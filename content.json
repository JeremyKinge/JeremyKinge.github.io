{"meta":{"title":"King哥","subtitle":"To know everything, no words don't talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）","description":"To know everything, no words don't talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）","author":"Jeremy Kinge","url":"http://kingge.top"},"pages":[{"title":"","date":"2017-08-14T09:28:56.000Z","updated":"2017-08-17T10:01:05.524Z","comments":true,"path":"about/index.html","permalink":"http://kingge.top/about/index.html","excerpt":"","text":"这个人真的很吊，什么都没留下，但是你不得不承认这个人他真的很吊啊。"},{"title":"分类","date":"2017-08-14T08:51:40.000Z","updated":"2017-08-14T08:52:32.618Z","comments":true,"path":"categories/index.html","permalink":"http://kingge.top/categories/index.html","excerpt":"","text":""},{"title":"","date":"2017-08-14T09:29:06.000Z","updated":"2017-08-17T10:02:42.294Z","comments":true,"path":"picture/index.html","permalink":"http://kingge.top/picture/index.html","excerpt":"","text":"该板块尚且还没有开发的心思,没有女朋友没有picture"},{"title":"标签","date":"2014-12-22T04:39:04.000Z","updated":"2017-08-14T09:29:28.873Z","comments":true,"path":"tags/index.html","permalink":"http://kingge.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hessian 多系统访问","slug":"Hessian 多系统访问","date":"2019-06-01T04:58:04.630Z","updated":"2017-08-31T09:44:24.038Z","comments":true,"path":"2019/06/01/Hessian 多系统访问/","link":"","permalink":"http://kingge.top/2019/06/01/Hessian 多系统访问/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"hadoop在使用中的常用优化手段","slug":"hadoop在使用中的常用优化手段","date":"2019-03-14T13:59:59.000Z","updated":"2019-08-01T13:54:38.121Z","comments":true,"path":"2019/03/14/hadoop在使用中的常用优化手段/","link":"","permalink":"http://kingge.top/2019/03/14/hadoop在使用中的常用优化手段/","excerpt":"","text":"一、前言我们知道影响MapReduce运算的因素很多，主要是机器性能、网络、磁盘读写速度、I/O 操作等等有关。 机器的问题属于外部因素，那么下面主要是介绍关于IO操作引发的性能问题： 主要是有几个以下方面 （1）数据倾斜 - 重点 （2）map和reduce数设置不合理 （3）map运行时间太长，导致reduce等待过久 （4）小文件过多 - 重点 （5）大量的不可分块的超大文件 （6）spill次数过多 （7）merge次数过多。 ​ MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。 下面想讲解小文件的处理方式： 1.1 HDFS小文件优化​ HDFS上每个文件都要在namenode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大是的索引速度变慢。 解决方案1）Hadoop Archive: 是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了namenode的内存使用。 2）Sequence file： sequence file由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。 3）CombineFileInputFormat： CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。（之前hadoop相关的章节讲解道，可以翻翻看看） 4）开启JVM重用 对于大量小文件Job，可以开启JVM重用会减少45%运行时间。 JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他map。 具体设置：mapreduce.job.jvm.numtasks值在10-20之间。 1.2 分阶段优化数据输入阶段 （1）合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致mr运行较慢。 （2）采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 数据传输阶段1）采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器。 2）使用SequenceFile二进制文件。 进入Map阶段 1）减少溢写（spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘IO。 2）减少合并（merge）次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。 3）在map之后，不影响业务逻辑前提下，先进行combine处理，减少 I/O。 进入Reduce阶段 暂无 数据倾斜 暂无总结 常用参数哟花 暂无","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop优化","slug":"hadoop优化","permalink":"http://kingge.top/tags/hadoop优化/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(十二)-数据压缩","slug":"hadoop大数据-十二-数据压缩","date":"2018-03-20T14:59:59.000Z","updated":"2019-08-01T13:41:44.559Z","comments":true,"path":"2018/03/20/hadoop大数据-十二-数据压缩/","link":"","permalink":"http://kingge.top/2018/03/20/hadoop大数据-十二-数据压缩/","excerpt":"","text":"4.1 概述压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在Hadoop下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下，I/O操作和网络数据传输要花大量的时间。还有，Shuffle与Merge过程同样也面临着巨大的I/O压力。 ​ 鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。 ​ 如果磁盘I/O和网络带宽影响了MapReduce作业性能，在任意MapReduce阶段启用压缩都可以改善端到端处理时间并减少I/O和网络流量。 压缩Mapreduce的一种优化策略：通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。 注意：压缩特性运用得当能提高性能，但运用不当也可能降低性能。 基本原则： （1）运算密集型的job，少用压缩 （2）IO密集型的job，多用压缩 4.2 MR支持的压缩编码 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFAULT 是，直接使用 DEFAULT .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFAULT .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 4.3 压缩方式选择4.3.1 Gzip压缩优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。 4.3.2 Bzip2压缩优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。 缺点：压缩/解压速度慢；不支持native。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 4.3.3 Lzo压缩优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。 4.3.4 Snappy压缩优点：高速压缩速度和合理的压缩率。 缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装； 应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。 4.4 压缩位置选择​ 压缩可以在MapReduce作用的任意阶段启用。 4.5 压缩配置参数要在Hadoop中启用压缩，可以配置如下参数： 参数 默认值 阶段 建议 io.compression.codecs （在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress（在mapred-site.xml中配置） false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec mapper输出 使用LZO或snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置） false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress. DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置） RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 4.6 压缩实战4.6.1 数据流的压缩和解压缩​ CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据。要想对正在被写入一个输出流的数据进行压缩，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流。相反，要想对从输入流读取而来的数据进行解压缩，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据。 测试一下如下压缩方式： DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec package com.kingge.mapreduce.compress;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.CompressionCodecFactory;import org.apache.hadoop.io.compress.CompressionInputStream;import org.apache.hadoop.io.compress.CompressionOutputStream;import org.apache.hadoop.util.ReflectionUtils;public class TestCompress &#123; public static void main(String[] args) throws Exception &#123; compress(\"e:/hello.txt\",\"org.apache.hadoop.io.compress.BZip2Codec\");// decompress(\"e:/hello.txt.bz2\"); &#125; // 压缩 private static void compress(String filename, String method) throws Exception &#123; // 1 获取输入流 FileInputStream fis = new FileInputStream(new File(filename)); Class codecClass = Class.forName(method); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); // 2 获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename +codec.getDefaultExtension())); CompressionOutputStream cos = codec.createOutputStream(fos); // 3 流的对拷 IOUtils.copyBytes(fis, cos, 1024*1024*5, false); // 4 关闭资源 fis.close(); cos.close(); fos.close(); &#125; // 解压缩 private static void decompress(String filename) throws FileNotFoundException, IOException &#123; // 0 校验是否能解压缩 CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration()); CompressionCodec codec = factory.getCodec(new Path(filename)); if (codec == null) &#123; System.out.println(\"cannot find codec for file \" + filename); return; &#125; // 1 获取输入流 CompressionInputStream cis = codec.createInputStream(new FileInputStream(new File(filename))); // 2 获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename + \".decoded\")); // 3 流的对拷 IOUtils.copyBytes(cis, fos, 1024*1024*5, false); // 4 关闭资源 cis.close(); fos.close(); &#125;&#125; 4.6.2 Map输出端采用压缩​ 即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置： 1）给大家提供的hadoop源码支持的压缩格式有：BZip2Codec 、DefaultCodec package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.BZip2Codec; import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); // 开启map端输出压缩 configuration.setBoolean(&quot;mapreduce.map.output.compress&quot;, true); // 设置map端输出压缩方式 configuration.setClass(&quot;mapreduce.map.output.compress.codec&quot;, BZip2Codec.class, CompressionCodec.class); Job job = Job.getInstance(configuration); job.setJarByClass(WordCountDriver.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 1 : 0); &#125;&#125; 2）Mapper保持不变 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割 String[] words = line.split(&quot; &quot;); // 3 循环写出 for(String word:words)&#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; 3）Reducer保持不变 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; // 1 汇总 for(IntWritable value:values)&#123; count += value.get(); &#125; // 2 输出 context.write(key, new IntWritable(count)); &#125;&#125; 7.10.3 Reduce输出端采用压缩基于wordcount案例处理 1）修改驱动 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.BZip2Codec;import org.apache.hadoop.io.compress.DefaultCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.io.compress.Lz4Codec;import org.apache.hadoop.io.compress.SnappyCodec;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); job.setJarByClass(WordCountDriver.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 设置reduce端输出压缩开启 FileOutputFormat.setCompressOutput(job, true); // 设置压缩的方式 FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); // FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); // FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); boolean result = job.waitForCompletion(true); System.exit(result?1:0); &#125;&#125; 2）Mapper和Reducer保持不变（详见4.6.2）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(十一)-Mapreduce框架原理","slug":"hadoop大数据-十一-Mapreduce框架原理","date":"2018-03-18T10:59:59.000Z","updated":"2019-06-17T13:45:43.508Z","comments":true,"path":"2018/03/18/hadoop大数据-十一-Mapreduce框架原理/","link":"","permalink":"http://kingge.top/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/","excerpt":"","text":"三 MapReduce框架原理3.1 MapReduce工作流程1）流程示意图 2.Submit()方法包含在这里面– 然后接着是切片处理数据（128M为一片）。很明显图例200M的文件需要切成两片处理。分配两个map进行计算操作 3.正式提交任务到yarn上，包含一些job的相关信息。 4．MrAppMaster进行资源调度。根据片块数分配相应数量的MapTask（这里分配两个MapTask） 5.然后MapTask根据InputFormat去读取文本数据。一行一行的经过Mapper程序的map()方法进行计算操作，最后输出到分区中，并有序的存储。 6.等到所有MapTask计算完毕后。启动MrAppMaster启动相对应分区数量的reduce数量进行统计操作。最后生成多个分区对应的统计文件。输出。 2）流程详解 上面的流程是整个mapreduce最全工作流程，但是shuffle过程只是从第7步开始到第16步结束，具体shuffle过程详解，如下： 1）maptask收集我们的map()方法输出的kv对，放到内存缓冲区中 2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 3）多个溢出文件会被合并成大的溢出文件 4）在溢出过程中，及合并的过程中，都要调用partitioner进行分区和针对key进行排序 5）reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据 6）reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序） 7）合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法） 3）注意 Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。 缓冲区的大小可以通过参数调整，参数：io.sort.mb 默认100M。 3.2 InputFormat数据输入3.2.1 Job提交流程和切片源码详解1）job提交流程源码详解 waitForCompletion()submit();// 1建立连接-主要的工作是建立集群环境，以便运行Job任务。同时会根据Configuration配置信息来辨别当前job是需要在本地LocalRunner上运行还是在真实的yarn上运行。 connect(); // 1）创建提交job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交jobsubmitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2）获取jobid ，并创建job路径 JobID jobId = submitClient.getNewJobID(); // 3）拷贝jar包到集群 – 如果是在本地运行那么就不需要提交jar包，但是如果是在远程服务器上运行，那么就需要提交jar包，防止找不到copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir);// 4）计算切片，生成切片规划文件-默认是切一片，会去读取配置文件，获取自定义的最小切片数。切片数最大值也是有一个默认值，最大值是Long.MAX_VALUEwriteSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);// 5）向Stag路径写xml配置文件writeConf(conf, submitJobFile); conf.writeXml(out);// 6）提交job,返回提交状态status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); 2）FileInputFormat源码解析(input.getSplits(job)) （1）找到你数据存储的目录。 ​ （2）开始遍历处理（规划切片）目录下的每一个文件 ​ （3）遍历第一个文件ss.txt ​ a）获取文件大小fs.sizeOf(ss.txt); ​ b）计算切片大小computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M ​ c）默认情况下，切片大小=blocksize ​ d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） ​ e）将切片信息写到一个切片规划文件中 ​ f）整个切片的核心过程在getSplit()方法中完成。 ​ g）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。 ​ h）注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分。 ​ （4）提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。 23.2.2 FileInputFormat切片机制1）FileInputFormat中默认的切片机制： （1）简单地按照文件的内容长度进行切片 （2）切片大小，默认等于block大小 （3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片(他会遍历输入目录里面的文件，一个一个处理，debug查看FileInputFormat的getSplits方法可知) 比如待处理数据有两个文件： file1.txt 320M file2.txt 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下： file1.txt.split1-- 0~128file1.txt.split2-- 128~256file1.txt.split3-- 256~320file2.txt.split1-- 0~10M 2）FileInputFormat切片大小的参数配置 通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize)); 切片主要由这几个值来运算决定 mapreduce.input.fileinputformat.split.minsize=1 默认值为1 mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue 因此，默认情况下，切片大小=blocksize。 maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。 minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 3）获取切片信息API // 根据文件类型获取切片信息FileSplit inputSplit = (FileSplit) context.getInputSplit();// 获取切片的文件名称String name = inputSplit.getPath().getName(); 3.2.3 CombineTextInputFormat切片机制1）关于大量小文件的优化策略1）默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。 2）优化策略​ （1）最好的办法，在数据处理系统的最前端（预处理/采集），将小文件先合并成大文件，再上传到HDFS做后续分析。 ​ （2）补救措施：如果已经是大量小文件在HDFS中了，可以使用另一种InputFormat来做切片（CombineTextInputFormat），它的切片逻辑跟TextFileInputFormat不同：它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个maptask。 ​ （3）优先满足最小切片大小，不超过最大切片大小 ​ CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m ​ CombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m ​ 举例：0.5m+1m+0.3m+5m=2m + 4.8m=2m + 4m + 0.8m ​ 0.5+1+0.3 = 1.8没有满足最小切片大小，所以向5借0.2M,最后合并成2+4.8，但是4.8大于最大切片数，所以拆成4+0.8 ，所以这个四个小文件最后合并成三个文件 3）具体实现步骤注意CombineTextInputFormat的jar包是： // 如果不设置InputFormat,它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class)CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4mCombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m 4）案例​ 大量小文件的切片优化（CombineTextInputFormat）。 4.1 数据准备准备5个小文件（这里准备五个txt文本） 4.2 我们依旧使用我们上一个章节使用的统计文本中单词出现个数的代码代码详见 《hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节案例》 先不进行任何的改造操作，直接用着五个小文件当做输入，运行后查看日志。 （1）不做任何处理，运行需求1中的wordcount程序，观察切片个数为5 （2）在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为1 // 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4mCombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m 3.2.4 InputFormat接口实现类MapReduce任务的输入文件一般是存储在HDFS里面。输入的文件格式包括：基于行的日志文件、二进制格式文件等。这些文件一般会很大，达到数十GB，甚至更大。那么MapReduce是如何读取这些数据的呢？下面我们首先学习InputFormat接口。 InputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。 1）TextInputFormat TextInputFormat是默认的InputFormat。每条记录是一行输入。键是LongWritable类型，存储该行在整个文件中的字节偏移量。值是这行的内容，不包括任何行终止符（换行符和回车符）。 以下是一个示例，比如，一个分片包含了如下4条文本记录。 Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 每条记录表示为以下键/值对： (0,Rich learning form)(19,Intelligent learning engine)(47,Learning more convenient)(72,From the real demand for more close to the enterprise) 很明显，键并不是行号。一般情况下，很难取得行号，因为文件按字节而不是按行切分为分片。 2）KeyValueTextInputFormat 每一行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “ “);来设定分隔符。默认分隔符是tab（\\t）。 以下是一个示例，输入是一个包含4条记录的分片。其中——&gt;表示一个（水平方向的）制表符。 line1 ——&gt;Rich learning formline2 ——&gt;Intelligent learning engineline3 ——&gt;Learning more convenientline4 ——&gt;From the real demand for more close to the enterprise 每条记录表示为以下键/值对： (line1,Rich learning form)(line2,Intelligent learning engine)(line3,Learning more convenient)(line4,From the real demand for more close to the enterprise) 此时的键是每行排在制表符之前的Text序列。 3）NLineInputFormat 如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数/N=切片数，如果不整除，切片数=商+1。 以下是一个示例，仍然以上面的4行输入为例。 Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 例如，如果N是2，则每个输入分片包含两行。开启2个maptask。 (0,Rich learning form) (19,Intelligent learning engine) 另一个 mapper 则收到后两行： (47,Learning more convenient) (72,From the real demand for more close to the enterprise) ​ 这里的键和值与TextInputFormat生成的一样。 3.2.5 自定义InputFormat1）概述（1）自定义一个类继承FileInputFormat。 （2）改写RecordReader，实现一次读取一个完整文件封装为KV。 （3）在输出时使用SequenceFileOutPutFormat输出合并文件。 2）案例​ 无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。将多个小文件合并成一个文件SequenceFile，SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。 小文件的优化无非以下几种方式： （1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS （2）在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并 （3）在mapreduce处理时，可采用CombineTextInputFormat提高效率 2.1 数据准备准备三个文本文件。 aa.txt:包含以下内容yongpeng weidong weinansanfeng luozong xiaomingbb.txt:包含以下内容longlong fanfanmazong kailun yuhang yixinlonglong fanfanmazong kailun yuhang yixincc.txt:包含以下内容shuaige changmo zhenqiang dongli lingu xuanxuan 最终预期文件格式： part-r-00000 2.2 代码实现使用自定义InputFormat的方式，处理输入小文件的问题。 （1）自定义一个类继承FileInputFormat （2）改写RecordReader，实现一次读取一个完整文件封装为KV （3）在输出时使用SequenceFileOutPutFormat输出合并文件 （1）自定义InputFromatpackage com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;// 定义类继承FileInputFormatpublic class WholeFileInputformat extends FileInputFormat&lt;NullWritable, BytesWritable&gt;&#123; @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; （2）自定义RecordReader package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class WholeRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt;&#123; private Configuration configuration; private FileSplit split; private boolean processed = false; private BytesWritable value = new BytesWritable(); @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.split = (FileSplit)split; configuration = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; // 1 定义缓存区 byte[] contents = new byte[(int)split.getLength()]; FileSystem fs = null; FSDataInputStream fis = null; try &#123; // 2 获取文件系统 Path path = split.getPath(); fs = path.getFileSystem(configuration); // 3 读取数据 fis = fs.open(path); // 4 读取文件内容 IOUtils.readFully(fis, contents, 0, contents.length); // 5 输出文件内容 value.set(contents, 0, contents.length); &#125; catch (Exception e) &#123; &#125;finally &#123; IOUtils.closeStream(fis); &#125; processed = true; return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return processed? 1:0; &#125; @Override public void close() throws IOException &#123; &#125;&#125; （3）SequenceFileMapper处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class SequenceFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;&#123; Text k = new Text(); @Override protected void setup(Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 1 获取文件切片信息 FileSplit inputSplit = (FileSplit) context.getInputSplit(); // 2 获取切片名称 String name = inputSplit.getPath().toString(); // 3 设置key的输出 k.set(name); &#125; @Override protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(k, value); &#125;&#125; （4）SequenceFileReducer处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, values.iterator().next()); &#125;&#125; （5）SequenceFileDriver处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;public class SequenceFileDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[] &#123; &quot;e:/input/inputinputformat&quot;, &quot;e:/output1&quot; &#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SequenceFileDriver.class); job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 设置输入的inputFormat job.setInputFormatClass(WholeFileInputformat.class); // 设置输出的outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.3 MapTask工作机制3.3.1 并行度决定机制1）问题引出 maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度。那么，mapTask并行任务是否越多越好呢？ 2）MapTask并行度决定机制 ​ 一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。 3.3.2 MapTask工作机制 ​ （1）Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。 ​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。 ​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner—调用用户自定义getPartition方法），并写入一个环形内存缓冲区中。 ​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 ​ 溢写阶段详情： ​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 ​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 ​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 ​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 ​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 ​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 https://blog.csdn.net/qq_41455420/article/details/79288764 好的总结： 3.4 Shuffle机制3.4.1 Shuffle机制Mapreduce确保每个reducer的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reducer）称为shuffle。 3.4.2 Partition分区 分区的行为在每一次的map操作都会调用一或者多次 0）问题引出：要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区） 默认只输出到一个分区，也就是结果输出到一个文件 1）默认partition分区 public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; ​ 默认分区是根据key的hashCode对reduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。（numReduceTasks默认是1，也就是说，默认返回0，也就是只创建一个分区，所以是part-r-00000） 2）自定义Partitioner步骤 ​ （1）自定义类继承Partitioner，重写getPartition()方法 public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123;// 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; ​ （2）在job驱动中，设置自定义partitioner： ​ job.setPartitionerClass(CustomPartitioner.class); ​ （3）自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce task ​ job.setNumReduceTasks(5); 3）注意： 如果reduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx； 如果1&lt;reduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception； 如果reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000； ​ 例如：假设自定义分区数为5，则 （1）job.setNumReduceTasks(1);会正常运行，只不过会产生一个输出文件 （2）job.setNumReduceTasks(2);会报错 （3）job.setNumReduceTasks(6);大于5，程序会正常运行，会产生空文件 4）案例4.1 案例1​ 将统计结果按照手机归属地不同省份输出到不同文件中（分区） 1）数据准备 phone.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 2）分析 （1）Mapreduce中会将map输出的kv对，按照相同key分组，然后分发给不同的reducetask。默认的分发规则为：根据key的hashcode%reducetask数来分发 （2）如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner 自定义一个CustomPartitioner继承抽象类：Partitioner （3）在job驱动中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class) 3）在&lt;hadoop大数据(十)-Mapreduce基础 章节的2.6.2 案例&gt;的基础上，增加一个分区类 package com.kingge.mapreduce.flowsum;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;//他的key和value就是map输出的kvpublic class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 在驱动函数中增加自定义数据分区设置和reduce task设置 package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 8 指定自定义数据分区 job.setPartitionerClass(ProvincePartitioner.class); // 9 同时指定相应数量的reduce task job.setNumReduceTasks(5); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 4.2 案例2​ 把单词按照ASCII码奇偶分区（Partitioner），结合&lt;hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节–统计一堆文件中单词出现的个数&gt; 只需要在此代码的基础上，添加自定义分区 package com.kingge.mapreduce.wordcount;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class WordCountPartitioner extends Partitioner&lt;Text, IntWritable&gt;&#123; @Override public int getPartition(Text key, IntWritable value, int numPartitions) &#123; // 1 获取单词key String firWord = key.toString().substring(0, 1); char[] charArray = firWord.toCharArray(); int result = charArray[0]; // int result = key.toString().charAt(0); // 2 根据奇数偶数分区 if (result % 2 == 0) &#123; return 0; &#125;else &#123; return 1; &#125; &#125;&#125; 在驱动类中配置加载分区，设置reducetask个数 job.setPartitionerClass(WordCountPartitioner.class);job.setNumReduceTasks(2);//想分多少个区，这里必须开多少个reduce，否则默认只会生成一个分区，那么自定义分区失效 5）总结l 结果输出文件，跟分区数量和reduce数量有关系 l getPartition方法是在map调用之后才会进入，而且是每一次map可能会调用多次getPartition。为什么说是多次调用分区方法呢？我们知道每一次进入map方法都是一行数据（例如 hello.txt的第一行hello kingge），那么经过分割后生成两个单词，调用两次**context.write（）所以为了确定这两个单词所属那个分区，那么就需要调用两次getPartition。也就说在这个例子中，一次map调用处理完后需要调用两次getPartition。（即：context.write（）内部会进行分区） l 如果job.setNumReduceTasks(1)（也就是保持默认值），那么就是生成一个分区，不会进入自定义的分区方法。Redeucetask必须大于1，自定义分区方法才会生效。 3.4.3 WritableComparable排序排序是MapReduce框架中最重要的操作之一。Map Task和Reduce Task均会对数据（按照key）进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 ​ 对于Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行一次合并，以将这些文件合并成一个大的有序文件。 ​ 对于Reduce Task，它从每个Map Task上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上，否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task统一对内存和磁盘上的所有数据进行一次合并。 每个阶段的默认排序 1）排序的分类：​ （1）部分排序： MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。例如输出文件到五个分区，那么部分排序能够保证各个五个分区的数据都是有序的。 ​ （2）全排序： 如何用Hadoop产生一个全局排序的文件？最简单的方法是使用一个分区，那么这个分区里面的数据全局都是排序的。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。 ​ 替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为上述文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。这种方式可以达到全排序的功能 （3）辅助排序：（GroupingComparator分组） ​ Mapreduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。 ​ （4）二次排序： ​ 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 2）自定义排序WritableComparable（1）原理分析 bean对象实现WritableComparable接口重写compareTo方法，就可以实现排序 @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 3）案例3.1 案例1在&lt;hadoop大数据(十)-Mapreduce基础 章节的2.6.2 案例&gt;输出结果的基础上增加一个新的需求 根据2.6.2 案例输出的结果：再次对总流量进行排序 1）数据准备 phone.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 2）分析 ​ （1）把程序分两步走，第一步正常统计总流量，第二步再把结果进行排序 ​ （2）context.write(总流量，手机号) ​ （3）FlowBean实现WritableComparable接口重写compareTo方法 @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 3）代码实现 （1）FlowBean对象在在需求2.6.2基础上增加了比较功能（compareTo） package com.kingge.mapreduce.sort;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; private long upFlow; private long downFlow; private long sumFlow; // 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public void set(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; /** * 序列化方法 * @param out * @throws IOException */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; /** * 反序列化方法 注意反序列化的顺序和序列化的顺序完全一致 * @param in * @throws IOException */ @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong(); &#125; @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; &#125; @Override public int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1; &#125;&#125; （2）编写mapper package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;&#123; FlowBean bean = new FlowBean(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 String phoneNbr = fields[0]; long upFlow = Long.parseLong(fields[1]); long downFlow = Long.parseLong(fields[2]); bean.set(upFlow, downFlow); v.set(phoneNbr); // 4 输出 context.write(bean, v); &#125;&#125; （3）编写reducer package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt;&#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; // 循环输出，避免总流量相同情况 for (Text text : values) &#123; context.write(text, key); &#125; &#125;&#125; （4）编写driver package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowCountSortDriver &#123; public static void main(String[] args) throws ClassNotFoundException, IOException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCountSortDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.2 案例2改造案例1的需求 ​ 要求每个省份手机号输出的文件中按照总流量内部排序。（部分排序） 2）做法 在案例1的基础上增加自定义分区类即可。 package com.kingge.mapreduce.sort;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; &#123; @Override public int getPartition(FlowBean key, Text value, int numPartitions) &#123; // 1 获取手机号码前三位 String preNum = value.toString().substring(0, 3); int partition = 4; // 2 根据手机号归属地设置分区 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; （2）在驱动类中添加分区类 // 加载自定义分区类job.setPartitionerClass(FlowSortPartitioner.class);// 设置Reducetask个数 job.setNumReduceTasks(5); 3.4.4 GroupingComparator分组（辅助排序）1）对reduce阶段的数据根据某一个或几个字段进行分组。 2）案例 ​ 求出每一个订单中最贵的商品（GroupingComparator） 1）需求 有如下订单数据 订单id 商品id 成交金额 0000001 Pdt_01 222.8 0000001 Pdt_06 25.8 0000002 Pdt_03 522.8 0000002 Pdt_04 122.4 0000002 Pdt_05 722.4 0000003 Pdt_01 222.8 0000003 Pdt_02 33.8 现在需要求出每一个订单中最贵的商品。 2）输入数据 goods.txt 0000001 Pdt_01 222.80000002 Pdt_06 722.40000001 Pdt_05 25.80000003 Pdt_01 222.80000003 Pdt_01 33.80000002 Pdt_03 522.80000002 Pdt_04 122.4 输出数据预期： ​ 3 222.8 2 722.4 1 222.8 3）分析 （1）利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce。 （2）在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值。 4）代码实现 （1）定义订单信息OrderBean package com.kingge.mapreduce.order;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int order_id; // 订单id号 private double price; // 价格 public OrderBean() &#123; super(); &#125; public OrderBean(int order_id, double price) &#123; super(); this.order_id = order_id; this.price = price; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; order_id = in.readInt(); price = in.readDouble(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + price; &#125; public int getOrder_id() &#123; return order_id; &#125; public void setOrder_id(int order_id) &#123; this.order_id = order_id; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; // 二次排序 @Override public int compareTo(OrderBean o) &#123; int result; if (order_id &gt; o.getOrder_id()) &#123; result = 1; &#125; else if (order_id &lt; o.getOrder_id()) &#123; result = -1; &#125; else &#123; // 价格倒序排序 result = price &gt; o.getPrice() ? -1 : 1; &#125; return result; &#125;&#125; （2）编写OrderSortMapper package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; OrderBean k = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 k.setOrder_id(Integer.parseInt(fields[0])); k.setPrice(Double.parseDouble(fields[2])); // 4 写出 context.write(k, NullWritable.get()); &#125;&#125; （3）编写OrderSortPartitioner package com.kingge.mapreduce.order;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Partitioner;public class OrderPartitioner extends Partitioner&lt;OrderBean, NullWritable&gt; &#123; @Override public int getPartition(OrderBean key, NullWritable value, int numReduceTasks) &#123; return (key.getOrder_id() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; （4）编写OrderSortGroupingComparator package com.kingge.mapreduce.order;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; //可以查看super的源代码，true是必须要传的，否则汇报空指针，因为我们在下面的compare方法中使用了强转的操作，那么如果不注明比较的bean的类型，那么就会有问题。 super(OrderBean.class, true); &#125; @SuppressWarnings(&quot;rawtypes&quot;) @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123; result = -1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125; （5）编写OrderSortReducer package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125;&#125; （6）编写OrderSortDriver package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class OrderDriver &#123; public static void main(String[] args) throws Exception, IOException &#123; // 1 获取配置信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 设置jar包加载路径 job.setJarByClass(OrderDriver.class); // 3 加载map/reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4 设置map输出数据key和value类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5 设置最终输出数据的key和value类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); // 6 设置输入数据和输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 10 设置reduce端的分组 job.setGroupingComparatorClass(OrderGroupingComparator.class); // 7 设置分区 job.setPartitionerClass(OrderPartitioner.class); // 8 设置reduce个数 job.setNumReduceTasks(3); // 9 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;//如果不使用GroupingComparator方法，那么就无法实现功能，因为我们知道进入reduce的数据，他们key一定是一样的。那么上面的OrderBean作为key很明显是不一样的，就算order_id相同，但是他们的price不相同。那么GroupingComparator就可以帮我们做到，假设某个值是相同的，那么他就认为整个key是相同的。那么OrderBean作为key就可以分组处理也就是说，我们通过在GroupingComparator方法中指明了，相同key的规则，那么就可以实现进入reduce的数据的分组情况尖叫提示： Map阶段结束后，马上进入GroupingComparator方法，进行判断key的逻辑。每判断一次完后，就调用reduce一次。循环此操作直到数据统计结束。 在进入GroupingComparator之前，map阶段输出的数据，已经按照订单分区，分区内的价格也已经按照大到小排序。 3.4.5 Combiner合并1）combiner是MR程序中Mapper和Reducer之外的一种组件。 2）combiner组件的父类就是Reducer。 3）combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行; Reducer是接收全局所有Mapper的输出结果； 4）combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。 5）combiner能够应用的前提是不能影响最终的业务逻辑，而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来。 Mapper3 5 7 -&gt;(3+5+7)/3=5 2 6 -&gt;(2+6)/2=4Reducer(3+5+7+2+6)/5=23/5 不等于 (5+4)/2=9/2 很明显，combiner不适合做求平均值这样的操作。他适合做汇总这样的业务场景。 6）自定义Combiner实现步骤： （1）自定义一个combiner继承Reducer，重写reduce方法 public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1 汇总操作 int count = 0; for(IntWritable v :values)&#123; count = v.get(); &#125; // 2 写出 context.write(key, new IntWritable(count)); &#125;&#125; （2）在job驱动类中设置： job.setCombinerClass(WordcountCombiner.class); 7）案例 ​ 前提：结合&lt;hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节–统计一堆文件中单词出现的个数&gt; 代码 数据输入也是同上 ​ 需求：统计过程中对每一个maptask的输出进行局部汇总，以减小网络传输量即采用Combiner功能。 方案一 1）增加一个WordcountCombiner类继承Reducer package com.kingge.mr.combiner;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1 汇总 int count = 0; for(IntWritable v :values)&#123; count += v.get(); &#125; // 2 写出 context.write(key, new IntWritable(count)); &#125;&#125; // 9 指定需要使用combiner，以及用哪个类作为combiner的逻辑 job.setCombinerClass(WordcountCombiner.class); // 9 指定需要使用combiner，以及用哪个类作为combiner的逻辑job.setCombinerClass(WordcountCombiner.class); 方案二 1）将WordcountReducer作为combiner在WordcountDriver驱动类中指定 // 指定需要使用combiner，以及用哪个类作为combiner的逻辑job.setCombinerClass(WordcountReducer.class); 运行程序 总结自定义Combiner的调用时机：是在MapTask阶段的split溢写阶段，需要写入到磁盘的之前进行。将有相同 key 的 key/value 对的 value 加起来，减少溢写到磁盘的数据量。调用完后进入**reduce**方法 ​ 3.5 ReduceTask工作机制1）设置ReduceTask并行度（个数） reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置： //默认值是1，手动设置为4 job.setNumReduceTasks(4); 2）注意 （1）reducetask=0 ，表示没有reduce阶段，输出文件个数和map个数一致。 ​ 例子7.1.1 job.setNumReduceTasks(0); 输出 ​ 生成一个分区，但是分区内的单词没有汇总 ​ （2）reducetask默认值就是1，所以输出文件个数为一个。 （3）如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜（也就是说，相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况） （4）reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask。 （5）具体多少个reducetask，需要根据集群性能而定。 （6）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在maptask的源码中，执行分区的前提是先判断reduceNum个数是否大于1。不大于1肯定不执行。 3）实验：测试reducetask多少合适。 （1）实验环境：1个master节点，16个slave节点：CPU:8GHZ，内存: 2G （2）实验结论： ​ 表1 改变reduce task （数据量为1GB） Map task =16 Reduce task 1 5 10 15 16 20 25 30 45 60 总时间 892 146 110 92 88 100 128 101 145 104 4）ReduceTask工作机制 ​ （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 ​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 ​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。 ​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 3.6 OutputFormat数据输出3.6.1 OutputFormat接口实现类 OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。 1）文本输出TextOutputFormat ​ 默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串。 2）SequenceFileOutputFormat SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。 3）自定义OutputFormat ​ 根据用户需求，自定义实现输出。 3.6.2 自定义OutputFormat为了实现控制最终文件的输出路径，可以自定义OutputFormat。 要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现。 1）自定义OutputFormat步骤（1）自定义一个类继承FileOutputFormat。 （2）改写recordwriter，具体改写输出数据的方法write()。 2）案例​ 修改日志内容及自定义日志输出路径（自定义OutputFormat）。 1）需求 ​ 过滤输入的log日志中是否包含kingge ​ （1）包含kingge的网站输出到e:/kingge.log ​ （2）不包含kingge的网站输出到e:/other.log 2）输入数据（pp.txt） http://www.baidu.comhttp://www.google.comhttp://cn.bing.comhttp://www.kingge.comhttp://www.sohu.comhttp://www.sina.comhttp://www.sin2a.comhttp://www.sin2desa.comhttp://www.sindsafa.com 输出预期： kingge.log文件包含： http://www.kingge.com other.log文件包含： http://cn.bing.comhttp://www.baidu.comhttp://www.google.comhttp://www.sin2a.comhttp://www.sin2desa.comhttp://www.sina.comhttp://www.sindsafa.comhttp://www.sohu.com 3）代码实现： （1）自定义一个outputformat package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; // 创建一个RecordWriter return new FilterRecordWriter(job); &#125;&#125; （2）具体的写数据RecordWriter package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream kinggeOut = null; FSDataOutputStream otherOut = null; public FilterRecordWriter(TaskAttemptContext job) &#123; // 1 获取文件系统 FileSystem fs; try &#123; fs = FileSystem.get(job.getConfiguration()); // 2 创建输出文件路径 Path kinggePath = new Path(&quot;e:/kingge.log&quot;); Path otherPath = new Path(&quot;e:/other.log&quot;); // 3 创建输出流 kinggeOut = fs.create(kinggePath); otherOut = fs.create(otherPath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断是否包含“kingge”输出到不同文件 if (key.toString().contains(&quot;kingge&quot;)) &#123; kinggeOut.write(key.toString().getBytes()); &#125; else &#123; otherOut.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; // 关闭资源 if (kinggeOut != null) &#123; kinggeOut.close(); &#125; if (otherOut != null) &#123; otherOut.close(); &#125; &#125;&#125; （3）编写FilterMapper package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FilterMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); k.set(line); // 3 写出 context.write(k, NullWritable.get()); &#125;&#125; （4）编写FilterReducer package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FilterReducer extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; String k = key.toString(); k = k + &quot;\\r\\n&quot;; context.write(new Text(k), NullWritable.get()); &#125;&#125; （5）编写FilterDriver package com.kingge.mapreduce.outputformat;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FilterDriver &#123; public static void main(String[] args) throws Exception &#123;args = new String[] &#123; &quot;e:/input/inputoutputformat&quot;, &quot;e:/output2&quot; &#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FilterDriver.class); job.setMapperClass(FilterMapper.class); job.setReducerClass(FilterReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要将自定义的输出格式组件设置到job中 job.setOutputFormatClass(FilterOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); // 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat // 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.7 Join多种应用3.7.1 Reduce join1）原理： Map端的主要工作：为来自不同表(文件)的key/value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。 Reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行合并就ok了。 2）该方法的缺点 这种方式的缺点很明显就是会造成map和reduce端也就是shuffle阶段出现大量的数据传输，效率很低。 3）案例​ reduce端表合并（数据倾斜） 通过将关联条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reducetask，在reduce中进行数据的串联。 1）代码实现 ​ 1.1 创建商品和订合并后的bean类 package com.kingge.mapreduce.table;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class TableBean implements Writable &#123; private String order_id; // 订单id private String p_id; // 产品id private int amount; // 产品数量 private String pname; // 产品名称 private String flag;// 表的标记 public TableBean() &#123; super(); &#125; public TableBean(String order_id, String p_id, int amount, String pname, String flag) &#123; super(); this.order_id = order_id; this.p_id = p_id; this.amount = amount; this.pname = pname; this.flag = flag; &#125; public String getFlag() &#123; return flag; &#125; public void setFlag(String flag) &#123; this.flag = flag; &#125; public String getOrder_id() &#123; return order_id; &#125; public void setOrder_id(String order_id) &#123; this.order_id = order_id; &#125; public String getP_id() &#123; return p_id; &#125; public void setP_id(String p_id) &#123; this.p_id = p_id; &#125; public int getAmount() &#123; return amount; &#125; public void setAmount(int amount) &#123; this.amount = amount; &#125; public String getPname() &#123; return pname; &#125; public void setPname(String pname) &#123; this.pname = pname; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(order_id); out.writeUTF(p_id); out.writeInt(amount); out.writeUTF(pname); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.order_id = in.readUTF(); this.p_id = in.readUTF(); this.amount = in.readInt(); this.pname = in.readUTF(); this.flag = in.readUTF(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + pname + &quot;\\t&quot; + amount + &quot;\\t&quot; ; &#125;&#125; 2）编写TableMapper程序 package com.kingge.mapreduce.table;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt;&#123; TableBean bean = new TableBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取输入文件类型 FileSplit split = (FileSplit) context.getInputSplit(); String name = split.getPath().getName(); // 2 获取输入数据 String line = value.toString(); // 3 不同文件分别处理 if (name.startsWith(&quot;order&quot;)) &#123;// 订单表处理 // 3.1 切割 String[] fields = line.split(&quot;\\t&quot;); // 3.2 封装bean对象 bean.setOrder_id(fields[0]); bean.setP_id(fields[1]); bean.setAmount(Integer.parseInt(fields[2])); bean.setPname(&quot;&quot;); bean.setFlag(&quot;0&quot;); k.set(fields[1]); &#125;else &#123;// 产品表处理 // 3.3 切割 String[] fields = line.split(&quot;\\t&quot;); // 3.4 封装bean对象 bean.setP_id(fields[0]); bean.setPname(fields[1]); bean.setFlag(&quot;1&quot;); bean.setAmount(0); bean.setOrder_id(&quot;&quot;); k.set(fields[0]); &#125; // 4 写出 context.write(k, bean); &#125;&#125; 3）编写TableReducer程序 package com.kingge.mapreduce.table;import java.io.IOException;import java.util.ArrayList;import org.apache.commons.beanutils.BeanUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 1准备存储订单的集合 ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;(); // 2 准备bean对象 TableBean pdBean = new TableBean(); for (TableBean bean : values) &#123; if (&quot;0&quot;.equals(bean.getFlag())) &#123;// 订单表 // 拷贝传递过来的每条订单数据到集合中 TableBean orderBean = new TableBean(); try &#123; BeanUtils.copyProperties(orderBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; orderBeans.add(orderBean); &#125; else &#123;// 产品表 try &#123; // 拷贝传递过来的产品表到内存中 BeanUtils.copyProperties(pdBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 3 表的拼接 for(TableBean bean:orderBeans)&#123; bean.setPname (pdBean.getPname()); // 4 数据写出去 context.write(bean, NullWritable.get()); &#125; &#125;&#125; 4）编写TableDriver程序 package com.kingge.mapreduce.table;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 指定本程序的jar包所在的本地路径 job.setJarByClass(TableDriver.class); // 3 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 4 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 5 指定最终输出的数据的kv类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 6 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3）运行程序查看结果 1001 小米 1 1001 小米 1 1002 华为 2 1002 华为 2 1003 格力 3 1003 格力 3 缺点：这种方式中，合并的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜 解决方案： map端实现数据合并 3.7.2 Map join（Distributedcache分布式缓存）1）使用场景：一张表十分小、一张表很大。 2）解决方案 在map端缓存多张表，提前处理业务逻辑，这样增加map端业务，减少reduce端数据的压力，尽可能的减少数据倾斜。 3）具体办法：采用distributedcache ​ （1）在mapper的setup阶段，将文件读取到缓存集合中。 ​ （2）在驱动函数中加载缓存。 job.addCacheFile(new URI(“file:/e:/mapjoincache/pd.txt”));// 缓存普通文件到task运行节点 4）案例：​ map端表合并（Distributedcache） - 结合上个案例代码（3.7.1 3 案例） 1）分析 适用于关联表中有小表的情形； 可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行合并并输出最终结果，可以大大提高合并操作的并发度，加快处理速度。 2）实操案例 （1）先在驱动模块中添加缓存文件 package test;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class DistributedCacheDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取job信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置加载jar包路径 job.setJarByClass(DistributedCacheDriver.class); // 3 关联map job.setMapperClass(DistributedCacheMapper.class); // 4 设置最终输出数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5 设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 加载缓存数据 job.addCacheFile(new URI(&quot;file:///e:/inputcache/pd.txt&quot;)); // 7 map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); // 8 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; （2）读取缓存的文件数据 package test;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(); @Override protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 1 获取缓存的文件 BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;pd.txt&quot;),&quot;UTF-8&quot;)); String line; while(StringUtils.isNotEmpty(line = reader.readLine()))&#123; // 2 切割 String[] fields = line.split(&quot;\\t&quot;); // 3 缓存数据到集合 pdMap.put(fields[0], fields[1]); &#125; // 4 关流 reader.close(); &#125; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 获取产品id String pId = fields[1]; // 4 获取商品名称 String pdName = pdMap.get(pId); // 5 拼接 k.set(line + &quot;\\t&quot;+ pdName); // 6 写出 context.write(k, NullWritable.get()); &#125;&#125; 3.8 数据清洗（ETL）1）概述 在运行核心业务Mapreduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行mapper程序，不需要运行reduce程序。 2）案例日志清洗（数据清洗）。 简单解析版1）需求： 去除日志中字段长度小于等于11的日志。 2）输入数据 里面的内容就是我们平时网站输出的日志。例如： 194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;101.226.68.137 - - [18/Sep/2013:06:49:45 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;60.208.6.156 - - [18/Sep/2013:06:49:48 +0000] &quot;GET /wp-content/uploads/2013/07/rcassandra.png HTTP/1.0&quot; 200 185524 &quot;http://cos.name/category/software/packages/&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] &quot;GET /images/my.jpg HTTP/1.1&quot; 200 19939 &quot;http://www.angularjs.cn/A00n&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;222.68.172.190 - - [18/Sep/2013:06:50:08 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot; 3）实现代码： （1）编写LogMapper package com.kingge.mapreduce.weblog;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取1行数据 String line = value.toString(); // 2 解析日志 boolean result = parseLog(line,context); // 3 日志不合法退出 if (!result) &#123; return; &#125; // 4 设置key k.set(line); // 5 写出数据 context.write(k, NullWritable.get()); &#125; // 2 解析日志 private boolean parseLog(String line, Context context) &#123; // 1 截取 String[] fields = line.split(&quot; &quot;); // 2 日志长度大于11的为合法 if (fields.length &gt; 11) &#123; // 系统计数器 context.getCounter(&quot;map&quot;, &quot;true&quot;).increment(1); return true; &#125;else &#123; context.getCounter(&quot;map&quot;, &quot;false&quot;).increment(1); return false; &#125; &#125;&#125; （2）编写LogDriver package com.kingge.mapreduce.weblog;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class LogDriver &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; &quot;e:/input/inputlog&quot;, &quot;e:/output1&quot; &#125;; // 1 获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 加载jar包 job.setJarByClass(LogDriver.class); // 3 关联map job.setMapperClass(LogMapper.class); // 4 设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置reducetask个数为0 job.setNumReduceTasks(0); // 5 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 提交 job.waitForCompletion(true); &#125;&#125; 复杂解析版1）需求： 对web访问日志中的各字段识别切分 去除日志中不合法的记录 根据统计需求，生成各类访问请求过滤数据 2）输入数据 输入同上一个案例 3）实现代码： （1）定义一个bean，用来记录日志数据中的各数据字段 package com.kingge.mapreduce.log;public class LogBean &#123; private String remote_addr;// 记录客户端的ip地址 private String remote_user;// 记录客户端用户名称,忽略属性&quot;-&quot; private String time_local;// 记录访问时间与时区 private String request;// 记录请求的url与http协议 private String status;// 记录请求状态；成功是200 private String body_bytes_sent;// 记录发送给客户端文件主体内容大小 private String http_referer;// 用来记录从那个页面链接访问过来的 private String http_user_agent;// 记录客户浏览器的相关信息 private boolean valid = true;// 判断数据是否合法 public String getRemote_addr() &#123; return remote_addr; &#125; public void setRemote_addr(String remote_addr) &#123; this.remote_addr = remote_addr; &#125; public String getRemote_user() &#123; return remote_user; &#125; public void setRemote_user(String remote_user) &#123; this.remote_user = remote_user; &#125; public String getTime_local() &#123; return time_local; &#125; public void setTime_local(String time_local) &#123; this.time_local = time_local; &#125; public String getRequest() &#123; return request; &#125; public void setRequest(String request) &#123; this.request = request; &#125; public String getStatus() &#123; return status; &#125; public void setStatus(String status) &#123; this.status = status; &#125; public String getBody_bytes_sent() &#123; return body_bytes_sent; &#125; public void setBody_bytes_sent(String body_bytes_sent) &#123; this.body_bytes_sent = body_bytes_sent; &#125; public String getHttp_referer() &#123; return http_referer; &#125; public void setHttp_referer(String http_referer) &#123; this.http_referer = http_referer; &#125; public String getHttp_user_agent() &#123; return http_user_agent; &#125; public void setHttp_user_agent(String http_user_agent) &#123; this.http_user_agent = http_user_agent; &#125; public boolean isValid() &#123; return valid; &#125; public void setValid(boolean valid) &#123; this.valid = valid; &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append(this.valid); sb.append(&quot;\\001&quot;).append(this.remote_addr); sb.append(&quot;\\001&quot;).append(this.remote_user); sb.append(&quot;\\001&quot;).append(this.time_local); sb.append(&quot;\\001&quot;).append(this.request); sb.append(&quot;\\001&quot;).append(this.status); sb.append(&quot;\\001&quot;).append(this.body_bytes_sent); sb.append(&quot;\\001&quot;).append(this.http_referer); sb.append(&quot;\\001&quot;).append(this.http_user_agent); return sb.toString(); &#125;&#125; （2）编写LogMapper程序 package com.kingge.mapreduce.log;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取1行 String line = value.toString(); // 2 解析日志是否合法 LogBean bean = pressLog(line); if (!bean.isValid()) &#123; return; &#125; k.set(bean.toString()); // 3 输出 context.write(k, NullWritable.get()); &#125; // 解析日志 private LogBean pressLog(String line) &#123; LogBean logBean = new LogBean(); // 1 截取 String[] fields = line.split(&quot; &quot;); if (fields.length &gt; 11) &#123; // 2封装数据 logBean.setRemote_addr(fields[0]); logBean.setRemote_user(fields[1]); logBean.setTime_local(fields[3].substring(1)); logBean.setRequest(fields[6]); logBean.setStatus(fields[8]); logBean.setBody_bytes_sent(fields[9]); logBean.setHttp_referer(fields[10]); if (fields.length &gt; 12) &#123; logBean.setHttp_user_agent(fields[11] + &quot; &quot;+ fields[12]); &#125;else &#123; logBean.setHttp_user_agent(fields[11]); &#125; // 大于400，HTTP错误 if (Integer.parseInt(logBean.getStatus()) &gt;= 400) &#123; logBean.setValid(false); &#125; &#125;else &#123; logBean.setValid(false); &#125; return logBean; &#125;&#125; （3）编写LogDriver程序 package com.kingge.mapreduce.log;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class LogDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 加载jar包 job.setJarByClass(LogDriver.class); // 3 关联map job.setMapperClass(LogMapper.class); // 4 设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 提交 job.waitForCompletion(true); &#125;&#125; 3.9 计数器应用​ Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。 1）API ​ （1）采用枚举的方式统计计数 enum MyCounter{MALFORORMED,NORMAL} //对枚举定义的自定义计数器加1 context.getCounter(MyCounter.MALFORORMED).increment(1); （2）采用计数器组、计数器名称的方式统计 context.getCounter(“counterGroup”, “countera”).increment(1); ​ 组名和计数器名称随便起，但最好有意义。 ​ （3）计数结果在程序运行后的控制台上查看。 2）案例 ​ 数据清洗的两个案例 3.10 MapReduce开发总结在编写mapreduce程序时，需要考虑的几个方面： 1）输入数据接口：InputFormat 默认使用的实现类是：TextInputFormat TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。 KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\\t）。 NlineInputFormat按照指定的行数N来划分切片。 CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。 用户还可以自定义InputFormat。 2）逻辑处理接口：Mapper 用户根据业务需求实现其中三个方法：map() setup() cleanup () 3）Partitioner分区 ​ 有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces ​ 如果业务上有特别的需求，可以自定义分区。 4）Comparable排序 ​ 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。 ​ 部分排序：对最终输出的每一个文件进行内部排序。 ​ 全排序：对所有数据进行排序，通常只有一个Reduce。 ​ 二次排序：排序的条件有两个。 5）Combiner合并 Combiner合并可以提高程序执行效率，减少io传输。但是使用时必须不能影响原有的业务处理结果。 6）reduce端分组：Groupingcomparator ​ reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数。 ​ 利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑。 ​ 自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果。然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）。这样，我们要取的最大值就是reduce()方法中传进来key。 7）逻辑处理接口：Reducer ​ 用户根据业务需求实现其中三个方法：reduce() setup() cleanup () 8）输出数据接口：OutputFormat ​ 默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对向目标文本文件中输出为一行。 SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。 用户还可以自定义OutputFormat。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(十)-Mapreduce基础","slug":"hadoop大数据-十-Mapreduce基础","date":"2018-03-16T11:59:59.000Z","updated":"2019-06-17T12:46:35.921Z","comments":true,"path":"2018/03/16/hadoop大数据-十-Mapreduce基础/","link":"","permalink":"http://kingge.top/2018/03/16/hadoop大数据-十-Mapreduce基础/","excerpt":"","text":"一 MapReduce入门1.1 MapReduce定义Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。 Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 1.2 MapReduce优缺点1.2.1 优点1**）MapReduce 易于编程。**它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 2**）良好的扩展性。**当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 3**）高容错性。**MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。 4**）适合PB**级以上海量数据的离线处理（他跟其他的分布式运行框架不同，例如spark等等）。这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。 1.2.2 缺点MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。 1）实时计算。MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 2）流式计算。流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 3）DAG（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 1.3 MapReduce核心思想 下面根据一个小小的案例来体现 mapreduce的运转流程。 根据块大小（128M）进行分片运算，每个maptask负责处理自己所属的块数据，把每个单词出现个数计算统计然后放到hashmap（实际上是放到磁盘上）中，key是单词，value是单词出现次数。 1）分布式的运算程序往往需要分成至少2个阶段。（map阶段和reduce阶段） 2）第一个阶段的maptask并发实例，完全并行运行，互不相干。 3）第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。 4）MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。 1.4 MapReduce进程一个完整的mapreduce程序在分布式运行时有三类实例进程： 1）MrAppMaster：负责整个程序的过程调度及状态协调。 2）MapTask：负责map阶段的整个数据处理流程。 3）ReduceTask：负责reduce阶段的整个数据处理流程。 1.5 MapReduce编程规范用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) 1）Mapper阶段​ （1）用户自定义的Mapper要继承自己的父类 ​ （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） ​ （3）Mapper中的业务逻辑写在map()方法中 ​ （4）Mapper的输出数据是KV对的形式（KV的类型可自定义） ​ （5）map()方法（maptask进程）对每一个调用一次 2）Reducer阶段​ （1）用户自定义的Reducer要继承自己的父类 ​ （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV ​ （3）Reducer的业务逻辑写在reduce()方法中 ​ （4）Reducetask进程对每一组相同k的组调用一次reduce()方法 3）Driver阶段整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象 4）案例​ 统计一堆文件中单词出现的个数（WordCount案例）。 在一堆给定的文本文件中统计输出每一个单词出现的总次数 1.数据准备 anly.text 包涵一下数据。hello worldkingge kinggehadoop sparkhello worldkingge kinggehadoop sparkhello worldhadoop spark 2.按照mapreduce编程规范，分别编写Mapper，Reducer，Driver。 简单案例分析 3.书写java代码（1）编写mapper类package com.kingge.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;//四个参数：前两个是map的输入参数类型，后两个数输出参数类型//很明显，执行一个map，数据的key值是long类型代表着数据所属的行号，那么value值就是string类型，对应Hadoop的序列化类型是text.//输出的结果是，每个单词对应的个数。那么输出的key应该是Text,代表单词,value应该是Int类型，代表这个单词的个数public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行-因为map是一行一行进行处理的 String line = value.toString(); // 2 切割 String[] words = line.split(&quot; &quot;); // 3 输出 for (String word : words) &#123; k.set(word); context.write(k, v); &#125; &#125;&#125; （2）编写reducer类package com.kingge.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; value, Context context) throws IOException, InterruptedException &#123; // 1 累加求和 int sum = 0; for (IntWritable count : value) &#123; sum += count.get(); &#125; // 2 输出 context.write(key, new IntWritable(sum)); &#125;&#125;执行到reduce阶段，那么经过map的计算和排序，最终会形成了一组一组的相同key的KV键值对（key group）。然后相同组的会进行reduce统计。一组接着一组进行计算。并不是所有组都通过reduce。//例如假设最终返回的KV值是：//hello 1//hello 1//word 1//word 1 那么 前两个hello为一组，经过reduce运算，然后返回，同时word为一组也经过统计返回。这两组并不会都由同一个reduce处理 （3）编写驱动类package com.kingge.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 设置map和Reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置Reduce输出 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 4）集群上测试 （1）将程序打成jar包，然后拷贝到hadoop集群中。 （2）启动hadoop集群 （3）执行wordcount程序 [kingge@hadoop102 software]$ hadoop jar wc.jar com.kingge.wordcount.WordcountDriver /user/kingge/input /user/kingge/output1 5）本地测试 （1）在windows环境上配置HADOOP_HOME环境变量。 （2）在eclipse上运行程序 （3）注意：如果eclipse打印不出日志，在控制台上只显示 1.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). 2.log4j:WARN Please initialize the log4j system properly. 3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 需要在项目的src目录下，新建一个文件，命名为“log4j.properties”，在文件中填入 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 经过debug发现，只有当map处理完所有数据，才会进入reduce，map处理数据是一行一行进行处理的，每一行数据的处理都会经过一次map方法，直到所有数据处理完毕。Map处理完所有数据后，会排序所有的key，进行分组。然后一组一组的经过reduce，进行统计操作。直到所有组统计完毕，然后输出数据。 二 Hadoop序列化2.1 为什么要序列化？​ 一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。 2.2 什么是序列化？序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。 反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。 2.3 为什么不用Java的序列化？​ Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效。 2.4 为什么序列化对Hadoop很重要？​ 因为Hadoop在集群之间进行通讯或者RPC调用的时候，需要序列化，而且要求序列化要快，且体积要小，占用带宽要小。所以必须理解Hadoop的序列化机制。 ​ 序列化和反序列化在分布式数据处理领域经常出现：进程通信和永久存储。然而Hadoop中各个节点的通信是通过远程调用（RPC）实现的，那么RPC序列化要求具有以下特点： 1）紧凑：紧凑的格式能让我们充分利用网络带宽，而带宽是数据中心最稀缺的资源 2）快速：进程通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本的； 3）可扩展：协议为了满足新的需求变化，所以控制客户端和服务器过程中，需要直接引进相应的协议，这些是新协议，原序列化方式能支持新的协议报文； 4）互操作：能支持不同语言写的客户端和服务端进行交互； 2.5 常用数据序列化类型常用的数据类型对应的hadoop数据序列化类型 Java**类型** Hadoop Writable**类型** boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable string Text map MapWritable array ArrayWritable 2.6 自定义bean对象实现序列化接口（Writable）1）自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项。 （1）必须实现Writable接口 （2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造 ​ public FlowBean() { super(); } （3）重写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; （4）重写反序列化方法 ​ @Overridepublic void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong();&#125; （5）注意反序列化的顺序和序列化的顺序完全一致 （6）要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。 （7）如果需要将自定义的bean放在key中传输，则还需要实现WritableComparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序。 ​ 《自定义的bean放在key中传输》是什么意思呢？因为我们知道map操作中输入数据的存储结构是-key-value的形式.上面的例子中统计文本单词数，那么文本文件中每一行的文本的序号就是key（0,1,2,3）每一行的文本，就是value的值。Map操作完后输出的数据结构也是key-value的形式。而且输出的数据会根据key排序，以便reduce处理。那么怎么排序在hadoop中有一个默认规则（如果key是2.5中的常用数据类型），如果使我们自定义的序列化数据类型作为key。那么默认排序规则就会失效，那么就需要我们制定一个排序规则就需要覆盖compareTo方法。** ​ @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 2）案例​ 每一个手机号耗费的总上行流量、下行流量、总流量（序列化）。 2.1 数据准备pd.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13560436666 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 输入数据格式： 输出数据格式 2.2 分析基本思路： Map阶段： （1）读取一行数据，切分字段 （2）抽取手机号、上行流量、下行流量 （3）以手机号为key，bean对象为value输出，即context.write(手机号,bean); Reduce阶段： （1）累加上行流量和下行流量得到总流量。 （2）实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输 （3）MR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key 所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable。然后重写key的compareTo方法。 2.3 编写mapreduce程序（1）编写流量统计的bean对象 package com.kingge.mapreduce.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;// 1 实现writable接口public class FlowBean implements Writable&#123; private long upFlow ; private long downFlow; private long sumFlow; //2 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; //3 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; //4 反序列化方法 //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 6 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125; （2）编写mapper package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); v.set(downFlow, upFlow); // 4 写出 context.write(new Text(phoneNum), new FlowBean(upFlow, downFlow)); &#125;&#125; （3）编写reducer package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_downFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sum_upFlow += flowBean.getSumFlow(); sum_downFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow); // 3 写出 context.write(key, resultBean); &#125;&#125; （4）编写驱动 package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(九)-yarn","slug":"hadoop大数据-九-yarn","date":"2018-03-14T14:59:59.000Z","updated":"2019-08-01T13:33:46.678Z","comments":true,"path":"2018/03/14/hadoop大数据-九-yarn/","link":"","permalink":"http://kingge.top/2018/03/14/hadoop大数据-九-yarn/","excerpt":"","text":"5.1 Hadoop1.x和Hadoop2.x架构区别在Hadoop1.x时代，Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。 ResourceManagement 资源管理 JobScheduling/JobMonitoring 任务调度监控 在Hadoop2.x时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。这样就能够各司其职 ResourceManger ApplicationMaster ​ 需要注意的是，在Yarn中我们把job的概念换成了application，因为在新的Hadoop2.x中，运行的应用不只是MapReduce了，还有可能是其它应用如一个DAG（有向无环图Directed Acyclic Graph，例如storm应用）。Yarn的另一个目标就是拓展Hadoop，使得它不仅仅可以支持MapReduce计算，还能很方便的管理诸如Hive、Hbase、Pig、Spark/Shark等应用。这种新的架构设计能够使得各种类型的应用运行在Hadoop上面，并通过Yarn从系统层面进行统一的管理，也就是说，有了Yarn，各种应用就可以互不干扰的运行在同一个Hadoop系统中，共享整个集群资源。 5.2 Yarn概述Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 5.3 Yarn基本架构​ YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。 5.4 Yarn工作机制1）Yarn运行机制 2）工作机制详解 ​ （0）Mr程序提交到客户端所在的节点。 ​ （1）Yarnrunner向Resourcemanager申请一个Application。 ​ （2）rm将该应用程序的资源路径返回给yarnrunner。 ​ （3）该程序将运行所需资源提交到HDFS上。 ​ （4）程序资源提交完毕后，申请运行mrAppMaster。 ​ （5）RM将用户的请求初始化成一个task。 ​ （6）其中一个NodeManager领取到task任务。 ​ （7）该NodeManager创建容器Container，并产生MRAppmaster。 ​ （8）Container从HDFS上拷贝资源到本地。 ​ （9）MRAppmaster向RM 申请运行maptask资源。 ​ （10）RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 ​ （11）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。 （12）MrAppMaster等待所有maptask运行完毕后，向RM申请容器，运行reduce task。 ​ （13）reduce task向maptask获取相应分区的数据。 ​ （14）程序运行完毕后，MR会向RM申请注销自己。 5.5 作业提交全过程1）作业提交过程之YARN 作业提交全过程详解 （1）作业提交第0步：client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。 第1步：client向RM申请一个作业id。 第2步：RM给client返回该job资源的提交路径和作业id。 第3步：client提交jar包、切片信息和配置文件到指定的资源提交路径。 第4步：client提交完资源后，向RM申请运行MrAppMaster。 （2）作业初始化第5步：当RM收到client的请求后，将该job添加到容量调度器中。 第6步：某一个空闲的NM领取到该job。 第7步：该NM创建Container，并产生MRAppmaster。 第8步：下载client提交的资源到本地。 （3）任务分配第9步：MrAppMaster向RM申请运行多个maptask任务资源。 第10步：RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 （4）任务运行第11步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。 第12步：MrAppMaster等待所有maptask运行完毕后，向RM申请容器，运行reduce task。 第13步：reduce task向maptask获取相应分区的数据。 第14步：程序运行完毕后，MR会向RM申请注销自己。 （5）进度和状态更新YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。 （6）作业完成除了向应用管理器请求作业进度外, 客户端每5分钟都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。 2）作业提交过程之MapReduce 3）作业提交过程之读数据 4）作业提交过程之写数据 5.6 资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2默认的资源调度器是Capacity Scheduler。 具体设置详见：yarn-default.xml文件 The class to use as the resource scheduler. yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler &lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 1）先进先出调度器（FIFO） 2）容量调度器（Capacity Scheduler） 3）公平调度器（Fair Scheduler） 5.7 任务的推测执行1）作业完成时间取决于最慢的任务完成时间 一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。 典型案例：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？ 2）推测执行机制： 发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。 3）执行推测任务的前提条件 （1）每个task只能有一个备份任务； （2）当前job已完成的task必须不小于0.05（5%） （3）开启推测执行参数设置。Hadoop2.7.2 mapred-site.xml文件中默认是打开的。 &lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; mapreduce.map.speculative true If true, then multiple instances of some map tasks may be executed in parallel. mapreduce.reduce.speculative true If true, then multiple instances of some reduce tasks may be executed in parallel. 4）不能启用推测执行机制情况 （1）任务间存在严重的负载倾斜； （2）特殊任务，比如任务向数据库中写数据。 5）算法原理：","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"yarn","slug":"yarn","permalink":"http://kingge.top/tags/yarn/"}]},{"title":"hadoop大数据(八)-namenode和resourcemanager高可用","slug":"hadoop大数据-八-namenode和resourcemanager高可用","date":"2018-03-12T12:59:59.000Z","updated":"2019-06-10T13:20:52.956Z","comments":true,"path":"2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/","link":"","permalink":"http://kingge.top/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/","excerpt":"","text":"HDFS 高可用高可用概述1）所谓HA（high available），即高可用（7*24小时不中断服务）。 2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。 3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。 4）NameNode主要在以下两个方面影响HDFS集群 ​ NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启 ​ NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用 HDFS HA功能通过配置Active/Standby两个nameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。 HDFS-HA工作机制1）通过双namenode消除单点故障 HDFS-HA工作要点1）元数据管理方式需要改变： 内存中各自保存一份元数据； Edits日志只有Active状态的namenode节点可以做写操作； 两个namenode都可以读取edits； 共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）； 2）需要一个状态管理功能模块 实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split（脑裂）现象的发生。 脑裂：集群中存在两台active状态的namenode。 3）必须保证两个NameNode之间能够ssh无密码登录。 4）隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务 怎么能够保证两台namenode，有一台是active另一台是standby，而不出现脑裂现象呢？ 假想一：两台namenode进行通信，周期请求对面，告知自己状态。在一定的条件下可以实现高可用，但是存在如下问题：1.两台namenode直接通信，如果namenode1（active）处理client请求时，没空响应namenode2那么nn2等待了一段时间，就认为nn1已经碟机，那么nn2启动（切换为active）。这个时候集群出现脑裂现象。2.nn1和nn2 因为网络问题，可能存在一定的延迟，无法实时的切换（nn1碟机，切换到nn2的时候，可能会等待一两分钟）。 假想二：使用zookeeper记录namenode 的状态。也会出现上面的问题。如果网络出现问题，nn1（active）无法正确汇报自己的状态到zookeeper，那么nn2启动，也会出现脑裂问题。 假想三：zkfailover，一个namenode的内部进程（解决网络交互问题） HDFS-HA自动故障转移工作机制前面学习了使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能： 1）故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。 2）现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。 ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责： 1）健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。 2）ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。 3）基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为active状态。 ​ zookeeper服务端 HDFS-HA集群配置环境准备1）修改IP 2）修改主机名及主机名和IP地址的映射 3）关闭防火墙 4）ssh免密登录 5）安装JDK，配置环境变量等 规划集群hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager NodeManager NodeManager NodeManager 配置Zookeeper集群0）集群规划 在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 1）解压安装 （1）解压zookeeper安装包到/opt/module/目录下 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ （2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData ​ mkdir -p zkData （3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg ​ mv zoo_sample.cfg zoo.cfg 2）配置zoo.cfg文件 ​ （1）具体配置 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 增加如下配置 ​ #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 （2）配置参数解读 Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器； B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 3）集群操作 （1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件 ​ touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 （2）编辑myid文件 ​ vi myid ​ 在文件中添加与server对应的编号：如2 （3）拷贝配置好的zookeeper到其他机器上 ​ scp -r zookeeper-3.4.10/ root@hadoop103.kingge.com:/opt/app/ ​ scp -r zookeeper-3.4.10/ root@hadoop104.kingge.com:/opt/app/ ​ 并分别修改myid文件中内容为3、4 （4）分别启动zookeeper ​ [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start （5）查看状态 [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower 配置HDFS-HA集群（手动故障转移配置） 为什么说是手动，因为假设某一台namenode 出现了问题，并不会自动的切换另一台namenode为active状态，需要我们手动切换 1）官方地址：http://hadoop.apache.org/ 2）在opt目录下创建一个ha文件夹 mkdir ha 3）将/opt/app/下的 hadoop-2.7.2拷贝到/opt/ha目录下 cp -r hadoop-2.7.2/ /opt/ha/ 4）配置hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 5）配置core-site.xml fs.defaultFS hdfs://mycluster //任意名字，代表着整个namenode集群，至于调用那个namenode，他会自己分配 hadoop.tmp.dir /opt/ha/hadoop-2.7.2/data/tmp &lt;configuration&gt;&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; //任意名字，代表着整个namenode集群，至于调用那个namenode，他会自己分配 &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6）配置hdfs-site.xml（因为有了热备namenode，那么就可以把secondary-namenode关闭，功能重复） &lt;configuration&gt; &lt;!—可以配置文件块备份数--&gt; &lt;!-- 完全分布式集群名称 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; //这个名字必须与上面的mycluster一致 &lt;/property&gt; &lt;!-- 集群中NameNode节点都有哪些 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode元数据（edit.log）在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 防止脑裂--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/kingge/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 声明journalnode服务器存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/jn&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭权限检查--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; &lt;!—可以配置文件块备份数–&gt; dfs.nameservices mycluster //这个名字必须与上面的mycluster一致 dfs.ha.namenodes.mycluster nn1,nn2 dfs.namenode.rpc-address.mycluster.nn1 hadoop102:9000 dfs.namenode.rpc-address.mycluster.nn2 hadoop103:9000 dfs.namenode.http-address.mycluster.nn1 hadoop102:50070 dfs.namenode.http-address.mycluster.nn2 hadoop103:50070 dfs.namenode.shared.edits.dir qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster dfs.ha.fencing.methods sshfence dfs.ha.fencing.ssh.private-key-files /home/atguigu/.ssh/id_rsa dfs.journalnode.edits.dir /opt/ha/hadoop-2.7.2/data/jn dfs.permissions.enable false dfs.client.failover.proxy.provider.mycluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider 7）可以关闭secondary namenode和关闭原先的namenode 的http访问模式 8）拷贝配置好的hadoop环境到其他节点 启动HDFS-HA集群1）在各个JournalNode节点上，输入以下命令启动journalnode服务：（在这里是hadoop102、hadoop103.、hadoop104 三台服务器都需要执行下面命令） ​ sbin/hadoop-daemon.sh start journalnode 2）在[nn1]上，对其进行格式化，并启动： ​ bin/hdfs namenode –format // ​ sbin/hadoop-daemon.sh start namenode //开启active namenode 3）在[nn2]上，同步nn1的元数据信息： ​ bin/hdfs namenode -bootstrapStandby 4）启动[nn2]：启动备用namenode ​ sbin/hadoop-daemon.sh start namenode 5）查看web页面显示 6）在[nn1]上，启动所有datanode ​ sbin/hadoop-daemons.sh start datanode 7）将[nn1]切换为Active ​ bin/hdfs haadmin -transitionToActive nn1 8）查看是否Active ​ bin/hdfs haadmin -getServiceState nn1 9）尝试kill 掉nn1的namenode Kill 7575 查看nn1和nn2的namenode 状态， 你会发现nn1挂掉后，nn2不还是standby状态，没有自动切换为active，需要手动切换为Active 配置HDFS-HA自动故障转移（上面的是手动故障转移，就是需要手动启动某个namenode为active）1）具体配置 ​ （1）在hdfs-site.xml中增加 &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; dfs.ha.automatic-failover.enabled true ​ （2）在core-site.xml文件中增加 &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;&lt;/property&gt; ha.zookeeper.quorum hadoop102:2181,hadoop103:2181,hadoop104:2181 2）启动 ​ （1）关闭所有HDFS服务： ​ sbin/stop-dfs.sh ​ （2）启动Zookeeper集群： ​ bin/zkServer.sh start ​ （3）初始化HA在Zookeeper中状态： ​ bin/hdfs zkfc -formatZK ​ （4）启动HDFS服务： ​ sbin/start-dfs.sh ​ （5）在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode ​ sbin/hadoop-daemin.sh start zkfc 3）验证 ​ （1）将Active NameNode进程kill ​ kill -9 namenode的进程id ​ （2）将Active NameNode机器断开网络 ​ service network stop YARN-高可用配置YARN-HA工作机制1）官方文档： http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html 2）YARN-HA工作机制 配置YARN-HA集群（也就是开两台resourcemanager）0）环境准备 （1）修改IP （2）修改主机名及主机名和IP地址的映射 （3）关闭防火墙 （4）ssh免密登录 （5）安装JDK，配置环境变量等 ​ （6）配置Zookeeper集群 1）规划集群 hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager ResourceManager NodeManager NodeManager NodeManager 2）具体配置 （1）yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--启用resourcemanager ha--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--声明两台resourcemanager的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster-yarn1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop102&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper集群的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复 – 当resourcemanager碟机后自动重启--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; yarn.nodemanager.aux-services mapreduce_shuffle yarn.resourcemanager.ha.enabled true yarn.resourcemanager.cluster-id cluster-yarn1 yarn.resourcemanager.ha.rm-ids rm1,rm2 yarn.resourcemanager.hostname.rm1 hadoop102 yarn.resourcemanager.hostname.rm2 hadoop103 yarn.resourcemanager.zk-address hadoop102:2181,hadoop103:2181,hadoop104:2181 yarn.resourcemanager.recovery.enabled true yarn.resourcemanager.store.class org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore ​ （2）同步更新其他节点的配置信息 3）启动hdfs （1）在各个JournalNode节点上，输入以下命令启动journalnode服务： ​ sbin/hadoop-daemon.sh start journalnode （2）在[nn1]上，对其进行格式化，并启动： ​ bin/hdfs namenode -format ​ sbin/hadoop-daemon.sh start namenode （3）在[nn2]上，同步nn1的元数据信息： ​ bin/hdfs namenode -bootstrapStandby （4）启动[nn2]： ​ sbin/hadoop-daemon.sh start namenode （5）启动所有datanode ​ sbin/hadoop-daemons.sh start datanode （6）将[nn1]切换为Active ​ bin/hdfs haadmin -transitionToActive nn1 4）启动yarn （1）在hadoop102中执行： sbin/start-yarn.sh （2）在hadoop103中执行： sbin/yarn-daemon.sh start resourcemanager （3）查看服务状态 bin/yarn rmadmin -getServiceState rm1 HDFS Federation架构设计1） NameNode架构的局限性 （1）Namespace（命名空间）的限制 由于NameNode在内存中存储所有的元数据（metadata），因此单个namenode所能存储的对象（文件+块）数目受到namenode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个datanode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个datanode从4T增长到36T，集群的尺寸增长到8000个datanode。存储的需求从12PB增长到大于100PB。 （2）隔离问题 由于HDFS仅有一个namenode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。 ​ （3）性能的瓶颈 ​ 由于是单个namenode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个namenode的吞吐量。 2）HDFS Federation架构设计 能不能有多个NameNode NameNode NameNode NameNode 元数据 元数据 元数据 Log machine 电商数据/话单数据 3）HDFS Federation应用思考 不同应用可以使用不同NameNode进行数据管理 ​ 图片业务、爬虫业务、日志审计业务 Hadoop生态系统中，不同的框架使用不同的namenode进行管理namespace。（隔离性）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"},{"name":"hadoop高可用","slug":"hadoop高可用","permalink":"http://kingge.top/tags/hadoop高可用/"}]},{"title":"hadoop大数据(七)-HDFS的Namenode和Datanode","slug":"hadoop大数据-七-HDFS的Namenode和Datanode","date":"2018-03-10T07:38:59.000Z","updated":"2019-06-10T12:56:33.609Z","comments":true,"path":"2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/","link":"","permalink":"http://kingge.top/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/","excerpt":"","text":"五 NameNode工作机制5.1 NameNode&amp;Secondary NameNode工作机制 1）第一阶段：namenode启动（1）第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存（初始化系统为上一次退出时的最新状态）。 （2）客户端对元数据进行增删改的请求 （3）namenode记录操作日志，更新滚动日志。 （4）namenode在内存中对数据进行增删改查 对于namenode而言最新的操作日志是 edits.in.progress(正在执行的日志) 2）第二阶段：Secondary NameNode工作 核心工作：检查是否需要合并namenode的编辑日志和镜像文件（checkpoint） ​ （1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。定时时间默认1小时，edits默认一百万次 ​ （2）Secondary NameNode请求执行checkpoint。（是否需要合并两个文件） ​ （3）namenode滚动正在写的edits日志（edits.in.progress） ​ （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode ​ （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 ​ （6）生成新的镜像文件fsimage.chkpoint ​ （7）拷贝fsimage.chkpoint到namenode ​ （8）namenode将fsimage.chkpoint重新命名成fsimage 也就是说，secondarynamenode的主要作用是帮助namenode分担他的压力，主要是帮助namenode合并镜像和操作日志，合并后，推给namenode。 总结正如上面所分析的，Hadoop文件系统会出现编辑日志（edits）不断增长的情况，尽管在NameNode运行期间不会对文件系统造成影响，但是如果NameNode重新启动，它将会花费大量的时间运行编辑日志中的每个操作，在此期间也就是我们前面所说的安全模式下，文件系统是不可用的。为了解决上述问题，Hadoop会运行一个Secondary NameNode进程，它的任务就是为原NameNode内存中的文件系统元数据产生检查点。其实说白了，就是辅助NameNode来处理fsimage文件与edits文件的一个进程。它从NameNode中复制fsimage与edits到临时目录并定期合并成一个新的fsimage并且删除原来的编辑日志edits。具体 步骤如下：（1）Secondary NameNode首先请求原NameNode进行edits的滚动，这样会产生一个新的编辑日志文件edits来保存对文件系统的操作（例如：上传新文件，删除文件，修改文件）。（2）Secondary NameNode通过Http方式读取原NameNode中的fsimage及edits。（3）Secondary NameNode将fsimage及edits进行合并产生新的fsimage（4）Secondary NameNode通过Http方式将新生成的fsimage发送到原来的NameNode中（5）原NameNode用新生成的fsimage替换掉旧的fsimage文件，新生成的edits文件也就是（1）生成的滚动编辑日志文件替换掉之前的edits文件 3）web端访问SecondaryNameNode​ （1）启动集群 ​ （2）浏览器中输入：http://hadoop102:50090/status.html ​ （3）查看SecondaryNameNode信息 4）chkpoint检查时间参数设置（1）通常情况下，SecondaryNameNode每隔一小时执行一次。 ​ [hdfs-default.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt; （2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt;&lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt;&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;&lt;/property&gt; 5.2 镜像文件和编辑日志文件1）概念​ namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件 edits_0000000000000000000 fsimage_0000000000000000000.md5 seen_txid VERSION （1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。 （2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。 （3）seentxid文件保存的是一个数字，就是最后一个edits的数字（最后一次操作的序号） （4）每次Namenode启动的时候都会将fsimage文件读入内存，并从00001开始到seen_txid中记录的数字依次执行每个edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成Namenode启动的时候就将fsimage和edits文件进行了合并。 2）oiv查看fsimage文件（1）查看oiv和oev命令 [kingge@hadoop102 current]$ hdfs oiv apply the offline fsimage viewer to an fsimage oev apply the offline edits viewer to an edits file （2）基本语法 hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径 （3）案例实操 [kingge@hadoop102 current]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current [kingge@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml [kingge@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml 将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。 ​ 总结 查看XML你会发现，里面存储了HDFS中文件或者文件夹的创建日期，权限，名称等等元数据信息。但是并没有存储文件保存的位置，也就是：并没有发现文件存储的DataNode节点信息信息那么当客户端请求读数据的时候，namenode是怎么返回数据所在块信息呢？原来他会一直跟datanode进行交互，获取数据所在块信息。 3）oev查看edits文件edits包括两类，edits_XXX,edits_inprogress_XXX edits_XXX：保存文件系统的操作，查看方式见下面语法&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;-&lt;EDITS&gt;&lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt;-&lt;RECORD&gt;&lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt;-&lt;DATA&gt;&lt;TXID&gt;7170&lt;/TXID&gt;&lt;/DATA&gt;&lt;/RECORD&gt;-&lt;RECORD&gt;&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;-&lt;DATA&gt;&lt;TXID&gt;7171&lt;/TXID&gt;&lt;LENGTH&gt;0&lt;/LENGTH&gt;&lt;INODEID&gt;17787&lt;/INODEID&gt;&lt;PATH&gt;/user/zpx/a.txt&lt;/PATH&gt;&lt;REPLICATION&gt;3&lt;/REPLICATION&gt;&lt;MTIME&gt;1489118864779&lt;/MTIME&gt;&lt;ATIME&gt;1489118864779&lt;/ATIME&gt;&lt;BLOCKSIZE&gt;数据的大小&lt;/BLOCKSIZE&gt;&lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_1295720148_1&lt;/CLIENT_NAME&gt;&lt;CLIENT_MACHINE&gt;192.168.231.1&lt;/CLIENT_MACHINE&gt;&lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt;-&lt;PERMISSION_STATUS&gt;&lt;USERNAME&gt;Administrator&lt;/USERNAME&gt;&lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt;&lt;MODE&gt;420&lt;/MODE&gt;&lt;/PERMISSION_STATUS&gt;&lt;RPC_CLIENTID&gt;0c9a5af9-26a8-45d9-8754-cd0e7e47f65b&lt;/RPC_CLIENTID&gt;&lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt;&lt;/DATA&gt;&lt;/RECORD&gt;&lt;/EDITS&gt;对Hdfs文件系统的每一个操作都保存在了edits文件中，每一个操作都是事务，有事务id——&lt;TXID&gt;7171&lt;/TXID&gt;，还有当前操作做了什么&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;，副本数，以及大小edits_inprogress_XXX：正在使用的过程，当前正在向前滚动。查看方式见下面语法 （1）基本语法 hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 （2）案例实操 [kingge@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml [kingge@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml 将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。 总结 你会发现，edits_inprogress，记录的是当前客户端请求执行的操作（增量记录当前操作） 5.3 滚动编辑日志正常情况HDFS文件系统有更新操作时，就会滚动编辑日志。也可以用命令强制滚动编辑日志。 1）滚动编辑日志（前提必须启动集群） [kingge@hadoop102 current]$ hdfs dfsadmin -rollEdits 2）镜像文件什么时候产生 Namenode启动时加载镜像文件和编辑日志 5.4 Namenode版本号1）查看namenode版本号 在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current这个目录下查看VERSION namespaceID=1933630176 clusterID=CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175 cTime=0 storageType=NAME_NODE blockpoolID=BP-97847618-192.168.10.102-1493726072779 layoutVersion=-63 2）namenode版本号具体解释 （1） namespaceID在HDFS上，会有多个Namenode，所以不同Namenode的namespaceID是不同的，分别管理一组blockpoolID。 （2）clusterID集群id，全局唯一 （3）cTime属性标记了namenode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 （4）storageType属性说明该存储目录包含的是namenode的数据结构。 （5）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。 （6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 5.5 SecondaryNameNode目录结构Secondary NameNode用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。 也即是说存在两种情况secondarynamenode会向namenode请求合并镜像文件和日志文件。（1）当上次请求时间已经间隔了一个小时后，会去请求（2）当操作数（edits，操作日志数）到达一百万次时，会去请求那么他怎么知道操作次数到达一百万次呢？答案是，一分钟请求namenode一次，查询操作次数是否到达一百万次。注意，这个检查操作数的时间设置最好不要跟 一致，不然他会默认执行第一种场景（间隔一个小时） 在/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/current这个目录中查看SecondaryNameNode目录结构。 edits_0000000000000000001-0000000000000000002 fsimage_0000000000000000002 fsimage_0000000000000000002.md5 VERSION SecondaryNameNode的namesecondary/current目录和主namenode的current目录的布局相同。 好处：在主namenode**发生故障时（假设没有及时备份数据），可以从SecondaryNameNode**恢复数据。 根据secondarynamenode恢复namenode方法一：将SecondaryNameNode中数据拷贝到namenode存储数据的目录； 方法二：使用-importCheckpoint选项启动namenode守护进程，从而将SecondaryNameNode中数据拷贝到namenode目录中。 1）案例实操（一）： 模拟namenode故障，并采用方法一，恢复namenode数据 （1）kill -9 namenode进程 （2）删除namenode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* （3）拷贝SecondaryNameNode中数据到原namenode存储数据目录 ​ [kingge@hadoop102 hadoop-2.7.2]$ scp -R /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* /opt/module/hadoop-2.7.2/data/tmp/dfs/name/ （4）重新启动namenode [kingge@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 2）案例实操（二）： 模拟namenode故障，并采用方法二，恢复namenode数据 （0）修改hdfs-site.xml中的 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;# 120秒checkpoint一次&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt;# namenode镜像文件和操作日志存放目录 （1）kill -9 namenode进程 （2）删除namenode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* （3）如果SecondaryNameNode不和Namenode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到Namenode存储数据的平级目录。Scp命令拷贝过来 [kingge@hadoop102 dfs]$ pwd/opt/module/hadoop-2.7.2/data/tmp/dfs[kingge@hadoop102 dfs]$ lsdata name namesecondary （4）导入检查点数据（等待一会ctrl+c结束掉） [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint （5）启动namenode [kingge@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode （6）如果提示文件锁了，可以删除in_use.lock ​ [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/in_use.lock 5.5.5 设置checkpoint检查时间默认的checkpoint period是1个小时。可以去hdfs-site.xml中修改 5.6 集群安全模式操作1）概述 Namenode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的fsimage文件和一个空的编辑日志。此时，namenode开始监听datanode请求。但是此刻，namenode运行在安全模式，即namenode的文件系统对于客户端来说是只读的。（可以解释为什么在namenode启动的时候，我们put数据到hdfs会提示，安全模式错误）因为这个时候namenode和datanode还没有联通对方，需要等待连通后，安全模式自动关闭，然后就可以上传文件了 系统中的数据块的位置并不是由namenode维护的，而是以块列表的形式存储在datanode中。在系统的正常操作期间，namenode会在内存中保留所有块位置的映射信息。在安全模式下，各个datanode会向namenode发送最新的块列表信息，namenode了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”，namenode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以namenode不会进入安全模式。 2）基本语法 集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。 （1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） （2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） （3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） （4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态） 3）案例 ​ 模拟等待安全模式 ​ 1）先进入安全模式 [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter ​ 2）执行下面的脚本 编辑一个脚本 #!/bin/bashbin/hdfs dfsadmin -safemode waitbin/hdfs dfs -put ~/hello.txt /root/hello.txt ​ 3）再打开一个窗口，执行 [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave 5.7 Namenode多目录配置1）namenode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。 2）具体配置如下： ​ hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 2.停止集群，删除数据文件 -- rm -rf data/logs (集群里有多少台服务器就删除多少台)3.格式化namenode4.调用xsync 分发脚本到各个集群 5.启动集群6.查看设置的本地目录name1、name2 你会发现里面的数据一模一样 测试NameNode场景：关闭namenode（stop-dfs.sh），关闭yarn（stop-yarn.sh），删除hadoop目录下的data目录和log目录。 1.格式化namenode – bin/hdfs namenode -format 2.启动hdfs和yarn 六 DataNode工作机制6.1 DataNode工作机制 1）一个数据块在datanode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 2）DataNode启动后向namenode注册，通过后，周期性（1小时）的向namenode上报所有的块信息。 3）心跳是每3秒一次，心跳返回结果带有namenode给该datanode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个datanode的心跳，则认为该节点不可用。 4）集群运行中可以安全加入和退出一些机器（在不关闭集群的情况下服役和退役服务器） 6.2 数据完整性1）当DataNode读取block的时候，它会计算checksum 2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。 3）client读取其他DataNode上的block。 4）datanode在其文件创建后周期验证checksum 6.3 掉线时限参数设置datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： ​ timeout = 2 dfs.namenode.heartbeat.recheck-interval + 10 dfs.heartbeat.interval。 ​ 而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。 ​ 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 &lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt; dfs.heartbeat.interval &lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 6.4 DataNode的目录结构和namenode不同的是，datanode的存储目录是初始阶段自动创建的，不需要额外格式化。 1）在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current这个目录下查看版本号 [kingge@hadoop102 current]$ cat VERSION storageID=DS-1b998a1d-71a3-43d5-82dc-c0ff3294921b clusterID=CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175 cTime=0 datanodeUuid=970b2daf-63b8-4e17-a514-d81741392165 storageType=DATA_NODE layoutVersion=-56 2）具体解释 ​ （1）storageID：存储id号 ​ （2）clusterID集群id，全局唯一 ​ （3）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 ​ （4）datanodeUuid：datanode的唯一识别码 ​ （5）storageType：存储类型 ​ （6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 3）在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-97847618-192.168.10.102-1493726072779/current这个目录下查看该数据块的版本号 [kingge@hadoop102 current]$ cat VERSION #Mon May 08 16:30:19 CST 2017 namespaceID=1933630176 cTime=0 blockpoolID=BP-97847618-192.168.10.102-1493726072779 layoutVersion=-56 4）具体解释 （1）namespaceID：是datanode首次访问namenode的时候从namenode处获取的storageID对每个datanode来说是唯一的（但对于单个datanode中所有存储目录来说则是相同的），namenode可用这个属性来区分不同datanode。 （2）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 （3）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。 （4）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 6.5 服役新数据节点0）需求： 随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。 1）环境准备 ​ （1）克隆一台虚拟机 ​ （2）修改ip地址和主机名称 ​ （3）修改xcall和xsync文件，增加新`增节点的同步ssh ​ （4）删除原来HDFS文件系统留存的文件 ​ /opt/module/hadoop-2.7.2/data 和 /opt/module/hadoop-2.7.2/log目录 2）服役新节点具体步骤（下面的操作建议在namenode所在节点进行操作） ​ （1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件 [kingge@hadoop105 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop105 hadoop]$ touch dfs.hosts （名字任意） [kingge@hadoop105 hadoop]$ vi dfs.hosts 添加如下主机名称（包含新服役的节点） hadoop102 hadoop103 hadoop104 hadoop105 ​ （2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性 &lt;property&gt;&lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; ​ （3）刷新namenode [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful ​ （4）更新resourcemanager节点 [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 操作完后，打开**hdfs文件系统，发现已经服役了一个新的data**节点 ​ （5）在namenode的slaves文件中增加新主机名称 ​ 增加105 不需要分发 hadoop102 hadoop103 hadoop104 hadoop105 ​ （6）单独命令启动新的数据节点和节点管理器 [kingge@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-datanode-hadoop105.out [kingge@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-kingge-nodemanager-hadoop105.out ​ （7）在web浏览器上检查是否ok 3）如果数据不均衡，可以用命令实现集群的再平衡 ​ [kingge@hadoop102 sbin]$ ./start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 6.6 退役旧数据节点1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件 ​ [kingge@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop102 hadoop]$ touch dfs.hosts.exclude [kingge@hadoop102 hadoop]$ vi dfs.hosts.exclude 添加如下主机名称（要退役的节点） hadoop105 2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 &lt;property&gt;&lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 3）刷新namenode、刷新resourcemanager [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 4）检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点。 5）等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役。· [kingge@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode stopping datanode [kingge@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager stopping nodemanager 6）从include文件中删除退役节点，再运行刷新节点的命令 ​ （1）从namenode的dfs.hosts文件中删除退役节点hadoop105 hadoop102 hadoop103 hadoop104 ​ （2）刷新namenode，刷新resourcemanager [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 7）从namenode的slave文件中删除退役节点hadoop105 hadoop102 hadoop103 hadoop104 8）如果数据不均衡，可以用命令实现集群的再平衡 [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 6.7 Datanode多目录配置1）datanode也可以配置成多个目录，每个目录存储的数据不一样，即是上传一个文本，那么文本存储在data，但是data2什么都没有（跟namenode多目录区别）。即：数据不是副本。 2）具体配置如下： ​ hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; 七 HDFS其他功能7.1 集群间数据拷贝1）scp实现两个远程主机之间的文件复制 ​ scp -r hello.txt root@hadoop103:/user/kingge/hello.txt // 推 push ​ scp -r root@hadoop103:/user/kingge/hello.txt hello.txt // 拉 pull ​ scp -r root@hadoop103:/user/kingge/hello.txt root@hadoop104:/user/kingge //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。 2）采用discp命令实现两个hadoop集群之间的递归数据复制 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp hdfs://haoop102:9000/user/kingge/hello.txt hdfs://hadoop103:9000/user/kingge/hello.txt 7.2 Hadoop存档1）理论概述 每个文件均按块存储，每个块的元数据存储在namenode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽namenode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。 Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少namenode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件可以用作MapReduce的输入。 2）案例实操 （1）需要启动yarn进程 ​ [kingge@hadoop102 hadoop-2.7.2]$ start-yarn.sh （2）归档文件 ​ 归档成一个叫做xxx.har的文件夹，该文件夹下有相应的数据文件。Xx.har目录是一个整体，该目录看成是一个归档文件即可。 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p /user/kingge /user/my （3）查看归档 ​ [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///myhar.har （4）解归档文件 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har /* /user/kingge 7.3 快照管理快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 1）基本语法 ​ （1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能） ​ （2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用） ​ （3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照） ​ （4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照） ​ （5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照） ​ （6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有可快照目录） ​ （7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处） ​ （8）hdfs dfs -deleteSnapshot （功能描述：删除快照） 2）案例实操 ​ （1）开启/禁用指定目录的快照功能 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/kingge/data [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/kingge/data ​ （2）对目录创建快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/kingge/data 通过web访问hdfs://hadoop102:9000/user/kingge/data/.snapshot/s…..// 快照和源文件使用相同数据块 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/kingge/data/.snapshot/ ​ （3）指定名称创建快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/kingge/data miao170508 ​ （4）重命名快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/kingge/data/ miao170508 kingge170508 ​ （5）列出当前用户所有可快照目录 [kingge@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir ​ （6）比较两个快照目录的不同之处 [kingge@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/kingge/data/ . .snapshot/kingge170508 ​ （7）恢复快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/kingge/input/.snapshot/s20170708-134303.027 /user 7.4 回收站1）默认回收站 默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。 默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。 要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。 2）启用回收站 修改core-site.xml，配置垃圾回收时间为1分钟。 &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 3）查看回收站 回收站在集群中的；路径：/user/kingge/.Trash/…. 4）修改访问垃圾回收站用户名称(如果不修改为想要查看该回收站的用户的名称，那么该用户试图进入回收站时会提示权限问题) ​ 进入垃圾回收站用户名称，默认是dr.who，修改为kingge用户 ​ [core-site.xml] &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;kingge&lt;/value&gt;&lt;/property&gt; 5）通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 Trash trash = New Trash(conf); trash.moveToTrash(path); 6）恢复回收站数据 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/kingge/.Trash/Current/user/kingge/input /user/kingge/input 7）清空回收站（他并不是真正删除文件，而是生成一个当前时间戳的文件夹然后把回收站里面的文件都放到这个文件夹里面） [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -expunge","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(六)-HDFS的读写数据流程","slug":"hadoop大数据-六-HDFS的读写数据流程","date":"2018-03-08T14:38:59.000Z","updated":"2019-06-09T04:48:50.181Z","comments":true,"path":"2018/03/08/hadoop大数据-六-HDFS的读写数据流程/","link":"","permalink":"http://kingge.top/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/","excerpt":"","text":"四 HDFS的数据流4.1 HDFS写数据流程4.1.1 剖析文件写入 1）客户端通过Distributed FileSystem模块向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。（存在覆盖，不存在创建） 2）namenode返回是否可以上传。 3）客户端请求第一个 block上传到哪几个datanode服务器上。 4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。（根据配置文件中指定的备份数量及机架感知原理进行文件分配） 5）客户端通过FSDataOutputStream模块请求dn1上传数据（建立RPC请求），dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。-应答成功，开始传输数据 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet（默认 64K）为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 128M他并不是一次性写入dn1，而是分包的形式，dn1接收到一个包，然后保存在自己所在服务器的本地缓存中。然后再写入自己磁盘（7_blk_1）的同时，传输给dn2，以此类推。直到传输完整个128M。第一块传输完毕。 8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器。（重复执行3-7步）。 4.1.2 网络拓扑概念​ 在本地网络中，两个节点被称为“彼此近邻”是什么意思？在海量数据处理中，其主要限制因素是节点之间数据的传输速率——带宽很稀缺。这里的想法是将两个节点间的带宽作为距离的衡量标准。 ​ 节点距离：两个节点到达最近的共同祖先的距离总和。 例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 Distance(/d1/r1/n1, /d1/r1/n1)=0（同一节点上的进程） Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点） Distance(/d1/r1/n1, /d1/r3/n2)=4（同一数据中心不同机架上的节点） Distance(/d1/r1/n1, /d2/r4/n2)=6（不同数据中心的节点） 大家算一算每两个节点之间的距离。 4.1.3 机架感知（副本节点选择） 一份数据如果存在三个副本，那么副本存放服务器的选择，应该采取怎么样的策略 1）官方ip地址： http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication 2）低版本Hadoop副本节点选择 第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于不相同机架的随机节点上。 第三个副本和第二个副本位于相同机架，节点随机。 3）Hadoop2.7.2副本节点选择 ​ 第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。 ​ 第二个副本和第一个副本位于相同机架，随机节点。 ​ 第三个副本位于不同机架，随机节点。 为什么第一个副本选择在客户端所在的节点，因为这样client**请求数据的时候，可以做到更快的响应，优先读取当前节点副本信息（设计网络拓扑概念），距离客户端越近的节点，数据传输速率越快** 4.2 HDFS读数据流程 1）客户端通过Distributed FileSystem向namenode请求下载文件，namenode通过查询元数据，找到文件块所在的datanode地址。 2）挑选一台datanode（就近原则，然后随机）服务器，请求读取数据。 3）datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验）。 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 4.3 一致性模型1）debug调试如下代码 @Test public void writeFile() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); fs = FileSystem.get(configuration); // 2 创建文件输出流 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”); FSDataOutputStream fos = fs.create(path); // 3 写数据 fos.write(“hello”.getBytes()); // 4 一致性刷新 fos.hflush(); fos.close(); } @Test public void writeFile() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); fs = FileSystem.get(configuration); // 2 创建文件输出流 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;); FSDataOutputStream fos = fs.create(path); // 3 写数据 fos.write(&quot;hello&quot;.getBytes()); // 4 一致性刷新 fos.hflush(); fos.close(); &#125; 2）总结 写入数据时，如果希望数据被其他client立即可见，调用如下方法 FsDataOutputStream. hflush (); //清理客户端缓冲区数据，被其他client立即可见 因为传统的流操作，只有在关闭流的时候，才会去执行flush**操作，那么可能在关闭流之前发生错误，导致数据没有存储到对应的节点。为了避免这种问题，可以手动显式的执行flush**写入磁盘操作。（IOUtils. copyBytes 内部已经实现刷新机制，不需要手动刷新）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(五)-HDFS概念和基本操作","slug":"hadoop大数据-五-HDFS概念和基本操作","date":"2018-03-06T12:31:59.000Z","updated":"2019-06-09T04:01:11.154Z","comments":true,"path":"2018/03/06/hadoop大数据-五-HDFS概念和基本操作/","link":"","permalink":"http://kingge.top/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/","excerpt":"","text":"一 HDFS简单介绍1.1 HDFS产生背景随着数据量越来越大，在一个操作系统管辖的范围内存不下了，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。 1.2 HDFS概念HDFS**，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的**，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS的设计适合一次写入，多次读出的场景，且不支持文件的修改（也是可以修改的，但是不建议）。适合用来做数据分析，并不适合用来做网盘应用。 1.3 HDFS 优缺点1.3.1 优点1）高容错性 ​ （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。 ​ （2）某一个副本丢失以后，它可以自动恢复，这是由 HDFS 内部机制实现的，我们不必关心。 2）适合大数据处理 ​ （1）数据规模：能够处理数据规模达到 GB、TB、甚至PB级别的数据。（因为他会切块存储，所以可以存储大文件-**参见HDFS**写数据流程） ​ （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 3）流式数据访问**（一点一点的处理数据，而不是一次性读取整个数据，这样会极大消耗内存）** ​ （1）一次写入，多次读取，不能修改，只能追加。 ​ （2）它能保证数据的一致性。 4）可构建在廉价机器上 ​ （1）它通过多副本机制，提高可靠性。 ​ （2）它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。 1.3.2 缺点1）不适合低延时数据访问 ​ （1）比如毫秒级的来存储数据，这是不行的，它做不到。 ​ （2）它适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况下是不行的，比如毫秒级以内读取数据，这样它是很难做到的。 2）无法高效的对大量小文件进行存储（HDFS默认的最基本的存储单位是128M的数据块。） ​ （1）存储大量小文件;)的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。 ​ （2）小文件存储的寻道时间会超过读取时间，它违反了HDFS的设计目标。 3）并发写入、文件随机修改 ​ （1）一个文件只能有一个写，不允许多个线程同时写。 ​ （2）仅支持数据 append（追加），不支持文件的随机修改。 1.4 HDFS架构HDFS的架构图 ​ 这种架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分。 1）Client：就是客户端。 ​ （1）文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的Block，然后进行存储。 ​ （2）与NameNode交互，获取文件的位置信息。 ​ （3）与DataNode交互，读取或者写入数据。 ​ （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS。 ​ （5）Client可以通过一些命令来访问HDFS。 2）NameNode：就是master，它是一个主管、管理者。 ​ （1）管理HDFS的名称空间。 ​ （2）管理数据块（Block）;)映射信息 ​ （3）配置副本策略;) ​ （4）处理客户端读写请求。 3） DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 ​ （1）存储实际的数据块。 ​ （2）执行数据块的读/写操作。 4） Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 ​ （1）辅助NameNode，分担其工作量。 ​ （2）定期合并fsimage和fsedits;)，并推送给NameNode。 ​ （3）在紧急情况下，可辅助恢复NameNode。 1.5 HDFS 文件块大小HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。 HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。 如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。 块的大小： 10ms*100*100M/s = 100M 块大小取决于磁盘传输速率 二 HFDS命令行操作操作HDFS的命令其实有三个： ​ Hadoop fs：使用面最广，可以操作任何文件系统。 ​ hadoop dfs与hdfs dfs：只能操作HDFS文件系统相关（包括与Local FS间的操作），前者已经Deprecated，一般使用后者。 推荐使用：hadoop fs 1）基本语法bin/hadoop fs 具体命令 2）参数大全​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs [-appendToFile … ] [-cat [-ignoreCrc] …] [-checksum …] [-chgrp [-R] GROUP PATH…] [-chmod [-R] PATH…] [-chown [-R] [OWNER][:[GROUP]] PATH…] [-copyFromLocal [-f] [-p] … ] [-copyToLocal [-p] [-ignoreCrc] [-crc] … ] [-count [-q] …] [-cp [-f] [-p] … ] [-createSnapshot []] [-deleteSnapshot ] [-df [-h] [ …]] [-du [-s] [-h] …] [-expunge] [-get [-p] [-ignoreCrc] [-crc] … ] [-getfacl [-R] ] [-getmerge [-nl] ] [-help [cmd …]] [-ls [-d] [-h] [-R] [ …]] [-mkdir [-p] …] [-moveFromLocal … ] [-moveToLocal ] [-mv … ] [-put [-f] [-p] … ] [-renameSnapshot ] [-rm [-f] [-r|-R] [-skipTrash] …] [-rmdir [–ignore-fail-on-non-empty] …] [-setfacl [-R] [{-b|-k} {-m|-x } ]|[–set ]] [-setrep [-R] [-w] …] [-stat [format] …] [-tail [-f] ] [-test -[defsz] ] [-text [-ignoreCrc] …] [-touchz …] [-usage [cmd …]] [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] &lt;path&gt; ...] [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] &lt;path&gt; ...] [-expunge] [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] &lt;file&gt;] [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]] 3）常用命令 （0）启动Hadoop集群（方便后续的测试） ​ [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -help rm ​ （2）-ls: 显示目录信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -ls / （3）-mkdir：在hdfs上创建目录 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /user/kingge/test （4）-moveFromLocal从本地剪切粘贴到hdfs [kingge@hadoop102 hadoop-2.7.2]$ touch jinlian.txt [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./jinlian.txt /user/kingge/test （5）–appendToFile ：追加一个文件到已经存在的文件末尾 [kingge@hadoop102 hadoop-2.7.2]$ touch ximen.txt [kingge@hadoop102 hadoop-2.7.2]$ vi ximen.txt 输入 wo ai jinlian [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile ximen.txt /user/kingge/test/jinlian.txt （6）-cat ：显示文件内容 （7）-tail：显示一个文件的末尾 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /user/kingge/test/jinlian.txt （8）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /hello.txt [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -chown someuser:somegrp /hello.txt （9）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /user/kingge/test （10）-copyToLocal：从hdfs拷贝到本地 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /user/kingge/test/jinlian.txt ./jinlian.txt （11）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /user/kingge/test/jinlian.txt /jinlian2.txt （12）-mv：在hdfs目录中移动文件 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /jinlian2.txt /user/kingge/test/ （13）-get：等同于copyToLocal，就是从hdfs下载文件到本地 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -get /user/kingge/test/jinlian2.txt ./ （14）-getmerge ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,… [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/kingge/test/* ./zaiyiqi.txt （15）-put：等同于copyFromLocal [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/kingge/test/ （16）-rm：删除文件或文件夹 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/kingge/test/jinlian2.txt （17）-rmdir：删除空目录 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test （18）-df ：统计文件系统的可用空间信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -df -h / （19）-du统计文件夹的大小信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/kingge/test 2.7 K /user/kingge/test ###查看文件夹下文件总大小 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/kingge/test 1.3 K /user/kingge/test/README.txt 15 /user/kingge/test/jinlian.txt 1.4 K /user/kingge/test/zaiyiqi.txt ##查看文件夹下各个文件大小 （20）-setrep：设置hdfs中文件的副本数量 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 2 /user/kingge/test/jinlian.txt 这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。 三 HDFS客户端操作-eclipse3.1 Eclipse环境准备注意：以下操作，是在hadoop集群中中进行的，也就是说，需要先启动linux的hadoop集群 3.1.1 jar包准备1）解压hadoop-2.7.2.tar.gz到非中文目录 2）进入share文件夹，查找所有jar包，并把jar包拷贝到_lib文件夹下 3）在全部jar包中查找sources.jar，并剪切到_source文件夹。 4）在全部jar包中查找tests.jar，并剪切到_test文件夹。 3.1.2 Eclipse准备1）配置HADOOP_HOME环境变量 2）采用Hadoop编译后的bin 、lib两个文件夹（如果不生效，重新启动eclipse） 3）创建第一个java工程 4）导入 编译后目录的jar包（可以在文章下方回复我，我私信给你们） public class HdfsClientDemo1 { public static void main(String[] args) throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 configuration.set(“fs.defaultFS”, “hdfs://hadoop102:9000”); FileSystem fileSystem = FileSystem.get(configuration); // 直接配置访问集群的路径和访问集群的用户名称 // FileSystem fileSystem = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 把本地文件上传到文件系统中 fileSystem.copyFromLocalFile(new Path(“f:/hello.txt”), new Path(“/hello1.copy.txt”)); // 3 关闭资源 fileSystem.close(); System.out.println(“over”); } } public class HdfsClientDemo1 &#123; public static void main(String[] args) throws Exception &#123; // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 - 如果不配置，默认使用的是本地的hadoop运行环境。 configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); FileSystem fileSystem = FileSystem.get(configuration); // 直接配置访问集群的路径和访问集群的用户名称// FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 把本地文件上传到文件系统中 fileSystem.copyFromLocalFile(new Path(&quot;f:/hello.txt&quot;), new Path(&quot;/hello1.copy.txt&quot;)); // 3 关闭资源 fileSystem.close(); System.out.println(&quot;over&quot;); &#125;&#125; 4）执行程序 运行时需要配置用户名称 客户端去操作hdfs时，是有一个用户身份的。默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=kingge，kingge为用户名称。 3.2 通过API操作HDFS3.2.1 HDFS获取文件系统1）详细代码 ​ @Test public void initHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); // 2 获取文件系统 FileSystem fs = FileSystem.get(configuration); // 3 打印文件系统 System.out.println(fs.toString()); } @Testpublic void initHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); // 2 获取文件系统 FileSystem fs = FileSystem.get(configuration); // 3 打印文件系统 System.out.println(fs.toString());&#125; 3.2.2 HDFS文件上传 @Test public void putFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration configuration = new Configuration(); // 2 设置参数 // 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 3 创建要上传文件所在的本地路径 Path src = new Path(&quot;e:/hello.txt&quot;); // 4 创建要上传到hdfs的目标路径 Path dst = new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;); // 5 拷贝文件 fs.copyFromLocalFile(src, dst); fs.close(); &#125; ​ @Test public void putFileToHDFS() throws Exception{ // 1 创建配置信息对象 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration configuration = new Configuration(); // 2 设置参数 // 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 configuration.set(“dfs.replication”, “2”); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 3 创建要上传文件所在的本地路径 Path src = new Path(“e:/hello.txt”); // 4 创建要上传到hdfs的目标路径 Path dst = new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”); // 5 拷贝文件 fs.copyFromLocalFile(src, dst); fs.close(); } 2）将core-site.xml拷贝到项目的根目录下 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; fs.defaultFS hdfs://hadoop102:9000 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）将hdfs-site.xml拷贝到项目的根目录下 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; dfs.replication 1 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 4）测试参数优先级 参数优先级： 1**、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3**、然后是服务器的默认配置 3.2.3 HDFS文件下载 @Test public void getFileFromHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // fs.copyToLocalFile(new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“d:/hello.txt”)); // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件效验 // 2 下载文件 fs.copyToLocalFile(false, new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“e:/hellocopy.txt”), true); fs.close(); } @Testpublic void getFileFromHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // fs.copyToLocalFile(new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;d:/hello.txt&quot;)); // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件效验 // 2 下载文件 fs.copyToLocalFile(false, new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;e:/hellocopy.txt&quot;), true); fs.close(); &#125; 3.2.4 HDFS目录创建​ @Test public void mkdirAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 创建目录 fs.mkdirs(new Path(“hdfs://hadoop102:9000/user/atguigu/output”)); } @Testpublic void mkdirAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 创建目录 fs.mkdirs(new Path(&quot;hdfs://hadoop102:9000/user/kingge/output&quot;));&#125; 3.2.5 HDFS文件夹删除@Testpublic void deleteAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 删除文件夹 ，如果是非空文件夹，参数2是否递归删除，true递归 fs.delete(new Path(&quot;hdfs://hadoop102:9000/user/kingge/output&quot;), true);&#125; ​ @Test public void deleteAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 删除文件夹 ，如果是非空文件夹，参数2是否递归删除，true递归 fs.delete(new Path(“hdfs://hadoop102:9000/user/atguigu/output”), true); } 3.2.6 HDFS文件名更改​ @Test public void renameAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 重命名文件或文件夹 fs.rename(new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“hdfs://hadoop102:9000/user/atguigu/hellonihao.txt”)); } @Testpublic void renameAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 重命名文件或文件夹 fs.rename(new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;hdfs://hadoop102:9000/user/kingge/hellonihao.txt&quot;));&#125; 3.2.7 HDFS文件详情查看查看文件名称、权限、长度、块信息-不是文件夹，是文件 @Test public void readListFiles() throws Exception { // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 思考：为什么返回迭代器，而不是List之类的容器 RemoteIterator listFiles = fs.listFiles(new Path(“/“), true); while (listFiles.hasNext()) { LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) { System.out.println(“block-offset:” + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) { System.out.println(host); } } System.out.println(“————–李冰冰的分割线————–”); } } @Testpublic void readListFiles() throws Exception &#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 思考：为什么返回迭代器，而不是List之类的容器 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) &#123; System.out.println(&quot;block-offset:&quot; + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(&quot;--------------李冰冰的分割线--------------&quot;); &#125; &#125; 3.2.8 HDFS文件和文件夹判断 不支持递归，只能查询当前某个目录下的文件或者或者文件夹，然后判断 @Testpublic void findAtHDFS() throws Exception, IllegalArgumentException, IOException&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 获取查询路径下的文件状态信息 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); // 3 遍历所有文件状态 for (FileStatus status : listStatus) &#123; if (status.isFile()) &#123; System.out.println(&quot;f--&quot; + status.getPath().getName()); &#125; else &#123; System.out.println(&quot;d--&quot; + status.getPath().getName()); &#125; &#125;&#125; @Test public void findAtHDFS() throws Exception, IllegalArgumentException, IOException{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 获取查询路径下的文件状态信息 FileStatus[] listStatus = fs.listStatus(new Path(“/“)); // 3 遍历所有文件状态 for (FileStatus status : listStatus) { if (status.isFile()) { System.out.println(“f–” + status.getPath().getName()); } else { System.out.println(“d–” + status.getPath().getName()); } } } 3.3 通过IO流操作HDFS3.3.1 HDFS文件上传​ @Test public void putFileToHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 创建输入流 FileInputStream inStream = new FileInputStream(new File(“e:/hello.txt”)); // 3 获取输出路径 String putFileName = “hdfs://hadoop102:9000/user/atguigu/hello1.txt”; Path writePath = new Path(putFileName); // 4 创建输出流 FSDataOutputStream outStream = fs.create(writePath); // 5 流对接 try{ IOUtils.copyBytes(inStream, outStream, 4096, false); }catch(Exception e){ e.printStackTrace(); }finally{ IOUtils.closeStream(inStream); IOUtils.closeStream(outStream); } } @Testpublic void putFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 创建输入流 FileInputStream inStream = new FileInputStream(new File(&quot;e:/hello.txt&quot;)); // 3 获取输出路径 String putFileName = &quot;hdfs://hadoop102:9000/user/kingge/hello1.txt&quot;; Path writePath = new Path(putFileName); // 4 创建输出流 FSDataOutputStream outStream = fs.create(writePath); // 5 流对接 try&#123; IOUtils.copyBytes(inStream, outStream, 4096, false); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; IOUtils.closeStream(inStream); IOUtils.closeStream(outStream); &#125;&#125; 3.3.2 HDFS文件下载​ @Test public void getFileToHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 获取读取文件路径 String filename = “hdfs://hadoop102:9000/user/atguigu/hello1.txt”; // 3 创建读取path Path readPath = new Path(filename); // 4 创建输入流 FSDataInputStream inStream = fs.open(readPath); // 5 流对接输出到控制台 try{ IOUtils.copyBytes(inStream, System.out, 4096, false); }catch(Exception e){ e.printStackTrace(); }finally{ IOUtils.closeStream(inStream); } } @Testpublic void getFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 获取读取文件路径 String filename = &quot;hdfs://hadoop102:9000/user/kingge/hello1.txt&quot;; // 3 创建读取path Path readPath = new Path(filename); // 4 创建输入流 FSDataInputStream inStream = fs.open(readPath); // 5 流对接输出到控制台 try&#123; IOUtils.copyBytes(inStream, System.out, 4096, false); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; IOUtils.closeStream(inStream); &#125;&#125; 3.3.3 定位文件读取1）下载第一块 @Test // 定位下载第一块内容 public void readFileSeek1() throws Exception { // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”), configuration, “atguigu”); // 2 获取输入流路径 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/tmp/hadoop-2.7.2.tar.gz”); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(“e:/hadoop-2.7.2.tar.gz.part1”); // 5 流对接 byte[] buf = new byte[1024]; for (int i = 0; i &lt; 128 1024; i++) { fis.read(buf); fos.write(buf); } // 6 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream*(fos); } @Test// 定位下载第一块内容public void readFileSeek1() throws Exception &#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kingge&quot;); // 2 获取输入流路径 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/tmp/hadoop-2.7.2.tar.gz&quot;); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;); // 5 流对接 byte[] buf = new byte[1024]; for (int i = 0; i &lt; 128 * 1024; i++) &#123; fis.read(buf); fos.write(buf); &#125; // 6 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos); &#125; 2）下载第二块 ​ @Test // 定位下载第二块内容 public void readFileSeek2() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”), configuration, “atguigu”); // 2 获取输入流路径 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/tmp/hadoop-2.7.2.tar.gz”); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(“e:/hadoop-2.7.2.tar.gz.part2”); // 5 定位偏移量（第二块的首位） fis.seek(1024 1024 128); // 6 流对接 IOUtils.copyBytes(fis, fos, 1024); // 7 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } @Test// 定位下载第二块内容public void readFileSeek2() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kingge&quot;); // 2 获取输入流路径 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/tmp/hadoop-2.7.2.tar.gz&quot;); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;); // 5 定位偏移量（第二块的首位） fis.seek(1024 * 1024 * 128); // 6 流对接 IOUtils.copyBytes(fis, fos, 1024); // 7 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos);&#125; 3）合并文件 在window命令窗口中执行 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 解压，发现，就是我们上传的压缩文件。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(四)-Hadoop编译源码","slug":"hadoop大数据-四-Hadoop编译源码","date":"2018-02-28T11:31:59.000Z","updated":"2019-06-09T02:46:53.773Z","comments":true,"path":"2018/02/28/hadoop大数据-四-Hadoop编译源码/","link":"","permalink":"http://kingge.top/2018/02/28/hadoop大数据-四-Hadoop编译源码/","excerpt":"","text":"五 Hadoop编译源码5.1 前期准备工作1）CentOS联网 配置CentOS能连接外网。Linux虚拟机ping www.baidu.com 是畅通的 注意：采用root角色编译，减少文件夹权限出现问题 2）jar包准备(hadoop源码、JDK7 、 maven、 ant 、protobuf) （1）hadoop-2.7.2-src.tar.gz （2）jdk-7u79-linux-x64.gz （3）apache-ant-1.9.9-bin.tar.gz （4）apache-maven-3.0.5-bin.tar.gz （5）protobuf-2.5.0.tar.gz 5.2 jar包安装0）注意：所有操作必须在root用户下完成 1）JDK解压、配置环境变量 JAVA_HOME和PATH，验证java-version(如下都需要验证是否配置成功) [root@hadoop101 software] # tar -zxf jdk-7u79-linux-x64.gz -C /opt/module/ [root@hadoop101 software]# vi /etc/profile #JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：java -version 2）Maven解压、配置 MAVEN_HOME和PATH。 [root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile #MAVEN_HOME export MAVEN_HOME=/opt/module/apache-maven-3.0.5 export PATH=$PATH:$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn -version 3）ant解压、配置 ANT _HOME和PATH。 [root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-ant-1.9.9]# vi /etc/profile #ANT_HOME export ANT_HOME=/opt/module/apache-ant-1.9.9 export PATH=$PATH:$ANT_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：ant -version 4）安装 glibc-headers 和 g++ 命令如下: [root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers [root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++ 5）安装make和cmake [root@hadoop101 apache-ant-1.9.9]# yum install make [root@hadoop101 apache-ant-1.9.9]# yum install cmake 6）解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0 然后相继执行命令： [root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/ [root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/ [root@hadoop101 protobuf-2.5.0]#./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make check [root@hadoop101 protobuf-2.5.0]# make install [root@hadoop101 protobuf-2.5.0]# ldconfig [root@hadoop101 hadoop-dist]# vi /etc/profile #LD_LIBRARY_PATH export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0 export PATH=$PATH:$LD_LIBRARY_PATH [root@hadoop101 software]#source /etc/profile 验证命令：protoc –version 7）安装openssl库 [root@hadoop101 software]#yum install openssl-devel 8）安装 ncurses-devel库： [root@hadoop101 software]#yum install ncurses-devel 到此，编译工具安装基本完成。 5.3 编译源码1）解压源码到/opt/tools目录 [root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/ 2）进入到hadoop源码主目录 [root@hadoop101 hadoop-2.7.2-src]# pwd /opt/hadoop-2.7.2-src 3）通过maven执行编译命令 [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 等待时间30分钟左右，最终成功是全部SUCCESS。 4）成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下。 [root@hadoop101 target]# pwd /opt/hadoop-2.7.2-src/hadoop-dist/target 5.4 常见的问题及解决方案1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） 2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐 2.7.0版本的问题汇总帖子 http://www.tuicool.com/articles/IBn63qf","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(三)-Hadoop三种部署和运行方式","slug":"hadoop大数据-三-Hadoop三种部署和运行方式","date":"2018-02-26T15:21:59.000Z","updated":"2019-06-09T02:46:10.040Z","comments":true,"path":"2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/","link":"","permalink":"http://kingge.top/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/","excerpt":"","text":"四 Hadoop运行模式1）官方网址 （1）官方网站： http://hadoop.apache.org/ （2）各个版本归档库地址 https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/ （3）hadoop2.7.2版本详情介绍 http://hadoop.apache.org/docs/r2.7.2/ 2）Hadoop运行模式 （1）本地模式（默认模式）： 不需要启用单独进程，直接可以运行，测试和开发时使用。 （2）伪分布式模式： 等同于完全分布式，只有一个节点。 （3）完全分布式模式： 多个节点一起运行。 4.1 本地运行Hadoop 案例4.1.1 官方grep案例1）创建在hadoop-2.7.2文件下面创建一个input文件夹 [kingge@hadoop101 hadoop-2.7.2]$mkdir input 2）将hadoop的xml配置文件复制到input [kingge@hadoop101 hadoop-2.7.2]$cp etc/hadoop/*.xml input 3）执行share目录下的mapreduce程序 [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output ‘dfs[a-z.]+’ 4）查看输出结果 [kingge@hadoop101 hadoop-2.7.2]$ cat output/* 4.1.2 官方wordcount案例1）创建在hadoop-2.7.2文件下面创建一个wcinput文件夹 [kingge@hadoop101 hadoop-2.7.2]$mkdir wcinput 2）在wcinput文件下创建一个wc.input文件 [kingge@hadoop101 hadoop-2.7.2]$cd wcinput [kingge@hadoop101 wcinput]$touch wc.input 3）编辑wc.input文件 [kingge@hadoop101 wcinput]$vim wc.input在文件中输入如下内容hadoop yarnhadoop mapreduce kinggekingge保存退出：：wq 4）回到hadoop目录/opt/module/hadoop-2.7.2 5）执行程序： [kingge@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput 6）查看结果： [kingge@hadoop101 hadoop-2.7.2]$cat wcoutput/part-r-00000 kingge 2 hadoop 2 mapreduce 1 yarn 1 4.1.3 总结第一个案例：统计input文件里面的文件，文件内容包含’dfs[a-z.]+’ 规则的文字，筛选出来。 第二个案例：统计wc.input 文件中单词出现的个数，特别注意，第二次运行之前必须删除已有的结果输出目录（wcoutput）（rm -rf wcoutput/），否则执行wordcount指令就会报错，提示文件已经存在 —- 操作设计的文件都是存储在linux 的文件系统中。本地操作，不支持联网操作 4.2 伪分布式运行Hadoop案例4.2.1 启动HDFS并运行MapReduce程序1）分析： ​ （1）准备1台客户机 ​ （2）安装jdk ​ （3）配置环境变量 ​ （4）安装hadoop ​ （5）配置环境变量 ​ （6）配置集群 ​ （7）启动、测试集群增、删、查 ​ （8）执行wordcount案例 2）执行步骤 需要配置hadoop文件如下 （1）配置集群 （a）配置：hadoop-env.sh ​ 1.Linux系统中获取jdk的安装路径： [kingge@hadoop100 hadoop-2.7.2]$ cd etc/hadoop/ [root@ hadoop101 ~]# echo $JAVA_HOME /opt/module/jdk1.8.0_144 ​ 2.修改Jhadoop-env.sh的JAVA_HOME 路径： export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置：core-site.xml &lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; （c）配置：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; （2）启动集群（a）格式化namenode（第一次启动时格式化，以后就不要总格式化） ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs namenode –format ​ 会在上面配置配置的存储目录生成 这两个文件 （b）启动namenode ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode ​ c）启动datanode ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode （3）查看集群​ （a）查看是否启动成功 [kingge@hadoop101 hadoop-2.7.2]$ jps 13586 NameNode 13668 DataNode 13786 Jps ​ （b）查看产生的log日志 当前目录：/opt/module/hadoop-2.7.2/logs [kingge@hadoop101 logs]$ ls hadoop-kingge-datanode-hadoop.kingge.com.log hadoop-kingge-datanode-hadoop.kingge.com.out hadoop-kingge-namenode-hadoop.kingge.com.log hadoop-kingge-namenode-hadoop.kingge.com.out SecurityAuth-root.audit [kingge@hadoop101 logs]# cat hadoop-kingge-datanode-hadoop101.log ​ （c）web端查看HDFS文件系统 ​ http://192.168.1.101:50070/dfshealth.html#tab-overview ​ 注意：如果不能查看，看如下帖子处理 http://www.cnblogs.com/zlslch/p/6604189.html （4）操作集群​ （a）在hdfs文件系统上创建一个input文件夹 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/kingge/input ​ （b）将测试文件内容上传到文件系统上 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -put wcinput/wc.input /user/kingge/input/ ​ （c）查看上传的文件是否正确 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -ls /user/kingge/input/ [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/ input/wc.input ​ （d）运行mapreduce程序(所有数据在HDFS上) ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input/ /user/kingge/output ​ （e）查看输出结果 命令行查看： [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/output/* 浏览器查看 ​ （f）将测试文件内容下载到本地 [kingge@hadoop101 hadoop-2.7.2]$ hadoop fs -get /user/kingge/ output/part-r-00000 ./wcoutput/ （g）删除输出结果 [kingge@hadoop101 hadoop-2.7.2]$ hdfs dfs -rmr /user/kingge/output 4.2.2 YARN上运行MapReduce 程序1）分析：​ （1）准备1台客户机 ​ （2）安装jdk ​ （3）配置环境变量 ​ （4）安装hadoop ​ （5）配置环境变量 ​ （6）配置集群yarn上运行 ​ （7）启动、测试集群增、删、查 ​ （8）在yarn上执行wordcount案例 2）执行步骤（1）配置集群 ​（a）配置yarn-env.sh ​ 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置yarn-site.xml &lt;!-- reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt; ​ （c）配置：mapred-env.sh ​ 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ （d）配置： (对mapred-site.xml.template重新命名为) mapred-site.xml [kingge@hadoop101 hadoop]$ mv mapred-site.xml.template mapred-site.xml [kingge@hadoop101 hadoop]$ vi mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; ​ （2）启动集群 （a）启动resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager （b）启动nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager ​ （3）集群操作 （a）yarn的浏览器页面查看 http://192.168.1.101:8088/cluster ​ （b）删除文件系统上的output文件 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/kingge/output ​ （c）执行mapreduce程序 ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input /user/kingge/output ​ （d）查看运行结果 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/output/* 4.2.3 配置临时文件存储路径1）停止进程 [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop namenode 2）修改hadoop.tmp.dir ​ [core-site.xml] &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 3）将/opt/module/hadoop-2.7.2路径中的logs文件夹删除掉 [kingge@hadoop101 hadoop-2.7.2]$ rm -rf logs/ 4）进入到tmp目录将tmp目录中hadoop-kingge目录删除掉 [kingge@hadoop101 tmp]$ rm -rf hadoop-kingge/ 5）格式化NameNode ​ [kingge@hadoop101 hadoop-2.7.2]$ hadoop namenode -format 6）启动所有进程 ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager ​ 7）查看/opt/module/hadoop-2.7.2/data/tmp这个目录下的内容。 4.2.4 配置历史服务器​ 1）配置mapred-site.xml [kingge@hadoop101 hadoop]$ vi mapred-site.xml &lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt; ​ 2）查看启动历史服务器文件目录： [kingge@hadoop101 hadoop-2.7.2]$ ls sbin/ | grep mr mr-jobhistory-daemon.sh ​ 3）启动历史服务器 [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver ​ 4）查看历史服务器是否启动 ​ [kingge@hadoop101 hadoop-2.7.2]$ jps ​ 5）查看jobhistory http://192.168.1.101:19888/jobhistory 4.2.5 配置日志的聚集日志聚集概念：应用运行完成以后，将日志信息上传到HDFS系统上。 开启日志聚集功能步骤： （1）配置yarn-site.xml [kingge@hadoop101 hadoop]$ vi yarn-site.xml &lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; （2）关闭nodemanager 、resourcemanager和historymanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh stop historyserver （3）启动nodemanager 、resourcemanager和historymanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver （4）删除hdfs上已经存在的hdfs文件 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/kingge/output （5）执行wordcount程序 [kingge@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input /user/kingge/output （6）查看日志 http://192.168.1.101:19888/jobhistory 4.2.6 配置文件说明Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。 （1）默认配置文件：存放在hadoop相应的jar包中 [core-default.xml] ​ hadoop-common-2.7.2.jar/ core-default.xml ​ [hdfs-default.xml] hadoop-hdfs-2.7.2.jar/ hdfs-default.xml ​ [yarn-default.xml] hadoop-yarn-common-2.7.2.jar/ yarn-default.xml ​ [core-default.xml] hadoop-mapreduce-client-core-2.7.2.jar/ core-default.xml ​ （2）自定义配置文件：存放在$HADOOP_HOME/etc/hadoop ​ core-site.xml ​ hdfs-site.xml ​ yarn-site.xml ​ mapred-site.xml 4.2.7 总结相比于本地运行模式，伪分布式模式支持互联网操作，不过集群的副本是1（不配置的话默认是3，详情查看hdfs-default.xml的dfs.replication属性）。 4.3 完全分布式部署Hadoop分析： ​ 1）准备3台客户机（关闭防火墙、静态ip、主机名称） ​ 2）安装jdk ​ 3）配置环境变量 ​ 4）安装hadoop ​ 5）配置环境变量 ​ 6）安装ssh ​ 7）配置集群 ​ 8）启动测试集群 4.3.1 虚拟机准备详见3.2-3.3章。 Hadoop运行环境搭建3.2-3.3 4.3.2 主机名设置Hadoop运行环境搭建3.4 4.3.3 scp1）scp可以实现服务器与服务器之间的数据拷贝。 操作一：hadoop101 主动推送数据到hadoop102 操作二：hadoop102主动从hadoop101获取数据到本地 操作三：hadoop101 控制将hadoop102的数据拷贝到hadoop103 ​ 接收方一般使用root**用户接受，因为有些文件夹的权限只有root用户才有，为保证传输成功，双方最好都切换到root用户** 2）案例实操 （1）将hadoop101中/opt/module和/opt/software文件拷贝到hadoop102、hadoop103和hadoop104上。 [root@hadoop101 /]# scp -r /opt/module/ root@hadoop102:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop102:/opt [root@hadoop101 /]# scp -r /opt/module/ root@hadoop103:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop103:/opt [root@hadoop101 /]# scp -r /opt/module/ root@hadoop104:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop105:/opt （2）将hadoop101服务器上的/etc/profile文件拷贝到hadoop102上。 [root@hadoop102 opt]# scp root@hadoop101:/etc/profile /etc/profile 例子1 [root@hadoop102 opt]# scp -r root@192.168.1.101:/opt/module/ /opt/–例子二 ​ （3）实现两台远程机器之间的文件传输（hadoop103主机文件拷贝到hadoop104主机上） ​ [kingge@hadoop102 test]$ scp kingge@hadoop103:/opt/test/haha kingge@hadoop104:/opt/test/ 注意：如果传递环境变量配置文件后需要source /etc/profile 一下，让其生效。同时可能需要修改一下文件的权限或者文件所属（chmod chown） 4.3.4 SSH无密码登录 针对执行ssh命令的 无密码操作 1）配置ssh （1）基本语法 ssh 另一台电脑的ip地址 （2）ssh连接时出现Host key verification failed的解决方法 [root@hadoop102 opt]# ssh 192.168.1.103 The authenticity of host ‘192.168.1.103 (192.168.1.103)’ can’t be established. RSA key fingerprint is cf:1e:de:d7:d0:4c:2d:98:60:b4:fd:ae:b1:2d:ad:06. Are you sure you want to continue connecting (yes/no)? Host key verification failed. （3）解决方案如下：直接输入yes 2）无密钥配置 （1）进入到我的home目录 ​ [kingge@hadoop102 opt]$ cd ~/.ssh （2）生成公钥和私钥： [kingge@hadoop102 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） （3）将公钥拷贝到要免密登录的目标机器上 [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop102 （给自己授权免密登录） [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop103 [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop104 查看103或者104的~/.ssh，发现多了authorized_keys 个文件 需求： A服务器需要访问B服务器，不需要输入密码 3）.ssh文件夹下的文件功能解释 ​ （1）~/.ssh/known_hosts ：记录ssh访问过计算机的公钥(public key) ​ （2）id_rsa ：生成的私钥 ​ （3）id_rsa.pub ：生成的公钥 ​ （4）authorized_keys ：存放授权过得无秘登录服务器公钥 注意： 如果你授权的免密登录用户，被切换了，那么还是需要输入密码才能够登录。 例子：hadoop100服务器的kingge用户生成了密匙，然后发给你了hadoop101，那么100服务器就可以免密登录101服务器，但是假设100服务器su root（切换为了root用户），那么当执行ssh hadoop101 操作时，就需要输入101服务器密码，而不能免密登录，所以要想在root用户下也能免密登录101服务器，就需要在root用户下重新走一遍免密登录流程 4.3.5 rsyncrsync远程同步工具，主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync**和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp**是把所有文件都复制过去。 （1）查看rsync使用说明 man rsync | more ​ （2）基本语法 rsync -rvl $pdir/$fname $user@hadoop$host:$pdir ​ 命令 命令参数 要拷贝的文件路径/名称 目的用户@主机:目的路径 ​ 选项 -r 递归 -v 显示复制过程 -l 拷贝符号连接 ​ （3）案例实操 ​ 把本机/opt/tmp目录同步到hadoop103服务器的root用户下的/opt/tmp目录 [kingge@hadoop102 opt]$ rsync -rvl /opt/tmp root@hadoop103:/opt/ 4.3.6 编写集群分发脚本xsync 场景：分布式系统中假设有6900**台服务器，那么假设我们需要同步配置信息，我们不可能一台台的执行scp/rsync 命令，效率极低，那怎么办呢？** 1）需求分析：循环复制文件到所有节点的相同目录下。 ​ （1）原始拷贝： rsync -rvl /opt/module root@hadoop103:/opt/ ​ （2）期望脚本： xsync 要同步的文件名称 ​ （3）在/usr/local/bin这个目录下存放的脚本，可以在系统任何地方直接执行。（就是在执行xsync命令时可以不用输入/usr/local/bin这样前缀） 2）案例实操： （1）在/usr/local/bin目录下创建xsync文件，文件内容如下： [root@hadoop102 bin]# touch xsync [root@hadoop102 bin]# vi xsync #!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do #echo $pdir/$fname $user@hadoop$host:$pdir echo --------------- hadoop$host ---------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone （2）修改脚本 xsync 具有执行权限 [root@hadoop102 bin]# chmod 777 xsync [root@hadoop102 bin]# chown kingge:kingge -R xsync （3）调用脚本形式：xsync 文件名称 [kingge@hadoop102 opt]$ xsync tmp/ 4.3.7 编写集群操作脚本xcall1）需求分析：在所有主机上同时执行相同的命令 xcall +命令 2）具体实现 （1）在/usr/local/bin目录下创建xcall文件，文件内容如下： [root@hadoop102 bin]# touch xcall [root@hadoop102 bin]# vi xcall #!/bin/bashpcount=$#if((pcount==0));then echo no args; exit;fiecho -------------localhost----------$@for((host=101; host&lt;=108; host++)); do echo ----------hadoop$host--------- ssh hadoop$host $@done （2）修改脚本xcall具有执行权限 ​ [root@hadoop102 bin]# chmod 777 xcall [root@hadoop102 bin]# chown kingge:kingge xcall （3）调用脚本形式： xcall 操作命令 [root@hadoop102 ~]# xcall rm -rf /opt/tmp/ 4.3.8 配置集群1）集群部署规划 hadoop102 hadoop103 hadoop104 HDFS NameNode DataNode DataNode SecondaryNameNode DataNode YARN NodeManager ResourceManager NodeManager NodeManager 集群规划的原则：NameNode/SecondaryNameNode/ ResourceManager必须要单独占据一个服务器(或者这三个不能在同一个节点上运行)，因为他是相当于目录，请求他的次数最大，所以不能跟其他插件部署在一起，不能跟NameNode抢占资源，因为他不能挂掉。Datanode就没有这些限制，挂掉也无所谓。 但是下面的例子中NameNode和DataNode都部署在hadoop102这个节点，因为我们是属于测试搭建环境下，所以无所谓，但是生产环境下必须按照规则 2）配置文件​ （1）core-site.xml [kingge@hadoop102 hadoop]$ vi core-site.xml &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; ​ （2）Hdfs ​ 2.1 hadoop-env.sh [kingge@hadoop102 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ 2.2 hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;# 如果不配置。默认是跟namenode同个位置 &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ​ 2.3 Slaves ​ 配置文件里面不能存在多余的空格或者换行 [kingge@hadoop102 hadoop]$ vi slaves hadoop102 hadoop103 hadoop104 ​ （3）yarn ​ yarn-env.sh [kingge@hadoop102 hadoop]$ vi yarn-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ yarn-site.xml ​ [kingge@hadoop102 hadoop]$ vi yarn-site.xml &lt;configuration&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ​ （4）mapreduce ​ mapred-env.sh [kingge@hadoop102 hadoop]$ vi mapred-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ mapred-site.xml [kingge@hadoop102 hadoop]$ vi mapred-site.xml &lt;configuration&gt; &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）在集群上分发以上所有文件[kingge@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/slaves 4）查看文件分发情况 [kingge@hadoop102 hadoop]$ xcall cat /opt/module/hadoop-2.7.2/etc/hadoop/slaves 4.3.9 集群启动及测试1）启动集群 ​ 清空之前启动namenode的数据 ​ Rm –rf data/ log/ ​ Data就是在core-site.xml中配置的文件存储目录，log就是hadoop的log目录。 ​ （0）如果集群是第一次启动，需要格式化namenode ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -format （1）启动HDFS： [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh –这一步跟我们之前的单个启动不一样（sbin/hadoop-daemon.sh start namenode**），这个操作是启动整个集群的namenode**和datanode—那么就需要配置ssh 无密码登录 [kingge@hadoop102 hadoop-2.7.2]$ jps 4166 NameNode 4482 Jps 4263 DataNode [kingge@hadoop103 hadoop-2.7.2]$ jps 3218 DataNode 3288 Jps [kingge@hadoop104 hadoop-2.7.2]$ jps 3221 DataNode 3283 SecondaryNameNode 3364 Jps （2）启动yarn （启动namemanager 和 resourceManager） [kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh 注意：Namenode和ResourceManger如果不是同一台机器，不能在NameNode上启动 yarn，应该在ResouceManager所在的机器上启动yarn。 因为我们在这个集群中ResourceManager是在hadoop103上面的，所以在103上启动 2）集群基本测试 （1）上传文件到集群 ​ 1.上传小文件 ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/kingge/input ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -put etc/hadoop/wc.input /user/kingge/input 上传完后 hadoop103和hadoop104 上面也会同步副本（即是，也会存在上面的/user/kingge/tep/conf 和 *-site.xml 这些东西） 查看HDFS系统的文件结构 我们可以看到他是存储在了块0，而且有三个副本101/102/103 ​ 接下来我们再上传一个大文件看看 ​ 2.上传大文件 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -put /opt/software/hadoop-2.7.2.tar.gz /user/kingge/input 为什么他这里会分为两块存储呢？因为默认块大小是128，当超过这个大小时就需要分块存储 （2）上传文件后查看文件存放在什么位置 ​ 文件存储路径 ​ [kingge@hadoop102 subdir0]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-938951106-192.168.10.107-1495462844069/current/finalized/subdir0/subdir0 ​ 查看文件内容(wc.input) [kingge@hadoop102 subdir0]$ cat blk_1073741825 hadoop kingge kingge 然后26 27 就是那个大文件，分为了两块 （3）拼接 -rw-rw-r–. 1 kingge kingge 134217728 5月 23 16:01 blk_1073741836 -rw-rw-r–. 1 kingge kingge 1048583 5月 23 16:01 blk_1073741836_1012.meta -rw-rw-r–. 1 kingge kingge 63439959 5月 23 16:01 blk_1073741837 -rw-rw-r–. 1 kingge kingge 495635 5月 23 16:01 blk_1073741837_1013.meta [kingge@hadoop102 subdir0]$ cat blk_1073741836&gt;&gt;tmp.file [kingge@hadoop102 subdir0]$ cat blk_1073741837&gt;&gt;tmp.file [kingge@hadoop102 subdir0]$ tar -zxvf tmp.file 已解压发现就是我们上传那个tar**大文件的解压版本，说明这两个文件就是存放着大文件** （4）下载 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -get /user/kingge/input/hadoop-2.7.2.tar.gz 3）性能测试集群 ​ 写海量数据 ​ 读海量数据 4.3.10 Hadoop启动停止方式1）各个服务组件逐一启动 ​ （1）分别启动hdfs组件 ​ hadoop-daemon.sh start|stop namenode|datanode|secondarynamenode ​ （2）启动yarn ​ yarn-daemon.sh start|stop resourcemanager|nodemanager 2）各个模块分开启动（配置ssh是前提）常用 ​ （1）整体启动/停止hdfs ​ start-dfs.sh ​ stop-dfs.sh ​ （2）整体启动/停止yarn ​ start-yarn.sh ​ stop-yarn.sh 3）全部启动（不建议使用） ​ start-all.sh ​ stop-all.sh 4.3.11 集群时间同步时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 配置时间同步实操： 1）时间服务器配置（必须root用户） （1）检查ntp是否安装 [root@hadoop102 桌面]# rpm -qa|grep ntp ntp-4.2.6p5-10.el6.centos.x86_64 fontpackages-filesystem-1.41-1.1.el6.noarch ntpdate-4.2.6p5-10.el6.centos.x86_64 （2）修改ntp配置文件 [root@hadoop102 桌面]# vi /etc/ntp.conf 修改内容如下 a）修改1 #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap为 restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap ​ b）修改2 注释时间服务器 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst为 #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst ​ c）添加3 自己的时间服务器 server 127.127.1.0 fudge 127.127.1.0 stratum 10 （3）修改/etc/sysconfig/ntpd 文件 [root@hadoop102 桌面]# vim /etc/sysconfig/ntpd 增加内容如下 SYNC_HWCLOCK=yes ​ （4）重新启动ntpd [root@hadoop102 桌面]# service ntpd status ntpd 已停 [root@hadoop102 桌面]# service ntpd start 正在启动 ntpd： [确定] ​ （5）执行： ​ [root@hadoop102 桌面]# chkconfig ntpd on 2）其他机器配置（必须root用户） ​ （1）在其他机器配置10分钟与时间服务器同步一次 ​ [root@hadoop103 hadoop-2.7.2]# crontab -e ​ 编写脚本 ​ /10 * /usr/sbin/ntpdate hadoop102 ​ （2）修改任意机器时间 ​ [root@hadoop103 hadoop]# date -s “2017-9-11 11:11:11” ​ （3）十分钟后查看机器是否与时间服务器同步 ​ [root@hadoop103 hadoop]# date 4.3.12 配置集群常见问题1）防火墙没关闭、或者没有启动yarn INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032 2）主机名称配置错误 3）ip地址配置错误 4）ssh没有配置好 5）root用户和kingge两个用户启动集群不统一 6）配置文件修改不细心 7）未编译源码 Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 17/05/22 15:38:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032 8）datanode不被namenode识别问题 Namenode在format初始化的时候会形成两个标识，blockPoolId和clusterId。新的datanode加入时，会获取这两个标识作为自己工作目录中的标识。 一旦namenode重新format后，namenode的身份标识已变，而datanode如果依然持有原来的id，就不会被namenode识别。 解决办法，删除datanode节点中的数据后，再次重新格式化namenode。 9）不识别主机名称 java.net.UnknownHostException: hadoop102: hadoop102 at java.net.InetAddress.getLocalHost(InetAddress.java:1475) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:146) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) 解决办法： （1）在/etc/hosts文件中添加192.168.1.102 hadoop102 ​ （2）主机名称不要起hadoop hadoop000等特殊名称 10）datanode和namenode进程同时只能工作一个。 11）执行命令 不生效，粘贴word中命令时，遇到-和长–没区分开。导致命令失效 解决办法：尽量不要粘贴word中代码。 12）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。 13）jps不生效。 原因：全局变量hadoop java没有生效，需要source /etc/profile文件。 14）8088端口连接不上 [kingge@hadoop102 桌面]$ cat /etc/hosts 注释掉如下代码 #127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 #::1 hadoop102","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(二)-运行环境搭建","slug":"hadoop大数据-二-运行环境搭建","date":"2018-02-24T02:21:59.000Z","updated":"2019-06-07T09:12:38.271Z","comments":true,"path":"2018/02/24/hadoop大数据-二-运行环境搭建/","link":"","permalink":"http://kingge.top/2018/02/24/hadoop大数据-二-运行环境搭建/","excerpt":"","text":"三 Hadoop运行环境搭建3.0 前置准备需要linux相关的知识，和安装虚拟机。所以还不了解的请看：Linux基础 3.1 虚拟机网络模式设置为NAT ​ 最后，重新启动系统。 ​ [root@hadoop101 ~]# sync ​ [root@hadoop101 ~]# reboot 3.2 克隆虚拟机1）克隆虚拟机 2）启动虚拟机 3.3 修改为静态ip1）在终端命令窗口中输入[root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules 进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址 2）修改IP地址[root@hadoop101 /]# vim /etc/sysconfig/network-scripts/ifcfg-eth0 需要修改的内容有5项： IPADDR=192.168.1.101 GATEWAY=192.168.1.2 ONBOOT=yes BOOTPROTO=static DNS1=192.168.1.2 ​ （1）修改前 ​ （2）修改后 ：wq 保存退出 3）执行[root@hadoop101 /]# service network restart 4）如果报错，reboot，重启虚拟机。​ [root@hadoop101 /]# reboot 3.4 修改主机名1）修改linux的hosts文件（1）进入Linux系统查看本机的主机名。通过hostname命令查看。 [root@hadoop100 /]# hostname hadoop100 （2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件。 [root@hadoop100~]# vi /etc/sysconfig/network 修改文件中主机名称 NETWORKING=yes NETWORKING_IPV6=no HOSTNAME= hadoop101 注意：主机名称不要有“_”下划线 （3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。 （4）保存退出。 （5）打开/etc/hosts [root@hadoop100 ~]# vim /etc/hosts 添加如下内容 192.168.1.100 hadoop100 192.168.1.101 hadoop101 192.168.1.102 hadoop102 192.168.1.103 hadoop103 192.168.1.104 hadoop104 192.168.1.105 hadoop105 192.168.1.106 hadoop106 192.168.1.107 hadoop107 192.168.1.108 hadoop108 192.168.1.109 hadoop109 192.168.1.110 hadoop110 （6）并重启设备，重启后，查看主机名，已经修改成功 2）修改window7的hosts文件(可以不改)只不过是方便于在windows服务器中，使用域名的方式访问hadopp相关的组件。 ​ （1）进入C:\\Windows\\System32\\drivers\\etc路径 ​ （2）打开hosts文件并添加如下内容 192.168.1.100 hadoop100 192.168.1.101 hadoop101 192.168.1.102 hadoop102 192.168.1.103 hadoop103 192.168.1.104 hadoop104 192.168.1.105 hadoop105 192.168.1.106 hadoop106 192.168.1.107 hadoop107 192.168.1.108 hadoop108 192.168.1.109 hadoop109 192.168.1.110 hadoop110 3.5 关闭防火墙1）查看防火墙开机启动状态 [root@hadoop101 ~]# chkconfig iptables –list 2）关闭防火墙 [root@hadoop101 ~]# chkconfig iptables off 3.6 在opt目录下创建文件1）创建kingge用户​ 在root用户里面执行如下操作 [root@hadoop101 opt]# adduser atguigu [root@hadoop101 opt]# passwd atguigu 更改用户 test 的密码 。 新的 密码： 无效的密码： 它没有包含足够的不同字符 无效的密码： 是回文 重新输入新的 密码： passwd： 所有的身份验证令牌已经成功更新。 2）设置kingge用户具有root权限修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： [root@hadoop101 kingge]# vi /etc/sudoers ## Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) ALL 修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。 3）在/opt目录下创建文件夹（1）在root用户下创建module、software文件夹 [root@hadoop101 opt]# mkdir module [root@hadoop101 opt]# mkdir software （2）修改module、software文件夹的所有者 [root@hadoop101 opt]# chown kingge:kingge module [root@hadoop101 opt]# chown kingge:kingge sofrware [root@hadoop101 opt]# ls -al 总用量 16 drwxr-xr-x. 6 root root 4096 4月 24 09:07 . dr-xr-xr-x. 23 root root 4096 4月 24 08:52 .. drwxr-xr-x. 4 kingge kingge 4096 4月 23 16:26 module drwxr-xr-x. 2 kingge kingge 4096 4月 23 16:25 software 3.7 安装jdk1）卸载现有jdk（1）查询是否安装java软件： [root@hadoop101 opt]# rpm -qa|grep java （2）如果安装的版本低于1.7，卸载该jdk： [root@hadoop101 opt]# rpm -e 软件包 2）复制文件用SecureCRT工具将jdk、Hadoop-2.7.2.tar.gz导入到opt目录下面的software文件夹下面 3）在linux系统下的opt目录中查看软件包是否导入成功。[root@hadoop101opt]# cd software/ [root@hadoop101software]# ls hadoop-2.7.2.tar.gz jdk-8u144-linux-x64.tar.gz 4）解压jdk到/opt/module目录下​ [root@hadoop101software]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/ 5）配置jdk环境变量​ （1）先获取jdk路径： [root@hadoop101 jdk1.8.0_144]# pwd /opt/module/jdk1.8.0_144 ​ （2）打开/etc/profile文件： [root@hadoop101 jdk1.8.0_144]# vi /etc/profile ​ 在profie文件末尾添加jdk路径： ​ ##JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin ​ （3）保存后退出： :wq ​ （4）让修改后的文件生效： [root@hadoop101 jdk1.8.0_144]# source /etc/profile ​ （5）重启（如果java -version可以用就不用重启）： [root@hadoop101 jdk1.8.0_144]# sync ​ [root@hadoop101 jdk1.8.0_144]# reboot 6）测试jdk安装成功[root@hadoop101 jdk1.8.0_144]# java -version java version “1.8.0_144” 3.8 安装Hadoop 这里使用的是已经编译过后的hadoop源码，官网下载的是非编译过后的，需要编译。如何编译参见。 Hadoop编译源码 1）进入到Hadoop安装包路径下： [root@hadoop101 ~]# cd /opt/software/ 2）解压安装文件到/opt/module下面 [root@hadoop101 software]# tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/ 3）查看是否解压成功 [root@hadoop101 software]# ls /opt/module/ hadoop-2.7.2 4）在/opt/module/hadoop-2.7.2/etc/hadoop路径下配置hadoop-env.sh （1）Linux系统中获取jdk的安装路径： [root@hadoop101 jdk1.8.0_144]# echo $JAVA_HOME /opt/module/jdk1.8.0_144 （2）修改hadoop-env.sh文件中JAVA_HOME 路径： [root@hadoop101 hadoop]# vi hadoop-env.sh 修改JAVA_HOME如下 export JAVA_HOME=/opt/module/jdk1.8.0_144 5）将hadoop添加到环境变量 （1）获取hadoop安装路径：[root@ hadoop101 hadoop-2.7.2]# pwd/opt/module/hadoop-2.7.2 （2）打开/etc/profile文件：[root@ hadoop101 hadoop-2.7.2]# vi /etc/profile 在profie文件末尾添加jdk路径：（shitf+g）\\##HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin （3）保存后退出：:wq （4）让修改后的文件生效：[root@ hadoop101 hadoop-2.7.2]# source /etc/profile（5）重启(如果hadoop命令不能用再重启)： [root@ hadoop101 hadoop-2.7.2]# sync [root@ hadoop101 hadoop-2.7.2]# reboot 6）修改/opt目录下的所有文件所有者为kingge ​ [root@hadoop101 opt]# chown kingge:kingge -R /opt/ 7）切换到kingge用户 ​ [root@hadoop101 opt]# su kingge","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(一)-理论知识了解","slug":"hadoop大数据-一-理论知识了解","date":"2018-02-20T16:31:59.000Z","updated":"2019-06-07T08:31:03.225Z","comments":true,"path":"2018/02/21/hadoop大数据-一-理论知识了解/","link":"","permalink":"http://kingge.top/2018/02/21/hadoop大数据-一-理论知识了解/","excerpt":"","text":"前言首先本人要先声明的是，这一系列的大数据总结，只是对于整个大数据生态的一个稍微深入的总结。并非是非常深入的，但是能够满足大部分人的需求。 一 大数据概念大数据的概念： 指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 这个概念的解释来源于百度百科，这样的解释太过空洞，那么就用更浅显易懂的解释是： 解决海量数据的存储和海量数据的分析计算 1.2 大数据的特点 图片来源于网上 1.3 大数据应用场景 4.给顾客推荐访问过的商品，我们使用淘宝之类的，当我们浏览某个商品之后，下次进来时，他会默认给你推荐你上次浏览过的商品。 第八：人工智能，目前最火的的风口 二 Hadoop框架2.1 Hadoop是什么1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构 2）主要解决，海量数据的存储和海量数据的分析计算问题（很明显只是一句废话，哈哈哈）。 3）广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈 2.2 Hadoop发展历史1）Lucene–Doug Cutting开创的开源软件，用java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎 2）2001年年底成为apache基金会的一个子项目 3）对于大数量的场景，Lucene面对与Google同样的困难 4）学习和模仿Google解决这些问题的办法 ：微型版Nutch 5）可以说Google是hadoop的思想之源(Google在大数据方面的三篇论文) ​ GFS —&gt;HDFS ​ Map-Reduce —&gt;MR ​ BigTable —&gt;Hbase 6）2003-2004年，Google公开了部分GFS和Mapreduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和Mapreduce机制，使Nutch性能飙升 7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。2006 年 3 月份，Map-Reduce和Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中 8）名字来源于Doug Cutting儿子的玩具大象 9）Hadoop就此诞生并迅速发展，标志这云计算时代来临 2.3 Hadoop三大发行版本Hadoop 三大发行版本: Apache、Cloudera、Hortonworks。 Apache版本最原始（最基础）的版本，对于入门学习最好。 Cloudera在大型互联网企业中用的较多。（因为他解决了hadoop各个版本和其他框架的兼容问题） Hortonworks文档较好。 1）Apache Hadoop 官网地址：http://hadoop.apache.org/releases.html 下载地址：https://archive.apache.org/dist/hadoop/common/ 2）Cloudera Hadoop 官网地址：https://www.cloudera.com/downloads/cdh/5-10-0.html 下载地址：http://archive-primary.cloudera.com/cdh5/cdh/5/ （1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。 （2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support （3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。 （4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。 （5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。 3）Hortonworks Hadoop 官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform （1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。 （2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。 （3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。 （4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。 （5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 （6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的microsoft Windows平台上本地运行。定价以集群为基础，每10个节点每年为12500美元。 2.4 Hadoop的优势1）高可靠性：因为Hadoop假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理。2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度（根据文件快开启相应数量的map任务处理，reduce的并行数量是可控的）。4）高容错性：自动保存多份副本数据，并且能够自动将失败的任务重新分配。 2.5 Hadoop组成2.5.0 四个组成1）Hadoop HDFS：一个高可靠、高吞吐量的分布式文件系统。2）Hadoop MapReduce：一个分布式的离线并行计算框架。3）Hadoop YARN：作业调度与集群资源管理的框架。4）Hadoop Common：支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作）。 2.5.1 HDFS 组成： namenode：存储文件元数据。例如文件名称，文件目录结构，文件属性（生成时间、副本数，文件权限），以及每个文件的快列表和快所在的datanode。hdfs的文件目录维护中心。datanode：真正存储文件的单位，也就是统称的块。secondary namenode：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS的元数据的快照。帮助namenode合并镜像文件和操作日志，合并完成后同步给namenode，减少namenode的处理任务的压力。他不能够替代namenode，只能够做备份（方便namenode意外挂掉后，能够恢复数据） 2.5.2 YARN1）ResourceManager(rm)：处理客户端请求、启动/监控ApplicationMaster、监控NodeManager、资源分配与调度； 2）NodeManager(nm)：单个节点上的资源管理、处理来自ResourceManager的命令、处理来自ApplicationMaster的命令； 3）ApplicationMaster：数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错。 4）Container：对任务运行环境的抽象，封装了CPU、内存等多维资源以及环境变量、启动命令等任务运行相关的信息。 2.5.3 MapReduce主要是获取HDFS存储的数据，通过拆分块的形式进行数据获取分析处理，最后输出自己想要的结果。 MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据 2）Reduce阶段对Map结果进行汇总 上图简单的阐明了map和reduce的两个过程或者作用，虽然不够严谨，但是足以提供一个大概的认知，map过程是一个蔬菜到制成食物前的准备工作，reduce将准备好的材料合并进而制作出食物的过程。 2.6 大数据整个结构 图片来源于网上 图中涉及的技术名词解释如下：1）Sqoop：sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 一般是将关系型数据库的数据导入到hive或者HDFS中，目的就是为了分析这些数据，因为我们知道关系型数据库当数据量变得很大时，通过sql去统计查询，那么就会卡死。就是因为整个架构他们本省就是不支持的，他们通常进行的是扫表操作。2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息（3）支持通过Kafka服务器和消费机集群来分区消息。（4）支持Hadoop并行数据加载。4）Storm：Storm为分布式实时计算提供了一组通用原语，可被用于“流处理”之中，实时处理消息并更新数据库。这是管理队列及工作者集群的另一种方式。 Storm也可被用于“连续计算”（continuous computation），对数据流做连续查询，在计算时就将结果以流的形式输出给用户。5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。Oozie协调作业就是通过时间（频率）和有效数据触发当前的Oozie工作流程。7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。8）Hive：hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。11）Mahout:Apache Mahout是个可扩展的机器学习和数据挖掘库，当前Mahout支持主要的4个用例：推荐挖掘：搜集用户动作并以此给用户推荐可能喜欢的事物。聚集：收集文件并进行相关文件分组。分类：从现有的分类文档中学习，寻找文档中的相似特征，并为无标签的文档进行正确的归类。频繁项集挖掘：将一组项分组，并识别哪些个别项会经常一起出现。12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 好了到这里我们已经了解了hadoop整个生态相关的概念和组成，以及架构。那么接下来让我们进行hadoop环境的搭建。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"关于2018年的计划","slug":"关于2018年的计划","date":"2018-02-18T16:00:00.000Z","updated":"2019-06-04T16:28:14.626Z","comments":true,"path":"2018/02/19/关于2018年的计划/","link":"","permalink":"http://kingge.top/2018/02/19/关于2018年的计划/","excerpt":"","text":"因为公司业务的转移和相关产品的开发，再加上大数据的火热，所以本人决定更新一些hadoop生态链相关的文章。所以敬请期待吧，哈哈哈哈哈。 Comming Soon！！！","categories":[{"name":"新年计划","slug":"新年计划","permalink":"http://kingge.top/categories/新年计划/"}],"tags":[{"name":"新年计划","slug":"新年计划","permalink":"http://kingge.top/tags/新年计划/"}]},{"title":"zookeeper知识学习","slug":"zookeeper知识学习","date":"2018-02-02T11:12:44.000Z","updated":"2019-06-02T13:24:38.033Z","comments":true,"path":"2018/02/02/zookeeper知识学习/","link":"","permalink":"http://kingge.top/2018/02/02/zookeeper知识学习/","excerpt":"","text":"引言 最近公司开发saas模式的企业应用软件服务，所以下面是我个人使用和总结（掺杂了大数据相关的总结） 一、正文0.1 下载1）官网首页： https://zookeeper.apache.org/ 2）下载截图 1.1 概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 1.2 模型构造和特点 1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。 2）Leader负责进行投票的发起和决议，更新系统状态。 3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票。 4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。（例如现在zookeeper集群现在有四台，那么挂掉两台后就不能正常工作了。假设初始时只有三台，那么最多也是挂掉两台后就不能工作了。也就是说，部署三台和部署四台的效用其实是一样的，所以一般都是部署奇数台zookeeper，节省资源） 5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。 6）更新请求顺序进行（全局数据一致性的提现），来自同一个client的更新请求按其发送顺序依次执行。 7）数据更新原子性，一次数据更新要么成功，要么失败。 8）实时性，在一定时间范围内（数据一致性的更新会有延迟），client能读到最新数据。 9）一次性监听（缺点）-这个缺点我们在后面可以用代码解决。 1.3 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。每个节点的存储的数据量，也决定了他的应用场景并不是存储大量的数据。下面的1.4章节会阐述到他的作用-应用场景 1.4 应用场景如果某个需求：当某个节点发生变化，通知其他关注这个节点的其他节点。那么就可以使用**zookeeper** 项目中常用到分布式锁和配置管理 1.4.1 统一命名服务 意思就是：我们通常使用域名来访问某个网站，但是域名对应的ip我们是不需要关注的为了系统的容错性，我们访问Baidu的这个请求是会随机寻找一个正常运行的服务器去处理。那么我们就可以使用zookeeper来进行管理。管理可以访问到Baidu这个网址的ip列表。客户端每次请求百度时，只需要去请求这个zookeeper获取可访问的ip即可。实现动态ip的上下线管理 1.4.2 统一配置管理 1.4.3 统一集群管理集群管理结构图如下所示。 1.4.4 服务器节点动态上下线 1.4.5 软负载均衡 控制某个服务器的访问数，达到资源合理分配 1.4.6 分布式锁（主要原理是同一路径下的节点名称不能重复，不能重复创建）有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。（在下面的3.2章节会讲到zookeeper顺序节点的相关内容） 好的博客： https://my.oschina.net/aidelingyu/blog/1600979 https://www.jianshu.com/p/5d12a01018e1 1.4.7 队列管理两种类型的队列： 1、 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 2、队列按照 FIFO 方式进行入队和出队操作。 第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。 二 Zookeeper安装2.1 本地模式安装部署1）安装前准备： （1）安装jdk （2）通过cshell工具拷贝zookeeper到linux系统下 （3）修改tar包权限 [kingge@hadoop102 software]$ chmod u+x zookeeper-3.4.10.tar.gz （4）解压到指定目录 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 2）配置修改 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； ​ 进入zoo.cfg文件：vim zoo.cfg ​ 修改dataDir路径为 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹 ​ [kingge@hadoop102 zookeeper-3.4.10]$ mkdir zkData 3）操作zookeeper （1）启动zookeeper [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start （2）查看进程是否启动 ​ [kingge@hadoop102 zookeeper-3.4.10]$ jps 4020 Jps 4001 QuorumPeerMain （3）查看状态： [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: standalone （4）启动客户端： [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkCli.sh （5）退出客户端： [zk: localhost:2181(CONNECTED) 0] quit （6）停止zookeeper [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh stop 2.2 配置参数解读解读zoo.cfg文件中参数含义 1）tickTime=2000：通信心跳数，Zookeeper服务器心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) 2）initLimit=10：Leader和Follower初始通信时限（10* tickTime – 也就是不要超过二十秒） 集群中的follower跟随者服务器与leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 投票选举新leader的初始化时间 Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 Leader允许Follower在initLimit时间内完成这个工作。 3）syncLimit=5：Leader和Follower同步通信时限（5* tickTime – 也就是不要超过十秒） 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。（默认超过十秒，leader就认为follwer已经碟机） 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 4）dataDir：数据文件目录+数据持久化路径 保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。 5）clientPort=2181：客户端连接端口 监听客户端连接的端口 2.3 分布式模式下的安装详见第四章节 三 Zookeeper内部原理3.1 选举机制1）半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。所以zookeeper**适合装在奇数台机器上**。 2）Zookeeper虽然在配置文件中并没有指定master和slave。但是，zookeeper工作时，是有一个节点为leader，其他则为follower，Leader是通过内部的选举机制临时产生的。 3）以一个简单的例子来说明整个选举的过程。 假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。 （1）服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是LOOKING状态。 （2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3 5/2=2.5 向上取整3)，所以服务器1、2还是继续保持LOOKING状态。 （3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的leader。 （4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。 （5）服务器5启动，同4一样当小弟。 3.2 节点类型1）Znode有两种类型： 短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 2）Znode有四种形式的目录节点（默认是persistent ） （1）持久化目录节点（PERSISTENT） ​ 客户端与zookeeper断开连接后，该节点依旧存在。 （2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） ​ 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。（保证创建的节点名称不会重复） （3）临时目录节点（EPHEMERAL） 客户端与zookeeper断开连接后，该节点被删除。 （4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 3）创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 4）在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序（应用场景1.4.6 分布式锁 第二种方式） 3.3 stat结构体1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid 每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。 事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。 2）ctime - znode被创建的毫秒数(从1970年开始) 3）mzxid - znode最后更新的zxid 4）mtime - znode最后修改的毫秒数(从1970年开始) 5）pZxid-znode最后更新的子节点zxid 6）cversion - znode子节点变化号，znode子节点修改次数 7）dataversion - znode数据变化号 8）aclVersion - znode访问控制列表的变化号 9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。 10）dataLength- znode的数据长度 11）numChildren - znode子节点数量 3.4 监听器原理 3.5 写数据流程 1.收到请求，先找到leader节点 2.广播请求给其他follower 3.哥哥follower写入数据，写入成功后，通知leader写入成功。（半数以上follower写入成功即为写入数据成功） 4.leader通知最初收到客户请求的server，数据写入成功，该server通知客户端写入数据成功 四 Zookeeper实战4.1 分布式安装部署0）集群规划 在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 1）解压安装 （1）解压zookeeper安装包到/opt/module/目录下 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ （2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData ​ mkdir -p zkData （3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg ​ mv zoo_sample.cfg zoo.cfg 2）配置zoo.cfg文件 ​ （1）具体配置 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 增加如下配置 ​ #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 （2）配置参数解读 Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器；（必须唯一） B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 3）集群操作 （1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件 ​ touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 （2）编辑myid文件 ​ vi myid ​ 在文件中添加与server对应的编号：如2 （3）拷贝配置好的zookeeper到其他机器上（可以用shell脚本进行分发数据） ​ scp -r zookeeper-3.4.10/ root@hadoop103.kingge.com:/opt/app/ ​ scp -r zookeeper-3.4.10/ root@hadoop104.kingge.com:/opt/app/ ​ 并分别修改myid文件中内容为3、4 （4）分别启动zookeeper ​ [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start （5）查看状态 [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower 分析：当第二个zookeeper启动时，因为2 &gt; 3/2=1.5，所以他被投票为了Leader，所以其他的节点就是follwer 4.2 客户端命令行操作 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建 -s 含有序列 -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 1）启动客户端（随便连接那个zookeeper都可以，因为他们内容都是一样的，下面连接的是103服务器） [kingge@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh 2）显示所有操作命令 [zk: localhost:2181(CONNECTED) 1] help 3）查看当前znode中所包含的内容 [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper] 4）查看当前节点数据并能看到更新次数等数据 [zk: localhost:2181(CONNECTED) 1] ls2 / [zookeeper] cZxid = 0x0 ctime = Thu Jan 01 08:00:00 CST 1970 mZxid = 0x0 mtime = Thu Jan 01 08:00:00 CST 1970 pZxid = 0x0 cversion = -1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 5）创建普通节点（注意创建节点时需要写入一些数据，否则创建不成功-例如create /app1 这样的话创建是没有效果的） [zk: localhost:2181(CONNECTED) 2] create /app1 “hello app1” Created /app1 [zk: localhost:2181(CONNECTED) 4] create /app1/server101 “192.168.1.101” Created /app1/server101 1.不支持递归创建节点，比如你要创建/app1/a,如果app1不存在，你就不能创建a( KeeperException.NoNode)。2.不可以再ephemeral类型的节点下创建子节点(KeeperException.NoChildrenForEphemerals)。（因为他本身是临时节点）3.如果指定的节点已经存在，会触发KeeperException.NodeExists 异常,当然了对于sequential类型的，不会抛出这个异常。（有编号类型的节点名称会自动递增）4.数据内容不能超过1M,否则将抛出KeeperException异常。 6）获得节点的值 [zk: localhost:2181(CONNECTED) 6] get /app1 hello app1 cZxid = 0x20000000a ctime = Mon Jul 17 16:08:35 CST 2017 mZxid = 0x20000000a mtime = Mon Jul 17 16:08:35 CST 2017 pZxid = 0x20000000b cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 10 numChildren = 1 [zk: localhost:2181(CONNECTED) 8] get /app1/server101 192.168.1.101 cZxid = 0x20000000b ctime = Mon Jul 17 16:11:04 CST 2017 mZxid = 0x20000000b mtime = Mon Jul 17 16:11:04 CST 2017 pZxid = 0x20000000b cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 13 numChildren = 0 7）创建短暂节点 [zk: localhost:2181(CONNECTED) 9] create -e /app-emphemeral 8888 （1）在当前客户端是能查看到的 [zk: localhost:2181(CONNECTED) 10] ls / [app1, app-emphemeral, zookeeper] （2）退出当前客户端然后再重启客户端 ​ [zk: localhost:2181(CONNECTED) 12] quit [kingge@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh （3）再次查看根目录下短暂节点已经删除 ​ [zk: localhost:2181(CONNECTED) 0] ls / [app1, zookeeper] 8）创建带序号的节点 ​ （1）先创建一个普通的根节点app2 ​ [zk: localhost:2181(CONNECTED) 11] create /app2 “app2” ​ （2）创建带序号的节点 ​ [zk: localhost:2181(CONNECTED) 13] create -s /app2/aa 888 Created /app2/aa0000000000 [zk: localhost:2181(CONNECTED) 14] create -s /app2/bb 888 Created /app2/bb0000000001 [zk: localhost:2181(CONNECTED) 15] create -s /app2/cc 888 Created /app2/cc0000000002 如果原节点下有1个节点，则再排序时从1开始，以此类推。 [zk: localhost:2181(CONNECTED) 16] create -s /app1/aa 888 Created /app1/aa0000000001 9）修改节点数据值 [zk: localhost:2181(CONNECTED) 2] set /app1 999 10）节点的值变化监听（一次性触发器）（Watch**的通知事件是从服务器发送给客户端的，是异步的**） ​ （1）在104主机上注册监听/app1节点数据变化（） 需要注意的是，注册一次监听，只能够响应一次，如果/app1节点的数据修改了两次，那么只显示第一次监听的信息，第二次不会有任何响应，想要得到响应，需要再次监听 [zk: localhost:2181(CONNECTED) 26] get /app1 watch ​ （2）在103主机上修改/app1节点的数据 [zk: localhost:2181(CONNECTED) 5] set /app1 777 ​ （3）观察104主机收到数据变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1 11）节点的子节点变化监听（路径变化） ​ （1）在104主机上注册监听/app1节点的子节点变化 [zk: localhost:2181(CONNECTED) 1] ls /app1 watch [aa0000000001, server101] ​ （2）在103主机/app1节点上创建子节点 [zk: localhost:2181(CONNECTED) 6] create /app1/bb 666 Created /app1/bb ​ （3）观察104主机收到子节点变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 12）删除节点 [zk: localhost:2181(CONNECTED) 4] delete /app1/bb 13）递归删除节点 [zk: localhost:2181(CONNECTED) 7] rmr /app2 14）查看节点状态 [zk: localhost:2181(CONNECTED) 12] stat /app1 cZxid = 0x20000000a ctime = Mon Jul 17 16:08:35 CST 2017 mZxid = 0x200000018 mtime = Mon Jul 17 16:54:38 CST 2017 pZxid = 0x20000001c cversion = 4 dataVersion = 2 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 3 numChildren = 2 15）exists 节点 这个函数很特殊，因为他可以监听一个尚未存在的节点，这是getData，getChildren不能做到的。exists可以监听一个节点的生命周期：从无到有，节点数据的变化，从有到无。 在传递给exists的watcher里，当path指定的节点被成功创建后，watcher会收到NodeCreated事件通知。当path所指定的节点的数据内容发送了改变后，wather会受到NodeDataChanged事件通知。 这里最需要注意的就是，exists可以监听一个未存在的节点，这是他与getData，getChildren本质的区别。 注意看上面的代码，其实我们已经实现了多次监听，解决了zookeeper单次监听的缺点。关键代码，我们在监听器里面，又再次声明了一次监听---zkClient.exists(&quot;eclipse&quot;,true) 16） getData 16） getChildren 4.3 API应用4.3.1 Eclipse环境搭建1）创建一个工程 2）解压zookeeper-3.4.10.tar.gz文件 3）拷贝zookeeper-3.4.10.jar、jline-0.9.94.jar、log4j-1.2.16.jar、netty-3.10.5.Final.jar、slf4j-api-1.6.1.jar、slf4j-log4j12-1.6.1.jar到工程的lib目录。并build一下，导入工程。 4）拷贝log4j.properties文件到项目根目录 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 4.3.2 创建ZooKeeper客户端private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void init() throws Exception &#123;//创建zookeeper连接的时候同时注册一个全局的默认的事件监听器 – // event.getType() 永远为null默认监听到None事件// //默认监听也可以使用register方法注册 //zkClient.register(watcherDefault); zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（用户的业务逻辑） System.out.println(event.getType() + &quot;--&quot; + event.getPath()); // 再次启动监听 - 解决zookeeper单次监听的缺点 try &#123; zkClient.getChildren(&quot;/&quot;, true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125;这里的watcher是该客户端总的监听方法，任何操作都会执行，而且是可以多次执行，并非单次。 4.3.3 创建子节点// 创建子节点@Testpublic void create() throws Exception &#123; // 数据的增删改查 // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型 String nodeCreated = zkClient.create(&quot;/eclipse&quot;, &quot;hello zk&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT);&#125; 4.3.4 获取子节点并监听// 获取子节点 @Test public void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren(&quot;/&quot;, true); for (String child : children) &#123; System.out.println(child); &#125; // 延时阻塞 Thread.sleep(Long.MAX_VALUE); &#125; 4.3.5 判断znode是否存在// 判断znode是否存在 @Test public void exist() throws Exception &#123; Stat stat = zkClient.exists(&quot;/eclipse&quot;, false); System.out.println(stat == null ? &quot;not exist&quot; : &quot;exist&quot;); &#125; 4.3.6 事件类型对照表 本表总结：exits和getData设置数据监视，而getChildren设置子节点监视 4.3.7 实现永久监听（伪）我们知道zookeeper的监听是一次性监听（on-time-trriger） 详情可查看 4.3.2代码 和 4.2 的15）exists 节点 4.4 案例总结1）需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线 2）需求分析 3）具体实现： （0）现在集群上创建/servers节点 [zk: localhost:2181(CONNECTED) 10] create /servers “servers” Created /servers （1）服务器端代码 package com.kingge.zkcase;import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.ZooDefs.Ids;public class DistributeServer &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException&#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); &#125; // 注册服务器 public void registServer(String hostname) throws Exception&#123; String create = zk.create(parentNode + \"/server\", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +\" is noline \"+ create); &#125; // 业务功能 public void business(String hostname) throws Exception&#123; System.out.println(hostname+\" is working ...\"); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 利用zk连接注册服务器信息 server.registServer(args[0]); // 启动业务功能 server.business(args[0]); &#125;&#125; （2）客户端代码 package com.kingge.zkcase;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = &quot;/servers&quot;; private volatile ArrayList&lt;String&gt; serversList = new ArrayList&lt;&gt;(); // 创建到zk的客户端连接 public void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 再次启动监听 try &#123; getServerList(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // public void getServerList() throws Exception &#123; // 获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); for (String child : children) &#123; byte[] data = zk.getData(parentNode + &quot;/&quot; + child, false, null); servers.add(new String(data)); &#125; // 把servers赋值给成员serverList，已提供给各业务线程使用 serversList = servers; System.out.println(serversList); &#125; // 业务功能 public void business() throws Exception &#123; System.out.println(&quot;client is working ...&quot;);Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 业务进程启动 client.business(); &#125;&#125; 4.5 zookeeper核心原理（事件） https://blog.csdn.net/yinwenjie/article/details/47685077 五 好的总结网站\\1. https://blog.csdn.net/liu857279611/article/details/70495413 \\2. https://www.jianshu.com/p/a1d7826073e6 \\3. https://blog.csdn.net/yinwenjie/article/details/47685077 \\4. https://www.jianshu.com/p/5d12a01018e1","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://kingge.top/categories/zookeeper/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"hadoop，linux","slug":"hadoop，linux","permalink":"http://kingge.top/tags/hadoop，linux/"}]},{"title":"聊聊分布式事务，再说说解决方案-cap","slug":"聊聊分布式事务，再说说解决方案-cap","date":"2017-10-18T09:57:58.000Z","updated":"2017-10-18T10:00:42.654Z","comments":true,"path":"2017/10/18/聊聊分布式事务，再说说解决方案-cap/","link":"","permalink":"http://kingge.top/2017/10/18/聊聊分布式事务，再说说解决方案-cap/","excerpt":"","text":"数据库事务 在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会经常使用到。但是即使如此，可能对于一些细节问题，很多人仍然不清楚。比如很多人都知道数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。但是再往下比如问到隔离性指的是什么的时候可能就不知道了，或者是知道隔离性是什么但是再问到数据库实现隔离的都有哪些级别，或者是每个级别他们有什么区别的时候可能就不知道了。 本文并不打算介绍这些数据库事务的这些东西，有兴趣可以搜索一下相关资料。不过有一个知识点我们需要了解，就是假如数据库在提交事务的时候突然断电，那么它是怎么样恢复的呢？ 为什么要提到这个知识点呢？ 因为分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，所以我们在做分布式系统的时候，最先考虑的就是这种情况。这些异常可能有 机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常等等… 我们接着说本地事务数据库断电的这种情况，它是怎么保证数据一致性的呢？我们使用SQL Server来举例，我们知道我们在使用 SQL Server 数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多。数据库进行任何写入操作的时候都是要先写日志的，同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。 接着，我们就说一下分布式事务。 分布式理论 当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况了，而在这种ACID的集群环境下，再想保证集群的ACID几乎是很难达到，或者即使能达到那么效率和性能会大幅下降，最为关键的是再很难扩展新的分区了，这个时候如果再追求集群的ACID会导致我们的系统变得很差，这时我们就需要引入一个新的理论原则来适应这种集群的情况，就是 CAP 原则或者叫CAP定理，那么CAP定理指的是什么呢？ CAP定理 CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性： 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成 具体地讲在分布式系统中，在任何数据库设计中，一个Web应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。 这个定理在迄今为止的分布式系统中都是适用的！ 为什么这么说呢？ 转载链接描述的很到位：http://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html","categories":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"数据库","slug":"数据库","permalink":"http://kingge.top/tags/数据库/"}]},{"title":"数据库中的undo和redo日志","slug":"数据库中的undo和redo日志","date":"2017-10-18T09:52:46.000Z","updated":"2017-10-18T09:55:01.852Z","comments":true,"path":"2017/10/18/数据库中的undo和redo日志/","link":"","permalink":"http://kingge.top/2017/10/18/数据库中的undo和redo日志/","excerpt":"","text":"转载好的博客解释1： http://blog.csdn.net/kobejayandy/article/details/50885693 转载好的博客解释2： http://www.cnblogs.com/Bozh/archive/2013/03/18/2966494.html","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"数据库","slug":"数据库","permalink":"http://kingge.top/tags/数据库/"}]},{"title":"vSphere与Workstation虚拟机交互的几种方法","slug":"vSphere与Workstation虚拟机交互的几种方法","date":"2017-10-18T08:11:28.000Z","updated":"2017-10-18T08:14:18.402Z","comments":true,"path":"2017/10/18/vSphere与Workstation虚拟机交互的几种方法/","link":"","permalink":"http://kingge.top/2017/10/18/vSphere与Workstation虚拟机交互的几种方法/","excerpt":"","text":"参见转载链接： http://wangchunhai.blog.51cto.com/225186/1884052","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"centos","slug":"centos","permalink":"http://kingge.top/tags/centos/"},{"name":"vmware","slug":"vmware","permalink":"http://kingge.top/tags/vmware/"}]},{"title":"查看虚拟机里的Centos7的IP","slug":"查看虚拟机里的Centos7的IP","date":"2017-10-18T07:48:43.000Z","updated":"2017-10-18T08:09:04.483Z","comments":true,"path":"2017/10/18/查看虚拟机里的Centos7的IP/","link":"","permalink":"http://kingge.top/2017/10/18/查看虚拟机里的Centos7的IP/","excerpt":"","text":"登录虚拟机 输入用户名和密码（用户名一般是root） 查看ip 指令 ip addr 指令： 查看当前虚拟机ip 我们发现ens32 没有 inet 这个属性，没有出现ip，那么说明在设置的时候没有开启，需要先去设置。 当前位置：[root@localhost ~]# pwd/root[root@localhost ~]# 接着来查看ens32网卡的配置： vi /etc/sysconfig/network-scripts/ifcfg-ens32 注意vi后面加空格. etc 文件夹的位置在于 [root@localhost ~]# cd ..[root@localhost /]# lsbin dev home lib64 mnt proc run srv tmp varboot etc lib media opt root sbin sys usr 查看 ifcfg-ens32 的内容 从配置清单中可以发现 CentOS 7 默认是不启动网卡的（ONBOOT=no）。 把这一项改为YES（ONBOOT=yes） – (按 i 进入编辑模式 ，修改完，按 esc退出编辑模式，然后 按 ctrl + shift + : 输入 wq 完成编辑) 然后重启网络服务： sudo service network restart 然后我们再输入 ip addr 命令 使用第三方工具登录 这里是用的是 xshell，你也可以用winscp（这个一般是用来传文件的） 然后点击连接，输入用户名和密码，便可以进入命令界面","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"centos","slug":"centos","permalink":"http://kingge.top/tags/centos/"},{"name":"vmware","slug":"vmware","permalink":"http://kingge.top/tags/vmware/"}]},{"title":"activity工作流框架——数据库表结构说明","slug":"activity工作流框架——数据库表结构说明","date":"2017-10-12T06:54:33.000Z","updated":"2017-10-12T07:08:08.890Z","comments":true,"path":"2017/10/12/activity工作流框架——数据库表结构说明/","link":"","permalink":"http://kingge.top/2017/10/12/activity工作流框架——数据库表结构说明/","excerpt":"","text":"本文转载于： http://www.jianshu.com/p/f9fd1cc02eae activity一共23张表 表的命名第一部分都是以 ACT_开头的。 表的命名第二部分是一个两个字符用例表的标识 act_ge_*： ‘ge’代表general（一般）。普通数据，各种情况都使用的数据。 act_gebytearray：二进制数据表，用来保存部署文件的大文本数据1.ID:资源文件编号，自增长2.REVINT:版本号3.NAME:资源文件名称4.DEPLOYMENTID:来自于父表act_redeployment的主键5.BYTES:大文本类型，存储文本字节流 act_geproperty：属性数据表，存储这整个流程引擎级别的数据。在初始化表结构时，会默认插入三条记录。1.NAME:属性名称2.VALUE_:属性值3.REV_INT:版本号 act_hi_*： hi’代表 history（历史）。就是这些表包含着历史的相关数据，如结束的流程实例、变量、任务、等等。 act_hiactinst：历史节点表1.ID : 标识2.PROC_DEFID :流程定义id3.PROC_INSTID : 流程实例id4.EXECUTIONID : 执行实例5.ACTID : 节点id6.ACTNAME : 节点名称7.ACTTYPE : 节点类型8.ASSIGNEE_ : 节点任务分配人9.STARTTIME : 开始时间10.ENDTIME : 结束时间11.DURATION : 经过时长 act_hi_attachment：历史附件表 act_hicomment：历史意见表1.ID :标识2.TYPE : 意见记录类型 为comment 时 为处理意见3.TIME : 记录时间4.USERID :5.TASKID ： 对应任务的id6.PROC_INSTID : 对应的流程实例的id7.ACTION ： 为AddComment 时为处理意见8.MESSAGE : 处理意见9.FULLMSG : act_hidetail：历史详情表，启动流程或者在任务complete之后,记录历史流程变量1.ID : 标识2.TYPE_ : variableUpdate 和 formProperty 两种值3.PROC_INSTID : 对应流程实例id4.EXECUTIONID : 对应执行实例id5.TASKID : 对应任务id6.ACT_INSTID : 对应节点id7.NAME : 历史流程变量名称，或者表单属性的名称8.VARTYPE : 定义类型9.REV : 版本10.TIME : 导入时间11.BYTEARRAYID12.DOUBLE : 如果定义的变量或者表单属性的类型为double，他的值存在这里13.LONG : 如果定义的变量或者表单属性的类型为LONG ,他的值存在这里14.TEXT : 如果定义的变量或者表单属性的类型为string，值存在这里15.TEXT2: act_hi_identitylink：历史流程人员表 act_hiprocinst： 历史流程实例表1.ID : 唯一标识2.PROC_INSTID : 流程ＩＤ3.BUSINESSKEY : 业务编号4.PROC_DEFID ： 流程定义id5.STARTTIME : 流程开始时间6.ENT_TIME : 结束时间7.DURATION : 流程经过时间8.START_USERID : 开启流程用户id9.START_ACTID : 开始节点10.END_ACTID： 结束节点11.SUPER_PROCESS_INSTANCEID : 父流程流程id12.DELETEREASON : 从运行中任务表中删除原因 act_hitaskinst： 历史任务实例表1.ID ： 标识2.PROC_DEFID ： 流程定义id3.TASK_DEFKEY : 任务定义id4.PROC_INSTID : 流程实例ｉｄ5.EXECUTIONID : 执行实例id6.PARENT_TASKID : 父任务id7.NAME : 任务名称8.DESCRIPTION : 说明9.OWNER : 拥有人（发起人）10.ASSIGNEE : 分配到任务的人11.START_TIME : 开始任务时间12.ENDTIME : 结束任务时间13.DURATION_ : 时长14.DELETEREASON :从运行时任务表中删除的原因15.PRIORITY_ : 紧急程度16.DUEDATE : act_hi_varinst：历史变量表 act_id_*： id’代表 identity（身份）。这些表包含着标识的信息，如用户、用户组、等等。 act_idgroup:用户组信息表，用来存储用户组信息。1.ID：用户组名2.REVINT:版本号3.NAME:用户组描述信息4.TYPE_:用户组类型 act_id_info：用户扩展信息表 act_id_membership：用户与用户组对应信息表，用来保存用户的分组信息1.USERID:用户名2.GROUPID:用户组名 act_iduser：用户信息表1.ID:用户名2.REVINT:版本号3.FIRST:用户名称4.LAST:用户姓氏5.EMAIL:邮箱6.PWD_:密码 act_re_*： ’re’代表 repository（仓库）。带此前缀的表包含的是静态信息，如，流程定义、流程的资源（图片、规则，等）。 act_redeployment:部署信息表,用来存储部署时需要持久化保存下来的信息1.ID:部署编号，自增长2.NAME_:部署包的名称3.DEPLOYTIME:部署时间 act_re_model 流程设计模型部署表 act_reprocdef:业务流程定义数据表1.ID:流程ID，由“流程编号：流程版本号：自增长ID”组成2.CATEGORY:流程命名空间（该编号就是流程文件targetNamespace的属性值）3.NAME:流程名称（该编号就是流程文件process元素的name属性值）4.KEY:流程编号（该编号就是流程文件process元素的id属性值）5.VERSION:流程版本号（由程序控制，新增即为1，修改后依次加1来完成的）6.DEPLOYMENTID:部署编号7.RESOURCENAME:资源文件名称8.DGRM_RESOURCENAME:图片资源文件名称9.HAS_START_FROMKEY:是否有Start From Key 注：此表和ACT_RE_DEPLOYMENT是多对一的关系，即，一个部署的bar包里可能包含多个流程定义文件，每个流程定义文件都会有一条记录在ACT_REPROCDEF表内，每个流程定义的数据，都会对于ACT_GE_BYTEARRAY表内的一个资源文件和PNG图片文件。和ACT_GE_BYTEARRAY的关联是通过程序用ACT_GE_BYTEARRAY.NAME与ACT_REPROCDEF.NAME完成的，在数据库表结构中没有体现。 act_ru_*： ’ru’代表 runtime（运行时）。就是这个运行时的表存储着流程变量、用户任务、变量、作业，等中的运行时的数据。 activiti 只存储流程实例执行期间的运行时数据，当流程实例结束时，将删除这些记录。这就使这些运行时的表保持 的小且快。 act_ru_event_subscr act_ruexecution：运行时流程执行实例表1.ID：主键，这个主键有可能和PROC_INSTID相同，相同的情况表示这条记录为主实例记录。2.REV_：版本，表示数据库表更新次数。3.PROC_INSTID：流程实例编号，一个流程实例不管有多少条分支实例，这个ID都是一致的。4.BUSINESSKEY：业务编号，业务主键，主流程才会使用业务主键，另外这个业务主键字段在表中有唯一约束。5.PARENTID：找到该执行实例的父级，最终会找到整个流程的执行实例6.PROC_DEFID：流程定义ID7.SUPEREXEC： 引用的执行模板，这个如果存在表示这个实例记录为一个外部子流程记录，对应主流程的主键ID。8.ACTID： 节点id，表示流程运行到哪个节点9.ISACTIVE： 是否活动流程实例10.ISCONCURRENT：是否并发。上图同步节点后为并发，如果是并发多实例也是为1。11.ISSCOPE： 主实例为1，子实例为0。12.TENANTID : 这个字段表示租户ID。可以应对多租户的设计。13.IS_EVENT_SCOPE: 没有使用到事件的情况下，一般都为0。14.SUSPENSIONSTATE：是否暂停。 act_ruidentitylink：运行时流程人员表，主要存储任务节点与参与者的相关信息1.ID： 标识2.REV_： 版本3.GROUPID： 组织id4.TYPE_： 类型5.USERID： 用户id6.TASKID： 任务id act_ru_job act_rutask：运行时任务节点表1.ID：2.REV_：3.EXECUTIONID： 执行实例的id4.PROC_INSTID： 流程实例的id5.PROC_DEFID： 流程定义的id,对应act_reprocdef 的id6.NAME_： 任务名称，对应 task 的name7.PARENT_TASKID : 对应父任务8.DESCRIPTION_：9.TASK_DEFKEY： task 的id10.OWNER : 发起人11.ASSIGNEE： 分配到任务的人12.DELEGATION : 委托人13.PRIORITY： 紧急程度14.CREATETIME： 发起时间15.DUETIME：审批时长 act_ruvariable：运行时流程变量数据表1.ID：标识2.REV：版本号3.TYPE：数据类型4.NAME_：变量名5.EXECUTIONID： 执行实例id6.PROC_INSTID： 流程实例id7.TASKID： 任务id8.BYTEARRAYID：9.DOUBLE：若数据类型为double ,保存数据在此列10.LONG： 若数据类型为Long保存数据到此列11.TEXT： string 保存到此列12.TEXT2： 结论及总结: 流程文件部署主要涉及到3个表，分别是：ACT_GE_BYTEARRAY、ACT_RE_DEPLOYMENT、ACT_RE_PROCDEF。主要完成“部署包”–&gt;“流程定义文件”–&gt;“所有包内文件”的解析部署关系。从表结构中可以看出，流程定义的元素需要每次从数据库加载并解析，因为流程定义的元素没有转化成数据库表来完成，当然流程元素解析后是放在缓存中的，具体的还需要后面详细研究。 流程定义中的java类文件不保存在数据库里 。 组织机构的管理相对较弱，如果要纳入单点登录体系内还需要改造完成，具体改造方法有待研究。 运行时对象的执行与数据库记录之间的关系需要继续研究 历史数据的保存及作用需要继续研究。","categories":[{"name":"activity","slug":"activity","permalink":"http://kingge.top/categories/activity/"}],"tags":[{"name":"activity","slug":"activity","permalink":"http://kingge.top/tags/activity/"},{"name":"工作流","slug":"工作流","permalink":"http://kingge.top/tags/工作流/"}]},{"title":"关于web.xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序","slug":"关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序","date":"2017-10-10T08:22:44.000Z","updated":"2017-10-10T09:25:05.991Z","comments":true,"path":"2017/10/10/关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序/","link":"","permalink":"http://kingge.top/2017/10/10/关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序/","excerpt":"","text":"前言 今天跑一个web项目，想做一些初始化工作，于是使用Filter来实现，但是发现ServletContextListener，Servlet也是能够实现的。但是肯定会有先后顺序执行的问题，那么接下来探讨这个问题。 作者规则：为了节省部分人的时间，先说结论。结论就是标题的顺序：ServletContext - ServletContextListener- Filter、Servlet web加载 启动一个WEB项目的时候，WEB容器会去读取它的配置文件web.xml。 加载产生Servlet上下文实例，ServletContext 这个web项目的所有部分都将共享这个上下文。容器将转换为键值对，并交给servletContext。L例如我们在使用spring的时候，会配置applicationContext.xml &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; 依次加载Servlet的事件监听器 - ServletContextListener 并依次调用public void contextInitialized(ServletContextEvent sce)方法。加载和调用多个Listener的顺序由在web.xml中配置的依次顺序决定的。 &lt;listener&gt; &lt;listener-class&gt;com.wlx.core.application.ApplicaltionListener&lt;/listener-class&gt;&lt;/listener&gt;&lt;listener&gt; &lt;listener-class&gt;com.wlx.core.application.ApplicaltionListener2&lt;/listener-class&gt;&lt;/listener&gt;先执行 ApplicaltionListener的contextInitialized方法后执行ApplicaltionListener2的contextInitialized方法 我们可以通过这个方法做一些初始化工作：例如初始化数据库连接池，初始化redis，启动定时器服务，启动线程池做一些socket通讯服务等等工作。 然后在contextDestroyed方法关闭这些服务即可。 .依次加载Servlet的过滤器-Filter 并依次调用public void init(FilterConfig filterConfig) throws ServletException;方法加载和调用多个filter的顺序由在web.xml中配置的依次顺序决定的。 &lt;filter&gt; &lt;filter-name&gt;appFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.wlx.core.application.AppFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;appFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 依次加载Servlet Load-on-startup元素在web应用启动的时候指定了servlet被加载的顺序，它的值必须是一个整数。如果它的值是一个负整数或是这个元素不存在，那么容器会在该servlet被调用的时候(例如下面代码访问-/servlet/UploadFile 为后缀的时候才会去初始化init，并不会在项目启动时候访问init)，加载这个servlet。如果值是正整数或零，容器在配置的时候就加载并初始化这个servlet，容器必须保证值小的先被加载。如果值相等，容器可以自动选择先加载谁。 在servlet的配置当中，&lt;load-on-startup&gt;5&lt;/load-on-startup&gt;的含义是：标记容器是否在启动的时候就加载这个servlet。当值为0或者大于0时，表示容器在应用启动时就加载这个servlet；当是一个负数时或者没有指定时，则指示容器在该servlet被选择时才加载。正数的值越小，启动该servlet的优先级越高。 项目启动时会去调用 UploadFile的init方法&lt;servlet&gt; &lt;servlet-name&gt;UploadFile&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wlx.core.application.servlet.UploadFile&lt;/servlet-class&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;UploadFile&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/UploadFile&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 项目启动时不会去调用 EServlet的init方法，访问匹配规则的网址时才会去调用init，而且只调用一次 &lt;servlet&gt; &lt;servlet-name&gt;EServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wlx.core.application.servlet.EServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;EServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/EServlet&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 总结 以上是Web容器在启动时加载的顺序，启动加载只会加载一次。web.xml 的加载顺序是：ServletContext-&gt; context-param -&gt;listener -&gt; filter -&gt; servlet. 扩展知识-请求执行循序 在上面中我们总结web加载的执行顺序，那么一个请求的执行循序呢？实际上就是一个责任链模式的问题 依次执行过滤器filter的方法public void doFilter(ServletRequest request, ServletResponse response,FilterChain chain)，这个方法应用了责任链模式，当在该方法中使用chain.doFilter(request, response);则这个过滤器就调用下一个过滤器，直到过滤器链条完成调用，进入Servlet处理，这个时候doFilter并未执行完成，仅仅在servlet之前进行一连串的过滤处理。 进入相应Servlet并调用public void service(ServletRequest req, ServletResponse res)方法，或者说是GET和POST方法。public void doGet(HttpServletRequest request, HttpServletResponse respose)进行请求响应的业务处理。 Servlet处理完成后，执行chain.doFilter(request, response);执行其他过滤器链条的后置过滤处理，然后执行自己的后置处理。 以上Filter和Servlet的执行顺序有点像Spring AOP 的前置通知和后置通知与业务方法关系。在Filter的doFilter方法中的chain.doFilter(request, response);之前做的业务逻辑就像前置通知，之后的逻辑像后置通知。业务方法是Sevlet中的public void service(ServletRequest req, ServletResponse res)方法。并且可以由多个有序的过滤链条进行Servlet的过滤。 Filter的过滤请求的Servlet的范围与配置有关,Filter在每次访问Servlet时都会拦截过滤。 代码例子： public class MyFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;执行MyFilter init&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;执行MyFilter doFilter&quot;); System.out.println(&quot;执行MyFilter doFilter before&quot;); chain.doFilter(request, response); System.out.println(&quot;执行MyFilter doFilter after&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;执行MyFilter destroy&quot;); &#125;&#125;-------------------------------------------------------------public class MyFilter1 implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;执行MyFilter1 init&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;执行MyFilter1 doFilter &quot;); System.out.println(&quot;执行MyFilter1 doFilter before&quot;); chain.doFilter(request, response); System.out.println(&quot;执行MyFilter1 doFilter after&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;执行MyFilter1 destroy&quot;); &#125;&#125;------------------------------------------------------------------------public class MyServlet1 extends HttpServlet &#123; private static final long serialVersionUID = 1L; public void init() throws ServletException &#123; System.out.println(&quot;执行Servlet1 init()&quot;); &#125; public void destroy() &#123; System.out.println(&quot;执行Servlet1 destroy()&quot;); &#125; public void doGet(HttpServletRequest request, HttpServletResponse respose) throws ServletException, IOException &#123; System.out.println(&quot;执行Servlet1 service&quot;); &#125;&#125; 省略在web.xml中的配置 输出： 执行MyFilter doFilter执行MyFilter doFilter before执行MyFilter1 doFilter执行MyFilter1 doFilter before执行Servlet service执行MyFilter1 doFilter after执行MyFilter doFilter after","categories":[{"name":"javaweb","slug":"javaweb","permalink":"http://kingge.top/categories/javaweb/"}],"tags":[{"name":"javaweb","slug":"javaweb","permalink":"http://kingge.top/tags/javaweb/"},{"name":"web.xml","slug":"web-xml","permalink":"http://kingge.top/tags/web-xml/"}]},{"title":"软技能-代码之外的生存指南-把自己当做一个企业去思考","slug":"软技能-代码之外的生存指南-把自己当做一个企业去思考","date":"2017-10-09T00:46:26.000Z","updated":"2017-10-09T00:51:47.087Z","comments":true,"path":"2017/10/09/软技能-代码之外的生存指南-把自己当做一个企业去思考/","link":"","permalink":"http://kingge.top/2017/10/09/软技能-代码之外的生存指南-把自己当做一个企业去思考/","excerpt":"","text":"《软技能》—— 把自己当做一个企业去思考","categories":[{"name":"读书系统","slug":"读书系统","permalink":"http://kingge.top/categories/读书系统/"}],"tags":[{"name":"软技能","slug":"软技能","permalink":"http://kingge.top/tags/软技能/"},{"name":"代码之外的生存指南","slug":"代码之外的生存指南","permalink":"http://kingge.top/tags/代码之外的生存指南/"}]},{"title":"java到底是值传递还是引用传递","slug":"java到底是值传递还是引用传递","date":"2017-09-26T07:13:17.000Z","updated":"2017-09-26T07:56:21.045Z","comments":true,"path":"2017/09/26/java到底是值传递还是引用传递/","link":"","permalink":"http://kingge.top/2017/09/26/java到底是值传递还是引用传递/","excerpt":"","text":"引言 我们先给本文定下基调，java是值传递 有一种说法，引用传递实际上也就是值传递。这个说法很有意思，实际上这种说法也是有道理的，传递引用，这个引用实际上就是一个地址，也即是一个值。 什么是值传递和引用传递 首先，不要纠结于 Pass By Value 和 Pass By Reference 的字面上的意义，否则很容易陷入所谓的“一切传引用其实本质上是传值”这种并不能解决问题无意义论战中。更何况，要想知道Java到底是传值还是传引用，起码你要先知道传值和传引用含义。 一：搞清楚 基本类型 和 引用类型的不同之处 int num = 10;String str = &quot;hello&quot;; num是基本类型，值就直接保存在变量中。而str是引用类型，变量中保存的只是实际对象的地址。一般称这种变量为”引用”，引用指向实际对象，实际对象中保存着内容。 二：搞清楚赋值运算符（=）的作用 num = 20;str = &quot;java&quot;; 对于基本类型 num ，赋值运算符会直接改变变量的值，原来的值被覆盖掉。对于引用类型 str，赋值运算符会改变引用中所保存的地址，原来的地址被覆盖掉。但是原来的对象不会被改变（重要）。 例子 参数传递基本上就是赋值操作 第一个例子：基本类型void foo(int value) &#123; value = 100;&#125;foo(num); // num 没有被改变第二个例子：没有提供改变自身方法的引用类型void foo(String text) &#123; text = &quot;windows&quot;;&#125;foo(str); // str 也没有被改变第三个例子：提供了改变自身方法的引用类型StringBuilder sb = new StringBuilder(&quot;iphone&quot;);void foo(StringBuilder builder) &#123; builder.append(&quot;4&quot;);&#125;foo(sb); // sb 被改变了，变成了&quot;iphone4&quot;。第四个例子：提供了改变自身方法的引用类型，但是不使用，而是使用赋值运算符。StringBuilder sb = new StringBuilder(&quot;iphone&quot;);void foo(StringBuilder builder) &#123; builder = new StringBuilder(&quot;ipad&quot;);&#125;foo(sb); // sb 没有被改变，还是 &quot;iphone&quot;。 重点理解为什么，第三个例子和第四个例子结果不同？ 例子5 public class Employee &#123; public int age;&#125;public class Main &#123; public static void changeEmployee(Employee employee3) &#123; employee3 = new Employee(); // flag 1 employee3.age = 1000; &#125; public static void main(String[] args) &#123; Employee employee = new Employee(); employee.age = 100; changeEmployee(employee); System.out.println(employee.age); &#125;&#125;输出： 100如果把 flag 1 位置代码注释，那么程序结果输出1000---原因同上 总结 = 号的理解是最重要的，他是一个动词，可能会引起左边变量值的改变 java中方法参数传递方式是按值传递。 如果参数是基本类型，传递的是基本类型的字面量值的拷贝。也就是你我没有半毛钱关系 如果参数是引用类型，传递的是该参量所引用的对象在堆中地址值的拷贝。你我可能存在关系 = 是赋值操作（任何包含=的如+=、-=、 /=等等，都内含了赋值操作）。不再是你以前理解的数学含义了，而+ - /和 = 在java中更不是一个级别，换句话说， = 是一个动作，一个可以改变内存状态的操作，一个可以改变变量的符号，而+ - /却不会。这里的赋值操作其实是包含了两个意思：1、放弃了原有的值或引用；2、得到了 = 右侧变量的值或引用。Java中对 = 的理解很重要啊！！可惜好多人忽略了，或者理解了却没深思过。 对于基本数据类型变量，= 操作是完整地复制了变量的值。换句话说，“=之后，你我已无关联”；至于基本数据类型，就不在这科普了。 对于非基本数据类型变量，= 操作是复制了变量的引用。换句话说，“嘿，= 左侧的变量，你丫别给我瞎动！咱俩现在是一根绳上的蚂蚱，除非你再被 = 一次放弃现有的引用！！上面说了 = 是一个动作，所以我把 = 当作动词用啦！！”。而非基本数据类型变量你基本上可以参数本身是变量 参数传递本质就是一种 = 操作。参数是变量，所有我们对变量的操作、变量能有的行为，参数都有。所以把C语言里参数是传值啊、传指针啊的那套理论全忘掉，参数传递就是 = 操作。","categories":[{"name":"java","slug":"java","permalink":"http://kingge.top/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java深入理解","slug":"java深入理解","permalink":"http://kingge.top/tags/java深入理解/"}]},{"title":"继承之上溯造型和下溯造型","slug":"继承之上溯造型和下溯造型","date":"2017-09-12T03:26:59.000Z","updated":"2017-09-12T07:01:12.151Z","comments":true,"path":"2017/09/12/继承之上溯造型和下溯造型/","link":"","permalink":"http://kingge.top/2017/09/12/继承之上溯造型和下溯造型/","excerpt":"","text":"前言 我们在平时的开发编码中，都会用到上溯造型和下溯造型，只是我们并不知道他的官方叫法而已， 上溯造型跟继承和多态，以及动态绑定的关系很密切 ，关于这几个概念后面会有涉及到他们的概念。 继承和合成 继承：它的本质就是为了使得代码复用（可以基于已经存在的类构造一个新类。继承已经存在的类就可以复用这些类的方法和域。在此基础上，可以添加新的方法和域，从而扩充了类的功能。） 合成：在新类里创建原有的对象称为合成。这种方式可以重复利用现有的代码而不更改它的形式。 -----继承关键字extends表明新类派生于一个已经存在的类。已存在的类称为父类或基类，新类称为子类或派生类。例如:class Dog extends Animal &#123;&#125;类Dog继承了Animal，Animal类称为父类或基类，Dog类称为子类或派生类。---合成合成比较简单，就是在一个类中创建一个已经存在的类。class Dog &#123; Animal animal;&#125; 上溯造型 这个术语缘于继承关系图的传统画法：将基类至于顶部，而向下发展的就是派生类(子类)，发送给父类的消息亦可发给衍生类，父类包含子类。假设把子类赋值给父类，这个过程就称之为上溯造型— 这个时候只能够调用父类父类的方法，子类特有的方法不能够调用，子类变窄 //父类abstract class Animal &#123; public abstract void speak(); public void eat()&#123; &#125; &#125;//子类特有方法interface DoorGod &#123; void guard(); &#125; //Dog 子类和 Cat 子类class Cat extends Animal &#123; @Override public void eat() &#123; try &#123; Thread.sleep( 1000 ); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // super .eat(); System.out.println(&quot;cat eat&quot;); &#125; @Override public void speak() &#123; System.out.println( &quot; 喵喵 &quot; ); &#125; &#125; class Dog extends Animal implements DoorGod&#123; @Override public void speak() &#123; System.out.println( &quot; 汪汪 &quot; ); &#125; public void guard() &#123; while ( true )&#123; System.out.println( &quot; 汪汪 &quot; ); &#125; &#125; &#125; //测试方法public class TestShangSu&#123; public static void upcasting(Animal animal)&#123; animal.speak(); animal.eat(); &#125; @Test public void test1()&#123; Animal dog1 = new Dog(); upcasting(dog1); Animal cat = new Cat(); upcasting(cat); &#125; &#125;//输出 汪汪 喵喵 cat eat 这个时候为什么输出是：子类覆盖父类的方法，而不是父类的方法，这个涉及到动态绑定。后面再讲 由于upcasting(Animal animal)方法的参数是 Animal类型的，因此如果传入的参数是 Animal的子类，传入的参数就会被转换成父类Animal类型，这样你创建的Dog对象能使用的方法只是Animal中的签名方法；也就是说，在上溯的过程中，Dog的接口变窄了，它本身的一些方法（例如实现了 DoorGod的guard方法）就不可见了。如果你想使用Dog中存在而Animal中不存在的方法（比如guard方法），编译时不能通过的。由此可见，上溯造型是安全的类型转换。 如果Dog在上溯造型过程中想使用 DoorGod的guard方法，那么需要配合下溯造型和安全检查，来进行强制转换，讲Animal 下溯为 Dog类型。 注意的是：下溯是不安全的，由父类转化为子类，所以需要加上判断。 下溯造型 将基类转化为衍生类，不安全的操作，可能会引发ClassCastException。 上面的例子只需要加上这一层判断即可 public static void upcasting(Animal animal)&#123; if( animal instanceof Dog )&#123;//下溯造型判断 Dog dog = (Dog) animal; dog.guard(); &#125; animal.speak(); animal.eat(); &#125; 我们在使用注解实现请求方法的登录控制 登录拦截器里面有段关键代码使用的就是下溯造型 为什么使用上溯和下溯造型 上面的例子我们发现，关键的代码是upcasting方法，为什么在调用upcasting方法时要有意忽略调用它的对象类型呢？如果让upcasting方法简单地获取Dog句柄似乎更加直观易懂，但是那样会使衍生自Animal类的每一个新类都要实现专属自己的upcasting方法：例如Cat会实现一个重复的upcasting(Cat cat )这样的方法。 实现多态的好处和代码复利用。 动态绑定 在上面的upcasting方法，测试例子输出的是子类的方法，而非是父类的方法，但是我们使用的是父类去调用这些方法，为什么输出不是父类的呢？ upcasting它接收的是Animal句柄，当执行speak和eat方法时时，它是如何知道Animal句柄指向的是一个Dog对象而不是Cat对象呢？编译器是无从得知的，这涉及到接下来要说明的绑定问题。 Java实现了一种方法调用机制，可在运行期间判断对象的类型，然后调用相应的方法，这种在运行期间进行，以对象的类型为基础的绑定称为动态绑定。除非一个方法被声明为final，Java中的所有方法都是动态绑定的。 静态方法的绑定 他跟普通的方法不同，子类和父类方法都是静态的，子类如果去掉父类编译会错误 package Test;class Person &#123; static void eat() &#123; System.out.println(&quot;Person.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Person.speak()&quot;); &#125;&#125;class Boy extends Person &#123; static void eat() &#123; System.out.println(&quot;Boy.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Boy.speak()&quot;); &#125;&#125;class Girl extends Person &#123; static void eat() &#123; System.out.println(&quot;Girl.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Girl.speak()&quot;); &#125;&#125;public class Persons &#123; public static Person randPerson() &#123; switch ((int)(Math.random() * 2)) &#123; default: case 0: return new Boy(); case 1: return new Girl(); &#125; &#125; public static void main(String[] args) &#123; Person[] p = new Person[4]; for (int i = 0; i &lt; p.length; i++) &#123; p[i] = randPerson(); // 随机生成Boy或Girl &#125; for (int i = 0; i &lt; p.length; i++) &#123; p[i].eat(); &#125; &#125;&#125;//输出Person.eat()Person.eat()Person.eat()Person.eat() 对于静态方法而言，不管父类引用指向的什么子类对象，调用的都是父类的方法。 总结 上溯造型和动态绑定实际上就是多态的体现，下溯造型是为了解决因为上溯而导致衍生类功能变小的问题，继承则是上溯和下溯以及动态编译的基础。","categories":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"继承","slug":"继承","permalink":"http://kingge.top/tags/继承/"},{"name":"多态","slug":"多态","permalink":"http://kingge.top/tags/多态/"}]},{"title":"注解实现请求方法的登录控制","slug":"注解实现请求方法的登录控制","date":"2017-09-06T02:37:08.000Z","updated":"2017-09-06T03:56:00.492Z","comments":true,"path":"2017/09/06/注解实现请求方法的登录控制/","link":"","permalink":"http://kingge.top/2017/09/06/注解实现请求方法的登录控制/","excerpt":"","text":"前言 之前一直使用的是，拦截器来统一验证当前用户是否登录，通过验证cookie或者session里面的是否存在已经登录标识来完成登录逻辑判断。但是会发现，这个很麻烦，而且有很多配置需要配置，例如免验证URL等等配置，无法实现可拔插式方法级别的控制。 public class RequestInterceptor extends HandlerInterceptorAdapter &#123; public String[] allowUrls;//配置不拦截的资源，所以在代码里面来排除. public void setAllowUrls(String[] allowUrls) &#123; this.allowUrls = allowUrls; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; // TODO Auto-generated method stub &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // TODO Auto-generated method stub request.setCharacterEncoding(&quot;UTF8&quot;); HttpSession session=request.getSession();//获取登录的SESSION String sessionid=request.getSession().getId();//获取登录的SESSIONID String requestPath=request.getServletPath();//获取客户请求页面 //先过滤掉不需要判断SESSION的请求 for(String url : allowUrls) &#123; if(requestPath.contains(url)) &#123; return true; &#125; &#125; Object attribute = request.getSession().getAttribute(&quot;sys_user&quot;); if( attribute == null )&#123; response.sendRedirect(&quot;/index.jsp&quot;); &#125; return true; &#125; 大体上是这样的，通过allowUrls来控制免登录url（上面的代码其实可以使用配置文件的方式来配置allowUrls的值，可以不通过setAllowUrls的方式来赋值，但是为了方面扩展就加入了。） 这里会面临一个问题，那就是如果网站网页多的话，那么allowUrls的值会变得很庞大，可能会缺漏。所以下面讲解本人用到的解决方式—-注解 和 spring配置方式（跟数组形式没有什么区别） spring 配置方式path 对所有的请求拦截使用/**，对某个模块下的请求拦截使用：/myPath/*&lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot; /&gt; &lt;bean class=&quot;com.kingge.oa.user.LoginInterceptor&quot; /&gt;&lt;/mvc:interceptor&gt; 或者 &lt;!-- 拦截是否登录 &lt;mvc:interceptor&gt; 需拦截的地址 二级目录 &lt;mvc:mapping path=&quot;/*/*&quot;/&gt; &lt;bean class=&quot;com.jk.ssm.interceptor.RequestInterceptor&quot; &gt; &lt;property name=&quot;allowUrls&quot;&gt; //回去调用拦截器的 setAllowUrls 方法 &lt;list&gt; 如果请求中包含以下路径，则不进行拦截 &lt;value&gt;/account/login.html&lt;/value&gt; &lt;value&gt;/captcha/image.html&lt;/value&gt; &lt;value&gt;/register/register.html&lt;/value&gt; &lt;value&gt;/error/400.html&lt;/value&gt; &lt;value&gt;/error/404.html&lt;/value&gt; &lt;value&gt;/error/500.html&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/mvc:interceptor&gt; 使用注解关于注解 官方说辞：JDK5开始，java增加了对元数据(MetaData)的支持，怎么支持？答：通过Annotation(注解）来实现。Annotation提供了为程序元素设置元数据的方法。元数据：描述数据的数据。 个人理解：首先什么是元数据，元数据就是对一类事物的统称，他不仅限于某个事物的描述。例如我们有ABC三个系统，分别使用oracle，mysql，db2，都有登录功能，他们的用户表字段名称是不一样的。那么有个需求，我想把A系统的用户数据pour到B系统中，那么进行映射操作？这个时候就需要一个描述用户数据的一个统一标识（元数据）这样我们就可以先把，A系统数据映射到元数据，然后再从元数据取数据映射到B系统中。 粗俗的理解，元数据就是一个类的属性，但是他所具备的职能的而应用范围，跟真正意义上类的属性数不一样的。传统的类的属性他只描述这个类，元数据可以描述多个具有共性的类。 再举个例子，我们现在常用的数据中心（DC）就是使用了元数据来作为数据传输的媒介。 元数据作用：：Annotation就像代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取。读取到了程序元素的元数据，就可以执行相应的处理。通过注解，程序开发人员可以在不改变原有逻辑的情况下，在源代码文件中嵌入一些补充信息。代码分析工具、开发工具和部署工具可以通过解析这些注解获取到这些补充信息，从而进行验证或者进行部署等。 到java8为止一共提供了五个 注解 unchecked异常：运行时异常。是RuntimeException的子类，不需要在代码中显式地捕获unchecked异常做处理。Java异常 @SafeVarargs (java7新增）：java7的“堆污染”警告与@SafeVarargs堆污染：把一个不带泛型的对象赋给一个带泛型的变量是，就会发生堆污染。例如：下面代码引起堆污染，会给出警告List l2 = new ArrayList&lt;Number&gt;();List&lt;String&gt; ls = l2;3中方式去掉这个警告 3种方式去掉这个警告：使用注解@SafeVarargs修饰引发该警告的方法或构造器。使用@SuppressWarnings(“unchecked”) 修饰。使用编译器参数命令：-Xlint:varargs @Functionlnterface （java8新增）：修饰函数式接口使用该注解修饰的接口必须是函数式接口，不然编译会出错。那么什么是函数式接口？答：如果接口中只有一个抽象方法（可以包含多个默认方法或static方法），就是函数式接口。 五个基本元注解 元注解：描述注解的注解（概念跟元数据类似）。 java提供了6个元注解（Meta Annotation)，在java.lang.annotation中。其中5个用于修饰其他的Annonation定义。而@Repeatable专门用于定义Java8新增的重复注解。所以要定义注解必须使用到5个元注解来定义( 五个注解用法 详情百度 ) @Inherited @Documented @Retention（英文：保留） @Target ( 目标) 自定义注解 参见下面，例子或者白度，具体就不阐述了。 使用注解解决登录问题定义一个枚举类 作用： 是否进行验证权限（因为后期可能会增加权限判断注解，而且是否登录也可以说是权限判断的一种，所以这里的枚举类的作用就是保存是否进行权限判断信息） public enum Action&#123; Normal(&quot;0&quot;,&quot;执行权限验证&quot;), Skip(&quot;1&quot;, &quot;跳过权限验证&quot;); private final String key; private final String desc; private Action(String key, String desc) &#123; this.key = key; this.desc = desc; &#125; //省略get set方法 定义登录和权限注解 Login属性是action ，属性类型是Action（上面的枚举类） @Target(ElementType.METHOD)@Documented@Retention(RetentionPolicy.RUNTIME)public @interface Login&#123; Action action() default Action.Normal;&#125; @Target(&#123;java.lang.annotation.ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Permission&#123; String value() default &quot;&quot;; // 这里我是保存一个权限代码，例如赋值为4000，表示当前用户的必须具备4000的权限才能够访问方法 Action action() default Action.Normal;&#125; 拦截器public class LoginInterceptor extends HandlerInterceptorAdapter&#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception&#123; if(handler instanceof HandlerMethod)&#123; //是否为请求方法 HandlerMethod handlerMethod = (HandlerMethod) handler; Login login = handlerMethod.getMethodAnnotation(Login.class);//当前请求方法是否添加了Login注解 if( login != null &amp;&amp; &quot;0&quot;.equals(login.action().getKey()) )&#123;//判断属性的值是否是0-表示需要进行登录验证 Object attribute = request.getSession().getAttribute(&quot;sys_user&quot;); if( attribute == null )&#123; response.sendRedirect(&quot;/index.jsp&quot;); &#125; &#125; return true; &#125; return true; &#125;&#125; 在spring中配置拦截器&lt;mvc:interceptors&gt;&lt;bean class=&quot;com.kingge.oa.user.LoginInterceptor&quot;&gt;&lt;/bean&gt;&lt;/mvc:interceptors&gt; 给请求方法添加权限控制 @Login(action=Action.Skip) //不需要进行登录校验 @Permission(value=&quot;4000&quot;,action=Action.normal)//需要进行权限号为4000的权限校验 @RequestMapping(&quot;/list&quot;) public String list(Model model,HttpServletRequest request) &#123; request.getSession().setAttribute(&quot;sys_user&quot;, &quot;denglule&quot;); List&lt;User&gt; userList = userService.findAllObjects(); System.out.println( userList ); model.addAttribute(&quot;userList&quot;,userList ); return &quot;list&quot;; &#125; @Login(action=Action.Normal)//添加操作，需要校验是否登录 @RequestMapping(value=&quot;/add&quot;, method=RequestMethod.POST) public String add( User user ) &#123; System.out.println( user ); userService.insert(user); return &quot;forward:/user/list&quot;; &#125;","categories":[{"name":"java","slug":"java","permalink":"http://kingge.top/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java注解","slug":"java注解","permalink":"http://kingge.top/tags/java注解/"},{"name":"登录控制","slug":"登录控制","permalink":"http://kingge.top/tags/登录控制/"}]},{"title":"java8新特性","slug":"java8新特性","date":"2017-08-29T04:27:16.000Z","updated":"2017-09-01T02:08:20.226Z","comments":true,"path":"2017/08/29/java8新特性/","link":"","permalink":"http://kingge.top/2017/08/29/java8新特性/","excerpt":"","text":"Java 8可谓是自Java 5以来最具革命性的版本了，她在语言、编译器、类库、开发工具以及Java虚拟机等方面都带来了不少新特性。我们来一一回顾一下这些特性。 一、Lambda表达式 Lambda表达式可以说是Java 8最大的卖点，她将函数式编程引入了Java。Lambda允许把函数作为一个方法的参数，或者把代码看成数据。Lambda 是一个匿名函数。 一个Lambda表达式可以由用逗号分隔的参数列表、–&gt;符号与函数体三部分表示。例如： 例子1 需求： 比较TreeSet中数据，按小到大输出 使用匿名内部类实现一个排序功能 //采用匿名内部类的方式-实现比较器 Comparator&lt;Integer&gt; comparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return Integer.compare(o1, o2);//关键代码 &#125; &#125;;//传入比较器 TreeSet&lt;Integer&gt; tree2 = new TreeSet&lt;&gt;(comparator ); tree2.add(12); tree2.add(-12); tree2.add(100);System.out.println(tree2) //输出 -12 12 100 我们不难发现上面的代码存在一个问题：其实关键代码只有第七行，其他代码都是冗余的 使用Lambda表达式实现同样功能 //使用Lambda表达式，抽取关键代码，减少代码量Comparator&lt;Integer&gt; comparator2 = (x, y) -&gt; Integer.compare(x, y); //关键代码 TreeSet&lt;Integer&gt; tree = new TreeSet&lt;&gt;(comparator2 ); tree.add(12); tree.add(-12); tree.add(100); tree.forEach(System.out::println);//代替System.out.println 代码瞬间就变得很简短，你可能觉得这个有什么，没什么感觉。那么我们在进入第二个例子 例子2 需求：1.获取公司中年龄小于 35 的员工信息2.获取公司中工资大于 5000 的员工信息。。。。。。 前期准备实现一个Employee类,有四个属性 private int id;private String name;private int age;private double salary;忽略get/set方法和构造器 初始化一个List： List&lt;Employee&gt; emps = Arrays.asList( new Employee(101, &quot;张三&quot;, 18, 9999.99), new Employee(102, &quot;李四&quot;, 59, 6666.66), new Employee(103, &quot;王五&quot;, 28, 3333.33), new Employee(104, &quot;赵六&quot;, 8, 7777.77), new Employee(105, &quot;田七&quot;, 38, 5555.55)); 常规方法实现实现两个方法，然后传入需要过滤的源数据，返回过滤后的结果集 //需求：获取公司中年龄小于 35 的员工信息public List&lt;Employee&gt; filterEmployeeAge(List&lt;Employee&gt; emps)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee emp : emps) &#123; if(emp.getAge() &lt;= 35)&#123;//比较代码 list.add(emp); &#125; &#125; return list;&#125;//需求：获取公司中工资大于 5000 的员工信息public List&lt;Employee&gt; filterEmployeeSalary(List&lt;Employee&gt; emps)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee emp : emps) &#123; if(emp.getSalary() &gt;= 5000)&#123;//比较代码 list.add(emp); &#125; &#125; return list;&#125; 我们不难发现上面的代码存在一个问题：那就是两个方法除了比较部分不同，其他逻辑是一样的，存在大量冗余，假设有新的需求（例如求得求得名字姓王的员工）那么就需要再创建一个 filterEmployee**方法对应新的需求。 使用策略设计模式实现 提供父借口 和 两个 实现类（两个需求对应的逻辑实现类） // 父接口 @FunctionalInterfacepublic interface MyPredicate&lt;T&gt; &#123; public boolean test(T t); &#125;//需求1 实现类-年龄小于35public class FilterEmployeeForAge implements MyPredicate&lt;Employee&gt;&#123; @Override public boolean test(Employee t) &#123; return t.getAge() &lt;= 35; &#125;&#125;//需求1 实现类-工资大于5000public class FilterEmployeeForSalary implements MyPredicate&lt;Employee&gt; &#123; @Override public boolean test(Employee t) &#123; return t.getSalary() &gt;= 5000; &#125;&#125; 测试代码 // 通用过滤方法 public List&lt;Employee&gt; filterEmployee(List&lt;Employee&gt; emps, MyPredicate&lt;Employee&gt; mp)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee employee : emps) &#123; if(mp.test(employee))&#123; list.add(employee); &#125; &#125; return list; &#125; @Test public void test4()&#123; //传入实现年龄过滤的实现类 List&lt;Employee&gt; list = filterEmployee(emps, new FilterEmployeeForAge()); for (Employee employee : list) &#123; System.out.println(employee); &#125; System.out.println(&quot;------------------------------------------&quot;); List&lt;Employee&gt; list2 = filterEmployee(emps, new FilterEmployeeForSalary()); for (Employee employee : list2) &#123; System.out.println(employee); &#125; &#125; 使用策略模式比上一个的好处是：代码很清晰，便于维护，新的需求我们只需要再实现对应的需求实现类即可，然后传入MyPredicate```接口即可。缺点是：需要实现对应的需求类然后实现``` MyPredicate&lt;T&gt;```接口### **匿名内部类**这种方法类似于例子1中的 Comparator这个接口的实现```JAVA//直接使用 MyPredicate&lt;Employee&gt;接口，不去实现对应的需求类（上面的FilterEmployeeForSalary 和 FilterEmployeeForAge ） @Test public void test5()&#123; List&lt;Employee&gt; list = filterEmployee(emps, new MyPredicate&lt;Employee&gt;() &#123; @Override public boolean test(Employee t) &#123; return t.getId() &lt;= 103; &#125; &#125;); for (Employee employee : list) &#123; System.out.println(employee); &#125; &#125; 我们不难发现上面的代码存在一个问题：跟例子1一样，存在大量的冗余。 Lambda 表达式实现前期准备public List&lt;Employee&gt; filterEmployee(List&lt;Employee&gt; emps, MyPredicate&lt;Employee&gt; mp)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee employee : emps) &#123; if(mp.test(employee))&#123; list.add(employee); &#125; &#125; return list;&#125; @Testpublic void test6()&#123; List&lt;Employee&gt; list = filterEmployee(emps, (e) -&gt; e.getAge() &lt;= 35); list.forEach(System.out::println); System.out.println(\"------------------------------------------\"); List&lt;Employee&gt; list2 = filterEmployee(emps, (e) -&gt; e.getSalary() &gt;= 5000); list2.forEach(System.out::println);&#125; 我们不难发现上面的代码存在一个问题：这个代码，是不是已经非常简短了，感觉已经是终极的最简代码。但是实际上还有更简短的代码（使用stream api）缺点：太过依赖 MyPredicate 这个接口，假设这个接口不存在，该怎么办呢？（我们这里仅仅是做个假设） 终极实现方式：Stream API@Testpublic void test7()&#123; emps.stream() .filter((e) -&gt; e.getAge() &lt;= 35) .forEach(System.out::println); System.out.println(&quot;----------------------------------------------&quot;); emps.stream() .filter((e) -&gt; e.getSalary() &gt;= 5000) .forEach(System.out::println); System.out.println(&quot;----------------------------------------------&quot;); // 可以使用map 指定输出那个属性的值，代替普通的便利输出 emps.stream() .map(Employee::getName) .limit(3)// 输出前三个 .sorted()//排序 .forEach(System.out::println); &#125; 输出 Employee [id=101, name=张三, age=18, salary=9999.99]Employee [id=103, name=王五, age=28, salary=3333.33]Employee [id=104, name=赵六, age=8, salary=7777.77]----------------------------------------------Employee [id=101, name=张三, age=18, salary=9999.99]Employee [id=102, name=李四, age=59, salary=6666.66]Employee [id=104, name=赵六, age=8, salary=7777.77]Employee [id=105, name=田七, age=38, salary=5555.55]----------------------------------------------张三李四王五 我们不难发现上面的代码存在一个问题：这个代码，是非常潇洒，舒服的，不依赖我们上面所说的接口。 函数式接口 为了使现有函数更好的支持Lambda表达式，Java 8引入了函数式接口的概念。函数式接口就是只有一个方法的普通接口。java.lang.Runnable与java.util.concurrent.Callable是函数式接口最典型的例子。为此，Java 8增加了一种特殊的注解@FunctionalInterface： –也就是说：这个接口里面只能够存在一个接口方法，多个就会报错 例子：@FunctionalInterfacepublic interface Functional &#123; void method();&#125; 认识Lambda表达式概念 一、Lambda 表达式的基础语法：Java8中引入了一个新的操作符 “-&gt;” 该操作符称为箭头操作符或 Lambda 操作符 箭头操作符将 Lambda 表达式拆分成两部分： 左侧：Lambda 表达式的参数列表 右侧：Lambda 表达式中所需执行的功能， 即 Lambda 体 上面的例子：List list = filterEmployee(emps, (e) -&gt; e.getAge() &lt;= 35); 第二个参数他会去找 MyPredicate&lt;T&gt; 接口里面的 public boolean test(T t);test方法，lambda表达式左边的(e) 对应的是test方法的入参, ambda表达式右边的e.getAge() &lt;= 35 对应得是test方法的实现 那么你可能会有疑问，假设MyPredicate接口里面有很多个接口方法，那么他会去调用那个呢？他怎么知道去找test方法呢？ 引入了：@FunctionalInterface这个函数式接口的概念，解决了这个问题。 * 语法格式一：无参数，无返回值 * () -&gt; System.out.println(&quot;Hello Lambda!&quot;); &gt; 例如 Runnable接口的 run方法就是无参数无返回值： @Test public void test1()&#123; int num = 0;//jdk 1.7 前，我们知道匿名内部引用局部变量必须声明为final //但jdk1.8，它默认给我们添加了final，不用显示声明。 Runnable r = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Hello World!&quot; + num); //这里如果改为 num++是会报错的，因为他本质上是一个final &#125; &#125;; r.run(); System.out.println(&quot;-------------------------------&quot;); Runnable r1 = () -&gt; System.out.println(&quot;Hello Lambda!&quot;); r1.run(); &#125;这两个是等效的 * * 语法格式二：有一个参数，并且无返回值* (x) -&gt; System.out.println(x)* 例子：Consumer这个类jdk自带--有参数无返回值@Testpublic void test2()&#123; Consumer&lt;String&gt; con = x -&gt; System.out.println(x); con.accept(&quot;我是你泽精哥！&quot;);&#125; * 语法格式三：若只有一个参数，小括号可以省略不写* x -&gt; System.out.println(x)* * 语法格式四：有两个以上的参数，有返回值，并且 Lambda 体中有多条语句* Comparator&lt;Integer&gt; com = (x, y) -&gt; &#123;* System.out.println(&quot;函数式接口&quot;);* return Integer.compare(x, y);* &#125;; * 语法格式五：若 Lambda 体中只有一条语句， return 和 大括号都可以省略不写* Comparator&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x, y);* * 语法格式六：Lambda 表达式的参数列表的数据类型可以省略不写，因为JVM编译器通过上下文推断出，数据类型，即“类型推断”* (Integer x, Integer y) -&gt; Integer.compare(x, y); 类型推断 : jdk1.8后，添加了这个功能String[] strs = {“aaa”, “bbb”, “ccc”} ; 它自动会转换里面的数据为String类型的数据改为： String[] strs;strs = &#123;&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;&#125;;//会报错--因为这样无法进行类型推断 类型推断例子2 public void show(Map&lt;String, Integer&gt; map)&#123;&#125;//方法 show(new HashMap&lt;&gt;());//调用方法我们发现在调用方法的时候入参我们并没有明确声明类型，但是在jdk1.8中是可以编译通过的。这里也是运用了类型推断（注意：jdk1.7中编译会失败） 热身例子一 //函数是接口@FunctionalInterfacepublic interface MyFun &#123; public Integer getValue(Integer num);&#125;//测试 //需求：对一个数进行运算 @Test public void test6()&#123; Integer num = operation(100, (x) -&gt; x * x); System.out.println(num); System.out.println(operation(200, (y) -&gt; y + 200)); &#125; public Integer operation(Integer num, MyFun mf)&#123; return mf.getValue(num); &#125; 热身例子二//函数接口 @FunctionalInterface //约束当前接口只能有一个方法public interface CalcLong&lt;K,T&gt;&#123; // public K getMultiply(T t, T tt); K getMultiply(T t, T tt);&#125;//需求：求得两个数的和 String result = getMuyl(10L,10L,(e,ee)-&gt;&#123; System.out.println(e+ &quot; &quot; + ee); return e+ee+&quot;&quot;; &#125;); System.out.println(result); public String getMuyl(Long l,Long ll,CalcLong&lt;String,Long&gt; mf)&#123; return mf.getMultiply(l, ll); &#125; 看到这里可能会有疑惑？我靠，使用lambda表达式还得声明一个函数接口，这么麻烦。实际上，java内部已经帮我们实现了很多个接口供我们使用，不需要重新自己定义，除非有特别操作。 java8内置四大函数式接口 为了解决接口需要自定义问题 /* * Java8 内置的四大核心函数式接口 * * Consumer&lt;T&gt; : 消费型接口 * void accept(T t); * * Supplier&lt;T&gt; : 供给型接口 * T get(); * * Function&lt;T, R&gt; : 函数型接口 * R apply(T t); * * Predicate&lt;T&gt; : 断言型接口 * boolean test(T t); * */ 例子消费型接口//Consumer&lt;T&gt; 消费型接口 :@Testpublic void test1()&#123; String p; happy(10000, (m) -&gt; System.out.println(&quot;桑拿，每次消费：&quot; + m + &quot;元&quot;));&#125; public void happy(double money, Consumer&lt;Double&gt; con)&#123; con.accept(money);&#125; Supplier 供给型接口 //Supplier&lt;T&gt; 供给型接口 :@Testpublic void test2()&#123; List&lt;Integer&gt; numList = getNumList(10, () -&gt; (int)(Math.random() * 100)); for (Integer num : numList) &#123; System.out.println(num); &#125;&#125;//需求：产生指定个数的整数，并放入集合中public List&lt;Integer&gt; getNumList(int num, Supplier&lt;Integer&gt; sup)&#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; num; i++) &#123; Integer n = sup.get(); list.add(n); &#125; return list;&#125; Function 函数型接口 //Function&lt;T, R&gt; 函数型接口：@Testpublic void test3()&#123; String newStr = strHandler(&quot;\\t\\t\\t 去除前后空格 &quot;, (str) -&gt; str.trim()); System.out.println(newStr); String subStr = strHandler(&quot;截取字符串你知不知道&quot;, (str) -&gt; str.substring(2, 5)); System.out.println(subStr);&#125;//需求：用于处理字符串public String strHandler(String str, Function&lt;String, String&gt; fun)&#123; return fun.apply(str);&#125; Predicate 断言型接口 //Predicate&lt;T&gt; 断言型接口：@Testpublic void test4()&#123; List&lt;String&gt; list = Arrays.asList(&quot;Hello&quot;, &quot;atguigu&quot;, &quot;Lambda&quot;, &quot;www&quot;, &quot;ok&quot;); List&lt;String&gt; strList = filterStr(list, (s) -&gt; s.length() &gt; 3); for (String str : strList) &#123; System.out.println(str); &#125;&#125;//需求：将满足条件的字符串，放入集合中public List&lt;String&gt; filterStr(List&lt;String&gt; list, Predicate&lt;String&gt; pre)&#123; List&lt;String&gt; strList = new ArrayList&lt;&gt;(); for (String str : list) &#123; if(pre.test(str))&#123; strList.add(str); &#125; &#125; return strList;&#125; 四大内置函数衍生的子函数 二、接口的默认方法与静态方法 我们可以在接口中定义默认方法，使用default关键字，并提供默认的实现。所有实现这个接口的类都会接受默认方法的实现，除非子类提供的自己的实现。例如：public interface DefaultFunctionInterface &#123; default String defaultFunction() &#123; return &quot;default function&quot;; &#125;&#125; 我们还可以在接口中定义静态方法，使用static关键字，也可以提供实现。例如：public interface StaticFunctionInterface &#123; static String staticFunction() &#123; return &quot;static function&quot;; &#125;&#125; 接口的默认方法和静态方法的引入，其实可以认为引入了C＋＋中抽象类的理念，以后我们再也不用在每个实现类中都写重复的代码了。 三、方法引用 通常与Lambda表达式联合使用，可以直接引用已有Java类或对象的方法。一般有四种不同的方法引用： 构造器引用 构造器引用。语法是Class::new，构造器的参数列表，需要与函数式接口中参数列表保持一致！也就是说，决定Class::new调用那一个构造器得是：接口函数的方法的参数 //构造器引用@Testpublic void test7()&#123; // Supplier 的接口方法 T get(); --所以调用无参构造器 Supplier&lt;Employee&gt; fun0 = Employee::new; //Function 的接口方法 R apply(T t);-调用一个参数构造器 Function&lt;String, Employee&gt; fun = Employee::new; //BiFunction 的接口方法 R apply(T t, U u); -调用二参构造器 BiFunction&lt;String, Integer, Employee&gt; fun2 = Employee::new;&#125; 对象静态方法引用（类名::静态方法） 静态方法引用。语法是Class::static_method，要求接受一个Class类型的参数； //类名 :: 静态方法名//max和compare 都是静态方法@Testpublic void test4()&#123; Comparator&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x, y); System.out.println(&quot;-------------------------------------&quot;); Comparator&lt;Integer&gt; com2 = Integer::compare; BiFunction&lt;Double, Double, Double&gt; fun = (x, y) -&gt; Math.max(x, y); System.out.println(fun.apply(1.5, 22.2)); System.out.println(&quot;------------------------------------&quot;); BiFunction&lt;Double, Double, Double&gt; fun2 = Math::max; System.out.println(fun2.apply(1.2, 1.5));&#125; 对象实例方法引用（对象引用::实例方法名） 特定类的任意对象方法引用。它的语法是Class::method。要求方法是没有参数的； //对象的引用 :: 实例方法名@Testpublic void test2()&#123; Employee emp = new Employee(101, &quot;张三&quot;, 18, 9999.99); Supplier&lt;String&gt; sup = () -&gt; emp.getName(); System.out.println(sup.get()); System.out.println(&quot;----------------------------------&quot;); Supplier&lt;String&gt; sup2 = emp::getName; System.out.println(sup2.get());&#125; 类名实例方法引用(类名::实例方法名) 我们知道一般是有对象才能够引用实例方法，但是有种特殊情况是可以直接使用类名引用实例方法若Lambda 的参数列表的第一个参数，是实例方法的调用者，第二个参数(或无参)是实例方法的参数时，格式： ClassName::MethodName //类名 :: 实例方法名//按照常规是String st = new String(&quot;123&quot;); st::equals,//对象调用实例方法，但是下面因为符合第四种引用的规则，//所以可以使用类名调用实例方法@Testpublic void test5()&#123;//第一个参数为实例方法调用者，第二个参数为为实例方法参数 BiPredicate&lt;String, String&gt; bp = (x, y) -&gt; x.equals(y); System.out.println(bp.test(&quot;abcde&quot;, &quot;abcde&quot;)); System.out.println(&quot;-------------------------------------&quot;); BiPredicate&lt;String, String&gt; bp2 = String::equals;System.out.println(bp2.test(&quot;abc&quot;, &quot;abc&quot;)); System.out.println(&quot;---------------------------------------&quot;); //第一个参数为实例方法调用者，第二个参数为空Function&lt;Employee, String&gt; fun = (e) -&gt; e.show();System.out.println(fun.apply(new Employee()));System.out.println(&quot;--------------------------------------&quot;); Function&lt;Employee, String&gt; fun2 = Employee::show; System.out.println(fun2.apply(new Employee())); &#125; 注意： ①方法体所引用的方法的参数列表与返回值类型，需要与函数式接口中抽象方法的参数列表和返回值类型保持一致！ ②若Lambda 的参数列表的第一个参数，是实例方法的调用者，第二个参数(或无参)是实例方法的参数时，格式：ClassName::MethodName (针对于第四种方法引用) 数组引用（类型[] :: new）//数组引用@Testpublic void test8()&#123; Function&lt;Integer, String[]&gt; fun = (args) -&gt; new String[args]; String[] strs = fun.apply(10); System.out.println(strs.length); System.out.println(&quot;--------------------------&quot;); Function&lt;Integer, Employee[]&gt; fun2 = Employee[] :: new; Employee[] emps = fun2.apply(20); System.out.println(emps.length);&#125; 四、重复注解在Java 5中使用注解有一个限制，即相同的注解在同一位置只能声明一次。Java 8引入重复注解，这样相同的注解在同一地方也可以声明多次。重复注解机制本身需要用@Repeatable注解。Java 8在编译器层做了优化，相同注解会以集合的方式保存，因此底层的原理并没有变化。 五、扩展注解的支持Java 8扩展了注解的上下文，几乎可以为任何东西添加注解，包括局部变量、泛型类、父类与接口的实现，连方法的异常也能添加注解。 六、OptionalJava 8引入Optional类来防止空指针异常，Optional类最先是由Google的Guava项目引入的。Optional类实际上是个容器：它可以保存类型T的值，或者保存null。使用Optional类我们就不用显式进行空指针检查了。 七、Stream前言 Stream API是把真正的函数式编程风格引入到Java中。其实简单来说可以把Stream理解为MapReduce，当然Google的MapReduce的灵感也是来自函数式编程。她其实是一连串支持连续、并行聚集操作的元素。从语法上看，也很像linux的管道、或者链式编程，代码写起来简洁明了，非常酷帅！ 八、Date/Time API (JSR 310)Java 8新的Date-Time API (JSR 310)受Joda-Time的影响，提供了新的java.time包，可以用来替代 java.util.Date和java.util.Calendar。一般会用到Clock、LocaleDate、LocalTime、LocaleDateTime、ZonedDateTime、Duration这些类，对于时间日期的改进还是非常不错的。 九、JavaScript引擎NashornNashorn允许在JVM上开发运行JavaScript应用，允许Java与JavaScript相互调用。 十、Base64在Java 8中，Base64编码成为了Java类库的标准。Base64类同时还提供了对URL、MIME友好的编码器与解码器。 除了这十大新特性之外，还有另外的一些新特性： 更好的类型推测机制：Java 8在类型推测方面有了很大的提高，这就使代码更整洁，不需要太多的强制类型转换了。 编译器优化：Java 8将方法的参数名加入了字节码中，这样在运行时通过反射就能获取到参数名，只需要在编译时使用-parameters参数。 并行（parallel）数组：支持对数组进行并行处理，主要是parallelSort()方法，它可以在多核机器上极大提高数组排序的速度。 并发（Concurrency）：在新增Stream机制与Lambda的基础之上，加入了一些新方法来支持聚集操作。 Nashorn引擎jjs：基于Nashorn引擎的命令行工具。它接受一些JavaScript源代码为参数，并且执行这些源代码。 类依赖分析器jdeps：可以显示Java类的包级别或类级别的依赖。 JVM的PermGen空间被移除：取代它的是Metaspace（JEP 122），元空间直接采用的是物理空间，也即是我们电脑的内存，电脑内存多大，元空间就有多大。","categories":[{"name":"Java8","slug":"Java8","permalink":"http://kingge.top/categories/Java8/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java8","slug":"java8","permalink":"http://kingge.top/tags/java8/"},{"name":"java8新特性","slug":"java8新特性","permalink":"http://kingge.top/tags/java8新特性/"}]},{"title":"哈希表冲突解决方式之开放地址法和链地址法","slug":"哈希表冲突解决方式之开放地址法和链地址法","date":"2017-08-29T03:07:51.000Z","updated":"2017-08-29T03:56:03.815Z","comments":true,"path":"2017/08/29/哈希表冲突解决方式之开放地址法和链地址法/","link":"","permalink":"http://kingge.top/2017/08/29/哈希表冲突解决方式之开放地址法和链地址法/","excerpt":"","text":"基本定义 散列技术是在记录的存储位置和它的关键字之间建立一个确定的对应关系 f，使得每个关键字key对应一个存储位置f(key)。 这种对应关系f称为散列或哈希函数 采用上述思想将数据存储在一块连续的存储空间中，这块连续的存储空间称为散列或哈希表 关键字对应的存储位置称为散列地址 如果碰到两个不同的关键字key1≠key2，但却有相同的f(key1)=f(key2)，这种现象称为冲突， 并把key1和key2 称为这个散列函数的同义词（synonym） 散列函数构造方法好的散列函数参考如下两个原则： 计算简单 散列地址分布均匀 最常用的方法是除留余数法，对于散列表长度为m的散列函数是 f(key)=key mod p (p≤m) 处理散列冲突 开放地址法 开放地址法就是一旦发生冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列表总能找到，并存入。开放地址法又分为线性探测法，二次探测法和随机探测法。 链地址法 将所有同义词的关键字存储在同一个单链表中，称这个单链表为同义词子表，在散列表中只存储同义词子表的头指针。只要有冲突，就在同义词的子表中增加结点。(java中的HashMap就是采用这种方法) 开放地址法","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://kingge.top/categories/数据结构/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"http://kingge.top/tags/数据结构/"},{"name":"哈希表","slug":"哈希表","permalink":"http://kingge.top/tags/哈希表/"},{"name":"哈希解决冲突","slug":"哈希解决冲突","permalink":"http://kingge.top/tags/哈希解决冲突/"}]},{"title":"java8之Hashmap","slug":"java8之Hashmap","date":"2017-08-28T14:27:16.000Z","updated":"2017-08-29T08:08:38.273Z","comments":true,"path":"2017/08/28/java8之Hashmap/","link":"","permalink":"http://kingge.top/2017/08/28/java8之Hashmap/","excerpt":"","text":"Java8-HashMap变化 数据的存储结构从：数组+链表 演变为了 数组+链表+红黑树 Map 家庭族谱 HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 总结对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 HashMap简介 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 我们知道HashMap的数据存储结构就是：数组加上链表。通过对于key的值做hash运算，获得对应的值找到对应的数组下标，然后再存储值。存储值的过程中可能当前数组已经存在值（这个称之为冲突） 然后再生成一个链表存储冲突的值。 HashCode() 和 Hash() 方法实现得足够好，能够尽可能地减少冲突的产生，那么对 HashMap 的操作几乎等价于对数组的随机访问操作，具有很好的性能。但是，如果 HashCode() 或者 Hash() 方法实现较差，在大量冲突产生的情况下，HashMap 事实上就退化为几个链表，对 HashMap 的操作等价于遍历链表，此时性能很差。 解决冲突的方法：开放地址法和链地址法 HashMap特点 允许null为key 输出无序 如果想要输出有序，那以使用继承他的LinkedHashMap，元素输出顺序跟输入顺序一致,他提供了一个节点保存输入的元素的顺序。想要对元素的值进行排序 推荐TreeMap（因为他继承了SortedMap) 非线程安全 数组+链表存储方式 Java8特性 HashMap是数组+链表+红黑树 存储算法： map.put(&quot;kingge&quot;,&quot;shuai&quot;)系统将调用kingge”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 好的hash算法和扩容机制是解决冲突和高效存取的命题 HashMap 重要的几个属性int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; Node[] table(Hash桶)始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 分析HashMap的put方法put方法图解，详情可以去看源码 public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 为什么说HashMap是线程不安全的 个人觉得有两个表现，如果还有其他的希望大家补充，或者以后等楼主源码研究透了再补充 表现一 我们知道当插入数据超过了threshold(threshold=length * Load factor),那么就会扩容，扩容会去调用resize和transfer方法，这个时候原先hash桶里面的所有数据都会重新计算，对应的位置–称之为rehash，这个成本很大 最根本的原因是出现时死循环-也就是在死锁问题出现在了transfer方法上面,而且是因为在扩容转换的过程中采用的是链表的头插法的形式进行插入数据。例如原来在数组arr[0]的位置又链表1–&gt;2–&gt;3 那么扩容后，采用头插法就变成了arr[0]：3–&gt;2–&gt;1 为什么采用头插法，因为时间复杂度为O(1)，想象一下尾插法，那么需要遍历找到最尾元素然后插入时间复杂度是O(n) 具体源码分析参见：http://www.importnew.com/22011.html 表现二多个线程同时操作一个hashmap就可能出现不安全的情况：比如A B两个线程(A线程获数据 B线程存数据) 同时操作myHashMap1.B线程执行存放数据modelHashMap.put(“1”,”2”);2.A线程执行get获取数据modelHashMap.get(“1”)A线程获取的值本来应该是2，但是如果A线程在刚到达获取的动作还没执行的时候，线程执行的机会又跳到线程B，此时线程B又对modelHashMap赋值 如：modelHashMap.put(“1”,”3”);然后线程虚拟机又执行线程A，A取到的值为3，这样map中第一个存放的值 就会丢失。。。。。—原子性 解决HashMap非线程安全其实上面我已经有提过了： 三个方法： Hashtable替换HashMap Collections.synchronizedMap将HashMap包装起来 private Map map = Collections.synchronizedMap(new HashMap());替换private HashMap map = new HashMap(); ConcurrentHashMap替换HashMap private ConcurrentHashMap map = new ConcurrentHashMap();替换private HashMap map = new HashMap(); 好的博文http://blog.csdn.net/lyg468088/article/details/49464121","categories":[{"name":"Java8","slug":"Java8","permalink":"http://kingge.top/categories/Java8/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java8","slug":"java8","permalink":"http://kingge.top/tags/java8/"},{"name":"java8新特性","slug":"java8新特性","permalink":"http://kingge.top/tags/java8新特性/"}]},{"title":"程序员未来规划","slug":"程序员未来规划","date":"2017-08-28T02:00:36.000Z","updated":"2017-08-28T02:06:13.437Z","comments":true,"path":"2017/08/28/程序员未来规划/","link":"","permalink":"http://kingge.top/2017/08/28/程序员未来规划/","excerpt":"","text":"http://www.jianshu.com/p/9d29a441ee17?utm_source=desktop&amp;utm_medium=timeline","categories":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/categories/心情/"}],"tags":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/tags/心情/"},{"name":"大龄程序员","slug":"大龄程序员","permalink":"http://kingge.top/tags/大龄程序员/"},{"name":"规划","slug":"规划","permalink":"http://kingge.top/tags/规划/"}]},{"title":"java之ClassLoader源码分析","slug":"java之ClassLoader源码分析","date":"2017-08-24T06:36:35.000Z","updated":"2017-08-24T08:36:55.128Z","comments":true,"path":"2017/08/24/java之ClassLoader源码分析/","link":"","permalink":"http://kingge.top/2017/08/24/java之ClassLoader源码分析/","excerpt":"","text":"类加载器ClassLoader的含义 不论多么简单的java程序，都是由一个或者多个java文件组成，java内部实现了程序所需要的功能逻辑，类之间可能还存在着依赖关系。当程序运行的时候，类加载器会把一部分类编译为class后加载到内存中，这样程序才能够调用里面的方法并运行。 类之间如果存在依赖关系，那么类加载会去帮你加载相关的类到内存中，这样才能够完成调用。如果找不到相关的类，那么他就会抛出我们在开发经常见到的异常：ClassNotFoundException Java中的所有类，必须被装载到jvm中才能运行，这个装载工作是由jvm中的类装载器完成的，类装载器所做的工作实质是把类文件从硬盘读取到内存中，JVM在加载类的时候，都是通过ClassLoader的loadClass（）方法来加载class的,与此同时在loadClass中存在着三种加载策略，loadClass使用双亲委派模式。 所以Classloader就是用来动态加载Class文件到内存当中用的。 Java默认提供的三个ClassLoader1.Bootstrap ClassLoader 称为启动类加载器，是Java类加载层次中最顶层的类加载器，负责加载JDK中的核心类库，预设上它负责搜寻JRE所在目录的classes或lib目录下（实际上是由系统参数sun.boot.class.path指定）。如：rt.jar、resources.jar、charsets.jar等，可通过如下程序获得该类加载器从哪些地方加载了相关的jar或class文件： URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i &lt; urls.length; i++) &#123; System.out.println(urls[i].toExternalForm()); &#125; &gt; 这两种输出的内容都是一样的。 System.out.println(System.getProperty(\"sun.boot.class.path\")); 输出 file:/F:/JDK/jdk1.8/jre/lib/resources.jarfile:/F:/JDK/jdk1.8/jre/lib/rt.jarfile:/F:/JDK/jdk1.8/jre/lib/sunrsasign.jarfile:/F:/JDK/jdk1.8/jre/lib/jsse.jarfile:/F:/JDK/jdk1.8/jre/lib/jce.jarfile:/F:/JDK/jdk1.8/jre/lib/charsets.jarfile:/F:/JDK/jdk1.8/jre/lib/jfr.jarfile:/F:/JDK/jdk1.8/jre/classesF:\\JDK\\jdk1.8\\jre\\lib\\resources.jar;F:\\JDK\\jdk1.8\\jre\\lib\\rt.jar;F:\\JDK\\jdk1.8\\jre\\lib\\sunrsasign.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jsse.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jce.jar;F:\\JDK\\jdk1.8\\jre\\lib\\charsets.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jfr.jar;F:\\JDK\\jdk1.8\\jre\\classes 2.Extension ClassLoader 称为扩展类加载器，负责加载Java的扩展类库，默认加载JAVA_HOME/jre/lib/ext/目下的所有jar（实际上是由系统参数java.ext.dirs指定） 3.App ClassLoader 称为系统类加载器，负责加载应用程序classpath目录下的所有jar和class文件（由系统参数java.class.path指定） 总结 Extension ClassLoader和App ClassLoader 这两个类加载器实际上都是继承了ClassLoader类，但是Bootstrap ClassLoader不继承自ClassLoader，因为它不是一个普通的Java类，底层由C++编写，已嵌入到了JVM内核当中，当JVM启动后，Bootstrap ClassLoader也随着启动，负责加载完核心类库后，并构造Extension ClassLoader和App ClassLoader类加载器，也就是说： Bootstrap Loader会在JVM启动之后载入，之后它会载入ExtClassLoader并将ExtClassLoader的parent设为Bootstrap Loader，然后BootstrapLoader再加载AppClassLoader，并将AppClassLoader的parent设定为 ExtClassLoader。 ClassLoader加载类的原理双亲委托加载模式 我们知道除了顶级的 Bootstrap Loader他的parent属性为null之外，其他的两个或者自定义的类加载器都是存在parent 的。 当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader 进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象 为什么要使用双亲委托这种模型呢 java是一门具有很高的安全性的语言，使用这种加载策略的目的是为了：防止重载，父类如果已经找到了需要的类并加载到了内存，那么子类加载器就不需要再去加载该类。安全性问题。两个原因 JVM在搜索类的时候，又是如何判定两个class是相同的 类名是否相同 否由同一个类加载器实例加载 看一个例子public class TestClassLoader&#123; public static void main(String[] args) &#123; ClassLoader loader = TestClassLoader.class.getClassLoader(); //获得加载ClassLoaderTest.class这个类的类加载器 while(loader != null) &#123; System.out.println(loader); loader = loader.getParent(); //获得父类加载器的引用 &#125; System.out.println(loader); &#125;&#125; 输出 sun.misc.Launcher$AppClassLoader@2a139a55sun.misc.Launcher$ExtClassLoader@7852e922null 结论 第一行结果说明：ClassLoaderTest的类加载器是AppClassLoader。 第二行结果说明：AppClassLoader的类加器是ExtClassLoader，即parent=ExtClassLoader。 第三行结果说明：ExtClassLoader的类加器是Bootstrap ClassLoader，因为Bootstrap ClassLoader不是一个普通的Java类，所以ExtClassLoader的parent=null，所以第三行的打印结果为null就是这个原因。 测试2 将ClassLoaderTest.class打包成ClassLoaderTest.jar，放到Extension ClassLoader的加载目录下（JAVA_HOME/jre/lib/ext），然后重新运行这个程序，得到的结果会是什么样呢？ 输出 sun.misc.Launcher$ExtClassLoader@7852e922null 结果分析： 为什么第一行的结果是ExtClassLoader呢？ 因为ClassLoader的委托模型机制，当我们要用ClassLoaderTest.class这个类的时候，AppClassLoader在试图加载之前，先委托给Bootstrcp ClassLoader，Bootstracp ClassLoader发现自己没找到，它就告诉ExtClassLoader，兄弟，我这里没有这个类，你去加载看看，然后Extension ClassLoader拿着这个类去它指定的类路径（JAVA_HOME/jre/lib/ext）试图加载，唉，它发现在ClassLoaderTest.jar这样一个文件中包含ClassLoaderTest.class这样的一个文件，然后它把找到的这个类加载到内存当中，并生成这个类的Class实例对象，最后把这个实例返回。所以ClassLoaderTest.class的类加载器是ExtClassLoader。 第二行的结果为null，是因为ExtClassLoader的父类加载器是Bootstrap ClassLoader。 测试3: 用Bootstrcp ClassLoader来加载ClassLoaderTest.class，有两种方式：1、在jvm中添加-Xbootclasspath参数，指定Bootstrcp ClassLoader加载类的路径，并追加我们自已的jar（ClassTestLoader.jar）2、将class文件放到JAVA_HOME/jre/classes/目录下（上面有提到）(将ClassLoaderTest.jar解压后，放到JAVA_HOME/jre/classes目录下，如下图所示：)提示：jre目录下默认没有classes目录，需要自己手动创建一个提供者：Java团长 自定义ClassLoader前言 实现自定义类加载的目的是，假设我们的类他不是存在特定的位置，可能是某个磁盘或者某个远程服务器上面，那么我们就需要自定义类加载器去加载这些类。 继承继承java.lang.ClassLoader 重写父类的findClass方法 在findClass()方法中调用defineClass()。 这个方法在编写自定义classloader的时候非常重要，它能将class二进制内容转换成Class对象，如果不符合要求的会抛出各种异常 注意： 一个ClassLoader创建时如果没有指定parent，那么它的parent默认就是AppClassLoader。 为什么不去重定义loadClass方法呢？其实也可以，但是loadClass方法内部已经实现了搜索类的策略。除非你是非常熟悉否则还是不建议这样去做。这里建议重载findClass方法，因为在loadClass中最后会去调用findClass方法去加载类。而且这个方法内部默认是空的。 分析loadClass方法源码/*** A class loader is an object that is responsible for loading classes. The class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to locate or generate data that constitutes a definition for the class. A typical strategy is to transform the name into a file name and then read a &quot;class file&quot; of that name from a file system.**/ 大致意思如下：class loader是一个负责加载classes的对象，ClassLoader类是一个抽象类，需要给出类的二进制名称，class loader尝试定位或者产生一个class的数据，一个典型的策略是把二进制名字转换成文件名然后到文件系统中找到该文件。 protected synchronized Class loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; // 首先检查该name指定的class是否有被加载 Class c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; //如果parent不为null，则调用parent的loadClass进行加载 c = parent.loadClass(name, false); &#125;else&#123; //parent为null，则调用BootstrapClassLoader进行加载 c = findBootstrapClass0(name); &#125; &#125;catch(ClassNotFoundException e) &#123; //如果仍然无法加载成功，则调用自身的findClass进行加载 c = findClass(name); // &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; 自定义类加载器package com.kingge.com;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.InputStream;import java.net.URL;public class PersonalClassLoader extends ClassLoader&#123; private String rootUrl; public PersonalClassLoader(String rootUrl) &#123; this.rootUrl = rootUrl; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; Class clazz = null;//this.findLoadedClass(name); // 父类已加载 //if (clazz == null) &#123; //检查该类是否已被加载过 byte[] classData = getClassData(name); //根据类的二进制名称,获得该class文件的字节码数组 if (classData == null) &#123; throw new ClassNotFoundException(); &#125; clazz = defineClass(name, classData, 0, classData.length); //将class的字节码数组转换成Class类的实例 //ClassLoader内置方法 /* * Converts an array of bytes into an instance of class * Before the &lt;tt&gt;Class&lt;/tt&gt; can be used it must be resolved.*/ //&#125; return clazz; &#125; private byte[] getClassData(String name) &#123; InputStream is = null; try &#123; String path = classNameToPath(name); URL url = new URL(path); byte[] buff = new byte[1024*4]; int len = -1; is = url.openStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); while((len = is.read(buff)) != -1) &#123; baos.write(buff,0,len); &#125; return baos.toByteArray(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (is != null) &#123; try &#123; is.close(); &#125; catch(IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return null; &#125; private String classNameToPath(String name) &#123; return rootUrl + &quot;/&quot; + name.replace(&quot;.&quot;, &quot;/&quot;) + &quot;.class&quot;; &#125;&#125; 测试类： package com.kingge.com;public class ClassLoaderTest&#123; public static void main(String[] args) &#123; try &#123; /*ClassLoader loader = ClassLoaderTest.class.getClassLoader(); //获得ClassLoaderTest这个类的类加载器 while(loader != null) &#123; System.out.println(loader); loader = loader.getParent(); //获得父加载器的引用 &#125; System.out.println(loader);*/ String rootUrl = &quot;http://localhost:8080/console/res&quot;; PersonalClassLoader networkClassLoader = new PersonalClassLoader(rootUrl); String classname = &quot;HelloWorld&quot;; Class clazz = networkClassLoader.loadClass(classname); System.out.println(clazz.getClassLoader()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出： com.kingge.com.PersonalClassLoader@65b54208 其中HelloWorld.class文件的位置在于： 其实很多服务器都自定义了类加载器 用于加载web应用指定目录下的类库（jar或class），如：Weblogic、Jboss、tomcat等，下面我以Tomcat为例，展示该web容器都定义了哪些个类加载器： 下面以tomcat为例子 1、新建一个web工程httpweb 2、新建一个ClassLoaderServletTest，用于打印web容器中的ClassLoader层次结构 一下代码来自网上：import java.io.IOException; import java.io.PrintWriter; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; public class ClassLoaderServletTest extends HttpServlet &#123; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.setContentType(&quot;text/html&quot;); PrintWriter out = response.getWriter(); ClassLoader loader = this.getClass().getClassLoader(); while(loader != null) &#123; out.write(loader.getClass().getName()+&quot;&lt;br/&gt;&quot;); loader = loader.getParent(); &#125; out.write(String.valueOf(loader)); out.flush(); out.close(); &#125; public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125; &#125; 3、配置Servlet，并启动服务 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app version=&quot;2.4&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;&gt; &lt;servlet&gt; &lt;servlet-name&gt;ClassLoaderServletTest&lt;/servlet-name&gt; &lt;servlet-class&gt;ClassLoaderServletTest&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ClassLoaderServletTest&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/ClassLoaderServletTest&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;/web-app&gt; 运行截图： 总结 这种自定义的方式目的就是为了，能够控制类的加载流程，那么这种远程加载类的方式类似于我们常用的Hessian 来访问多个系统获取类 好的网站http://blog.csdn.net/briblue/article/details/54973413","categories":[{"name":"java深入理解","slug":"java深入理解","permalink":"http://kingge.top/categories/java深入理解/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"ClassLoader","slug":"ClassLoader","permalink":"http://kingge.top/tags/ClassLoader/"},{"name":"类加载器","slug":"类加载器","permalink":"http://kingge.top/tags/类加载器/"},{"name":"自定义类加载器","slug":"自定义类加载器","permalink":"http://kingge.top/tags/自定义类加载器/"},{"name":"类加载器源码分析","slug":"类加载器源码分析","permalink":"http://kingge.top/tags/类加载器源码分析/"}]},{"title":"为什么毕业后选择留在小城市","slug":"为什么毕业后选择留在小城市","date":"2017-08-21T06:13:44.000Z","updated":"2017-08-23T01:12:42.498Z","comments":true,"path":"2017/08/21/为什么毕业后选择留在小城市/","link":"","permalink":"http://kingge.top/2017/08/21/为什么毕业后选择留在小城市/","excerpt":"","text":"前言经常看到大学要毕业的学生，会有一种纠结感。越是临近毕业，这种感觉就越是强烈这种感觉实际上就是一种选择恐惧症，又或者说是小城综合征。他们对于毕业后究竟是选择去一线城市就业还是去二三线城市就业，产生了很大的选择恐惧。 为什么会产生这种心理大致的原因有那么几个： 身边的认识的人，都是选择回到自己的家乡进行就业，自己收到了影响。 大城市的生活节奏会比小城市更加的紧凑，你会很忙。 父母希望自己回去，离家近的地方工作。 恋人不跟随自己，她或他选择了回到了故乡就业，自己左右为难。 有些人已经在大城市实习过，对于大城市已经厌倦。 作者的选择 本人就是属于最后一种，大三的时候去的深圳实习，在一家IT互联网公司上班，加班很多，虽然加班这种现象在深圳是一种常态。但是每天加班到晚上一点多，也是很累，所以毕业后也就选择回到了自己的家乡就业。 回到了","categories":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/categories/心情/"}],"tags":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/tags/心情/"},{"name":"总结","slug":"总结","permalink":"http://kingge.top/tags/总结/"},{"name":"有感","slug":"有感","permalink":"http://kingge.top/tags/有感/"}]},{"title":"max-allowed-packet的问题","slug":"max-allowed-packet的问题","date":"2017-08-17T02:37:15.000Z","updated":"2017-08-17T09:44:39.015Z","comments":true,"path":"2017/08/17/max-allowed-packet的问题/","link":"","permalink":"http://kingge.top/2017/08/17/max-allowed-packet的问题/","excerpt":"","text":"前言 近期，因启动项目有个批量插入的sql结果太大，超过了mysql自带的缓存，报了这个错误 修改： 定位到mysql的安装目录下面，然后修改my.ini 的max_allowed_packet = 8M默认是1M","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"异常","slug":"异常","permalink":"http://kingge.top/tags/异常/"},{"name":"sql异常","slug":"sql异常","permalink":"http://kingge.top/tags/sql异常/"}]},{"title":"YUM仓库配置","slug":"YUM仓库配置","date":"2017-08-06T14:12:30.000Z","updated":"2019-06-02T06:17:59.046Z","comments":true,"path":"2017/08/06/YUM仓库配置/","link":"","permalink":"http://kingge.top/2017/08/06/YUM仓库配置/","excerpt":"","text":"1.1 概述YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。-需要联网 1.2 为什么要制作本地YUM源YUM源虽然可以简化我们在Linux上安装软件的过程，但是生产环境通常无法上网，不能连接外网的YUM源，说以就无法使用yum命令安装软件了。为了在内网中也可以使用yum安装相关的软件，就要配置yum源。 YUM源其实就是一个保存了多个RPM包的服务器，可以通过http的方式来检索、下载并安装相关的RPM包。 1.3 yum的常用命令1）基本语法： yum install -y rpm软件包 （功能描述：安装httpd并确认安装） yum list （功能描述：列出所有可用的package和package组） yum clean all （功能描述：清除所有缓冲数据） yum deplist rpm软件包 （功能描述：列出一个包所有依赖的包） yum remove rpm软件包 （功能描述：删除httpd） 2）案例实操 ​ yum install -y tree //安装文档树结构展示插件 1.4 关联网络yum源1）前期文件准备 （1）前提条件linux系统必须可以联网 （2）在Linux环境中访问该网络地址：http://mirrors.163.com/.help/centos.html，在使用说明中点击CentOS6-&gt;再点击保存 （3）查看文件保存的位置 在打开的终端中输入如下命令，就可以找到文件的保存位置。 [kingge@hadoop101 下载]$ pwd /home/kingge/下载 2）替换本地yum文件 ​ （1）把下载的文件移动到/etc/yum.repos.d/目录 [root@hadoop101 下载]# mv CentOS6-Base-163.repo /etc/yum.repos.d/ ​ （2）进入到/etc/yum.repos.d/目录 [root@hadoop101 yum.repos.d]# pwd /etc/yum.repos.d ​ （3）用CentOS6-Base-163.repo替换CentOS-Base.rep [root@hadoop101 yum.repos.d]# mv CentOS6-Base-163.repo CentOS-Base.rep 3）安装命令 ​ （1）[root@hadoop101 yum.repos.d]#yum clean all ​ （2）[root@hadoop101 yum.repos.d]#yum makecache ​ （3）[root@hadoop101 yum.repos.d]# yum install -y createrepo （4）[root@hadoop101 yum.repos.d]#yum install -y httpd 1.5 制作本地yum源1.5.1 制作只有本机能访问的本地YUM源（1）准备一台Linux服务器，版本CentOS-6.8-x86_64-bin-DVD1.iso （2）配置好这台服务器的IP地址 （3）将CentOS-6.8-x86_64-bin-DVD1.iso镜像挂载到/mnt/cdrom目录 [root@hadoop101 /]# mkdir /mnt/cdrom [root@hadoop101 /]# mount -t iso9660 /dev/cdrom /mnt/cdrom （4）安装相应的软件 [root@hadoop101 yum.repos.d]#yum install -y httpd ​ （5）启动httpd服务 [root@hadoop101 yum.repos.d]#service httpd start （6）使用浏览器访问http://192.168.1.101:80（如果访问不通，检查防火墙是否开启了80端口或关闭防火墙），测试网络是否畅通 （7）将YUM源配置到httpd（Apache Server）中 [root@hadoop101 html]# mkdir Packages [root@hadoop101 html]# chown kingge:kingge Packages/ [root@hadoop101 html]# cp -r /mnt/cdrom/Packages/* /var/www/html/Packages/ （8）执行创建仓库命令：createrepo 路径 [root@hadoop101 Packages]# createrepo ./ （9）修改本机上的YUM源配置文件，将源指向自己 备份原有的YUM源的配置文件 [root@hadoop101 /]# cd /etc/yum.repos.d/ [root@hadoop101 yum.repos.d]# cp CentOS-Base.repo CentOS-Base.repo.bak ​ 编辑CentOS-Base.repo文件 [root@hadoop101 yum.repos.d]# vi CentOS-Base.repo [base] name=CentOS-Local baseurl=file:///var/www/html/Packages gpgcheck=0 enabled=1 #增加改行，使能 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 添加上面内容保存退出 （10）清除YUM缓存 [root@hadoop101 yum.repos.d]# yum clean all （11）列出可用的YUM仓库 [root@hadoop101 yum.repos.d]# yum repolist （12）安装相应的软件 [root@hadoop101 yum.repos.d]# yum install -y tree [root@hadoop101 Packages]# yum install -y firefox-45.0.1-1.el6.centos.x86_64.rpm 1.5.2 制作其他主机通过网络能访问的本地YUM源前期准备：检查yum源服务器的httpd服务是否启动： ​ Ps aux | grep httpd –查看该进程是否存在 或者 netstat –anp | grep 80 直接查看是否监听80端口 ​ 启动 httpd服务： service httpd start/restart 启动后，可能使用yum源的服务器访问不到yum仓库（例如下面的101服务器访问不到102服务器的yum），那么有可能是101服务器防火墙屏蔽了80端口，应该设置一下防火墙规则 （1）让其他需要安装RPM包的服务器指向这个YUM源，准备一台新的服务器，备份或删除原有的YUM源配置文件 备份原有的YUM源的配置文件 [root@hadoop102 /]#cd /etc/yum.repos.d/ [root@hadoop102 yum.repos.d]# cp CentOS-Base.repo CentOS-Base.repo.bak ​ 编辑CentOS-Base.repo文件 [root@hadoop102 yum.repos.d]# vi CentOS-Base.repo [base] name=CentOS-hadoop101 baseurl=http://192.168.1.101/Packages gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 添加上面内容保存退出 （2）在这台新的服务器上执行YUM的命令 [root@hadoop102 yum.repos.d]# yum clean all [root@hadoop102 yum.repos.d]# yum repolist （3）安装软件 [root@hadoop102 yum.repos.d]# yum install -y httpd","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"Shell编程","slug":"Shell编程","date":"2017-08-05T16:00:00.000Z","updated":"2019-06-02T06:10:23.295Z","comments":true,"path":"2017/08/06/Shell编程/","link":"","permalink":"http://kingge.top/2017/08/06/Shell编程/","excerpt":"","text":"一、引言因为公司遜要开发一款saas系统，需要使用自动部署脚本，部署web项目，所以就去了解了一下相关的知识，这里并不做深入的研究。 1.1 概述Shell是一个命令行解释器，它为用户提供了一个向Linux内核发送请求以便运行程序的界面系统级程序，用户可以用Shell来启动、挂起、停止甚至是编写一些程序。 Shell还是一个功能相当强大的编程语言，易编写、易调试、灵活性强。Shell是解释执行的脚本语言，在Shell中可以调用Linux系统命令。 1.2 shell脚本的执行方式1）echo输出内容到控制台 ​ （1）基本语法：​ echo [选项] [输出内容] 选项： -e： 支持反斜线控制的字符转换 控制字符 作 用 \\ 输出\\本身 \\a 输出警告音 \\b 退格键，也就是向左删除键 \\c 取消输出行末的换行符。和“-n”选项一致 \\e ESCAPE键 \\f 换页符 \\n 换行符 \\r 回车键 \\t 制表符，也就是Tab键 \\v 垂直制表符 \\0nnn 按照八进制ASCII码表输出字符。其中0为数字零，nnn是三位八进制数 \\xhh 按照十六进制ASCII码表输出字符。其中hh是两位十六进制数 ​（2）案例 ​ [kingge@hadoop101 sbin]$ echo &quot;helloworld&quot; helloworld 2）脚本格式 （1）脚本以 #!/bin/bash 开头（2）脚本必须有可执行权限 3）第一个Shell脚本 （1）需求：创建一个Shell脚本，输出helloworld （2）实操： [kingge@hadoop101 datas]$ touch helloworld.sh [kingge@hadoop101 datas]$ vi helloworld.sh ​ 在helloworld.sh中输入如下内容 #!**/**bin**/**bash echo &quot;helloworld&quot; 4）脚本的常用执行方式 第一种：输入脚本的绝对路径或相对路径 （1）首先要赋予helloworld.sh 脚本的+x权限 [kingge@hadoop101 datas]$ chmod 777 helloworld.sh （2）执行脚本 ​ /root/helloWorld.sh ​ ./helloWorld.sh 第二种：bash或sh+脚本（不用赋予脚本+x权限） ​ sh /root/helloWorld.sh ​ sh helloWorld.sh 1.3 shell中的变量1）Linux Shell中的变量分为，系统变量和用户自定义变量。 2）系统变量：$HOME、$PWD、$SHELL、$USER等等 3）显示当前shell中所有变量：set 1.3.1 定义变量1）基本语法： ​ （1）定义变量：变量=值 （2）撤销变量：unset 变量 （3）声明静态变量：readonly变量，注意：不能unset 2）变量定义规则 ​ （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头。 ​ （2）等号两侧不能有空格 ​ （3）变量名称一般习惯为大写 3）案例 ​ （1）定义变量A A=8 ​ （2）撤销变量A unset A ​ （3）声明静态的变量B=2，不能unset readonly B=2 ​ （4）可把变量提升为全局环境变量，可供其他shell程序使用 export 变量名 1.3.2 将命令的返回值赋给变量​ （1）A=ls -la 反引号，运行里面的命令，并把结果返回给变量A ​ （2）A=$(ls -la) 等价于反引号 1.3.3 设置环境变量1）基本语法： ​ （1）export 变量名=变量值 （功能描述：设置环境变量的值） （2）source 配置文件 （功能描述：让修改后的配置信息立即生效） （3）echo $变量名 （功能描述：查询环境变量的值） 2）案例： ​ （1）在/etc/profile文件中定义JAVA_HOME环境变量 ​ export JAVA_HOME=/opt/module/jdk1.7.0_79 ​ export PATH=$PATH:$JAVA_HOME/bin （2）查看环境变量JAVA_HOME的值 ​ [kingge@hadoop101 datas]$ echo $JAVA_HOME /opt/module/jdk1.7.0_79 1.3.4 位置参数变量1）基本语法 ​ $n （功能描述：n为数字，$0代表命令本身，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如$&#123;10&#125;）$* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）$@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）$# （功能描述：这个变量代表命令行中所有参数的个数） 2）案例 （1）输出输入的的参数1，参数2，所有参数，参数个数 #!/bin/bashecho &quot;$0 $1 $2&quot;echo &quot;$*&quot;echo &quot;$@&quot;echo &quot;$#&quot; （2）$*与$@的区别 #!/bin/bash for i in &quot;$*&quot; #$*中的所有参数看成是一个整体，所以这个for循环只会循环一次 do echo &quot;The parameters is: $i&quot; done x=1 for y in &quot;$@&quot; #$@中的每个参数都看成是独立的，所以“$@”中有几个参数，就会循环几次 do echo &quot;The parameter$x is: $y&quot; x=$(( $x +1 )) done a）$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数 b）当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“​$1 $2 …​$n”的形式输出所有参数；“$@”会将各个参数分开，以“​$1” “$2”…”​$n”的形式输出所有参数 1.3.5 预定义变量1）基本语法： （1）“$((运算式))”或“$[运算式]”（2）expr m + n 注意expr运算符间要有空格（3）expr m - n（4）expr \\*, /, % 乘，除，取余 2）案例 计算（2+3）X4的值 （1）采用$[运算式]方式[root@hadoop101 datas]# S=$[(2+3)*4] [root@hadoop101 datas]# echo $S （2）expr分布计算 S=`expr 2 + 3` expr $S \\* 4 （3）expr一步完成计算 expr `expr 2 + 3` \\* 4 1.4 运算符1）基本语法： （1）“$((运算式))”或“$[运算式]” （2）expr m + n 注意expr运算符间要有空格 （3）expr m - n （4）expr *, /, % 乘，除，取余 2）案例：计算（2+3）X4的值 ​ （1）采用$[运算式]方式 ​ [root@hadoop101 datas]# S=$[(2+3)*4] ​ [root@hadoop101 datas]# echo $S ​ （2）expr分布计算 ​ S=expr 2 + 3 ​ expr $S * 4 ​ （3）expr一步完成计算 ​ expr expr 2 + 3 * 4 1.5 条件判断1.5.1 判断语句1）基本语法： [ condition ]（注意condition前后要有空格） #非空返回true，可使用$?验证（0为true，&gt;1为false） 2）案例： [kingge] 返回true [] 返回false [condition] &amp;&amp; echo OK || echo notok 条件满足，执行后面的语句 1.5.2 常用判断条件1）两个整数之间比较 = 字符串比较 -lt 小于 -le 小于等于 -eq 等于 -gt 大于 -ge 大于等于 -ne 不等于 2）按照文件权限进行判断 -r 有读的权限 -w 有写的权限 -x 有执行的权限 3）按照文件类型进行判断 -f 文件存在并且是一个常规的文件 -e 文件存在 -d 文件存在并是一个目录 4）案例 ​ （1）23是否大于等于22 ​ [ 23 -ge 22 ] ​ （2）student.txt是否具有写权限 ​ [ -w student.txt ] ​ （3）/root/install.log目录中的文件是否存在 ​ [ -e /root/install.log ] 1.6 流程控制1.6.1 if判断1）基本语法： if [ 条件判断式 ];then 程序 fi 或者 if [ 条件判断式 ] then ​ 程序 fi ​ 注意事项：（1）[ 条件判断式 ]，中括号和条件判断式之间必须有空格 2）案例 #!/bin/bashif [ $1 -eq &quot;123&quot; ]then echo &quot;123&quot;elif [ $1 -eq &quot;456&quot; ]then echo &quot;456&quot;fi 1.6.2 case语句1）基本语法： case $变量名 in “值1”） ​ 如果变量的值等于值1，则执行程序1 ​ ;; “值2”） ​ 如果变量的值等于值2，则执行程序2 ​ ;; …省略其他分支… *） ​ 如果变量的值都不是以上的值，则执行此程序 ​ ;; esac 2）案例 !/bin/bashcase $1 in&quot;1&quot;) echo &quot;1&quot;;;&quot;2&quot;) echo &quot;2&quot;;;*) echo &quot;other&quot;;;esac 1.6.3 for循环1）基本语法1： for 变量 in 值1 值2 值3… do ​ 程序 done 2）案例： ​ （1）打印输入参数 #!/bin/bash#打印数字for i in &quot;$*&quot; do echo &quot;The num is $i &quot; donefor j in &quot;$@&quot; do echo &quot;The num is $j&quot; done 3）基本语法2： ​ for (( 初始值;循环控制条件;变量变化 )) do ​ 程序 done 4）案例 （1）从1加到100 #!/bin/bashs=0for((i=0;i&lt;=100;i++))do s=$[$s+$i]doneecho &quot;$s&quot; 1.6.4 while循环1）基本语法： while [ 条件判断式 ] do ​ 程序 done 2）案例 ​ （1）从1加到100 #!/bin/bashs=0i=1while [ $i -le 100 ]do s=$[$s+$i] i=$[$i+1]doneecho $s 1.7 read读取控制台输入1）基本语法： ​ read(选项)(参数) ​ 选项： -p：指定读取值时的提示符； -t：指定读取值时等待的时间（秒）。 参数 ​ 变量：指定读取值的变量名 2）案例 ​ 读取控制台输入的名称 #!/bin/bashread -t 7 -p &quot;please 7 miao input your name &quot; NAMEecho $NAME 1.8 函数1.8.1 系统函数1）basename基本语法 basename [pathname] [suffix] basename [string] [suffix] （功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。 选项： suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。 2）案例 [kingge@hadoop101 opt]$ basename /opt/test.txt test.txt [kingge@hadoop101 opt]$ basename /opt/test.txt .txt test 3）dirname基本语法 ​ dirname 文件绝对路径 （功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）） 4）案例 ​ [kingge@hadoop101 opt]$ dirname /opt/test.txt /opt 1.8.2 自定义函数1）基本语法： [ function ] funname[()]&#123; Action; [return int;]&#125; funname 注意： ​ （1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。 ​ （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255) 2）案例 ​ （1）计算输入参数的和 #!/bin/bashfunction sum()&#123; s=0 s=$[ $1 + $2 ] echo &quot;$s&quot;&#125;read -p &quot;Please input the number1: &quot; n1;read -p &quot;Please input the number2: &quot; n2;sum $n1 $n2;","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"实现多数据量的导入数据库","slug":"实现多数据量的导入数据库","date":"2017-08-01T06:37:15.000Z","updated":"2017-08-17T08:21:15.695Z","comments":true,"path":"2017/08/01/实现多数据量的导入数据库/","link":"","permalink":"http://kingge.top/2017/08/01/实现多数据量的导入数据库/","excerpt":"","text":"引言 在做一个项目的时候，涉及到需要从一个表格中获取百万条数据然后插入到数据库中，最后采用JDBC的executeBantch方法实现这个功能。 采取的策略 尽量关闭字段索引（因为再插入数据的时候还是需要维护索引的，在创建索引和维护索引 会耗费时间,随着数据量的增加而增加，可以在插入数据后再去为字段创建索引） 虽然索引可以提高查询速度但是，插入数据的时候会导致索性的更新。索性越多，插入会越慢。可以看文档描述:Although it can be tempting to create an indexes for every possible column used in a query, unnecessary indexes waste space and waste time for MySQL to determine which indexes to use. Indexes also add to the cost of inserts, updates, and deletes because each index must be updated. You must find the right balance to achieve fast queries using the optimal set of indexes. 分批次提交数据 在分布式条件下，还可以考虑在不同的数据库结点提交，有底层的消息系统完成数据扩展 过滤预处理数据 预处理数据的场景：为了避免插入的数据（假设ListA）跟数据库中某些数据重复，那么我们会把要插入的数据去数据库中查询是否已经存在，获得返回的已经存在数据（ListB）。然后在插入数据的时候判断当前数据是否在ListB中，那么当前数据不能够插入数据库。过滤出来，最后得到一个可以插入数据库的ListC 代码关键代码/*数据分析结束*/ /*往数据库写数据开始*/ Connection conn=null; PreparedStatement idsUserAdd=null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\") ; conn = DriverManager.getConnection(ConfigTool.getProperty(\"jdbc.url\").toString() , ConfigTool.getProperty(\"jdbc.username\").toString() , ConfigTool.getProperty(\"jdbc.password\").toString()); conn.setAutoCommit(false); //构造预处理statement idsUserAdd = conn.prepareStatement(\"INSERT INTO dc_matedata (\"+ \" ID,`NAME`, DATATYPE,`CODE`,TYPE_ID,`LENGTH`, \"+ \" DATANAME, VALUEAREA,`RESTRICT`, REMARK,MD_DATE)\"+ \" values(?,?,?,?,?,?,?,?,?,?,now())\"); //最大列表的数目当做循环次数 int xhcs=addMetadataList.size();//addMetadataList需要插入的数据 for(int i=0;i&lt;xhcs;i++)&#123; idsUserAdd.setString(1,addMetadataList.get(i).get(\"id\").toString()); idsUserAdd.setString(2,addMetadataList.get(i).get(\"name\").toString()); idsUserAdd.setString(3,addMetadataList.get(i).get(\"dataType\").toString()); idsUserAdd.setString(4,addMetadataList.get(i).get(\"code\").toString()); idsUserAdd.setString(5,addMetadataList.get(i).get(\"typeId\").toString()); idsUserAdd.setString(6,addMetadataList.get(i).get(\"dataLength\").toString()); idsUserAdd.setString(7,addMetadataList.get(i).get(\"dataName\").toString()); idsUserAdd.setString(8,addMetadataList.get(i).get(\"valueArea\").toString()); idsUserAdd.setString(9,addMetadataList.get(i).get(\"dataRestrict\").toString()); idsUserAdd.setString(10,addMetadataList.get(i).get(\"dataRemark\").toString()); idsUserAdd.addBatch(); //每10000次提交一次 if(i%10000==0||i==xhcs-1)&#123;//可以设置不同的大小；如50，100，500，1000等等 i==xhcs-1（最后一次） idsUserAdd.executeBatch(); conn.commit(); idsUserAdd.clearBatch(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw e; &#125;finally &#123; try &#123; if(idsUserAdd!=null) idsUserAdd.close(); if(conn!=null) conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); throw e; &#125; &#125; /*往数据库写数据结束*/ 完整代码/** * 校验需要导入的元数据信息，封装错误信息并批量插入数据库 */ @Override public List&lt;Map&lt;String, Object&gt;&gt; saveDCMetadataBatch(List&lt;Map&lt;String, Object&gt;&gt; list, boolean valid, boolean addError) throws Exception&#123; List&lt;Map&lt;String,Object&gt;&gt; errorList=new ArrayList&lt;Map&lt;String,Object&gt;&gt;();//获得不能够添加成功的数据 Map&lt;String,Object&gt; map=new HashMap&lt;String,Object&gt;();//查询条件 Map&lt;String,String&gt; codeMap=new HashMap&lt;String,String&gt;();//每个分类对应的元数据的编号最大值 Map&lt;String,Object&gt; metaName=new HashMap&lt;String,Object&gt;();//查询数据库中是否存在相同的数据（这里校验的是：元数据的中文简称） Map&lt;String,Object&gt; metaDataName=new HashMap&lt;String,Object&gt;();//查询数据库中是否存在相同的数据（这里校验的是：元数据的数据项名称） map.put(\"metaName\",list);//需要查询的元数据中文名称 map.put(\"metaDataTypeId\",list);//导入的元数据的编号 List&lt;Map&lt;String, Object&gt;&gt; metaExistList = dCMatedataDao.getDCMetadata(map);//根据元数据名称查询当前分类下是否存在同样元数据 map.put(\"metaName\",null);//置空 map.put(\"metaDataName\",list); List&lt;Map&lt;String, Object&gt;&gt; metaExistListTwo = dCMatedataDao.getDCMetadata(map);//根据元数据数据项名称查询存在的元数据 //保存重复的信息 for(int i=0;i&lt;metaExistList.size();i++) metaName.put(metaExistList.get(i).get(\"name\").toString()+metaExistList.get(i).get(\"code\").toString() ,metaExistList.get(i).get(\"id\"));//添加父类的编号为后缀-唯一性保证 for(int i=0;i&lt;metaExistListTwo.size();i++) metaDataName.put(metaExistListTwo.get(i).get(\"dataname\").toString()+metaExistListTwo.get(i).get(\"code\").toString(), metaExistListTwo.get(i).get(\"id\")); /*整理出来的数据-开始*/ List&lt;Map&lt;String,Object&gt;&gt; addMetadataList=new ArrayList&lt;Map&lt;String,Object&gt;&gt;(); /*整理出来的数据-结束*/ for (int i = 0; i &lt; list.size(); i++) &#123; Map&lt;String, Object&gt; MetadataObj = list.get(i); try &#123; String metadatId = StringUtil.getUUID();//元数据id /*校验开始*/ if (valid)&#123; if(validUser(MetadataObj,\"name\",addError)!=null)&#123;//验证输入的数据是否符合格式和必填。 errorList.add(MetadataObj); continue; &#125; &#125; /*前端校验结束*/ /*校验是否存在同名的元数据*/ String dataCodeCheck = MetadataObj.get(\"dataCode\").toString().trim(); //元数据父分类编号 String name = MetadataObj.get(\"name\").toString().trim();//元数据中文简称 if (metaName.containsKey(name+dataCodeCheck)) &#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"中文简称已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; /*校验是否存在相同数据项的元数据*/ String dataName = MetadataObj.get(\"dataName\").toString().trim();//数据项名 if (metaDataName.containsKey(dataName+dataCodeCheck)) &#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"数据项名已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; String dataCode = MetadataObj.get(\"dataCode\").toString().trim(); //元数据父分类编号 List&lt;Map&lt;String, Object&gt;&gt; footCount = dCMatedataDao.getFootCount(dataCode); if( footCount.size() &gt; 0)&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"分类编码不是最后一级分类\"); &#125; errorList.add(MetadataObj); continue; &#125; Map&lt;String, Object&gt; typeByCode = dCMatedataDao.getMetadataTypeByCode(dataCode); if( typeByCode == null || typeByCode.size() &lt; 1)&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"分类编码不存在，请先添加分类\"); &#125; errorList.add(MetadataObj); continue; &#125; //校验是在添加的List中是否存在相同的数据项名或者中文简称 //校验导入文件中是否存在一样的中文简称或者数据项名 boolean nameExist = false; boolean dataNameExist = false; for (int j = 0; j &lt; addMetadataList.size(); j++)&#123; Map&lt;String, Object&gt; map2 = addMetadataList.get(j); String typeId = map2.get(\"typeId\").toString(); String nameE = map2.get(\"name\").toString(); String dataNameE = map2.get(\"dataName\").toString(); if( typeId.equals(typeByCode.get(\"id\").toString()) &amp;&amp; nameE.equals(name))&#123; nameExist=true; break; &#125; if( typeId.equals(typeByCode.get(\"id\").toString()) &amp;&amp; dataNameE.equals(dataName))&#123; dataNameExist=true; break; &#125; &#125; if( nameExist )&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"中文简称已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; if( dataNameExist )&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"数据项名已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; //进入这里说明校验结束，开始填充添加的数据 String type_id = typeByCode.get(\"id\").toString();//元数据所属分类id String dataType = MetadataObj.get(\"dataType\").toString().trim(); //元数据类型 String dataLength = MetadataObj.get(\"dataLength\").toString().trim(); //元数据长度 String code = \"\"; //// if( codeMap.get(dataCode) == null||StringUtil.isEmpty(codeMap.get(dataCode)) )&#123;//表示当前分类不存在已经添加的元数据--因为编码map中不存在对应分类的最大编码 Map maxCodeByPid = this.selectMetadataMaxCode(type_id); if( maxCodeByPid == null )&#123;//表示当前分类下不存在任何子分类 code = StringUtil.getCode(\"0\", dataCode);//则从01开始编号 codeMap.put(dataCode, \"01\");//保存当前分类下元数据编号最大值 &#125;else&#123; String object = (String) maxCodeByPid.get(\"codeNum\");//当前分类节点下的元数据的编号最大值。 int pSituation = object.indexOf(dataCode); int pLength = pSituation+dataCode.length() ; String substring = object.substring(pLength); //截取出最大编号值得最大值 code = StringUtil.getCode(substring, dataCode); int temp = Integer.parseInt(substring);//保存当前分类下元数据编号最大值 temp+=1; codeMap.put(dataCode, temp+\"\"); &#125; &#125;else&#123; String maxCode = codeMap.get(dataCode); code = StringUtil.getCode(maxCode, dataCode); //保存当前分类下元数据编号最大值 int temp = Integer.parseInt(maxCode); temp+=1; codeMap.put(dataCode, temp+\"\"); &#125; /// Map&lt;String, Object&gt; metadatList = new LinkedHashMap&lt;String, Object&gt;(); metadatList.put(\"id\", metadatId); metadatList.put(\"name\",name); metadatList.put(\"dataType\",dataType); metadatList.put(\"code\",code); metadatList.put(\"typeId\",type_id); metadatList.put(\"dataLength\",dataLength); metadatList.put(\"dataName\",dataName); metadatList.put(\"valueArea\", MetadataObj.get(\"valueArea\")==null?\"\":MetadataObj.get(\"valueArea\") ); metadatList.put(\"dataRestrict\",MetadataObj.get(\"dataRestrict\")==null?\"\":MetadataObj.get(\"dataRestrict\")); metadatList.put(\"dataRemark\",MetadataObj.get(\"dataRemark\")==null?\"\":MetadataObj.get(\"dataRemark\")); metadatList.put(\"mdDate\",new Date()); addMetadataList.add(metadatList); &#125; catch (Exception e)&#123; if(addError) &#123; MetadataObj.put(\"errInfo\", e.getMessage()); &#125; errorList.add(MetadataObj); &#125; &#125; /*数据分析结束*/ /*往数据库写数据开始*/ Connection conn=null; PreparedStatement idsUserAdd=null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\") ; conn = DriverManager.getConnection(ConfigTool.getProperty(\"jdbc.url\").toString() , ConfigTool.getProperty(\"jdbc.username\").toString() , ConfigTool.getProperty(\"jdbc.password\").toString()); conn.setAutoCommit(false); //构造预处理statement idsUserAdd = conn.prepareStatement(\"INSERT INTO dc_matedata (\"+ \" ID,`NAME`, DATATYPE,`CODE`,TYPE_ID,`LENGTH`, \"+ \" DATANAME, VALUEAREA,`RESTRICT`, REMARK,MD_DATE)\"+ \" values(?,?,?,?,?,?,?,?,?,?,now())\"); //最大列表的数目当做循环次数 int xhcs=addMetadataList.size(); for(int i=0;i&lt;xhcs;i++)&#123; idsUserAdd.setString(1,addMetadataList.get(i).get(\"id\").toString()); idsUserAdd.setString(2,addMetadataList.get(i).get(\"name\").toString()); idsUserAdd.setString(3,addMetadataList.get(i).get(\"dataType\").toString()); idsUserAdd.setString(4,addMetadataList.get(i).get(\"code\").toString()); idsUserAdd.setString(5,addMetadataList.get(i).get(\"typeId\").toString()); idsUserAdd.setString(6,addMetadataList.get(i).get(\"dataLength\").toString()); idsUserAdd.setString(7,addMetadataList.get(i).get(\"dataName\").toString()); idsUserAdd.setString(8,addMetadataList.get(i).get(\"valueArea\").toString()); idsUserAdd.setString(9,addMetadataList.get(i).get(\"dataRestrict\").toString()); idsUserAdd.setString(10,addMetadataList.get(i).get(\"dataRemark\").toString()); idsUserAdd.addBatch(); //每10000次提交一次 if(i%10000==0||i==xhcs-1)&#123;//可以设置不同的大小；如50，100，500，1000等等 idsUserAdd.executeBatch(); conn.commit(); idsUserAdd.clearBatch(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw e; &#125;finally &#123; try &#123; if(idsUserAdd!=null) idsUserAdd.close(); if(conn!=null) conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); throw e; &#125; &#125; /*往数据库写数据结束*/ return errorList; &#125; 总结 有些网友发现使用StringBuffer 来拼接入参，不通过prepareStatement的预处理，虽然前者速度很快，但是使用prepareStatement可以防止SQL注入 有的好的建议大家都可以提出来","categories":[{"name":"JDBC","slug":"JDBC","permalink":"http://kingge.top/categories/JDBC/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"JDBC","slug":"JDBC","permalink":"http://kingge.top/tags/JDBC/"},{"name":"批量导入","slug":"批量导入","permalink":"http://kingge.top/tags/批量导入/"},{"name":"SSM","slug":"SSM","permalink":"http://kingge.top/tags/SSM/"},{"name":"项目经验","slug":"项目经验","permalink":"http://kingge.top/tags/项目经验/"}]},{"title":"Java工程师书单（初级、中级、高级）","slug":"Java工程师书单（初级、中级、高级）","date":"2017-06-21T06:13:44.000Z","updated":"2017-08-24T07:33:21.781Z","comments":true,"path":"2017/06/21/Java工程师书单（初级、中级、高级）/","link":"","permalink":"http://kingge.top/2017/06/21/Java工程师书单（初级、中级、高级）/","excerpt":"","text":"当你的能力承受不住你的欲望，你就应该静下心来读书 初级书籍《编写高质量代码——改善Java程序的151个建议》 这是一本值得入门java的人放在床头的书。此书内容广泛、要点翔实。大多数优秀程序设计书籍都需要看老外写的，但是这本讲述提高java编程水平的书还是不错的，适合具有基本java编程能力的人。对于程序猿而言，工作久了，就感觉编程习惯对一个人很重要。习惯好，不仅工作效率告，而且bug少。这本书对提高个人的好的编程习惯很有帮助。 《Java程序员修炼之道》 此书涵盖了Java7的新特性和Java开发的关键技术，对当前大量开源技术并存，多核处理器、并发以及海量数据给Java开发带来的挑战作出了精辟的分析，提供了实践前沿的深刻洞见，涉及依赖注入、现代并发、类与字节码、性能调优等底层概念的剖析。**书中的道理很浅显，可是对于菜鸟却是至理名言。基本为你勾勒了一个成熟软件程序员专家所需要的所有特性。。 《Java8实战》 没看过。嘻嘻嘻 《有效的单元测试》 此书由敏捷技术实践专家撰写，系统且深入地阐释单元测试用于软件设计的工具、方法、原则和佳实践；深入剖析各种测试常见问题，包含大量实践案例，可操作性强，能为用户高效编写测试提供系统实践指南。**介绍了单元测试的各个方面，TDD、test double、测试的坏味道、可测试的设计等等，每个主题需要深入的话，还需要配合其它书籍和实践，非常适合入门单元测试。书中例子非常全面，看完对使用 Junit 进行单元测试会有一个大的长进，而且用java语言编写，内容很新 《Java核心技术：卷1》 不推荐卷2，因为这个作为初级书单来讲，太难了。 《代码整洁之道》 没看过 《数据结构与算法分析-Java语言描述》 本书是java数据结构与算法方面的三宝之一，除了这三本其他的已经没有意义了。这三宝分别是:**黑宝书《数据结构与算法分析java语言描述》mark allen weiss蓝宝书《java数据结构和算法》robert lafore红宝书《算法》robert sedgewick黑宝书胜在公式推理和证明以及算法的简洁和精炼，此外习题较多。蓝宝书胜在对算法的深入浅出的讲解，演示和举例，让艰涩的理论变得很容易理解。红宝书胜在系出名门斯坦福，演示通俗易懂，内容丰富。有了这三宝，算法不用愁，学完以后再看《算法导论》就容易多了。本书从讲解什么是数据结构开始，延伸至高级数据结构和算法分析，强调数据结构和问题求解技术。本书的目的是从抽象思维和问题求解的观点提供对数据结构的实用介绍，试图包含有关数据结构、算法分析及其Java实现的所有重要的细节 中级书单《重构：改善既有代码的设计》 重构，绝对是写程序过程中最重要的事之一。在写程序之前我们不可能事先了解所有的需求，设计肯定会有考虑不周的地方，而且随着项目需求的修改，也有可能原来的设计已经被改得面目全非了。更何况，我们很少有机会从头到尾完成一个项目，基本上都是接手别人的代码，我们要做的是重构，从小范围的重构开始。**重构是设计,设计是art,重构也是art. 一个函数三行只是语不惊人死不休的说法,是对成百上千行代码的矫枉过正。 更一个般的看法是一个函数应该写在一页纸内。 《Effective Java》 必读 《Java并发编程实战》 没看过： 本书深入浅出地介绍了Java线程和并发，是一本完美的Java并发参考手册。书中从并发性和线程安全性的基本概念出发，介绍了如何使用类库提供的基本并发构建块，用于避免并发危险、构造线程安全的类及验证线程安全的规则，如何将小的线程安全类组合成更大的线程安全类，如何利用线程来提高并发应用程序的吞吐量。**java进阶必看，多线程的最佳书籍。 实战Java高并发程序设计》 没看过 《算法》 没看过 《Head First 设计模式》 这是我看过最幽默最搞笑最亲切同时又让我收获巨大的技术书籍！ 森森的膜拜Freeman(s)！Amen！ 深入浅出，娓娓道来，有的地方能笑死你！写得很有趣，图文并茂，比起四人帮的那本，好懂了不知道多少倍。计算机世界的head first系列基本都是经典。不过只看书学明白设计模式是不可能的，这些只是前人的总结，我们唯有实践实践再实践了。**读这本书不仅仅是学习知识，而是在学习一种思考的方法，学习一种认知的技巧，学习一种成长的阶梯。 总之，用你闲暇的时间来读这本书，并不亚于你专注的工作或学习。笔者强烈推荐此书，要成长为一名高级程序员，设计模式已经是必备技能了。 《Java编程思想》 没看过 高级书单 《深入理解Java虚拟机》 没看过 《Java性能权威指南》 没看过 《深入分析Java Web技术内幕》 没看过 《大型网站系统与Java中间件实践》 没看过 《大型网站技术架构：核心原理与案例分析》 没看过 《企业应用架构模式》 没看过 Spring3.x企业应用开发实战 这本书适合初学者看或者当做一本参考书。对于提高者而言，略看就行 Spring揭秘 没看过 Java程序性能优化:让你的Java程序更快、更稳定 没看过 总结talk is less show me your code，希望大家有好的书籍也可以推荐","categories":[{"name":"读书系统","slug":"读书系统","permalink":"http://kingge.top/categories/读书系统/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"书籍推荐","slug":"书籍推荐","permalink":"http://kingge.top/tags/书籍推荐/"}]},{"title":"linux-PRM软件包管理工具","slug":"linux-PRM软件包管理工具","date":"2017-06-13T12:12:30.000Z","updated":"2019-06-02T05:47:38.887Z","comments":true,"path":"2017/06/13/linux-PRM软件包管理工具/","link":"","permalink":"http://kingge.top/2017/06/13/linux-PRM软件包管理工具/","excerpt":"","text":"1.1 概述RPM（RedHat Package Manager），Rethat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。 RPM包的名称格式 Apache-1.3.23-11.i386.rpm - “apache” 软件名称 - “1.3.23-11”软件的版本号，主版本和此版本 - “i386”是软件所运行的硬件平台 - “rpm”文件扩展名，代表RPM包 1.2 常用命令1.2.1 查询（rpm -qa）1）基本语法： rpm -qa （功能描述：查询所安装的所有rpm软件包） 过滤 rpm -qa | grep rpm软件包 2）案例 [root@hadoop101 Packages]# rpm -qa |grep firefox firefox-45.0.1-1.el6.centos.x86_64 1.2.2 卸载（rpm -e）1）基本语法： （1）rpm -e RPM软件包 或者（2） rpm -e –nodeps 软件包 –nodeps 如果该RPM包的安装依赖其它包，即使其它包没装，也强迫安装。 2）案例 [root@hadoop101 Packages]# rpm -e firefox 1.2.3 安装（rpm -ivh）1）基本语法： ​ rpm -ivh RPM包全名 ​ -i=install，安装 ​ -v=verbose，显示详细信息 ​ -h=hash，进度条 ​ –nodeps，不检测依赖进度 2）案例 [root@hadoop101 Packages]# pwd /media/CentOS_6.8_Final/Packages [root@hadoop101 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm warning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY Preparing… ########################################### [100%] 1:firefox ########################################### [100%]","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"linux基本操作命令","slug":"linux基本操作命令","date":"2017-06-13T07:34:03.000Z","updated":"2019-06-02T05:52:23.561Z","comments":true,"path":"2017/06/13/linux基本操作命令/","link":"","permalink":"http://kingge.top/2017/06/13/linux基本操作命令/","excerpt":"","text":"一、常用基本命令此文章紧接linux基础文章，如果没有安装linux，那么情先参见（linux基础 ） 1.1 帮助命令1.1.1 man 获得帮助信息1）基本语法： ​ man [命令或配置文件] （功能描述：获得帮助信息） ​ （1）显示说明 NAME 命令的名称和单行描述 SYNOPSIS 怎样使用命令 DESCRIPTION 命令功能的深入讨论 EXAMPLES 怎样使用命令的例子 SEE ALSO 相关主题（通常是手册页） （2）数字说明 1.用户在shell环境中可以操作的命令或是可执行的文件 2.系统内核(kernel)可以调用的函数 3.常用的函数or函数库 4.设备配置文件 5.配置文件的格式 6.游戏相关 7.linux网络协议和文件系统 8.系统管理员可以用的命令 9.跟内核有关系的文件 2）案例 [root@hadoop106 home]# man ls 1.1.2 help 获得shell内置命令的帮助信息1）基本语法： ​ help 命令 （功能描述：获得shell内置命令的帮助信息） 2）案例： ​ [root@hadoop101 bin]# help cd 1.1.3 常用快捷键1）ctrl + c：停止进程 2）ctrl+l：清屏 3）ctrl + q：退出 4）善于用tab键 5）上下键：查找执行过的命令 6）ctrl +alt：linux和Windows之间切换 1.2 文件目录类1.2.1 pwd 显示当前工作目录的绝对路径1）基本语法： ​ pwd （功能描述：显示当前工作目录的绝对路径） ​ 2）案例 [root@hadoop101 home]# pwd /home 1.2.2 ls 列出目录的内容1）基本语法： ls [选项] [目录或是文件] 选项： -a ：全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用) -l ：长数据串列出，包含文件的属性与权限等等数据；(常用) 每行列出的信息依次是： 文件类型与权限 链接数 文件属主 文件属组 文件大小用byte来表示 建立或最近修改的时间 名字 2）案例 [kingge@hadoop101 ~]$ ls -al 总用量 44 drwx——. 5 kingge kingge 4096 5月 27 15:15 . drwxr-xr-x. 3 root root 4096 5月 27 14:03 .. drwxrwxrwx. 2 root root 4096 5月 27 14:14 hello -rwxrw-r–. 1 kingge kingge 34 5月 27 14:20 test.txt 1.2.3 mkdir 创建一个新的目录1）基本语法： ​ mkdir [-p] 要创建的目录 ​ 选项： -p：创建多层目录 2）案例 [root@hadoop101 opt]# mkdir test [root@hadoop101 opt]# mkdir -p user/kingge 1.2.4 rmdir 删除一个空的目录1）基本语法： ​ rmdir 要删除的空目录 2）案例 [root@hadoop101 opt]# mkdir test [root@hadoop101 opt]# rmdir test 1.2.5 touch 创建空文件1）基本语法： ​ touch 文件名称 2）案例 [root@hadoop101 opt]# touch test.java 1.2.6 cd 切换目录1）基本语法： ​ （1）cd 绝对路径 ​ （2）cd 相对路径 ​ （3）cd ~或者cd （功能描述：回到自己的家目录） ​ （4）cd - （功能描述：回到上一次所在目录） ​ （5）cd .. （功能描述：回到当前目录的上一级目录） ​ （6）cd -P （功能描述：跳转到实际物理路径，而非快捷方式路径） 2）案例 （1）使用 mkdir 命令创建kingge目录 [root@hadoop101 ~]# mkdir kingge （2）使用绝对路径切换到kingge目录 [root@hadoop101 ~]# cd /root/kingge/ （3）使用相对路径切换到kingge目录 [root@hadoop101 ~]# cd ./kingge/ （4）表示回到自己的家目录，亦即是 /root 这个目录 [root@hadoop101 kingge]# cd ~ （5）cd- 回到上一次所在目录 [root@hadoop101 kingge]# cd - （6）表示回到当前目录的上一级目录，亦即是 /root 的上一级目录的意思； [root@hadoop101 ~]# cd .. 1.2.7 cp 复制文件或目录1）基本语法： （1）cp source dest （功能描述：复制source文件到dest） （2）cp -r sourceFolder targetFolder （功能描述：递归复制整个文件夹） 2）案例 （1）复制文件 [root@hadoop101 opt]# cp test.java test （2）递归复制整个文件夹 [root@hadoop101 opt]# cp -r test test1 1.2.8 rm 移除文件或目录1）基本语法 ​ （1）rmdir deleteEmptyFolder （功能描述：删除空目录） （2）rm -rf deleteFile （功能描述：递归删除目录中所有内容） 2）案例 ​ 1）删除空目录 [root@hadoop101 opt]# rmdir test 2）递归删除目录中所有内容 [root@hadoop101 opt]# rm -rf test1 1.2.9 mv 移动文件与目录或重命名1）基本语法： ​ （1）mv oldNameFile newNameFile （功能描述：重命名） ​ （2）mv /temp/movefile /targetFolder （功能描述：递归移动文件） 2）案例： ​ 1）重命名 [root@hadoop101 opt]# mv test.java test1.java 2）移动文件 [root@hadoop101 opt]# mv test1.java test1 1.2.10 cat 查看文件内容查看文件内容，从第一行开始显示。 1）基本语法 ​ cat [选项] 要查看的文件 选项： -A ：相当于 -vET 的整合选项，可列出一些特殊字符而不是空白而已； -b ：列出行号，仅针对非空白行做行号显示，空白行不标行号！ -E ：将结尾的断行字节 $ 显示出来； -n ：列出行号，连同空白行也会有行号，与 -b 的选项不同； -T ：将 [tab] 按键以 ^I 显示出来； -v ：列出一些看不出来的特殊字符 2）案例 [kingge@hadoop101 ~]$ cat -A test.txt hellda $ dasadf ^I$ da^I^I^I$ das$ 1.2.11 tac查看文件内容查看文件内容，从最后一行开始显示，可以看出 tac 是 cat 的倒著写。 1）基本语法： ​ tac [选项参数] 要查看的文件 2）案例 [root@hadoop101 test1]# cat test1.java hello kingge kingge1 [root@hadoop101 test1]# tac test1.java kingge1 kingge hello 1.2.12 more 查看文件内容查看文件内容，一页一页的显示文件内容。 1）基本语法： ​ more 要查看的文件 2）功能使用说明 空白键 (space)：代表向下翻一页； Enter:代表向下翻『一行』； q:代表立刻离开 more ，不再显示该文件内容。 Ctrl+F 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 :f 输出文件名和当前行的行号 3）案例 [root@hadoop101 test1]# more test1.java 1.2.13 less 查看文件内容less 的作用与 more 十分相似，都可以用来浏览文字档案的内容，不同的是 less 允许使用[pageup] [pagedown]往回滚动。 1）基本语法： ​ less 要查看的文件 2）功能使用说明 空白键 ：向下翻动一页； [pagedown]：向下翻动一页； [pageup] ：向上翻动一页； /字串 ：向下搜寻『字串』的功能；n：向下查找；N：向上查找； ?字串 ：向上搜寻『字串』的功能；n：向上查找；N：向下查找； q ：离开 less 这个程序； 3）案例 [root@hadoop101 test1]# less test1.java 1.2.14 head查看文件内容查看文件内容，只看头几行。 1）基本语法 head -n 10 文件 （功能描述：查看文件头10行内容，10可以是任意行数） 2）案例 [root@hadoop101 test1]# head -n 2 test1.java hello kingge 1.2.15 tail 查看文件内容查看文件内容，只看尾巴几行。 1）基本语法 （1）tail -n 10 文件 （功能描述：查看文件头10行内容，10可以是任意行数） （2）tail -f 文件 （功能描述：实时追踪该文档的所有更新） 2）案例 （1）查看文件头1行内容 [root@hadoop101 test1]# tail -n 1 test1.java kingge （2）实时追踪该档的所有更新 [root@hadoop101 test1]# tail -f test1.java hello kingge kingge 1.2.16 重定向命令1）基本语法： （1）ls -l &gt;文件 （功能描述：列表的内容写入文件a.txt中（覆盖写）） （2）ls -al &gt;&gt;文件 （功能描述：列表的内容追加到文件aa.txt的末尾） 2）案例 ​ （1）[root@hadoop101 opt]# ls -l&gt;t.txt （2）[root@hadoop101 opt]# ls -l&gt;&gt;t.txt （3）[root@hadoop101 test1]# echo hello&gt;&gt;test1.java 1.2.17 echo1）基本语法： （1）echo 要显示的内容 &gt;&gt; 存储内容的的文件 （功能描述：将要显示的内容，存储到文件中） ​ （2）echo 变量 （功能描述：显示变量的值） 2）案例 [root@hadoop101 test1]# echo $JAVA_HOME /opt/module/jdk1.7.0_79 1.2.18 ln软链接1）基本语法： ln -s [原文件] [目标文件] （功能描述：给原文件创建一个软链接，软链接存放在目标文件目录） 删除软链接： rm -rf kingge，而不是rm -rf kingge/ 2）案例： [root@hadoop101 module]# ln -s /opt/module/test.txt /opt/t.txt [root@hadoop101 opt]# ll lrwxrwxrwx. 1 root root 20 6月 17 12:56 t.txt -&gt; /opt/module/test.txt 创建一个软链接 [kingge@hadoop101 opt]$ ln -s /opt/module/hadoop-2.7.2/ /opt/software/hadoop cd不加参数进入是软链接的地址 [kingge@hadoop101 software]$ cd hadoop [kingge@hadoop101 hadoop]$ pwd /opt/software/hadoop cd加参数进入是实际的物理地址 [kingge@hadoop101 software]$ cd -P hadoop [kingge@hadoop101 hadoop-2.7.2]$ pwd /opt/module/hadoop-2.7.2 1.2.19 history查看所敲命令历史1）基本语法： ​ history 2）案例 [root@hadoop101 test1]# history 1.3 时间日期类1）基本语法 date [OPTION]… [+FORMAT] 1.3.1 date显示当前时间1）基本语法： ​ （1）date （功能描述：显示当前时间） ​ （2）date +%Y （功能描述：显示当前年份） （3）date +%m （功能描述：显示当前月份） （4）date +%d （功能描述：显示当前是哪一天） （5）date +%Y%m%d date +%Y/%m/%d … （功能描述：显示当前年月日各种格式 ） ​ （6）date “+%Y-%m-%d %H:%M:%S” （功能描述：显示年月日时分秒） 2）案例 [root@hadoop101 /]# date 2017年 06月 19日 星期一 20:53:30 CST [root@hadoop101 /]# date +%Y%m%d 20170619 [root@hadoop101 /]# date “+%Y-%m-%d %H:%M:%S” 2017-06-19 20:54:58 1.3.2 date显示非当前时间1）基本语法： （1）date -d ‘1 days ago’ （功能描述：显示前一天日期） （2）date -d yesterday +%Y%m%d （同上） （3）date -d next-day +%Y%m%d （功能描述：显示明天日期） （4）date -d ‘next monday’ （功能描述：显示下周一时间） 2）案例： [root@hadoop101 /]# date -d ‘1 days ago’ 2017年 06月 18日 星期日 21:07:22 CST [root@hadoop101 /]# date -d next-day +%Y%m%d 20170620 [root@hadoop101 /]# date -d ‘next monday’ 2017年 06月 26日 星期一 00:00:00 CST 1.3.3 date设置系统时间1）基本语法： ​ date -s 字符串时间 2）案例 ​ [root@hadoop106 /]# date -s “2017-06-19 20:52:18” 1.3.4 cal查看日历1）基本语法： cal [选项] （功能描述：不加选项，显示本月日历） 选项： -3 ，显示系统前一个月，当前月，下一个月的日历 具体某一年，显示这一年的日历。 2）案例： [root@hadoop101 /]# cal [root@hadoop101 /]# cal -3 ​ [root@hadoop101 /]# cal 2016 1.4 用户管理命令1.4.1 useradd 添加新用户1）基本语法： ​ useradd 用户名 （功能描述：添加新用户） 2）案例： ​ [root@hadoop101 opt]# user kingge 1.4.2 passwd 设置用户密码1）基本语法： ​ passwd 用户名 （功能描述：设置用户密码） 2）案例 ​ [root@hadoop101 opt]# passwd kingge 1.4.3 id 判断用户是否存在1）基本语法： ​ id 用户名 2）案例： ​ [root@hadoop101 opt]#id kingge 1.4.4 su 切换用户1）基本语法： su 用户名称 （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量） su - 用户名称 （功能描述：切换到用户并获得该用户的环境变量及执行权限） 2）案例 [root@hadoop101 opt]#su kingge [root@hadoop101 opt]#su - kingge 1.4.5 userdel 删除用户1）基本语法： ​ （1）userdel 用户名 （功能描述：删除用户但保存用户主目录） （2）userdel -r 用户名 （功能描述：用户和用户主目录，都删除） 2）案例： （1）删除用户但保存用户主目录 ​ [root@hadoop101 opt]#userdel kingge （2）删除用户和用户主目录，都删除 ​ [root@hadoop101 opt]#userdel -r kingge 1.4.6 who 查看登录用户信息1）基本语法 ​ （1）whoami （功能描述：显示自身用户名称） （2）who am i （功能描述：显示登录用户的用户名） （3）who （功能描述：看当前有哪些用户登录到了本台机器上） 2）案例 [root@hadoop101 opt]# whoami [root@hadoop101 opt]# who am i ​ [root@hadoop101 opt]# who 1.4.7 设置kingge普通用户具有root权限1）修改配置文件 修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) ALL或者配置成采用sudo命令时，不需要输入密码 Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) NOPASSWD:ALL修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。 2）案例 [kingge@hadoop101 opt]$ sudo mkdir module [root@hadoop101 opt]# chown kingge:kingge module/ 1.4.8 cat /etc/passwd 查看创建了哪些用户cat /etc/passwd 1.4.9 usermod修改用户1）基本语法： usermod -g 用户组 用户名 2）案例： 将用户kingge加入dev用户组 [root@hadoop101 opt]#usermod -g dev kingge 1.5 用户组管理命令每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同， 如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 1.5.1 groupadd 新增组1）基本语法 groupadd 组名 2）案例： ​ 添加一个kingge组 [root@hadoop101 opt]#groupadd kingge 1.5.2 groupdel删除组1）基本语法： groupdel 组名 2）案例 [root@hadoop101 opt]# groupdel kingge 1.5.3 groupmod修改组1）基本语法： groupmod -n 新组名 老组名 2）案例 ​ 修改kingge组名称为kingge1 [root@hadoop101 kingge]# groupmod -n kingge1 kingge 1.5.4 cat /etc/group 查看创建了哪些组cat /etc/group 1.5.5 综合案例[root@hadoop101 kingge]# groupadd dev [root@hadoop101 kingge]# groupmod -n device dev [root@hadoop101 kingge]# usermod -g device kingge [root@hadoop101 kingge]# su kingge [kingge@hadoop101 ~]$ mkdir kingge [kingge@hadoop101 ~]$ ls -l drwxr-xr-x. 2 kingge device 4096 5月 27 16:31 kingge [root@hadoop101 kingge]# usermod -g kingge kingge 1.6 文件权限类1.6.1 文件属性Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls –l命令来显示一个文件的属性以及文件所属的用户和组。 1）从左到右的10个字符表示： 如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示: （1）0首位表示类型 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等 - 代表文件 d 代表目录 c 字符流，装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置) s socket p 管道 l 链接文档(link file)； b 设备文件，装置文件里面的可供储存的接口设备(可随机存取装置) （2）第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User （3）第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group （4）第7-9位确定其他用户拥有该文件的权限 —Other 文件类型 属主权限 属组权限 其他用户权限 0 1 2 3 4 5 6 7 8 9 d R w x R - x R - x 目录文件 读 写 执行 读 写 执行 读 写 执行 2）rxw作用文件和目录的不同解释 （1）作用到文件： [ r ]代表可读(read): 可以读取，查看 [ w ]代表可写(write): 可以修改，但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件. [ x ]代表可执行(execute):可以被系统执行 （2）作用到目录： [ r ]代表可读(read): 可以读取，ls查看目录内容 [ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录 [ x ]代表可执行(execute):可以进入该目录 3）案例 [kingge@hadoop101 ~]$ ls -l 总用量 8 drwxrwxr-x. 2 kingge kingge 4096 5月 27 14:14 hello -rw-rw-r–. 1 kingge kingge 34 5月 27 14:20 test.txt （1）如果查看到是文件：链接数指的是硬链接个数。创建硬链接方法 ln [原文件] [目标文件] [root@hadoop101 xiyou]# ln sunhouzi/shz.txt ./shz.txt （2）如果查看的是文件夹：链接数指的是子文件夹个数。 [root@hadoop101 xiyou]# ls -al kingge/ 总用量 8 drwxr-xr-x. 2 root root 4096 9月 3 19:02 . drwxr-xr-x. 5 root root 4096 9月 3 21:21 .. 1.6.2 chmod改变权限1）基本语法： ​ chmod [{ugoa}{+-=}{rwx}] [文件或目录] [mode=421 ] [文件或目录] 2）功能描述 改变文件或者目录权限 文件: r-查看；w-修改；x-执行文件 目录: r-列出目录内容；w-在目录中创建和删除；x-进入目录 删除一个文件的前提条件:该文件所在的目录有写权限，你才能删除该文件。 3）案例 [root@hadoop101 test1]# chmod u+x test1.java [root@hadoop101 test1]# chmod g+x test1.java [root@hadoop101 test1]# chmod o+x test1.java [root@hadoop101 test1]# chmod 777 test1.java [root@hadoop101 test1]# chmod -R 777 testdir 1.6.3 chown改变所有者1）基本语法： chown [最终用户] [文件或目录] （功能描述：改变文件或者目录的所有者） 2）案例 [root@hadoop101 test1]# chown kingge test1.java [root@hadoop101 test1]# ls -al -rwxr-xr-x. 1 kingge kingge 551 5月 23 13:02 test1.java 修改前： [root@hadoop101 xiyou]# ll drwxrwxrwx. 2 root root 4096 9月 3 21:20 sunhouzi 修改后 [root@hadoop101 xiyou]# chown -R kingge:kingge sunhouzi/ [root@hadoop101 xiyou]# ll drwxrwxrwx. 2 kingge kingge 4096 9月 3 21:20 sunhouzi 1.6.4 chgrp改变所属组1）基本语法： ​ chgrp [最终用户组] [文件或目录] （功能描述：改变文件或者目录的所属组） 2）案例 [root@hadoop101 test1]# chgrp kingge test1.java [root@hadoop101 test1]# ls -al -rwxr-xr-x. 1 root kingge 551 5月 23 13:02 test1.java 1.6.5 su 切换用户1）基本语法： su -username （功能描述：切换用户） 2）案例 [root@hadoop101 kingge]# su kingge [kingge@hadoop101 ~]$ [kingge@hadoop101 ~]$ su root 密码： [root@hadoop101 kingge]# 1.7 磁盘分区类1.7.1 fdisk查看分区1）基本语法： ​ fdisk -l （功能描述：查看磁盘分区详情） ​ 注意：在root用户下才能使用 2）功能说明： ​ （1）Linux分区 这个硬盘是20G的，有255个磁面；63个扇区；2610个磁柱；每个 cylinder（磁柱）的容量是 8225280 bytes=8225.280 K（约为）=8.225280M（约为）； Device Boot Start End Blocks Id System 分区序列 引导 从X磁柱开始 到Y磁柱结束 容量 分区类型ID 分区类型 （2）Win7分区 3）案例 [root@hadoop101 /]# fdisk -l Disk /dev/sda: 21.5 GB, 21474836480 bytes 255 heads, 63 sectors/track, 2610 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x0005e654 Device Boot Start End Blocks Id System /dev/sda1 * 1 26 204800 83 Linux Partition 1 does not end on cylinder boundary. /dev/sda2 26 1332 10485760 83 Linux /dev/sda3 1332 1593 2097152 82 Linux swap / Solaris 1.7.2 df查看硬盘1）基本语法： ​ df 参数 （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况） 参数： -a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统； -k ：以 KBytes 的容量显示各文件系统； -m ：以 MBytes 的容量显示各文件系统； -h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示； -H ：以 M=1000K 取代 M=1024K 的进位方式； -T ：显示文件系统类型，连同该 partition 的 filesystem 名称 (例如 ext3) 也列出； -i ：不用硬盘容量，而以 inode 的数量来显示 2）案例 [root@hadoop101 ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda2 15G 3.5G 11G 26% / tmpfs 939M 224K 939M 1% /dev/shm /dev/sda1 190M 39M 142M 22% /boot 1.7.3 mount/umount挂载/卸载对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构 Linux中每个分区都是用来组成整个文件系统的一部分，她在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。 0**）挂载前准备（必须要有光盘或者已经连接镜像文件）** 1**）挂载光盘语法：** mount [-t vfstype] [-o options] device dir （1）-t vfstype 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。 常用类型有： 光盘或光盘镜像：iso9660 DOS fat16文件系统：msdos Windows 9x fat32文件系统：vfat Windows NT ntfs文件系统：ntfs Mount Windows文件网络共享：smbfs UNIX(LINUX) 文件网络共享：nfs （2）-o options 主要用来描述设备或档案的挂接方式。常用的参数有： loop：用来把一个文件当成硬盘分区挂接上系统 ro：采用只读方式挂接设备 rw：采用读写方式挂接设备 iocharset：指定访问文件系统所用字符集 （3）device 要挂接(mount)的设备 （4）dir设备在系统上的挂接点(mount point) 2**）案例** （1）光盘镜像文件的挂载 [root@hadoop101 ~]# mkdir /mnt/cdrom/ 建立挂载点 [root@hadoop101 ~]# mount -t iso9660 /dev/cdrom /mnt/cdrom/ 设备/dev/cdrom挂载到 挂载点 ： /mnt/cdrom中 [root@hadoop101 ~]# ll /mnt/cdrom/ 3**）卸载光盘语法：** [root@hadoop101 ~]# umount 设备文件名或挂载点 4**）案例** [root@hadoop101 ~]# umount /mnt/cdrom 5**）开机自动挂载语法：** [root@hadoop101 ~]# vi /etc/fstab 添加红框中内容，保存退出。 1.8 搜索查找类1.8.1 find 查找文件或者目录1）基本语法： ​ find [搜索范围] [匹配条件] 2）案例 （1）按文件名：根据名称查找/目录下的filename.txt文件。 [root@hadoop101 ~]# find /opt/ -name *.txt （2）按拥有者：查找/opt目录下，用户名称为-user的文件 [root@hadoop101 ~]# find /opt/ -user kingge ​ （3）按文件大小：在/home目录下查找大于200m的文件（+n 大于 -n小于 n等于） [root@hadoop101 ~]find /home -size +204800 1.8.2 grep 过滤查找及“|”管道符0）管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理 1）基本语法 grep+参数+查找内容+源文件 参数： -c：只输出匹配行的计数。 -I：不区分大小写(只适用于单字符)。 -h：查询多文件时不显示文件名。 -l：查询多文件时只输出包含匹配字符的文件名。 -n：显示匹配行及行号。 -s：不显示不存在或无匹配文本的错误信息。 -v：显示不包含匹配文本的所有行。 2）案例 [root@hadoop101 opt]# ls | grep -n test 4:test1 5:test2 1.8.3 which 文件搜索命令1）基本语法： ​ which 命令 （功能描述：搜索命令所在目录及别名信息） 2）案例 ​ [root@hadoop101 opt]# which ls ​ /bin/ls 1.9 进程线程类进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。 1.9.1 ps查看系统中所有进程1）基本语法： ​ ps -aux （功能描述：查看系统中所有进程） 2）功能说明 ​ USER：该进程是由哪个用户产生的 ​ PID：进程的ID号 %CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源； %MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源； VSZ：该进程占用虚拟内存的大小，单位KB； RSS：该进程占用实际物理内存的大小，单位KB； TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。 STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台 START：该进程的启动时间 TIME：该进程占用CPU的运算时间，注意不是系统时间 COMMAND：产生此进程的命令名 3）案例 ​ [root@hadoop101 datas]# ps -aux 1.9.2 top查看系统健康状态1）基本命令 ​ top [选项] ​ （1）选项： ​ -d 秒数：指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令： -i：使top不显示任何闲置或者僵死进程。 -p：通过指定监控进程ID来仅仅监控某个进程的状态。 ​ （2）操作选项： P： 以CPU使用率排序，默认就是此项 M： 以内存的使用率排序 N： 以PID排序 q： 退出top ​ （3）查询结果字段解释 第一行信息为任务队列信息 内容 说明 12:26:46 系统当前时间 up 1 day, 13:32 系统的运行时间，本机已经运行1天 13小时32分钟 2 users 当前登录了两个用户 load average: 0.00, 0.00, 0.00 系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。 第二行为进程信息 Tasks: 95 total 系统中的进程总数 1 running 正在运行的进程数 94 sleeping 睡眠的进程 0 stopped 正在停止的进程 0 zombie 僵尸进程。如果不是0，需要手工检 查僵尸进程 第三行为CPU信息 Cpu(s): 0.1%us 用户模式占用的CPU百分比 0.1%sy 系统模式占用的CPU百分比 0.0%ni 改变过优先级的用户进程占用的CPU百分比 99.7%id 空闲CPU的CPU百分比 0.1%wa 等待输入/输出的进程的占用CPU百分比 0.0%hi 硬中断请求服务占用的CPU百分比 0.1%si 软中断请求服务占用的CPU百分比 0.0%st st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。 第四行为物理内存信息 Mem: 625344k total 物理内存的总量，单位KB 571504k used 已经使用的物理内存数量 53840k free 空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了 65800k buffers 作为缓冲的内存数量 第五行为交换分区（swap）信息 Swap: 524280k total 交换分区（虚拟内存）的总大小 0k used 已经使用的交互分区的大小 524280k free 空闲交换分区的大小 409280k cached 作为缓存的交互分区的大小 2）案例 ​ [root@hadoop101 kingge]# top -d 1 [root@hadoop101 kingge]# top -i [root@hadoop101 kingge]# top -p 2575 执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。 1.9.3 pstree查看进程树1）基本语法： ​ pstree [选项] ​ 选项 -p： 显示进程的PID -u： 显示进程的所属用户 2）案例： ​ [root@hadoop101 datas]# pstree -u [root@hadoop101 datas]# pstree -p 1.9.4 kill终止进程1）基本语法： ​ kill -9 pid进程号 ​ 选项 -9 表示强迫进程立即停止 2）案例： ​ 启动mysql程序 ​ 切换到root用户执行 ​ [root@hadoop101 桌面]# kill -9 5102 1.9.5 netstat显示网络统计信息1）基本语法： ​ netstat -anp （功能描述：此命令用来显示整个系统目前的网络情况。例如目前的连接、数据包传递数据、或是路由表内容） ​ 选项： ​ -an 按一定顺序排列输出 ​ -p 表示显示哪个进程在调用 ​ -nltp 查看tcp协议进程端口号 2）案例 查看端口50070的使用情况 [root@hadoop101 hadoop-2.7.2]# netstat -anp | grep 50070 tcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 6816/java ​ 端口号 进程号 1.9.6 前后台进程切换1）基本语法： fg %1 （功能描述：把后台进程转换成前台进程） ctrl+z bg %1 （功能描述：把前台进程发到后台） 1.10 压缩和解压类1.10.1 gzip/gunzip压缩1）基本语法： gzip+文件 （功能描述：压缩文件，只能将文件压缩为*.gz文件） gunzip+文件.gz （功能描述：解压缩文件命令） 2）特点： （1）只能压缩文件不能压缩目录 （2）不保留原来的文件 3）案例 （1）gzip压缩 [root@hadoop101 opt]# ls test.java [root@hadoop101 opt]# gzip test.java [root@hadoop101 opt]# ls test.java.gz （2）gunzip解压缩文件 [root@hadoop101 opt]# gunzip test.java.gz [root@hadoop101 opt]# ls test.java 1.10.2 zip/unzip压缩1）基本语法： zip + 参数 + XXX.zip + 将要压缩的内容 （功能描述：压缩文件和目录的命令，window/linux通用且可以压缩目录且保留源文件） 参数： -r 压缩目录 2）案例： （1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip [root@hadoop101 opt]# zip test.zip test1.java test.java adding: test1.java (stored 0%) adding: test.java (stored 0%) [root@hadoop101 opt]# ls test1.java test.java test.zip （2）解压 mypackage.zip [root@hadoop101 opt]# unzip test.zip Archive: test.zip extracting: test1.java extracting: test.java ​ [root@hadoop101 opt]# ls test1.java test.java test.zip 1.10.3 tar打包1）基本语法： tar + 参数 + XXX.tar.gz + 将要打包进去的内容 （功能描述：打包目录，压缩后的文件格式.tar.gz） 参数： -c 产生.tar打包文件 -v 显示详细信息 -f 指定压缩后的文件名 -z 打包同时压缩 -x 解包.tar文件 2）案例 （1）压缩：tar -zcvf XXX.tar.gz n1.txt n2.txt ​ 压缩多个文件 [root@hadoop101 opt]# tar -zcvf test.tar.gz test1.java test.java test1.java test.java [root@hadoop101 opt]# ls test1.java test.java test.tar.gz 压缩目录 [root@hadoop101 opt]# tar -zcvf test.java.tar.gz test1 test1/ test1/hello test1/test1.java test1/test/ test1/test/test.java [root@hadoop106 opt]# ls test1 test.java.tar.gz （2）解压：tar -zxvf XXX.tar.gz ​ 解压到当前目录 [root@hadoop101 opt]# tar -zxvf test.tar.gz 解压到/opt目录 [root@hadoop101 opt]# tar -zxvf test.tar.gz -C /opt .11 后台服务管理类.11.1 service后台服务管理1）service network status 查看指定服务的状态 2）service network stop 停止指定服务 3）service network start 启动指定服务 4）service network restart 重启指定服务 5）service –status-all 查看系统中所有的后台服务 .11.2 chkconfig设置后台服务的自启配置1）chkconfig 查看所有服务器自启配置 2）chkconfig iptables off 关掉指定服务的自动启动 3）chkconfig iptables on 开启指定服务的自动启动 1.12 crond系统定时任务1.12.1 crond服务管理[root@hadoop101 ~]# service crond restart （重新启动服务） 1.12.2 crontab定时任务设置1）基本语法 crontab [选项] 选项： -e： 编辑crontab定时任务 -l： 查询crontab任务 -r： 删除当前用户所有的crontab任务 2）参数说明 ​ [root@hadoop101 ~]# crontab -e （1）进入crontab编辑界面。会打开vim编辑你的工作。 * 执行的任务 项目 含义 范围 第一个“*” 一小时当中的第几分钟 0-59 第二个“*” 一天当中的第几小时 0-23 第三个“*” 一个月当中的第几天 1-31 第四个“*” 一年当中的第几月 1-12 第五个“*” 一周当中的星期几 0-7（0和7都代表星期日） （2）特殊符号 特殊符号 含义 * 代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。 ， 代表不连续的时间。比如“0 8,12,16 * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 - 代表连续的时间范围。比如“0 5 1-6命令”，代表在周一到周六的凌晨5点0分执行命令 */n 代表每隔多久执行一次。比如“/10 * 命令”，代表每隔10分钟就执行一遍命令 （3）特定时间执行命令 时间 含义 45 22 * 命令 在22点45分执行命令 0 17 1 命令 每周1 的17点0分执行命令 0 5 1,15 命令 每月1号和15号的凌晨5点0分执行命令 40 4 1-5 命令 每周一到周五的凌晨4点40分执行命令 /10 4 命令 每天的凌晨4点，每隔10分钟执行一次命令 0 0 1,15 * 1 命令 每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。 3）案例： /5 * /bin/echo ”11” &gt;&gt; /tmp/test","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"linux基础","slug":"linux基础","date":"2017-06-12T01:17:03.000Z","updated":"2019-06-02T04:09:46.634Z","comments":true,"path":"2017/06/12/linux基础/","link":"","permalink":"http://kingge.top/2017/06/12/linux基础/","excerpt":"","text":"一 、Linux入门概述linux 系统也是接触了许久，不过一直没有机会总结一下，所以决定出个linux相关安装和配置以及常用指令的专栏。 1.1 概述​ Linux内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。 ​ Linux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。Linux能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 ​ 目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debain、Fedora、SuSE、OpenSUSE ​ 下面的操作，我使用的是Centos 1.2 下载地址centos下载地址： 网易镜像：http://mirrors.163.com/centos/6/isos/ 1.3 Linux特点 Linux里面一切皆是文件 Linux里面没有后缀名这一说 1.4 Linux和Windows区别目前国内Linux更多的是应用在服务器上，而桌面操作系统更多使用的是window。主要区别如下。 二 、VM安装相关（运行环境）2.1 安装VMWare虚拟机 详情这里就不说了，自行百度。 2.2 安装CentOS ​ 需要注意的是：下面的步骤，按需省略。 1 检查BIOS虚拟化支持 2 新建虚拟机 3 新建虚拟机向导 4 创建虚拟空白光盘 5 安装Linux系统对应的CentOS版 6 虚拟机命名和定位磁盘位置 7 处理器配置，看自己是否是双核、多核 8 设置内存为2GB 9 网络设置NAT 10 选择IO控制器类型 11 选择磁盘类型 12 新建虚拟磁盘 13 设置磁盘容量 14 你在哪里存储这个磁盘文件 15 新建虚拟机向导配置完成 16 VM设置 17 加载ISO 18 加电并安装配置CentOS 19 加电后初始化欢迎进入页面 回车选择第一个开始安装配置，此外，在Ctrl+Alt可以实现Windows主机和VM之间窗口的切换 20 是否对CD媒体进行测试，直接跳过**Skip** 21 CentOS欢迎页面，直接点击Next 22 选择简体中文进行安装 23 选择语言键盘 23 选择存储设备 24 给计算机起名 25 设置网络环境 安装成功后再设置。 26 选择时区 27 设置root密码 （一定记住） 28 硬盘分区-1 29 根分区新建 l Boot l swap分区设置 l 分区完成 30 程序引导，直接下一步 31 现在定制系统软件 32 Web环境 33 可扩展文件系统支持 34 基本系统 35 应用程序 36 开发、弹性存储、数据库、服务器 可以都不勾，有需要，以后使用中有需要再手动安装 37 桌面 除了KDE，其他都选就可以了。 38 语言支持 39 系统管理、虚拟化、负载平衡器、高可用性可以都不选 40 完成配置，开始安装CentOS 41 等待安装完成，等待等待等待等待……20分钟左右 42 安装完成，重新引导 43 欢迎引导页面 44 许可证 45 创建用户，可以先不创建，用root账户登录就行 46 时间和日期 47 Kdump,去掉 48 重启后用root登录 2.3 安装VMTools工具1）什么是VMtools VM tools顾名思义就是Vmware的一组工具。主要用于虚拟主机显示优化与调整，另外还可以方便虚拟主机与本机的交互，如允许共享文件夹，甚至可以直接从本机向虚拟主机拖放文件、鼠标无缝切换、显示分辨率调整等，十分实用。 安装过程自行百度 三 、Linux目录结构3.1 概览 3.2 树状目录结构 /bin：是Binary的缩写，这个目录存放着系统必备执行命令/boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件，自己的安装别放这里/dev：Device(设备)的缩写，该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。/etc：所有的系统管理所需要的配置文件和子目录。/home：存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。/lib：系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。/lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。/media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。/misc: 该目录可以用来存放杂项文件或目录，即那些用途或含义不明确的文件或目录可以存放在该目录下。/mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。/net 存放着和网络相关的一些文件./opt：这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。/proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。/root：该目录为系统管理员，也称作超级权限者的用户主目录。/sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。/selinux：这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙/srv：service缩写，该目录存放一些服务启动之后需要提取的数据。/sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。/tmp：这个目录是用来存放一些临时文件的。/usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。/var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 四 VI/VIM编辑器4.1 概述所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。但是目前我们使用比较多的是 vim 编辑器。 Vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 简单的来说vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。vim 则可以说是程序开发者的一项很好用的工具。连vim 的官方网站 (http://www.vim.org) 自己也说 vim 是一个程序开发工具而不是文字处理软件。 4.2 一般模式以 vi 打开一个档案就直接进入一般模式了(这是默认的模式)。在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、粘贴』来处理你的文件数据。 常用语法 1）yy （功能描述：复制光标当前一行） y数字y （功能描述：复制一段(从第几行到第几行)） 2）p （功能描述：箭头移动到目的行粘贴） 3）u （功能描述：撤销上一步） 4）dd （功能描述：删除光标当前行） d数字d （功能描述：删除光标(含)后多少行） 5）x （功能描述：删除一个字母，相当于del） X （功能描述：删除一个字母，相当于Backspace） 6）yw （功能描述：复制一个词） 7）dw （功能描述：删除一个词） 8）shift+^ （功能描述：移动到行头） 9）shift+$ （功能描述：移动到行尾） 10）1+shift+g （功能描述：移动到页头，数字） 11）shift+g （功能描述：移动到页尾） 12）数字N+shift+g （功能描述：移动到目标行） 4.3 编辑模式在一般模式中可以进行删除、复制、贴上等等的动作，但是却无法编辑文件内容的！ 要等到你按下『i, I, o, O, a, A, r, R』等任何一个字母之后才会进入编辑模式。 注意了！通常在 Linux 中，按下这些按键时，在画面的左下方会出现『INSERT 或 REPLACE 』的字样，此时才可以进行编辑。而如果要回到一般模式时， 则必须要按下『Esc』这个按键即可退出编辑模式。 常用语法 1）进入编辑模式 （1）i 当前光标前 （2）a 当前光标后 （3）o 当前光标行的下一行 2）退出编辑模式 按『Esc』键 4.4 指令模式在一般模式当中，输入『 : / ?』3个中的任何一个按钮，就可以将光标移动到最底下那一行。 在这个模式当中， 可以提供你『搜寻资料』的动作，而读取、存盘、大量取代字符、离开 vi 、显示行号等动作是在此模式中达成的！ 常用语法 1）基本语法 （1）: 选项 ​ 选项： w 保存 q 退出 ！ 感叹号强制执行 （2）/ 查找，/被查找词，n是查找下一个，shift+n是往上查找 （3）? 查找，?被查找词，n是查找上一个，shift+n是往下查找 2）案例 :wq! 强制保存退出 五 系统管理操作5.1 查看网络IP和网关1）查看虚拟网络编辑器 2）修改ip地址 3）查看网关 5.2 配置网络ip地址 0）查看当前ip基本语法： &gt; &gt; &gt; [root@hadoop101 /]# ifconfig&gt; &gt; 1）在终端命令窗口中输入（如果不是克隆的虚拟机可以跳过这一步）*******&gt; &gt; [root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules&gt; &gt; 进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址&gt; 2）修改IP地址 [root@hadoop101 /]#vim /etc/sysconfig/network-scripts/ifcfg-eth0需要修改的内容有5项：IPADDR=192.168.1.101GATEWAY=192.168.1.2ONBOOT=yesBOOTPROTO=staticDNS1=192.168.1.2 （1）修改前 ​ （2）修改后 ：wq 保存退出 3）执行service network restart 3）执行service network restart 4）如果报错，reboot，重启虚拟机 5.3 配置主机名0）查看主机名基本语法： [root@hadoop101 /]#hostname 1）修改linux的主机映射文件（hosts文件） （1）进入Linux系统查看本机的主机名。通过hostname命令查看[root@hadoop101 ~]# hostnamehadoop100（2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件[root@hadoop101 /]# vi /etc/sysconfig/network文件中内容NETWORKING=yesNETWORKING_IPV6=noHOSTNAME= hadoop101注意：主机名称不要有“_”下划线（3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。（4）保存退出。（5）打开/etc/hosts[root@hadoop101 /]# vim /etc/hosts添加如下内容192.168.1.101 hadoop101（6）并重启设备，重启后，查看主机名，已经修改成功 2）修改window7的主机映射文件（hosts文件）–方面在电脑使用域名进行访问hadoop相关的组件-例如hdfs，mapreduce等等。 ​ （1）进入C:\\Windows\\System32\\drivers\\etc路径 （2）打开hosts文件并添加如下内容192.168.1.101 hadoop101192.168.1.102 hadoop102192.168.1.103 hadoop103192.168.1.104 hadoop104192.168.1.105 hadoop105192.168.1.106 hadoop106192.168.1.107 hadoop107192.168.1.108 hadoop108 5.4 防火墙1）基本语法： service iptables status （功能描述：查看防火墙状态）chkconfig iptables -list （功能描述：查看防火墙开机启动状态）service iptables stop （功能描述：临时关闭防火墙）chkconfig iptables off （功能描述：关闭防火墙开机启动）chkconfig iptables on （功能描述：开启防火墙开机启动） 2）扩展 Linux系统有7个运行级别(runlevel)运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆运行级别2：多用户状态(没有NFS)运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式运行级别4：系统未使用，保留运行级别5：X11控制台，登陆后进入图形GUI模式运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 5.5 关机重启在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机 。 正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt 1）基本语法： ​ （1）sync （功能描述：将数据由内存同步到硬盘中）​ （2）shutdown [选项] 时间 ​ 选项：​ -h：关机​ -r：重启（3）halt （功能描述：关闭系统，等同于shutdown -h now 和 poweroff）（4）reboot （功能描述：就是重启，等同于 shutdown -r now） 2）案例 （1）将数据由内存同步到硬盘中[root@hadoop101 /]#sync （2）计算机将在10分钟后关机，并且会显示在登录用户的当前屏幕中[root@hadoop101 /]#shutdown -h 10 ‘This server will shutdown after 10 mins’（3）立马关机[root@hadoop101 /]# shutdown -h now （4）系统立马重启[root@hadoop101 /]# shutdown -r now（5）重启（等同于 shutdown -r now）[root@hadoop101 /]# reboot （6）关机（等同于shutdown -h now 和 poweroff）[root@hadoop101 /]#halt 注意：不管是重启系统还是关闭系统，首先要运行sync命令，把内存中的数据写到磁盘中。 5.6 找回root密码重新安装系统吗？当然不用！进入单用户模式更改一下root密码即可。 1）重启Linux，见到下图，在3秒钟之内按下回车 2）三秒之内要按一下回车，出现如下界面 3）按下e键就可以进入下图 4）移动到下一行，再次按e键 5）移动到下一行，进行修改 修改完成后回车键，然后按b键进行重新启动进入系统 6）移动到下一行，进行修改 最终修改完密码，reboot一下即可。 6.1 安装远程连接linux服务器工具Linux一般作为服务器使用，而服务器一般放在机房，你不可能在机房操作你的Linux服务器。这时我们就需要远程登录到Linux服务器来管理维护系统。 Linux系统中是通过SSH服务实现的远程登录功能，默认ssh服务端口号为 22。Window系统上 Linux 远程登录客户端有SecureCRT, Putty, SSH Secure Shell,XShell等 我这里安装的是 xshell。安装流程自行百度，比较简单。 # 好了到此，linux 相关的环境安装就已经结束了，linux相关的指令操作，可以参考下一章节。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"Mysql索引详解","slug":"Mysql索引详解","date":"2016-08-01T02:37:15.000Z","updated":"2017-08-17T09:35:34.792Z","comments":true,"path":"2016/08/01/Mysql索引详解/","link":"","permalink":"http://kingge.top/2016/08/01/Mysql索引详解/","excerpt":"","text":"前言 索引对查询的速度有着至关重要的影响，理解索引也是进行数据库性能调优的起点。考虑如下情况，假设数据库中一个表有10^6条记录，DBMS的页面大小为4K，并存储100条记录。如果没有索引，查询将对整个表进行扫描，最坏的情况下，如果所有数据页都不在内存，需要读取10^4个页面，如果这10^4个页面在磁盘上随机分布，需要进行10^4次I/O，假设磁盘每次I/O时间为10ms(忽略数据传输时间)，则总共需要100s(但实际上要好很多很多)。如果对之建立B-Tree索引，则只需要进行log100(10^6)=3次页面读取，最坏情况下耗时30ms。这就是索引带来的效果，很多时候，当你的应用程序进行SQL查询速度很慢时，应该想想是否可以建索引。进入正题： 有些硬啃的干货还是得了解的，下面先了解索引的基本知识 索引分类 单列索引 主键索引 唯一索引 普通索引 组合索引用到的表CREATE TABLE `award` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '用户id', `aty_id` varchar(100) NOT NULL DEFAULT '' COMMENT '活动场景id', `nickname` varchar(12) NOT NULL DEFAULT '' COMMENT '用户昵称', `is_awarded` tinyint(1) NOT NULL DEFAULT 0 COMMENT '用户是否领奖', `award_time` int(11) NOT NULL DEFAULT 0 COMMENT '领奖时间', `account` varchar(12) NOT NULL DEFAULT '' COMMENT '帐号', `password` char(32) NOT NULL DEFAULT '' COMMENT '密码', `message` varchar(255) NOT NULL DEFAULT '' COMMENT '获奖信息', `created_time` int(11) NOT NULL DEFAULT 0 COMMENT '创建时间', `updated_time` int(11) NOT NULL DEFAULT 0 COMMENT '更新时间', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='获奖信息表'; 单列索引普通索引 这个是最基本的索引 创建语法：其sql格式是： 第一种方式 : CREATE INDEX IndexName ON `TableName`(`字段名`(length)) 第二种方式 : ALTER TABLE TableName ADD INDEX IndexName(`字段名`(length)) 创建例子：第一种方式 : CREATE INDEX account_Index ON `award`(`account`);第二种方式: ALTER TABLE award ADD INDEX account_Index(`account`) 唯一索引 与普通索引类似,但是不同的是唯一索引要求所有的类的值是唯一的,这一点和主键索引一样.但是他允许有空值 创建语法：其sql格式是： 第一种方式 : CREATE UNIQUE INDEX IndexName ON `TableName`(`字段名`(length)); 第二种方式 : ALTER TABLE TableName ADD UNIQUE (column_list) 创建例子：CREATE UNIQUE INDEX account_UNIQUE_Index ON `award`(`account`); 主键索引 他与唯一索引的不同在于不允许有空值(在B+TREE中的InnoDB引擎中,主键索引起到了至关重要的地位) 创建语法：其sql格式是： 第一种方式 : CREATE UNIQUE INDEX IndexName ON `TableName`(`字段名`(length)); 第二种方式 : ALTER TABLE TableName ADD UNIQUE (column_list) 创建例子：CREATE UNIQUE INDEX account_UNIQUE_Index ON `award`(`account`); 单列索引的总结mysql&gt;SELECT ｀uid｀ FROM people WHERE lname｀='Liu' AND ｀fname｀='Zhiqun' AND ｀age｀=26因为我们不想扫描整表，故考虑用索引。单列索引：ALTER TABLE people ADD INDEX lname (lname);将lname列建索引，这样就把范围限制在lname='Liu'的结果集1上，之后扫描结果集1，产生满足fname='Zhiqun'的结果集2，再扫描结果集2，找到 age=26的结果集3，即最终结果。由 于建立了lname列的索引，与执行表的完全扫描相比，效率提高了很多，但我们要求扫描的记录数量仍旧远远超过了实际所需 要的。虽然我们可以删除lname列上的索引，再创建fname或者age 列的索引，但是，不论在哪个列上创建索引搜索效率仍旧相似。&gt; 所以就需要组合索引 组合索引 一个表中含有多个单列索引不代表是组合索引,通俗一点讲 组合索引是:包含多个字段但是只有索引名称 创建语法：其sql格式是： CREATE INDEX IndexName On `TableName`(`字段名`(length),`字段名`(length),...); 创建例子：CREATE INDEX nickname_account_createdTime_Index ON `award`(`nickname`, `account`, `created_time`); 如果你建立了 组合索引(nickname_account_createdTime_Index) 那么他实际包含的是3个索引 (nickname) (nickname,account)(nickname,account,created_time) 组合索引的最左前缀 上面的例子中给nickname,account,created_time 这三个字段建立索引他会去创建三个索引，但是在执行查询的时候只会用其中一个索引去查询，mysql会选择一个最严格(获得结果集记录数最少)的索引，所以where子句中使用最频繁的一列放在最左边。所谓最左前缀原则就是先要看第一列，在第一列满足的条件下再看左边第二列 全文索引 文本字段上(text)如果建立的是普通索引,那么只有对文本的字段内容前面的字符进行索引,其字符大小根据索引建立索引时申明的大小来规定.如果文本中出现多个一样的字符,而且需要查找的话,那么其条件只能是 where column lick &apos;%xxxx%&apos; 这样做会让索引失效.这个时候全文索引就祈祷了作用了ALTER TABLE tablename ADD FULLTEXT(column1, column2)有了全文索引，就可以用SELECT查询命令去检索那些包含着一个或多个给定单词的数据记录了。ELECT * FROM tablenameWHERE MATCH(column1, column2) AGAINST(‘xxx′, ‘sss′, ‘ddd′)这条命令将把column1和column2字段里有xxx、sss和ddd的数据记录全部查询出来。 总结使用索引的优点 可以通过建立唯一索引或者主键索引,保证数据库表中每一行数据的唯一性. 建立索引可以大大提高检索的数据,以及减少表的检索行数 在表连接的连接条件 可以加速表与表直接的相连 在分组和排序字句进行数据检索,可以减少查询时间中 分组 和 排序时所消耗的时间(数据库的记录会重新排序) 建立索引,在查询中使用索引 可以提高性能 使用索引的缺点 在创建索引和维护索引 会耗费时间,随着数据量的增加而增加 索引文件会占用物理空间,除了数据表需要占用物理空间之外,每一个索引还会占用一定的物理空间 当对表的数据进行 INSERT,UPDATE,DELETE 的时候,索引也要动态的维护,这样就会降低数据的维护速度,(建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快)。 使用索引需要注意的地方 在经常需要搜索的列上,可以加快索引的速度 主键列上可以确保列的唯一性 在表与表的而连接条件上加上索引,可以加快连接查询的速度 在经常需要排序(order by),分组(group by)和的distinct 列上加索引 可以加快排序查询的时间, (单独order by 用不了索引，索引考虑加where 或加limit) 在一些where 之后的 &lt; &lt;= &gt; &gt;= BETWEEN IN 以及某个情况下的like 建立字段的索引(B-TREE) like语句的 如果你对nickname字段建立了一个索引.当查询的时候的语句是 nickname lick ‘%ABC%’ 那么这个索引讲不会起到作用.而nickname lick ‘ABC%’ 那么将可以用到索引 索引不会包含NULL列,如果列中包含NULL值都将不会被包含在索引中,复合索引中如果有一列含有NULL值那么这个组合索引都将失效,一般需要给默认值0或者 ‘ ‘字符串 使用短索引,如果你的一个字段是Char(32)或者int(32),在创建索引的时候指定前缀长度 比如前10个字符 (前提是多数值是唯一的..)那么短索引可以提高查询速度,并且可以减少磁盘的空间,也可以减少I/0操作. 不要在列上进行运算,这样会使得mysql索引失效,也会进行全表扫描 选择越小的数据类型越好,因为通常越小的数据类型通常在磁盘,内存,cpu,缓存中 占用的空间很少,处理起来更快 什么情况下不建立索引 查询中很少使用到的列 不应该创建索引,如果建立了索引然而还会降低mysql的性能和增大了空间需求. 很少数据的列也不应该建立索引,比如 一个性别字段 0或者1,在查询中,结果集的数据占了表中数据行的比例比较大,mysql需要扫描的行数很多,增加索引,并不能提高效率 定义为text和image和bit数据类型的列不应该增加索引 当表的修改(UPDATE,INSERT,DELETE)操作远远大于检索(SELECT)操作时不应该创建索引,这两个操作是互斥的关系 好的文章转：SQL优化转：MySQL索引原理及慢查询优化","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"索引","slug":"索引","permalink":"http://kingge.top/tags/索引/"}]},{"title":"C++文件流操作的读与写","slug":"C-文件流操作的读与写","date":"2014-11-08T13:04:00.000Z","updated":"2017-08-17T06:33:56.710Z","comments":true,"path":"2014/11/08/C-文件流操作的读与写/","link":"","permalink":"http://kingge.top/2014/11/08/C-文件流操作的读与写/","excerpt":"","text":"对文件的写入put和&lt;&lt; 写入方式 put的操作：是对文件进行写入的操作，写入一个字符（可以使字母也可以是asci码值） file.put(' A');file.put('\\n');file &lt;&lt; \"xiezejing1994\"; 输出： &nbsp;&nbsp;&nbsp;&nbsp;A// 注意到A这里有几个空格 但是不影响左对齐xiezejing1994// 也就是说A的前面不会有空格 ##操作和&lt;&lt; 读写方式区别 put操作和 file &lt;&lt;‘A’这个基本上是一样的，但是有个区别就是他不可以这样file &lt;&lt;’ A’;（A的前面有空格）因为他是格式化输入 所以中间不能有”空格“但是这样file &lt;&lt;”‘ A”;（也就是以字符串的格式输入则会有空格） 文件的读操作1.getline（） getline（ cin ，string类型 ） getline( cin, z ); file1 &lt;&lt; z; （file1 为文件流对象） 例子： char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ); cout &lt;&lt; c; &#125; 假设文件1.txt内有' A xiezejing1994 这样文本它的输出：' Axiezejing1994 也就是说他没有读到换行的功能 不会输出' A xiezejing1994（原因就是getlibe其实里面有三个参数，第三个参数默认为'\\n'） 2.getline（ fstream，string ）while ( getline( file,z ) )&#123; cout &lt;&lt; z;&#125; 3.get（） char c[100]; while ( !file.eof() ) &#123; //file.getline( c,100 ,'\\0'); file.get( c,100 ,'\\0'); cout &lt;&lt; c; &#125;输出同getline一样----必须要写三个参数 否则只会输出一行（第三个参数为'\\n'也是只会输出一行）。非常严格的输出。 4.get操作 char c; file.get(c); while ( !file.eof() ) &#123; cout &lt;&lt; c; file.get(c); &#125;-----和getline的区别在于 他是读取单个字符的，所以会读取到结束符号故会输出' Axiezejing1994 对文件是否读到末尾的判断1.feof（） 该函数只有“已经读取了”结束标志时 feof（）才会返回非零值 也就是说当文件读取到文件结束标志位时他的返回值不是非零还是零 故还要在进行一次读. 例子 假设在1.txt中只有abc三个字符在进行 while（！feof(fp)） &#123; ch = getc(fp); putchar(ch); &#125;//实际上输出的是四个字符改为ch = getc（fp）；while （ ！feof（fp））&#123; putchar（ch）； ch = getc（fp）；&#125;// 这样就可以正常运行3. 可以不调用函数eof 直接就是 while （ file ） // file 就是文件流的对象&#123; 。。。。操作&#125;4.char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ,'\\0'); cout &lt;&lt; c; &#125;这个 和char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ,'\\n'); cout &lt;&lt; c; &#125;假设文本为上面的。输出分别为' A xiezejing1994' Axiezejing1994 读写1.read( 数组名，接收的个数 )2.write( 数组名，gcount函数 )#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main()&#123; ifstream file( \"D:\\\\jjj.txt\"); ofstream file1( \"D:\\\\j.txt\" , ios::app); string z; if ( !file ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125; char c[100]; while ( !file.eof() ) &#123; file.read( c,100 ); file1.write( c, file.gcount() ); &#125; file.close(); file.close(); return 0;&#125; **判断打开是否正确** 1. if( !file )2.if ( !file.good() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;3. if ( !file.is_open() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;4. if ( file.fail() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;","categories":[{"name":"c++","slug":"c","permalink":"http://kingge.top/categories/c/"}],"tags":[{"name":"文件","slug":"文件","permalink":"http://kingge.top/tags/文件/"},{"name":"C++","slug":"C","permalink":"http://kingge.top/tags/C/"},{"name":"文件读写","slug":"文件读写","permalink":"http://kingge.top/tags/文件读写/"}]},{"title":"文章例子","slug":"ceshi","date":"2013-12-02T07:30:16.000Z","updated":"2017-08-16T02:36:44.437Z","comments":true,"path":"2013/12/02/ceshi/","link":"","permalink":"http://kingge.top/2013/12/02/ceshi/","excerpt":"前言使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；","text":"前言使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的； 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行； 博客内容可以轻松打包、转移、发布到其它平台； 等等；","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://kingge.top/categories/默认分类/"}],"tags":[{"name":"tag1","slug":"tag1","permalink":"http://kingge.top/tags/tag1/"},{"name":"tag2","slug":"tag2","permalink":"http://kingge.top/tags/tag2/"},{"name":"tag3","slug":"tag3","permalink":"http://kingge.top/tags/tag3/"}]}]}