{"meta":{"title":"King哥","subtitle":"To know everything, no words don't talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）","description":"To know everything, no words don't talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）","author":"Jeremy Kinge","url":"http://kingge.top"},"pages":[{"title":"","date":"2017-08-14T09:28:56.000Z","updated":"2017-08-17T10:01:05.524Z","comments":true,"path":"about/index.html","permalink":"http://kingge.top/about/index.html","excerpt":"","text":"这个人真的很吊，什么都没留下，但是你不得不承认这个人他真的很吊啊。"},{"title":"分类","date":"2017-08-14T08:51:40.000Z","updated":"2017-08-14T08:52:32.618Z","comments":true,"path":"categories/index.html","permalink":"http://kingge.top/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2014-12-22T04:39:04.000Z","updated":"2017-08-14T09:29:28.873Z","comments":true,"path":"tags/index.html","permalink":"http://kingge.top/tags/index.html","excerpt":"","text":""},{"title":"","date":"2017-08-14T09:29:06.000Z","updated":"2017-08-17T10:02:42.294Z","comments":true,"path":"picture/index.html","permalink":"http://kingge.top/picture/index.html","excerpt":"","text":"该板块尚且还没有开发的心思,没有女朋友没有picture"}],"posts":[{"title":"Hessian 多系统访问","slug":"Hessian 多系统访问","date":"2019-06-01T04:58:04.630Z","updated":"2017-08-31T09:44:24.038Z","comments":true,"path":"2019/06/01/Hessian 多系统访问/","link":"","permalink":"http://kingge.top/2019/06/01/Hessian 多系统访问/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"springcloud个人总结","slug":"SpringCloud个人总结","date":"2019-05-01T02:21:59.000Z","updated":"2019-08-25T03:18:32.975Z","comments":true,"path":"2019/05/01/SpringCloud个人总结/","link":"","permalink":"http://kingge.top/2019/05/01/SpringCloud个人总结/","excerpt":"","text":"一下内容就是个人学习sc微服务架构中的学习总结，整个架构的东西很多，大家可以在需要某个组件时再去学习。 一、为什么需要微服务 我么那首先思考下面这些问题，为什么需要微服务，微服务能够解决什么痛点，它有什么优缺点？微服务和微服务架构是什么关系？什么是分布式？什么是集群？为了解决这些问题我们得从实现一个系统的架构的到底发生了那些演变说起。 1.1分布式的演化 1.1.1单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 优点： 适用于小型网站，小型管理系统，将所有功能都部署到一个工程里，简单易用，易于开发 缺点： 1、性能扩展比较难 2、协同开发问题 3、不利于升级维护 4、 只能采用同一种技术，很难用不同的语言或者语言不同版本开发不同模块； 5、系统耦合性强，一旦其中一个模块有问题，整个系统就瘫痪了；一旦升级其中一个模块，整个系统就停机了； 6、 集群只能是复制整个系统，即使只是其中一个模块压力大。（可能整个订单处理，仅仅是支付模块压力过大，按道理只需要升级支付模块，但是在单一场景里面是不能的） 那么这个时候我么那肯定产生了想法，就是把所有功能模块切开，分而治之，那么就演变成了下面的架构。 1.1.2 垂直应用架构​ 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率，这样就可以单独修改某个模块而不用重启或者影响其他模块，同时也可以给某个访问量剧增的模块，单独添加服务器部署集群。此时，用于加速前端页面开发的Web框架(MVC)是关键。 ​ 通过切分业务来实现各个模块独立部署，降低了维护和部署的难度，团队各司其职更易管理，性能扩展也更方便，更有针对性。 缺点： 公用模块无法重复利用，开发性的浪费（存在重复开发的问题） 面对突变的应用场景，可能某个模块对于web界面会频繁修改，但是模块业务功能没有变化，这样会造成单个应用频繁修改。所以需要界面+业务逻辑的实现分离。 没有处理好应用之间的交互问题，系统之间相互独立，例如订单模块可能会需要查询商品模块的信息。 这个时候，虽然切分了各个模块，但是没有很好地考虑到服务之间的引用等等问题。 1.1.3 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 例如我们常见的springcloud和dubbo就是属于分布式服务架构，但是严格上来讲dubbo并不是属于分布式架构，因为他并不具备分布式架构的某些特性，例如服务的分布式配置，服务网关，数据流，批量任务等等。一般认为Dubbo只是相当于SpringCloud中的Eureka模块（服务注册中心） 分布式服务框架很好的解决了垂直应用架构的缺点，实现界面和服务的分离，实现界面和服务，以及服务与服务之间的调度。 ​ 但是存在问题，那就是没有一个统一管理服务的机制和基于访问压力的调度中心(服务注册中心，负载均衡)，容易造成资源浪费，什么意思呢？假设用户服务部署了200台服务器，但是在某个时间段，他的访问压力很小，订单服务的访问压力剧增，服务器不够用。那么就会造成资源浪费和倾斜，存在服务器闲置或者请求量少的情况。 1.1.4 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)Service Oriented Architecture]是关键。 以前出现了什么问题？ 服务越来越多，需要管理每个服务的地址 调用关系错综复杂，难以理清依赖关系 服务过多，服务状态难以管理，无法根据服务情况动态管理 服务治理要做什么？ 服务注册中心，实现服务自动注册和发现，无需人为记录服务地址 服务自动订阅，服务列表自动推送，服务调用透明化，无需关心依赖关系 动态监控服务状态监控报告，人为控制服务状态 缺点： 服务间会有依赖关系，一旦某个环节出错会影响较大 服务关系复杂，运维、测试部署困难，不符合DevOps思想  1.1.5 微服务架构前面说的SOA，英文翻译过来是面向服务。微服务，似乎也是服务，都是对系统进行拆分。因此两者非常容易混淆，但其实却有一些差别： 微服务的特点： 单一职责：微服务中每一个服务都对应唯一的业务能力，做到单一职责 微：微服务的服务拆分粒度很小，例如一个用户管理就可以作为一个服务。每个服务虽小，但“五脏俱全”。 面向服务：面向服务是说每个服务都要对外暴露Rest风格服务接口API。并不关心服务的技术实现，做到与平台和语言无关，也不限定用什么技术实现，只要提供Rest的接口即可。 自治：自治是说服务间互相独立，互不干扰 团队独立：每个服务都是一个独立的开发团队，人数不能过多。 技术独立：因为是面向服务，提供Rest接口，使用什么技术没有别人干涉 前后端分离：采用前后端分离开发，提供统一Rest接口，后端不用再为PC、移动段开发不同接口 数据库分离：每个服务都使用自己的数据源 部署独立，服务间虽然有调用，但要做到服务重启不影响其它服务。有利于持续集成和持续交付。每个服务都是独立的组件，可复用，可替换，降低耦合，易维护 微服务结构图： 1.2 基本概念梳理 分布式：一个业务分拆多个子业务，部署在不同的服务器上 集群： 同一个业务，部署在多个服务器上 微服务： 微服务化的核心就是将传统的一站式应用,根据业务拆分成一个一个的服务,彻底地去耦合,每一个微服务提供单个业务功能的服务,一个服务做一件事,从技术角度看就是一种小而独立的处理过程,类似进程概念,能够自行单独启动或销毁，可以拥有自己独立的数据库。（我们之前使用springboot开发的项目就是属于一个微服务，他是单一进程，处理单一服务。-他关注的是单一业务的实现细节） 微服务架构：微服务架构是一种架构模式,它提倡将单一应用程序划分成一组小的服务,服务之间互相协调、互相配合,为用户提供最终价值.每个服务运行在其独立的进程中,服务与服务间采用轻量级的通信机制互相协作(通常是基于HTTP协议的RESTful API).每个服务都围绕着具体业务进行构建,并且能够被独立的部署到生产环境、类生产环境等.另外,应当尽量避免统一的、集中式的服务管理机制,对具体的一个服务而言,应根据业务上下文,选择合适的语言、工具对其进行构建.（springcloud就是一个微服务架构，通过一系列措施，管理微服务，实现系统整体的功能-他关注的是整体项目的实现和架构） 微服务提出者：马丁.福勒(Martin Fowler) 论文网址:https://martinfowler.com/articles/microservices.html ​ 1.3 微服务优缺点优点： 每个服务足够内聚,足够小,代码容易理解这样能聚焦一个指定的业务功能或业务需求 开发简单、开发效率提高,一个服务可能就是专一的只干一件事. 微服务能够被小团队单独开服,这个小团队是2到5人的开发人员组成 微服务是松耦合的,是有功能意义的服务,无论是在开发阶段或部署阶段都是独立的. 微服务能试用不同的语言开发 易于和第三方集成,微服务允许容易且灵活的方式集成自动部署,通过持续集成工具,如Jenkins,Hudson,bamboo. 微服务易于被一个开发人员理解,修改和维护,这样小团队能够更关注自己的工作成果.无需通过合作才能体现价值 微服务允许你利用融合最新技术. 微服务只是业务逻辑的代码,不会和HTML,CSS或其他界面组件混合. 每个微服务都有自己的存储能力,可以有自己的数据库.也可以有统一的数据库 缺点： 开发人员要处理分布式系统的复杂性 多服务运维难度,随着服务的增加,运维的压力也在增大 系统部署依赖 服务间通信成本 数据一致性 系统集成测试 性能监控 1.4 常见微服务架构 微服务条目 落地技术 服务开发 SpringBoot,Spring,SpringMVC 服务配置与管理 Netflix公司的Archaius、阿里的Diamond等 服务注册与发现 Eureka、Consul、Zookeeper等 服务调用 Rest、RPC、gRPC 服务熔断器 Hystrix、Envoy等 负载均衡 Ribbon、Nginx等 服务接口调用（客户端调用服务的简化工具） Feign等 消息队列 Kafka、RabbitMQ、ActiveMQ等 服务配置中心管理 SpringCloudConfig、Chef等 服务路由（API网关） Zuul等 服务监控 Zabbix、Nagios、Metrics、Specatator等 全链路追踪 Zipkin、Brave、Dapper等 服务部署 Docker、OpenStack、Kubernetes等 数据流操作开发包 SpringCloud Stream(封装与Redis，Rabbit，Kafka等发送接收消息) 事件消息总线 SpringCloud Bus 二、springcloud和dubbo为什么现在流行的微服务架构是springcloud而不是dubbo，最主要的是dubbo在这之前停止更新过几年的时间，这个时候springcloud异军突起，很好地抢占了先机，整体解决方案和框架成熟度，社区热度，可维护性，学习曲线也是它更加火爆的原因： 最主要的是，Dubbo 的定位始终是一款 RPC 框架，目的是提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案 然而：Spring Cloud 的目标是微服务架构下的一站式解决方案，换句话说，dubbo更像是springcloud的Eureka模块。 接下来我么你看一段关于Dubbo目前负责人刘军的一段采访 https://www.oschina.net/question/2896879_2272652?sort=time 当前各大IT公司用的微服务架构有哪些 阿里Dubbo/HSF京东JSF新浪微博Motan当当网DubboX 1.各微服务的框架对比 功能点/服务框架 Netflix/SpringCloud Motan gRPC Thrift Dubbo/DubboX 功能定位 完整的微服务架构 RPC框架，但整合了ZK或Consul，实现集群环境的基本服务注册/发现 RPC框架 RPC框架 服务框架 支持Rest 是，Ribbon支持多种可插拔的序列化选择 否 否 否 否 支持RPC 否 是 是 是 是 支持多语言 是（Rest形式） 否 是 是 否 服务注册/发现 是（Eureka） Eureka服务注册表，Karyon服务端框架支持服务自注册和健康检查 是（zookeeper/consul） 否 否 是 负载均衡 是（服务端zuul+客户端Ribbon） zuul-服务，动态路由 云端负载均衡 Eureka（针对中间层服务器） 是（客户端） 否 否 是（客户端） 配置服务 Netflix Archaius SpringCloud Config Server集中配置 是（zookeeper提供） 否 否 否 服务调用链监控 是（zuul） Zuul提供边缘服务，API网关 否 否 否 否 高可用/容错 是（服务端Hystrix+客户端Ribbon） 是（客户端） 否 否 是（客户端） 典型应用案例 Netflix Sina Google Facebook 社区活跃度 高 一般 高 一般 2017年7月才重启 学习难度 中等 一般 高 一般 低 文档丰富度 高 一般 一般 一般 高 其他 Spring Cloud Bus为我们应用程序带来了更多管理端点 支持降级 Netflix内部在开发集成gRPC IDL定义 实践公司比较多 2. springcloud VS Dubbo社区活跃度 https://github.com/dubbohttps://github.com/springcloud 功能对比 Dubbo Spring 服务注册中心 Zookeeper Spring Cloud Netfilx Eureka 服务调用方式 RPC REST API 服务监控 Dubbo-monitor Spring Boot Admin 断路器 不完善 Spring Cloud Netflix Hystrix 服务网关 无 Spring Cloud Netflix Zuul 分布式配置 无 Spring Cloud Config 服务跟踪 无 Spring Cloud Sleuth 消息总线 无 Spring Cloud Bus 数据流 无 Spring Cloud Stream 批量任务 无 Spring Cloud Task 最大区别： Spring Cloud抛弃了RPC通讯，采用基于HTTP的REST方式。Spring Cloud牺牲了服务调用的性能，但是同时也避免了原生RPC带来的问题。REST比RPC更为灵活，不存在代码级别的强依赖，在强调快速演化的微服务环境下，显然更合适。 ==一句话：Dubbo像组装机，Spring Cloud像一体机== 社区的支持与力度：Dubbo曾经停运了5年，虽然重启了，但是对于技术发展的新需求，还是需要开发者自行去拓展，对于中小型公司，显然显得比较费时费力，也不一定有强大的实力去修改源码 总结 解决的问题域不一样：Dubbo的定位是一款RPC框架，Spring Cloud的目标是微服务架构下的一站式解决方案 三、服务调用方式1.RPC和HTTP无论是微服务还是SOA，都面临着服务间的远程调用。那么服务间的远程调用方式有哪些呢？ 常见的远程调用方式有以下2种： RPC：Remote Produce Call远程过程调用，类似的还有RMI。自定义数据格式，基于原生TCP通信，速度快，效率高。早期的webservice，现在热门的dubbo，都是RPC的典型代表 Http：http其实是一种网络传输协议，基于TCP，规定了数据传输的格式。现在客户端浏览器与服务端通信基本都是采用Http协议，也可以用来进行远程服务调用。缺点是消息封装臃肿，优势是对服务的提供和调用方没有任何技术限定，自由灵活，更符合微服务理念。 现在热门的Rest风格，就可以通过http协议来实现。 如果你们公司全部采用Java技术栈，那么使用Dubbo作为微服务架构是一个不错的选择。 相反，如果公司的技术栈多样化，而且你更青睐Spring家族，那么SpringCloud搭建微服务是不二之选。在我们的项目中，我们会选择SpringCloud套件，因此我们会使用Http方式来实现服务间调用。 2.Http客户端工具既然微服务选择了Http，那么我们就需要考虑自己来实现对请求和响应的处理。不过开源世界已经有很多的http客户端工具，能够帮助我们做这些事情，例如： HttpClient OKHttp URLConnection 接下来，不过这些不同的客户端，API各不相同 3.Spring的RestTemplateSpring提供了一个RestTemplate模板工具类，对基于Http的客户端进行了封装，并且实现了对象与json的序列化和反序列化，非常方便。RestTemplate并没有限定Http的客户端类型，而是进行了抽象，目前常用的3种都有支持： HttpClient OkHttp JDK原生的URLConnection（默认的） RestTemplate简单使用 首先在项目中注册一个RestTemplate对象，可以在启动类位置注册： @SpringBootApplicationpublic class HttpDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(HttpDemoApplication.class, args); &#125; @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 在测试类中直接@Autowired注入： @RunWith(SpringRunner.class)@SpringBootTest(classes = HttpDemoApplication.class)public class HttpDemoApplicationTests &#123; @Autowired private RestTemplate restTemplate; @Test public void httpGet() &#123; // 调用springboot案例中的rest接口 User user = this.restTemplate.getForObject(\"http://localhost/user/1\", User.class); System.out.println(user); &#125;&#125; 通过RestTemplate的getForObject()方法，传递url地址及实体类的字节码，RestTemplate会自动发起请求，接收响应，并且帮我们对响应结果进行反序列化。 学习完了Http客户端工具，接下来就可以正式学习微服务了。 四、初识SpringCloud微服务是一种架构方式，最终肯定需要技术架构去实施。 微服务的实现方式很多，但是最火的莫过于Spring Cloud了。为什么？ 后台硬：作为Spring家族的一员，有整个Spring全家桶靠山，背景十分强大。 技术强：Spring作为Java领域的前辈，可以说是功力深厚。有强力的技术团队支撑，一般人还真比不了 群众基础好：可以说大多数程序员的成长都伴随着Spring框架，试问：现在有几家公司开发不用Spring？SpringCloud与Spring的各个框架无缝整合，对大家来说一切都是熟悉的配方，熟悉的味道。 使用方便：相信大家都体会到了SpringBoot给我们开发带来的便利，而SpringCloud完全支持SpringBoot的开发，用很少的配置就能完成微服务框架的搭建 1.简介SpringCloud是Spring旗下的项目之一，官网地址：http://projects.spring.io/spring-cloud/ 官网介绍:https://spring.io/ SpringCloud,基于springboot提供了一套为服务解决方案. Spring最擅长的就是集成，把世界上最好的框架拿过来，集成到自己的项目中。 SpringCloud也是一样，它将现在非常流行的一些技术整合到一起，实现了诸如：配置管理，服务发现，智能路由，负载均衡，熔断器，控制总线，集群状态等等功能。其主要涉及的组件包括： Eureka：服务治理组件，包含服务注册中心，服务注册与发现机制的实现。（服务治理，服务注册/发现） Zuul：网关组件，提供智能路由，访问过滤功能 Ribbon：客户端负载均衡的服务调用组件（客户端负载） Feign：服务调用，给予Ribbon和Hystrix的声明式服务调用组件 （声明式服务调用） Hystrix：容错管理组件，实现断路器模式，帮助服务依赖中出现的延迟和为故障提供强大的容错能力。(熔断、断路器，容错) 架构图： 以上只是其中一部分。 SpringCloud,基于springboot提供了一套为服务解决方案,包括服务注册与发现,配置中心,全链路监控,服务网关,负载均衡,熔断器等组件,除了基于NetFlix的开源组件做高度抽象封装之外,还有一些选型中立的开源组件. SpringCloud利用springboot的开发便利性巧妙地简化了分布式系统基础设施的开发,SpringCloud为开发人员提供了快速构建分布式系统的一些工具,包括配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等,它们都可以利用Springboot的开发风格做到一键启动和部署. SpringBoot并没有重复制造轮子,它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来,通过Springboot风格进行再封装屏蔽掉了复杂的配置和实现原理,最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包. 2.SpringCloud和springboot是什么关系Springboot专注于快速方便的开发单个个体微服务. SpringCloud是关注全局的微服务协调整理治理框架,它将Springboot开发的一个个单体微服务整合并管理起来,为各个微服务质检提供,配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务 Springboot可以离开SpringCloud独立使用开发项目,但是SpringCloud离不开Springboot,属于依赖的关系.Springboot专注于快速、方便的开发单个微服务个体,SpringCloud关注全局的服务治理框架. 3.springcloud的版本因为Spring Cloud不同其他独立项目，它拥有很多子项目的大项目。所以它的版本是版本名+版本号 （如Angel.SR6）。 版本名：是伦敦的地铁名 版本号：SR（Service Releases）是固定的 ,大概意思是稳定版本。后面会有一个递增的数字。 所以 Edgware.SR3就是Edgware的第3个Release版本。 我们在项目中，会是以Finchley的版本。 其中包含的组件，也都有各自的版本，如下表： Component Edgware.SR3 Finchley.RC1 Finchley.BUILD-SNAPSHOT spring-cloud-aws 1.2.2.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-bus 1.3.2.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-cli 1.4.1.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-commons 1.3.3.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-contract 1.2.4.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-config 1.4.3.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-netflix 1.4.4.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-security 1.2.2.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-cloudfoundry 1.1.1.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-consul 1.3.3.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-sleuth 1.3.3.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-stream Ditmars.SR3 Elmhurst.RELEASE Elmhurst.BUILD-SNAPSHOT spring-cloud-zookeeper 1.2.1.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-boot 1.5.10.RELEASE 2.0.1.RELEASE 2.0.0.BUILD-SNAPSHOT spring-cloud-task 1.2.2.RELEASE 2.0.0.RC1 2.0.0.RELEASE spring-cloud-vault 1.1.0.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-gateway 1.0.1.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-openfeign 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT 4.SpringCloud的参考资料五、springcloud的实现准备为了下面使用springcloud微服务架构各个优秀的组件，我们先搭建一个基本的分布式项目工程。 案例使用的springcloud和springboot版本分别是： &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Dalston.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt; 1 新建父工程-microservicecloudmicroservicecloud 打包方式设置为pom 设置pom.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;lombok.version&gt;1.16.18&lt;/lombok.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Dalston.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.0.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.31&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 查看项目 新建父工程目的：定义pom文件，统一各个子模块的jar依赖版本，方面管理，避免每个子模块使用相同组件不同版本，造成项目测试运行出现问题。 2 根据父工程，新建api公共模块-microservicecloud-api目的：抽取出所有子项目公共的bean或者方法。例如在下面的创建的服务提供者和服务消费者都是用到了Person这个javabean，那么我们就需要把这个bean放在这里。然后服务提供者和消费者就可以依赖这个api公共模块，从而实用Person实体类。 创建完毕 2.1 查看父工程pom 发现多了这一行，因为我们是在父工程microserviceproject下新建的，表示microservicecloud-api工程为父工程子模块。 2.2 修改microservicecloud-api的pom文件修改内容如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;dependencies&gt;&lt;!-- 当前Module需要用到的jar包，按自己需求添加，如果父类已经包含了，可以不用写版本号 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt;//这个组件可用可不用 &lt;/dependencies&gt;&lt;/project&gt; Lombok组件在真实项目中不建议使用，虽然他简化了javabean的开发，但是代码的可读性也产生了严重的影响。 2.3 新建公共bean-Person 生成get/set方法 2.4 项目结构 3 根据父工程，新建微服务提供者-providermicroservicecloud-provider-person-8001 –&gt; 8001表示服务暴露的端口号 新建方法同新建-microservicecloud-api 模块 3.1 修改pom文件&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;microservicecloud-provider-person-8001&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!-- 引入自己定义的api通用包，可以使用Person用户Entity --&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 3.2 新建application.yml文件 内容是：详细的参见代码 server: port: 8001mybatis: config-location: classpath:mybatis/mybatis.cfg.xml # mybatis配置文件所在路径 type-aliases-package: com.kingge.entity # 所有Entity别名类所在包 mapper-locations: - classpath:mybatis/mapper/**/*.xml # mapper映射文件spring: application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://127.0.0.1:3306/test # 数据库名称 username: root password: 123 dbcp2: min-idle: 5 # 数据库连接池的最小维持连接数 initial-size: 5 # 初始化连接数 max-total: 5 # 最大连接数 max-wait-millis: 200 注意每个属性后面必须是有空格-yml文件的格式 3.3 新建person数据表DROP TABLE IF EXISTS `person`;CREATE TABLE `person` ( `deptno` int(255) NOT NULL AUTO_INCREMENT, `dname` varchar(255) DEFAULT NULL, `db_source` varchar(255) DEFAULT NULL, //这和字段标识当前数据来源于那个数据库，后面讲解Eureka集群时会使用到 PRIMARY KEY (`deptno`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;-- ------------------------------ Records of person-- ----------------------------INSERT INTO `person` VALUES (&apos;1&apos;, &apos;开发部&apos;, &apos;test&apos;);INSERT INTO `person` VALUES (&apos;2&apos;, &apos;人事部&apos;, &apos;test&apos;);INSERT INTO `person` VALUES (&apos;3&apos;, &apos;集成部&apos;, &apos;test&apos;);INSERT INTO `person` VALUES (&apos;4&apos;, &apos;市场部&apos;, &apos;test&apos;); 3.4 新建dao和mapper 内容 import com.kingge.entity.Person;import org.apache.ibatis.annotations.Mapper;import java.util.List;@Mapperpublic interface PersonDao &#123; public boolean addDept(Person dept); public Person findById(Long id); public List&lt;Person&gt; findAll();&#125; 3.4.1 新建mybatis.cfg.xml实际上我们不需要这个xml文件，因为我们的配置一般都是已经放置在了application.yml文件中，但是为了整体架构的扩展性，这里也新建了改文件 3.5 新建PersonMapper.xml 存放mapper.xml文件的位置我们在上面application.yml中已经声明 内容 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.kingge.dao.PersonDao&quot;&gt; &lt;select id=&quot;findById&quot; resultType=&quot;Person&quot; parameterType=&quot;Long&quot;&gt; select deptno,dname,db_source from person where deptno=#&#123;deptno&#125;; &lt;/select&gt; &lt;select id=&quot;findAll&quot; resultType=&quot;Person &quot;&gt; select deptno,dname,db_source from person; &lt;/select&gt; &lt;insert id=&quot;addDept&quot; parameterType=&quot;Person &quot;&gt; INSERT INTO person(dname,db_source) VALUES(#&#123;dname&#125;,DATABASE()); &lt;/insert&gt;&lt;/mapper&gt; 3.6 新建PersonService接口和接口实现类impl PersonService 接口 public interface PersonService&#123; publicboolean add(Person dept); public Personget(Long id); publicList&lt;Person&gt; list();&#125; PersonServiceImpl 实现类 @Servicepublic class PersonServiceImpl implements PersonService&#123; @Autowired private PersonDao dao; @Override public boolean add(Person person)&#123; return dao.addDept(person); &#125; @Override public Person get(Long id)&#123; return dao.findById(id); &#125; @Override public List&lt;Person&gt; list()&#123; return dao.findAll(); &#125;&#125; 3.7 controller控制层-PersonController代码实现： @RestControllerpublic class PersonController&#123; @Autowired private PersonService service;//全部使用restful风格，返回json字符串 @RequestMapping(value = &quot;/person/add&quot;, method = RequestMethod.POST) public boolean add(@RequestBody Person person) &#123; return service.add(person); &#125; @RequestMapping(value = &quot;/person/get/&#123;id&#125;&quot;, method = RequestMethod.GET) public Person get(@PathVariable(&quot;id&quot;) Long id) &#123; return service.get(id); &#125; @RequestMapping(value = &quot;/person/list&quot;, method = RequestMethod.GET) public List&lt;Person&gt; list() &#123; return service.list(); &#125;&#125; 3.8 springboot启动类@SpringBootApplicationpublic class ApplicationBootStart &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart.class, args); &#125;&#125; 启动服务并测试访问。 3.9 项目整体结构 4 根据父工程，新建微服务消费者-consumermicroservicecloud-consumer-person-80 方法同上 4.1 修改pom文件&lt;dependencies&gt; &lt;dependency&gt;&lt;!-- 自己定义的api --&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;version&gt;2.1.7.RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.2 添加application.yml文件添加端口配置 server: port: 80 4.3 新建@Configuration注解的配置类@Configurationpublic class ConfigBean&#123; @Bean public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125;&#125; 它的作用就是我们整合SSM的时候，对应的applicationContext.xml 4.4 新建消费者控制器，访问服务提供者获取数据通过RestTemplate获取消息提供者暴露的服务信息。 @RestControllerpublic class ConsumerController &#123; @Autowired private RestTemplate restTemplate; private static final String REST_URL_PREFIX = &quot;http://localhost:8001&quot;;//这里通过书写固定的服务提供者的地址，后面我们学习到了Eureka服务注册中心，那么注重修改这里 @RequestMapping(value = &quot;/consumer/person/add&quot;) public boolean add(Person person) &#123; return restTemplate.postForObject(REST_URL_PREFIX + &quot;/person/add&quot;, person, Boolean.class); &#125; @RequestMapping(value = &quot;/consumer/person/get/&#123;id&#125;&quot;) public Person get(@PathVariable(&quot;id&quot;) Long id) &#123; return restTemplate.getForObject(REST_URL_PREFIX + &quot;/person/get/&quot; + id, Person.class); &#125; @SuppressWarnings(&quot;unchecked&quot;) @RequestMapping(value = &quot;/consumer/person/list&quot;) public List&lt;Person&gt; list() &#123; return restTemplate.getForObject(REST_URL_PREFIX + &quot;/person/list&quot;, List.class); &#125;&#125; 4.5 新建启动类@SpringBootApplicationpublic class ApplicationBootStart &#123; public static void main(String[] args)&#123; SpringApplication.run(ApplicationBootStart.class, args); &#125;&#125; 4.6 完整项目结构 5 同时启动服务提供者和服务消费者也就是运行ApplicationBootStart8001和ApplicationBootStart80启动类即可 测试服务是否可以消费，访问消费者 测试成功,以上就是成功的搭建了一个简单的微服务架构，下面我们会逐步的加入springcloud的其他组件，完善这个架构 5.上面的项目存在什么问题存在什么问题？ 在consumer中，我们把url地址硬编码到了代码中，不方便后期维护 consumer需要记忆provider的地址，如果出现变更，可能得不到通知，地址将失效 consumer不清楚provider的状态，服务宕机也不知道 provider只有1台服务，不具备高可用性 即便provider形成集群，consumer还需自己实现负载均衡 其实上面说的问题，概括一下就是分布式服务必然要面临的问题： 服务管理 如何自动注册和发现 如何实现状态监管 如何实现动态路由 服务如何实现负载均衡 服务如何解决容灾问题 服务如何实现统一配置 以上的问题，我们都将在SpringCloud中得到答案。 六、Eureka注册中心​ 首先我们来解决第一问题，服务的管理 - 利用Eureka解决服务治理问题 1.Eureka概念 ​ Eureka是Netflix的一个子模块，也是核心模块之一。Eureka是一个基于REST的服务，用于定位服务，以实现云端中间层服务发现和故障转移。服务注册与发现对于微服务架构来说是非常重要的，有了服务发现与注册，只需要使用服务的标识符，就可以访问到服务(解决上诉案例在consumer中，我们把服务提供者的url地址硬编码到了代码中)，而不需要修改服务调用的配置文件了。功能类似于dubbo的注册中心，比如Zookeeper。 ​ Netflix在设计Eureka时遵守的就是CAP规则中的AP原则。 CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得 特别提示：同样作为dubbo服务注册中心的zookeeper遵守的是CP原则。 2.Eureka架构和原理Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务注册和发现(请对比Zookeeper)。 Eureka 采用了 C-S 的设计架构。Eureka Server 作为服务注册功能的服务器，它是服务注册中心。 而系统中的其他微服务，使用 Eureka 的客户端（上诉案例中的，服务提供者和服务消费者相对于EurekaServer都是属于客户端，前者是向EurekaServer注册服务，后者是向EurekaServer获取服务）连接到 Eureka Server并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。SpringCloud 的一些其他模块（比如Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关的逻辑。 基本架构： Eureka包含两个组件：Eureka Server和Eureka Client Eureka Server提供服务注册服务各个节点启动后，会在EurekaServer中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观的看到 EurekaClient是一个Java客户端，用于简化Eureka Server的交互，客户端同时也具备一个内置的、使用轮询(round-robin)负载算法的负载均衡器。在应用启动后，将会向Eureka Server发送心跳(默认周期为30秒)。如果Eureka Server在多个心跳周期内没有接收到某个节点的心跳，EurekaServer将会从服务注册表中把这个服务节点移除（默认90秒） Eureka：就是服务注册中心（可以是一个集群），对外暴露自己的地址 提供者：启动后向Eureka注册自己信息（地址，提供什么服务） 消费者：向Eureka订阅服务，Eureka会将对应服务的所有提供者地址列表发送给消费者，并且定期更新 心跳(续约)：提供者定期通过http方式向Eureka刷新自己的状态 3. 实现EurekaServer接下来我们通过加入springcloud的Eureka服务治理组件，改造之前的例子，解决动态路由（上个例子中我们是把服务提供者的url硬编码到消费者中）、注册发现，动态监管的问题 3.1 根据父工程实现单节点Eureka服务注册中心根据父工程microservicecloud 创建 microservicecloud-eureka-7001模块 3.1.1 修改pom文件导入EurekaServer依赖&lt;dependencies&gt; &lt;!--eureka-server服务端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies 3.1.2 修改application.yml配置文件server: port: 7001eureka: instance: hostname: localhost #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ # #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 #等同于 http://localhost:7001/eureka/ 3.1.3 创建启动类并添加@EnableEurekaServer注解package com.kingge.entity;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServer ////EurekaServer服务器端启动类,接受其它微服务注册进来public class ApplicationBootStart7001 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart7001.class, args); &#125;&#125; 3.1.4 完整项目结构 3.1.5 运行启动类，启动EurekaServer访问网址：http://localhost:7001/ No application available 没有服务被发现 —- 因为没有注册服务进来当然不可能有服务被发现 接下来我门把8001模块服务注册进来 3.2 修改 服务提供者也就是修改上面我们实现的：microservicecloud-provider-person-8001 模块，将人员服务注册进EurekaServer中 3.2.1 修改pom文件修改内容： &lt;!-- 将微服务provider端注册进eureka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; 3.2.2 修改application.yml配置文件eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://localhost:7001/eureka #这个地址就是我们在3.1.2定义的EurekaServer对外暴露的连接地址。 3.2.3 修改启动类添加@EnableEurekaClient注解@SpringBootApplication@EnableEurekaClient////本服务启动后会自动注册进 eureka服务中public class ApplicationBootStart8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart8001.class, args); &#125;&#125; 3.3 修改服务消费者通过访问服务名称的方式消费服务，解决硬编码服务提供者url的问题 也就是修改上面我们实现的：microservicecloud-consumer-person-80 模块 3.3.1 修改pom文件修改内容： &lt;!-- 将微服务provider端注册进eureka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; 3.3.2 修改application.yml配置文件eureka: client: register-with-eureka: false service-url: defaultZone: http://localhost:7001/eureka/ 3.3.3 修改启动类添加@EnableEurekaClient注解@SpringBootApplication@EnableEurekaClient//public class ApplicationBootStart80 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80.class, args); &#125;&#125; 3.3.4修改ConsumerController代码修改内容如下 // private static final String REST_URL_PREFIX = &quot;http://localhost:8001&quot;; private static final String REST_URL_PREFIX = &quot;http://MICROSERVICECLOUD-PERSON&quot;; 把url更改为服务名称。 3.4 启动EurekaServer和服务提供者，以及服务消费者运行ApplicationBootStart7001和ApplicationBootStart8001、ApplicationBootStart80 查看地址：http://localhost:7001/ 注册成功！！！ 服务名称就是我们在服务提供者的application.yml配置文件中配置的： spring: application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 我们发现生成的实例名称是没有任何意义的，而且实例的介绍地址不是ip的形式 下面就讲解修改这些小细节 4.actuator与注册微服务信息完善4.1 修改服务实例名和实例名访问路径显示ip修改microservicecloud-provider-person-8001 模块的配置文件，添加以下内容 instance: instance-id: microservicecloud-person8001 prefer-ip-address: true #访问路径可以显示IP地址 注意instance属性是eureka的子属性 重启服务提供者ApplicationBootStart8001 查看EurekaServer 这里涉及到了Eureka 的自我保护机制，下一章节我们会讲到。 4.2 修改服务实例的详情页info默认我么你点击服务实例名，跳转到的界面是： 接下来我们要定制一下这界面，显示一下当前服务实例的一些说明信息。 （1）修改microservicecloud-provider-person-8001 模块的pom文件，添加以下内容 &lt;!-- actuator监控信息完善 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改microservicecloud-provider-person-8001 模块的配置文件，添加以下内容 info: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个提供查询部门人员信息的服务 （3）修改父工程的pom文件，添加以下内容 &lt;build&gt; &lt;finalName&gt;microservicecloud&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;delimiters&gt; &lt;delimit&gt;$&lt;/delimit&gt; &lt;/delimiters&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 如果不添加那么就无法解析像这样的动态赋值${spring.application.name} 重启EurekaServer和服务提供者 再次查看服务实例名的info界面 5.Eureka详解5.1.基础架构Eureka架构中的三个核心角色： 服务注册中心 Eureka的服务端应用，提供服务注册和发现功能，就是刚刚我们建立的7001模块。 服务提供者 提供服务的应用，可以是SpringBoot应用，也可以是其它任意技术实现，只要对外提供的是Rest风格服务即可。本例中就是我们实现的8001模块。 服务消费者 消费应用从注册中心获取服务列表，从而得知每个服务方的信息，知道去哪里调用服务方。本例中就是我们实现的80模块。 服务提供和服务消费者相对于服务注册中心，他们都是客户端。所以他们访问EurekaServer导入的依赖是相同的都是spring-cloud-starter-netflix-eureka-client 5.2.服务提供者服务提供者要向EurekaServer注册服务，并且完成服务续约等工作。 服务注册 服务提供者在启动时，会检测配置属性中的：eureka.client.register-with-eureka=true参数是否正确，事实上默认就是true。如果值确实为true，则会向EurekaServer发起一个Rest请求，并携带自己的元数据信息，Eureka Server会把这些信息保存到一个双层Map结构中。 第一层Map的Key就是服务id，一般是配置中的spring.application.name属性 第二层Map的key是服务的实例id。一般host+ serviceId + port，例如：locahost:service-provider:8081 值则是服务的实例对象，也就是说一个服务，可以同时启动多个不同实例，形成集群。 服务续约 在注册服务完成以后，服务提供者会维持一个心跳（定时向EurekaServer发起Rest请求），告诉EurekaServer：“我还活着”。这个我们称为服务的续约（renew）； 有两个重要参数可以修改服务续约的行为： eureka: instance: lease-expiration-duration-in-seconds: 90 lease-renewal-interval-in-seconds: 30 lease-renewal-interval-in-seconds：服务续约(renew)的间隔，默认为30秒 lease-expiration-duration-in-seconds：服务失效时间，默认值90秒 也就是说，默认情况下每个30秒服务会向注册中心发送一次心跳，证明自己还活着。如果超过90秒没有发送心跳，EurekaServer就会认为该服务宕机，会从服务列表中移除，这两个值在生产环境不要修改，默认即可。 但是在开发时，这个值有点太长了，经常我们关掉一个服务，会发现Eureka依然认为服务在活着。所以我们在开发阶段可以适当调小。 eureka: instance: lease-expiration-duration-in-seconds: 10 # 10秒即过期 lease-renewal-interval-in-seconds: 5 # 5秒一次心跳 5.3.服务消费者 获取服务列表 当服务消费者启动时，会检测eureka.client.fetch-registry=true参数的值，如果为true，则会拉取Eureka Server服务的列表只读备份，然后缓存在本地。并且每隔30秒会重新获取并更新数据。我们可以通过下面的参数来修改： eureka: client: registry-fetch-interval-seconds: 5 生产环境中，我们不需要修改这个值。 但是为了开发环境下，能够快速得到服务的最新状态，我们可以将其设置小一点。 5.4.失效剔除和自我保护 服务下线 当服务进行正常关闭操作时，它会触发一个服务下线的REST请求给Eureka Server，告诉服务注册中心：“我要下线了”。服务中心接受到请求之后，将该服务置为下线状态。 失效剔除 有些时候，我们的服务提供方并不一定会正常下线，可能因为内存溢出、网络故障等原因导致服务无法正常工作。Eureka Server需要将这样的服务剔除出服务列表。因此它会开启一个定时任务，每隔60秒对所有失效的服务（超过90秒未响应）进行剔除。 可以通过eureka.server.eviction-interval-timer-in-ms参数对其进行修改，单位是毫秒，生产环境不要修改。 这个会对我们开发带来极大的不变，你对服务重启，隔了60秒Eureka才反应过来。开发阶段可以适当调整，比如：10秒 自我保护 我们关停一个服务，就会在Eureka面板看到一条警告： 这是触发了Eureka的自我保护机制。当一个服务未按时进行心跳续约时，Eureka会统计最近15分钟心跳失败的服务实例的比例是否超过了85%。在生产环境下，因为网络延迟等原因，心跳失败实例的比例很有可能超标，但是此时就把服务剔除列表并不妥当，因为服务可能没有宕机。Eureka就会把当前实例的注册信息保护起来，不予剔除。生产环境下这很有效，保证了大多数服务依然可用。 也就是好死不如赖活着，这个就是用EurekaServer的AP原则，保证可用性 但是这给我们的开发带来了麻烦， 因此开发阶段我们都会关闭自我保护模式：（itcast-eureka） eureka: server: enable-self-preservation: false # 关闭自我保护模式（缺省为打开） eviction-interval-timer-in-ms: 1000 # 扫描失效服务的间隔时间（缺省为60*1000ms） 综上，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留），也不盲目注销任何健康的微服务。使用自我保护模式，可以让Eureka集群更加的健壮、稳定。 一句话：某时刻某一个微服务不可用了，eureka不会立刻清理，依旧会对该微服务的信息进行保存 6.消费者获取服务信息如果我们想要在消费者端获取服务者提供的服务实例列表，那么应该怎么做？对于注册进eureka里面的微服务，可以通过服务发现来获得该服务的信息 既然是消费者端想查看服务端暴露的服务信息，那么就需要在服务提供者实现一个查询暴露服务实例的列表的接口 1.修改服务端PersonController 添加如下代码，查询服务名称为MICROSERVICECLOUD-PERSON的服务实例列表信息 @Autowiredprivate DiscoveryClient client;@RequestMapping(value = &quot;/person/discovery&quot;, method = RequestMethod.GET)public Object discovery()&#123; List&lt;String&gt; list = client.getServices(); System.out.println(&quot;**********&quot; + list); List&lt;ServiceInstance&gt; srvList = client.getInstances(&quot;MICROSERVICECLOUD-PERSON&quot;); for (ServiceInstance element : srvList) &#123; System.out.println(element.getServiceId() + &quot;\\t&quot; + element.getHost() + &quot;\\t&quot; + element.getPort() + &quot;\\t&quot; + element.getUri()); &#125; return this.client;&#125; 2.服务提供者启动类添加注解@EnableDiscoveryClient （后来测试发现其实这一步是多余的） 因为@EnableEurekaClient注解已经包含了@EnableDiscoveryClient 注解 也就是说当服务注册中心是Eureka的时候那么官方已经为了包装了一个注解替代了@EnableDiscoveryClient，但是如果注册中心不是Eureka的话，那么建议使用@EnableDiscoveryClient注解实现服务发现，因为这里注册中心是Eureka那么就是用官方推荐的@EnableEurekaClient 3.修改ConsumerController代码，访问服务提供者提供的接口： // 测试@EnableDiscoveryClient,消费端可以调用服务发现@RequestMapping(value = \"/consumer/person/discovery\")public Object discovery()&#123; return restTemplate.getForObject(REST_URL_PREFIX + \"/person/discovery\", Object.class);&#125; 4.启动服务提供者和服务消费者 消费者访问接口 7.EurekaServer集群单个的EurekaServer很明显是不符合HA，高可用原则，所以下面再加两台EurekaServer-8002和8003构成EurekaServer集群 基本原理 上图是来自eureka的官方架构图，这是基于集群配置的eureka； - 处于不同节点的eureka通过Replicate进行数据同步 - Application Service为服务提供者 - Application Client为服务消费者 - Make Remote Call完成一次服务调用 服务启动后向Eureka注册，Eureka Server会将注册信息向其他Eureka Server进行同步，当服务消费者要调用服务提供者，则向服务注册中心获取服务提供者地址，然后会将服务提供者地址缓存在本地，下次再调用时，则直接从本地缓存中取，完成一次调用。 当服务注册中心Eureka Server检测到服务提供者因为宕机、网络原因不可用时，则在服务注册中心将服务置为DOWN状态，并把当前服务提供者状态向订阅者发布，订阅过的服务消费者更新本地缓存。 服务提供者在启动后，周期性（默认30秒）向Eureka Server发送心跳，以证明当前服务是可用状态。Eureka Server在一定的时间（默认90秒）未收到客户端的心跳，则认为服务宕机，注销该实例。 7.1 根据microservicecloud-eureka-7001创建两个相同的工程分别是microservicecloud-eureka-7002和microservicecloud-eureka-7003 按照7001为模板粘贴POM 修改7002和7003的主启动类 完整工程如下 7.2 修改映射配置-实现唯一的eureka服务端的实例名称为了模拟EurekaServer集群，不同的EurekaServer在不同的机器，而且拥有不同的实例名称 找到C:\\Windows\\System32\\drivers\\etc路径下的hosts文件 修改映射配置添加进hosts文件 127.0.0.1 peer1 127.0.0.1 peer2 127.0.0.1 peer3 7.3 修改7001-7003三台EurekaServer的配置文件7001 修改内容如下： server: port: 7001eureka: instance: hostname: peer1 #peer1 #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://peer2:7002/eureka/,http://peer3:7003/eureka/ #注册7002和7003 自己不用声明 7002 修改内容如下： server: port: 7002eureka: instance: hostname: peer2 #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://peer1:7001/eureka/,http://peer3:7003/eureka/ #注册7001和7003 自己不用声明 #等同于http://localhost:7001/eureka/ 7003修改内容如下 server: port: 7003eureka: instance: hostname: peer3 #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/ #注册7001和7002 自己不用声明 #等同于http://localhost:7001/eureka/ 需要注意的是service-url:defaultZone的值，都是包含其他EurekaServer的值，不用书写自己的。 7.4 修改服务提供者的配置文件也就是修改microservicecloud-provider-person-8001模块修改内容如下 server: port: 8001mybatis: config-location: classpath:mybatis/mybatis.cfg.xml # mybatis配置文件所在路径 type-aliases-package: com.kingge.entity # 所有Entity别名类所在包 mapper-locations: - classpath:mybatis/mapper/**/*.xml # mapper映射文件spring: application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://127.0.0.1:3306/test # 数据库名称 username: root password: 123 dbcp2: min-idle: 5 # 数据库连接池的最小维持连接数 initial-size: 5 # 初始化连接数 max-total: 5 # 最大连接数 max-wait-millis: 200 # 等待连接获取的最大超时时间#eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/# http://localhost:7001/eureka #单机版本使用# defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ instance: instance-id: microservicecloud-person8001 #自定义服务实例名 prefer-ip-address: true #访问路径可以显示IP地址#info: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个提供查询部门人员信息的服务 实际上就是修改了service-url:defaultZone的值，修改为了EurekaServer集群的地址，其他配置没有改变 7.5重启7001-7003服务器也就是分别运行ApplicationBootStart7001、ApplicationBootStart7002、ApplicationBootStart7003 访问查看 部署成功！！！！！！！ 8.Eureka和Zookeeper-服务注册中心比较著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性P在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。 那么分布式系统，必然要求分区容错性，也就是P原则，那么Zookeeper选择了C，Eureka选择了A 因此Zookeeper保证的是CP,Eureka则是AP。 8.1 Zookeeper保证CP​ 当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。 8.2 Eureka保证AP​ Eureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的 ，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： ​ Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 ​ Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用) ​ 当网络稳定时，当前实例新的注册信息会被同步到其它节点中 因此， Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。 那么既然保证了保证了可用性，那么数据的一致性肯定是不能够保证了，所以这个就是自我保护的机制。所以到底是AP还是CP，又或者是AC（数据库），要看业务场景来定。 七、Ribbon负载均衡接下来解决第二个问题，那就是假设在多个服务提供者提供服务的情况下，怎么做到负载均衡，解决需要把服务提供者url硬编码到消费者端的问题。 7.1 Ribbon概念Spring Cloud Ribbon是基于Netflix Ribbon实现的一套客户端 负载均衡的工具 。 ​ 简单的说，Ribbon是Netflix发布的开源项目，主要功能是提供客户端的软件负载均衡算法，将Netflix的中间层服务连接在一起。Ribbon客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出Load Balancer（简称LB）后面所有的机器，Ribbon会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器。我们也很容易使用Ribbon实现自定义的负载均衡算法。 7.2 什么叫LB（负载均衡） LB，即负载均衡(Load Balance)，在微服务或分布式集群中经常用的一种应用。 负载均衡简单的说就是将用户的请求平摊的分配到多个服务上，从而达到系统的HA。 常见的负载均衡有软件Nginx，LVS，硬件 F5等。 相应的在中间件，例如：dubbo和SpringCloud中均给我们提供了负载均衡，SpringCloud的负载均衡算法可以自定义。 两种负载均衡： 集中式LB：偏硬件，服务的消费方和提供方之间使用独立的LB设施，由该设施负责把访问请求以某种策略转发至服务的提供方。 进程内LB：偏软件， 将LB逻辑集成到消费方，消费方从服务注册中心指导哪些地址可用，再自己选择一个合适的服务器。 Ribbon就属于进程内LB，它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。 7.3 Ribbon负载均衡实现因为在上面的例子中只存在一个8001模块在提供服务，那么为了能够演示负载均衡的例子，这里需要再增加两个服务提供者8002和8003 7.3.1 根据8001模块复制新建两份分别命名为8002和8003请看完整项目结构图 8002模块 8003模块 7.3.2 新建数据库test2、test3，让各自微服务分别连各自的数据库我们知道一个微服务可能是一套完整的系统，那么也就意味着他可能拥有自己的数据库。而且为了方便测试负载均衡，我们让8001-8003这三个服务提供者各自连接自己的数据库，也方便验证负载均衡是否实现。 给test2数据库导入数据： DROP TABLE IF EXISTS `person`;CREATE TABLE `person` ( `deptno` int(255) NOT NULL AUTO_INCREMENT, `db_source` varchar(255) DEFAULT NULL, `dname` varchar(255) DEFAULT NULL, PRIMARY KEY (`deptno`)) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8;-- ------------------------------ Records of person-- ----------------------------INSERT INTO `person` VALUES (&apos;5&apos;, &apos;test2&apos;, &apos;开发部&apos;);INSERT INTO `person` VALUES (&apos;6&apos;, &apos;test2&apos;, &apos;人事部&apos;);INSERT INTO `person` VALUES (&apos;7&apos;, &apos;test2&apos;, &apos;集成部&apos;);INSERT INTO `person` VALUES (&apos;8&apos;, &apos;test2&apos;, &apos;市场部&apos;);INSERT INTO `person` VALUES (&apos;9&apos;, &apos;test2&apos;, &apos;hr&apos;); db_source字段标识，数据来源那个数据库 给test3数据库导入数据： DROP TABLE IF EXISTS `person`;CREATE TABLE `person` ( `deptno` int(255) NOT NULL AUTO_INCREMENT, `db_source` varchar(255) DEFAULT NULL, `dname` varchar(255) DEFAULT NULL, PRIMARY KEY (`deptno`)) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8;-- ------------------------------ Records of person-- ----------------------------INSERT INTO `person` VALUES (&apos;5&apos;, &apos;test3&apos;, &apos;开发部&apos;);INSERT INTO `person` VALUES (&apos;6&apos;, &apos;test3&apos;, &apos;人事部&apos;);INSERT INTO `person` VALUES (&apos;7&apos;, &apos;test3&apos;, &apos;集成部&apos;);INSERT INTO `person` VALUES (&apos;8&apos;, &apos;test3&apos;, &apos;市场部&apos;);INSERT INTO `person` VALUES (&apos;9&apos;, &apos;test3&apos;, &apos;hr&apos;); 7.3.3 修改8002/8003各自application.yml配置文件实际上只需要修改三个地方：服务暴露的端口号，服务连接的数据库，服务的实例名 注意：服务名称不能够修改 因为这三个服务都是提供同样的业务，那么就不会归属到一个服务组下，也就是说我们想要的是：microservicecloud-person 这个服务名称（服务组）下面有三个服务实例（8001-8003）提供服务，这样负载均衡才能够演示 8002模块修改如下 8003模块修改如下 7.3.4 启动EurekaServer集群和8001-8003服务模块 查看EurekaServer http://peer1:7001/ http://peer2:7002/ http://peer3:7003/ 服务提供者集群创建成功 7.3.5 自测启动的服务是否可用 7.3.6 修改服务消费者-采取负载均衡方式访问服务也就是修改microservicecloud-consumer-person-80模块 （1）修改pom文件添加Robbin依赖 &lt;!-- Ribbon相关 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改application.yml文件 server: port: 80 #eureka: client: register-with-eureka: false service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ 也就是修改服务注册中心地址为集群地址 （3）修改ConfigBean配置类添加@LoadBalanced注解，获取rest服务的时候添加ribbon @Configurationpublic class ConfigBean&#123; @Bean @LoadBalanced public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125;&#125; （4）主启动类ApplicationBootStart80添加@EnableEurekaClient @SpringBootApplication@EnableEurekaClient//这里建议使用@EnableDiscoveryClient 替换@EnableEurekaClientpublic class ApplicationBootStart80 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80.class, args); &#125;&#125; 7.3.7 修改服务消费者修改ConsumerController代码 修改内容如下 // private static final String REST_URL_PREFIX = &quot;http://localhost:8001&quot;; private static final String REST_URL_PREFIX = &quot;http://MICROSERVICECLOUD-PERSON&quot;; 把url更改为服务名称 Ribbon和Eureka整合后Consumer可以直接调用服务而不用再关心地址和端口号 7.3.8 启动7001-7003,8001-8003，80 启动完成后测试： 第一次访问 第二次访问： 第三次访问： 第四次访问： 我们注意db_source的值是变换的，说明负载均衡成功。但是当我们访问一轮后，发现他又从头开始： test2-&gt;test3-&gt;test-&gt;test2-&gt;test3-&gt;test 说明Ribbon默认采用的是轮询的负载均衡策略 7.3.9 总结 Ribbon在工作时分成两步 第一步先选择 EurekaServer ,它优先选择在同一个区域内负载较少的server. 第二步再根据用户指定的策略，在从server取到的服务注册列表中选择一个地址。 其中Ribbon提供了多种策略：比如轮询、随机和根据响应时间加权。 Ribbon其实就是一个软负载均衡的客户端组件， 他可以和其他所需请求的客户端结合使用，和eureka结合只是其中的一个实例。 7.4 Ribbon负载均衡实现核心接口IRule7.4.1.源码跟踪为什么我们只输入了service名称就可以访问了呢？之前还要获取ip和端口。 显然有人帮我们根据service名称，获取到了服务实例的ip和端口。它就是LoadBalancerInterceptor 在consumer的ConsumerController如下代码打断点： 一路源码跟踪：RestTemplate.getForObject –&gt; RestTemplate.execute –&gt; RestTemplate.doExecute： 点击进入AbstractClientHttpRequest.execute –&gt; AbstractBufferingClientHttpRequest.executeInternal –&gt; InterceptingClientHttpRequest.executeInternal –&gt; InterceptingClientHttpRequest.execute: 继续跟入：LoadBalancerInterceptor.intercept方法 获取请求的服务名称，我们发现执行this.loadBalancer.execute()方法的loadBalancer是一个接口LoadBalancerClient，那么很明显执行execute（）方法的只能是LoadBalancerClient的实现类。 继续跟入execute方法发现执行该方法的类是：RibbonLoadBalancerClient负载均衡客户端类： 发现获取了8003端口的服务 我们查看一下获取的负载均衡器的信息： 7.4.2.负载均衡策略Ribbon默认的负载均衡策略是简单的轮询，我们可以测试一下： 编写测试类，在刚才的源码中我们看到拦截中是使用RibbonLoadBalanceClient来进行负载均衡的，其中有一个choose方法，找到choose方法的接口方法，是这样介绍的： 现在这个就是负载均衡获取实例的方法。 我们注入这个类的对象，然后对其测试： 测试内容： @RunWith(SpringRunner.class)@SpringBootTest(classes = ApplicationBootStart80.class)public class LoadBalanceTest &#123; @Autowired private RibbonLoadBalancerClient client; @Test public void testLoadBalance()&#123; for (int i = 0; i &lt; 100; i++) &#123; ServiceInstance instance = this.client.choose(\"MICROSERVICECLOUD-PERSON\"); System.out.println(instance.getHost() + \":\" +instance.getPort()); &#125; &#125;&#125; 结果： 符合了我们的预期推测，确实是轮询方式。 我们是否可以修改负载均衡的策略呢？ 继续跟踪源码，发现这么一段代码： 我们看看这个rule是谁： 这里的rule默认值是一个RoundRobinRule，看类的介绍： 这不就是轮询的意思嘛。 我们注意到，这个类其实是实现了接口IRule的，查看一下： 定义负载均衡的规则接口。 它有以下实现： 七大方法IRule是一个接口，七大方法是其实现类 RoundRobinRule：轮询（默认方法） RandomRule：随机 AvailabilityFilteringRule：先过滤掉由于多次访问故障而处于断路器跳闸状态的服务，还有并发的连接数量超过阈值的服务，然后对剩余的服务进行轮询 WeightedResponseTimeRule：根据平均响应时间计算服务的权重。统计信息不足时会按照轮询，统计信息足够会按照响应的时间选择服务 RetryRule：正常时按照轮询选择服务，若过程中有服务出现故障，在轮询一定次数后依然故障，则会跳过故障的服务继续轮询。 BestAvailableRule：先过滤掉由于多次访问故障而处于断路器跳闸状态的服务，然后选择一个并发量最小的服务 ZoneAvoidanceRule：默认规则，符合判断server所在的区域的性能和server的可用性选择服务 7.4.3 负载均衡自定义1.修改某个服务的负载均衡策略例如我们只想修改 MICROSERVICECLOUD-PERSON服务的负载均衡策略 这里一共有两种方法实现一种是使用yml配置的方式声明-一种是使用注解的方式声明 第一种使用配置方式（建议使用）（1）修改消费者端（microservicecloud-consumer-person-80）的application.yml配置文件 修改内容如下： MICROSERVICECLOUD-PERSON: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 格式是：{服务名称}.ribbon.NFLoadBalancerRuleClassName，值就是IRule的实现类。 （2）运行上面的LoadBalanceTest 第二种使用配置方式前提：注释掉第一种方式实现的 配置信息（不注释掉也可以，因为第一种方式跟第二种方式同时存在时，以第二种方式为主） （1）修改消费者端启动类 @SpringBootApplication#@EnableEurekaClient@EnableDiscoveryClient //服务发现@RibbonClient(name = &quot;MICROSERVICECLOUD-PERSON&quot;,configuration = OwnRule.class)public class ApplicationBootStart80 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80.class, args); &#125;&#125; 添加一下注解 @RibbonClient(name = &quot;MICROSERVICECLOUD-PERSON&quot;,configuration = OwnRule.class) （2）新建OwnRule自定义侧略类 首先新建包com.myrule @Configurationpublic class OwnRule &#123; @Bean public IRule getIuIRule()&#123; System.out.println(&quot;进入了自定义负载均衡策略&quot;); return new RandomRule(); &#125;&#125; 注意： 官方文档明确给出了警告： 这个自定义配置类不能放在@ComponentScan所扫描的当前包下以及子包下， 否则我们自定义的这个配置类就会被所有的Ribbon客户端所共享，也就是说 我们达不到特殊化定制的目的了。 所以上面的OwnRule类是不在启动类同级包或者子包下的。 所以我们也可以利用这个特性，修改全局的所有服务的获取策略为某个策略 （3）运行上面的LoadBalanceTest 2. 修改全局的服务访问策略（替换默认的轮询策略）很简单，直接在IOC容器注入想要替换成的负载均衡策略即可 @Configurationpublic class ConfigBeanapplicationContext.xml&#123; @Bean @LoadBalanced public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125; @Bean public IRule getIuIRule()&#123; return new RandomRule(); &#125;&#125; 3.自定义负载均衡策略上上面的两个章节实际上使用的都是Ribbon提供的负载均衡策略，所以接下来我们要实现一个负载均衡策略 提出需求： 问题：修改MICROSERVICECLOUD-PERSON服务的负载均衡策略：依旧使用轮询策略，但是加上新需求，每个服务器（现在有三台8001-8003）要求被调用5次。也即 以前是每台机器一次，现在是每台机器5次 在实现之前拜读一下官方的RandomRule 源代码，然后再修改出符合我们需求的策略类 https://github.com/Netflix/ribbon/blob/master/ribbon-loadbalancer/src/main/java/com/netflix/loadbalancer/RandomRule.java public class RandomRule extends AbstractLoadBalancerRule &#123; /** * Randomly choose from all living servers */ @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = &quot;RCN_REDUNDANT_NULLCHECK_OF_NULL_VALUE&quot;) public Server choose(ILoadBalancer lb, Object key) &#123; if (lb == null) &#123; return null; &#125; Server server = null;//需要返回的服务 while (server == null) &#123;//使用while循环知道获取服务 if (Thread.interrupted()) &#123;//如果当前线程已经中断，那么直接返回null return null; &#125; List&lt;Server&gt; upList = lb.getReachableServers();//获取所有可达的服务列表 List&lt;Server&gt; allList = lb.getAllServers();//获取所有服务列表 int serverCount = allList.size();//得到服务列表里服务实例的数量 if (serverCount == 0) &#123; /* * No servers. End regardless of pass, because subsequent passes * only get more restrictive. */ return null; &#125; int index = chooseRandomInt(serverCount);//根据服务数量所及获取服务下标 等同于Random.rand.nextInt(serverCount); server = upList.get(index);//根据随机获取到的下标，从可用服务列表实例中获取服务 if (server == null) &#123;//获取不到时，暂停当前正在执行的线程对象(及放弃当前拥有的cup资源),并执行其他线程 //然后继续while循环获取 /* * The only time this should happen is if the server list were * somehow trimmed. This is a transient condition. Retry after * yielding. */ Thread.yield(); continue; &#125; if (server.isAlive()) &#123; return (server); &#125; // Shouldn&apos;t actually happen.. but must be transient or a bug. server = null; Thread.yield(); &#125; return server; &#125; protected int chooseRandomInt(int serverCount) &#123; return ThreadLocalRandom.current().nextInt(serverCount); &#125; @Override public Server choose(Object key) &#123; return choose(getLoadBalancer(), key); &#125;&#125; 代码其实很简单 （1）新建RandomRuleModify类package com.myrule;import java.util.List; import com.netflix.client.config.IClientConfig; import com.netflix.loadbalancer.AbstractLoadBalancerRule; import com.netflix.loadbalancer.ILoadBalancer; import com.netflix.loadbalancer.Server;public class RandomRuleModify extends AbstractLoadBalancerRule&#123; // total = 0 // 当total==5以后，我们指针才能往下走， // index = 0 // 当前对外提供服务的服务器地址， // total需要重新置为零，但是已经达到过一个5次，我们的index = 1 // 分析：我们5次，但是微服务只有8001 8002 8003 三台，OK？ // private int total = 0; // 总共被调用的次数，目前要求每台被调用5次 private int currentIndex = 0; // 当前提供服务的机器号 public Server choose(ILoadBalancer lb, Object key) &#123; if (lb == null) &#123; return null; &#125; Server server = null; while (server == null) &#123; if (Thread.interrupted()) &#123; return null; &#125; List&lt;Server&gt; upList = lb.getReachableServers(); List&lt;Server&gt; allList = lb.getAllServers(); int serverCount = allList.size(); if (serverCount == 0) &#123; return null; &#125; if(total &lt; 5) &#123; server = upList.get(currentIndex); total++; &#125;else &#123; total = 0; currentIndex++; if(currentIndex &gt;= upList.size()) &#123; currentIndex = 0; &#125; &#125; if (server == null) &#123; Thread.yield(); continue; &#125; if (server.isAlive()) &#123; return (server); &#125; // Shouldn&apos;t actually happen.. but must be transient or a bug. server = null; Thread.yield(); &#125; return server; &#125; @Override public Server choose(Object key) &#123; return choose(getLoadBalancer(), key); &#125; @Override public void initWithNiwsConfig(IClientConfig clientConfig) &#123; // TODO Auto-generated method stub &#125;&#125; 这个类不能够放在启动类的同级或者子包下，否则将被设置为全局的负载均衡策略，起不到为MICROSERVICECLOUD-PERSON服务定制策略的作用 （2）OwnRule返回我们实现的RandomRuleModify@Configurationpublic class OwnRule &#123; @Bean public IRule getIuIRule()&#123; System.out.println(&quot;进入了自定义负载均衡策略&quot;); return new RandomRuleModify();//RandomRule(); &#125;&#125; （3）修改启动类@SpringBootApplication//@EnableEurekaClient@EnableDiscoveryClient //服务发现@RibbonClient(name = &quot;MICROSERVICECLOUD-PERSON&quot;,configuration = OwnRule.class)public class ApplicationBootStart80 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80.class, args); &#125;&#125; （4）运行测试类LoadBalanceTest 轮询的同时每个服务器调用五次！！！ （5）总结我们也可以使用配置类的方式为某个服务配置特定的负载均衡策略实现类（参考7.4.3.1 章节的第一种配置方式），这样就可以省略上面的第二和第三步骤 通过配置的方式更加，灵活 然后运行测试类LoadBalanceTest，结果一模一样。 八、 Feign负载均衡8.1.简介Feign出现的原因是什么，既然他也是提供负载均衡的功能，那么他跟Ribbon有什么区别？ 项目主页： https://github.com/OpenFeign/feign 官网解释： http://projects.spring.io/spring-cloud/spring-cloud.html#spring-cloud-feign Feign是一个声明式WebService客户端。使用Feign能让编写Web Service客户端更加简单, 它的使用方法是定义一个接口，然后在上面添加注解，同时也支持JAX-RS标准的注解。Feign也支持可拔插式的编码器和解码器。Spring Cloud对Feign进行了封装，使其支持了Spring MVC标准注解和HttpMessageConverters。Feign可以与Eureka和Ribbon组合使用以支持负载均衡。 Feign的出现的目的是：为了迎合我们平时面向接口编程和调用的习惯。例如我们在Controller通过注入Service层的接口调用相关的业务。但是在上面的Ribbon例子中我们是通过RestTemplate和URL的方式调用某个服务： Feign的实现其实也很简单：只需要创建一个接口，然后在上面添加注解即可。 有道词典的英文解释： 为什么叫伪装？ Feign可以把Rest的请求进行隐藏，伪装成类似SpringMVC的Controller一样。你不用再自己拼接url，拼接参数等等操作，一切都交给Feign去做。 总的来说：Feign通过封装Ribbon实现了我们常用的面向接口编程 8.2 实现Feign其实就是复制microservicecloud-consumer-person-80工程代码，在做一些修改。 （1）根据父工程新建microservicecloud-consumer-person-80-feign模块（2）复制80模块代码到80-feign模块 修改启动类的名字。 （3）修改80-feign模块的pom文件，添加Feign依赖添加如下内容： &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; （4）修改api公共模块也就是修改microservicecloud-api，因为我们知道我们抽象出来的服务，有可能其他模块也会调用，不仅仅是80-feign模块。所以公共的东西我们都放在api模块 1.修改pom文件，添加feign依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; 2.新建PersonClientService接口并新增注解@FeignClient @FeignClient(value = &quot;MICROSERVICECLOUD-PERSON&quot;)public interface PersonClientService&#123; @RequestMapping(value = &quot;/person/get/&#123;id&#125;&quot;, method = RequestMethod.GET) public Person get(@PathVariable(&quot;id&quot;) long id); @RequestMapping(value = &quot;/person/list&quot;, method = RequestMethod.GET) public List&lt;Person&gt; list(); @RequestMapping(value = &quot;/person/add&quot;, method = RequestMethod.POST) public boolean add(Person person);&#125; • 首先这是一个接口，Feign会通过动态代理，帮我们生成实现类。这点跟mybatis的mapper很像 • @FeignClient，声明这是一个Feign客户端，类似@Mapper注解。同时通过value属性指定服务名称 • 接口中的定义方法，完全采用SpringMVC的注解，Feign会根据注解帮我们生成URL，并访问获取结果 （5）feign工程修改Controller，添加上一步新建的PersonClientService接口@RestControllerpublic class ConsumerController &#123;// @Autowired// private RestTemplate restTemplate;// private static final String REST_URL_PREFIX = &quot;http://localhost:8001&quot;;// private static final String REST_URL_PREFIX = &quot;http://MICROSERVICECLOUD-PERSON&quot;;@Autowiredprivate PersonClientService personClientService; @RequestMapping(value = &quot;/consumer/person/add&quot;) public boolean add(Person person) &#123;// return restTemplate.postForObject(REST_URL_PREFIX + &quot;/person/add&quot;, person, Boolean.class); return personClientService.add(person); &#125; @RequestMapping(value = &quot;/consumer/person/list&quot;) public List&lt;Person&gt; list() &#123;// return restTemplate.getForObject(REST_URL_PREFIX + &quot;/person/list&quot;, List.class); return personClientService.list(); &#125; @RequestMapping(value = &quot;/consumer/person/get/&#123;id&#125;&quot;) public Person get(@PathVariable(&quot;id&quot;) Long id) &#123; return this.personClientService.get(id); &#125;&#125; 可以看到我们注释掉了以往的RestTemplate+URL请求服务的方式，通过注入接口调用的方式，实现了面向借口编程。 （6）feign工程修改主启动类添加@EnableFeignClients // 开启feign客户端 @SpringBootApplication@EnableDiscoveryClient //服务发现@EnableFeignClients // 开启feign客户端public class ApplicationBootStart80feign &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80feign.class, args); &#125;&#125; （7）启动EurekaServer7001-7003和服务提供集群8001-8003，启动feign工程进行测试 请求 默认使用轮询的负载均衡方式 8.3 总结 Feign通过接口的方法调用Rest服务（之前是Ribbon+RestTemplate） ， 该请求发送给Eureka服务器（http://MICROSERVICECLOUD-PERSON/person/list）, 通过Feign直接找到服务接口，由于在进行服务调用的时候融合了Ribbon技术，所以也支持负载均衡作用。 也就是说Feign只是封装了Ribbon，改造成了我们习惯的面向接口编程的方式。 自定义负载均衡的方式跟之前使用Ribbon 的一样，也就是支持注解和配置两种方式实现某个服务或者全局服务的负载均衡策略自定义 例如： @SpringBootApplication@EnableDiscoveryClient //服务发现@EnableFeignClients // 开启feign客户端@RibbonClient(name = &quot;MICROSERVICECLOUD-PERSON&quot;,configuration = OwnRule.class)public class ApplicationBootStart80feign &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80feign.class, args); &#125;&#125; 8.4.请求压缩(了解)Spring Cloud Feign 支持对请求和响应进行GZIP压缩，以减少通信过程中的性能损耗。通过下面的参数即可开启请求与响应的压缩功能： feign: compression: request: enabled: true # 开启请求压缩 response: enabled: true # 开启响应压缩 同时，我们也可以对请求的数据类型，以及触发压缩的大小下限进行设置： feign: compression: request: enabled: true # 开启请求压缩 mime-types: text/html,application/xml,application/json # 设置压缩的数据类型 min-request-size: 2048 # 设置触发压缩的大小下限 注：上面的数据类型、压缩大小下限均为默认值。 8.5.日志级别(了解)前面讲过，通过logging.level.xx=debug来设置日志级别。然而这个对Fegin客户端而言不会产生效果。因为@FeignClient注解修改的客户端在被代理时，都会创建一个新的Fegin.Logger实例。我们需要额外指定这个日志的级别才可以。 1）设置com.kingge包下的日志级别都为debug logging: level: com.kingge: debug 2）新建FeignLogConfiguration配置类，定义日志级别 内容： @Configurationpublic class FeignLogConfiguration &#123; @Bean Logger.Level feignLoggerLevel()&#123; return Logger.Level.FULL; &#125;&#125; 这里指定的Level级别是FULL，Feign支持4种级别： • NONE：不记录任何日志信息，这是默认值。 • BASIC：仅记录请求的方法，URL以及响应状态码和执行时间 • HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息 • FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。 3）在FeignClient中指定配置类： @FeignClient(value = “MICROSERVICECLOUD-PERSON” configuration = FeignLogConfiguration.class) public interface UserFeignClient { @GetMapping(“/user/{id}”) User queryUserById(@PathVariable(“id”) Long id); } 4）重启项目，即可看到每次访问的日志： 8.6.Hystrix支持参加下面服务降级的案例 九、Hystrix断路器​ 首先我们来解决第三问题，服务的容灾处理 跟Ribbon和Feign是客户端技术不同的是Hystrix是服务端的技术，也就是他是作用在服务提供端 1.分布式系统面临的问题复杂分布式体系结构中的应用程序有数十个依赖关系，每个依赖关系在某些时候将不可避免地失败 1.雪崩问题 微服务中，服务间调用关系错综复杂，一个请求，可能需要调用多个微服务接口才能实现，会形成非常复杂的调用链路： 如图，一次业务请求，需要调用A、P、H、I四个服务，这四个服务又可能调用其它服务（这个就是所谓的扇出）。 如果此时，某个服务出现异常： 例如微服务I发生异常，请求阻塞，用户不会得到响应，则tomcat的这个线程不会释放，于是越来越多的用户请求到来，越来越多的线程会阻塞： 服务器支持的线程和并发数有限，请求一直阻塞，会导致服务器资源耗尽，从而导致所有其它服务都不可用，形成雪崩效应，所以我们需要阻断故障的传播，这个就是断路器。 这就好比，一个汽车生产线，生产不同的汽车，需要使用不同的零件，如果某个零件因为种种原因无法使用，那么就会造成整台车无法装配，陷入等待零件的状态，直到零件到位，才能继续组装。 此时如果有很多个车型都需要这个零件，那么整个工厂都将陷入等待的状态，导致所有生产都陷入瘫痪。一个零件的波及范围不断扩大。 备注：一般情况对于服务依赖的保护主要有3中解决方案： （1）熔断模式：这种模式主要是参考电路熔断，如果一条线路电压过高，保险丝会熔断，防止火灾。放到我们的系统中，如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。 （2）（降级）隔离模式：这种模式就像对系统请求按类型划分成一个个小岛的一样，当某个小岛被火少光了，不会影响到其他的小岛。例如可以对不同类型的请求使用线程池来资源隔离，每种类型的请求互不影响，如果一种类型的请求线程资源耗尽，则对后续的该类型请求直接返回，不再调用后续资源。这种模式使用场景非常多，例如将一个服务拆开，对于重要的服务使用单独服务器来部署，再或者公司最近推广的多中心。 （3）限流模式：上述的熔断模式和隔离模式都属于出错后的容错处理机制，而限流模式则可以称为预防模式。限流模式主要是提前对各个类型的请求设置最高的QPS阈值，若高于设置的阈值则对该请求直接返回，不再调用后续资源。这种模式不能解决服务依赖的问题，只能解决系统整体资源分配问题，因为没有被限流的请求依然有可能造成雪崩效应。 springcloud的Hystrix就是提供了前两种解决方式： Hystix解决雪崩问题的手段有两个： • 线程隔离（降级） • 服务熔断 2.Hystrix简介​ Hystrix是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统里，许多依赖不可避免的会调用失败，比如超时、异常等，Hystrix能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性。 Hystix是Netflix开源的一个延迟和容错库，用于隔离访问远程服务、第三方库，防止出现级联失败。 ​ “断路器”本身是一种开关装置，当某个服务单元发生故障之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个符合预期的、可处理的备选响应（FallBack）（解决思路） ，而不是长时间的等待或者抛出调用方无法处理的异常 ，这样就保证了服务调用方的线程不会被长时间、不必要地占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。 官网资料 https://github.com/Netflix/Hystrix/wiki/How-To-Use 3.服务端的服务熔断熔断机制是应对雪崩效应的一种微服务链路保护机制。他是在服务端实现。 ​ 当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回”错误”的响应信息。 当检测到该节点微服务调用响应正常后恢复调用链路。在SpringCloud框架里熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败就会启动熔断机制。熔断机制的注解是@HystrixCommand。 3.1 熔断原理熔断器，也叫断路器，其英文单词为：Circuit Breaker 熔断状态机3个状态： • Closed：关闭状态，所有请求都正常访问。 • Open：打开状态，所有请求都会被降级。Hystix会对请求情况计数，当一定时间内失败请求百分比达到阈值，则触发熔断，断路器会完全打开。默认失败比例的阈值是50%，请求次数最少不低于20次。 • Half Open：半开状态，open状态不是永久的，打开后会进入休眠时间（默认是5S）。随后断路器会自动进入半开状态。此时会释放部分请求通过，若这些请求都是健康的，则会完全关闭断路器，否则继续保持打开，再次进行休眠计时 ​ 当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回”错误”的响应信息。 当检测到该节点微服务调用响应正常后恢复调用链路。在SpringCloud框架里熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败就会启动熔断机制。熔断机制的注解是@HystrixCommand。 3.2 案例演示3.2.1 参考microservicecloud-provider-person-8001 新建microservicecloud-provider-person-hystrix-8001赋值pom文件和代码已经application.yml文件 3.2.2 修改新建hystrix-8001模块的pom文件和启动类（1）pom文件添加hystrix依赖 &lt;!-- hystrix --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改启动类名称并添加@EnableCircuitBreaker注解//对hystrixR熔断机制的支持 @SpringBootApplication//@EnableEurekaClient@EnableDiscoveryClient //服务发现@EnableCircuitBreaker//对hystrixR熔断机制的支持public class ApplicationBootStart8001hystrix &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart8001hystrix.class, args); &#125;&#125; 3.2.3 修改配置文件其实就是修改服务实例名字，避免跟8001模块冲突 3.2.4 修改PersonController，添加熔断处理在某个方法使用@HystrixCommand注解，模拟报异常后如何处理 一旦调用服务方法失败并抛出了错误信息/请求超时后，会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法 下面以get方法为例，代码内容（下面模拟的是出现异常） @RestControllerpublic class PersonController&#123; @Autowired private PersonService service;//全部使用restful风格，返回json字符串 @RequestMapping(value = &quot;/person/add&quot;, method = RequestMethod.POST) public boolean add(@RequestBody Person person) &#123; return service.add(person); &#125; @RequestMapping(value = &quot;/person/get/&#123;id&#125;&quot;, method = RequestMethod.GET) @HystrixCommand(fallbackMethod = &quot;processHystrix_Get&quot;) //关键代码 public Person get(@PathVariable(&quot;id&quot;) Long id) &#123; Person person = service.get(id); if(null == person) &#123; throw new RuntimeException(&quot;该ID：&quot;+id+&quot;没有没有对应的信息&quot;); &#125; return person; &#125; public Person processHystrix_Get(@PathVariable(&quot;id&quot;) Long id)//关键代码 &#123; Person person = new Person(); person.setDeptno(id);person.setDname(&quot;该ID：&quot;+id+&quot;没有没有对应的信息,null--@HystrixCommand&quot;); person.setDb_source(&quot;no this database in MySQL&quot;); return person; &#125; @RequestMapping(value = &quot;/person/list&quot;, method = RequestMethod.GET) public List&lt;Person&gt; list() &#123; return service.list(); &#125;&#125; 注意，配置的fallbackMethod方法必须与被@HystrixCommand注解的方法有相同的入参和返回值 3.2.5 启动EurekaServer集群和8001-hystrix模块、80模块 消费者访问 http://localhost/consumer/person/get/5 输出：{“deptno”:5,”dname”:”开发部”,”db_source”:”test”} 我们假设获取某个不存在的用户 http://localhost/consumer/person/get/55 输出：{“deptno”:55,”dname”:”该ID：55没有没有对应的信息,null–@HystrixCommand”,”db_source”:”no this database in MySQL”} 4.客户端的服务降级​ 既然有了服务熔断，为什么还需要服务降级？ 服务降级处理是在客户端实现完成的,与服务端没有关系，客户端自带一个异常处理机制。上面的服务熔断是在服务端实现 客户端的服务降级，能够快速的响应用户的请求，当服务不可达，那么立即返回定制的错误信息。 整体资源快不够了,忍痛将某些服务先关掉,待渡过难关,再开启回来 优先保证核心服务，而非核心服务不可用或弱可用。 用户的请求故障时，不会被阻塞，更不会无休止的等待或者看到系统崩溃，至少可以看到一个执行结果（例如返回友好的提示信息 。 服务降级虽然会导致请求失败，但是不会导致阻塞，而且最多会影响这个依赖服务对应的线程池中的资源，对其它服务没有响应。 而且上面的服务端熔断案例存在一些缺点：控制层每实现一个方法，就要实现对应的fallBack方法处理相关的异常逻辑，那么代码量会越来越大，而且跟业务逻辑偶合在一起。所以我们需要解耦，我们把熔断的的fallback方法都放在一个类中，去除控制层的@HystrixCommand，这样能够保证业务的纯粹。 4.1 案例演示4.1.1 修改microservicecloud-api的工程, 根据已有的PersonClientService接口新建一个实现类 FallbackFactory接口的类PersonClientServiceFallbackFactory​ @Component//不要忘记添加public class PersonClientServiceFallbackFactory implements FallbackFactory&lt;PersonClientService&gt; &#123; @Override public PersonClientService create(Throwable throwable) &#123; return new PersonClientService() &#123; @Override public Person get(long id) &#123; Person person = new Person(); person.setDeptno(id);person.setDname(&quot;该ID&quot;+id+&quot;没有对应的信息,Consumer客户端提供的降级信息,此服务Provider已经关闭&quot;); person.setDb_source(&quot;no this database in MySQL&quot;); return person; &#125; @Override public List&lt;Person&gt; list() &#123; return null; &#125; @Override public boolean add(Person person) &#123; return false; &#125; &#125;; &#125;&#125; ​ 4.1.2 修改microservicecloud-api工程,PersonClientService接口在注解@FeignCLient中添加fallbackFactory属性值​ @FeignClient(value = &quot;MICROSERVICECLOUD-PERSON&quot;,fallbackFactory = PersonClientServiceFallbackFactory.class)public interface PersonClientService&#123; @RequestMapping(value = &quot;/person/get/&#123;id&#125;&quot;, method = RequestMethod.GET) public Person get(@PathVariable(&quot;id&quot;) long id); @RequestMapping(value = &quot;/person/list&quot;, method = RequestMethod.GET) public List&lt;Person&gt; list(); @RequestMapping(value = &quot;/person/add&quot;, method = RequestMethod.POST) public boolean add(Person person);&#125; ​ 4.1.3 修改80-feign消费者端配置文件，开启服务熔断​ microservicecloud-consumer-person-80-feign工程修改YML feign: hystrix: enabled: true 4.1.4 测试​ 启动3个eureka先启动和微服务microservicecloud-provider-person-8001启动 microservicecloud-consumer-person-80-feign启动 正常访问测试 http://localhost/consumer/dept/get/5 输出：{“deptno”:5,”dname”:”hr”,”db_source”:”test”} ​ 故意关闭微服务microservicecloud-provider-person-8001 访问测试 http://localhost/consumer/dept/get/5 输出：”deptno”:5,”dname”:”该ID5没有对应的信息,Consumer客户端提供的降级信息,此服务Provider已经关闭”,”db_source”:”no this database in MySQL”} 成功！！！ ​ 此时服务端provider已经down了,但是我们做了服务降级处理,让客户端在服务端不可用时也会获得提示信息而不会挂起耗死服务器 也就是说，如果服务端能够正常调用那么就返回值，如果不能够调用那么就返回由fallbackFactory定义的值 5. 服务监控HystrixDashboard ​ 除了隔离依赖服务的调用以外,Hystrix还提供了准实时的调用监控(Hystrix Dashboard),Hystrix 会持续地记录所有通过Hystrix发起的请求的执行信息,并以统计报表和图形的形式展示给用户,包括每秒执行多少请求多少成功,多少失败等.Netflix通过hystrix-metrics-event-stream项目实现了对以上指标的监控.Spring Cloud也提供了Hystrix Dashboard的整合.对监控内容转化成可视化界面. 也就是说，服务端必须是集成了Hystrix组件，才能够被监控，也即是：启动类添加@EnableCircuitBreaker//对hystrixR熔断机制的支持。例如我们的服务8002和8003是监控不到的 ​ 记下来看案例实现 5.1 案例实现5.1.1 新建 microservicecloud-consumer-hystrix-dashboard-9001 模块根据父工程 microservicecloud新建服务监控模块（ 参考microservicecloud-consumer-person-80-feign ） ​ （1）复制80-feign模块pom文件并添加dashboard依赖 &lt;!-- hystrix和 hystrix-dashboard相关--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; ​ （2）修改application.yml配置文件 server: port: 9001 ​ （3）启动类ApplicationBootStart9001dashboard添加@EnableHystrixDashboard ​ @SpringBootApplication@EnableHystrixDashboardpublic class ApplicationBootStart9001dashboard &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart9001dashboard.class, args); &#125;&#125; ​ （4）所有Provider微服务提供类(8001/8002/8003)都需要监控依赖配置 ​ &lt;!-- actuator监控信息完善 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; ​ （5）完整工程 ​ ​ 5.1.2 启动dashboard-9001模块​ http://localhost:9001/hystrix 部署成功 5.1.3 启动3个eureka集群5.1.4启动microservicecloud-provider-person-hystrix-8001和microservicecloud-consumer-person-80-feign 服务端和消费者5.1.4dashboard填写需要监控的服务地址 1：Delay：该参数用来控制服务器上轮询监控信息的延迟时间，默认为2000毫秒，可以通过配置该属性来降低客户端的网络和CPU消耗。 2：Title：该参数对应了头部标题Hystrix Stream之后的内容，默认会使用具体监控实例的URL，可以通过配置该信息来展示更合适的标题。 点击Monitor Stream 开始监控 怎么查看这张图：关键的部位我已经用红色方框，框住。核心就是：7色（服务的状态），1圈，1线。 ​ 1圈 ​ 实心圆:共有两种含义.它通过颜色的变化代表了 实例健康程度,它的健康度从绿色&lt;黄色&lt;橙色&lt;红色递减.该实心圆除了颜色的变化之外,它的大小也会根据实例的请求流量发生变化,流量越大该实心圆就越大.所以通过该实心圆的展示,就可以在大量的实例中快速的发现故障实例和高压力实例. ​ 1线 ​ 曲线:用来记录2分钟内流量的相对变化,可以通过它来观察到流量的上升和下降趋势. 5.1.5 多次刷新http://localhost/consumer/person/get/1也即是多次请求8001服务，然后查看监控的状态 显示请求的get方法的情况。圆圈变大，曲线上升。（如果请求多个方法会增加图形说明） 十、 zuul路由网关​ 通过前面的学习，使用Spring Cloud实现微服务的架构基本成型，大致是这样的： 我们使用Spring Cloud Netflix中的Eureka实现了服务注册中心以及服务注册与发现；而服务间通过Ribbon或Feign实现服务的消费以及均衡负载。为了使得服务集群更为健壮，使用Hystrix的融断机制来避免在微服务架构中个别服务出现异常时引起的故障蔓延。 在该架构中，我们的服务集群包含：内部服务Service A和Service B，他们都会注册与订阅服务至Eureka Server，而Open Service是一个对外的服务，通过均衡负载公开至服务调用方。我们把焦点聚集在对外服务这块，直接暴露我们的服务地址，这样的实现是否合理，或者是否有更好的实现方式呢？ 先来说说这样架构需要做的一些事儿以及存在的不足： • 破坏了服务无状态特点。 ​ 为了保证对外服务的安全性，我们需要实现对服务访问的权限控制，而开放服务的权限控制机制将会贯穿并污染整个开放服务的业务逻辑，这会带来的最直接问题是，破坏了服务集群中REST API无状态的特点。 ​ 从具体开发和测试的角度来说，在工作中除了要考虑实际的业务逻辑之外，还需要额外考虑对接口访问的控制处理。 • 无法直接复用既有接口。 ​ 当我们需要对一个即有的集群内访问接口，实现外部服务访问时，我们不得不通过在原有接口上增加校验逻辑，或增加一个代理调用来实现权限控制，无法直接复用原有的接口。 面对类似上面的问题，我们要如何解决呢？答案是：服务网关！ 为了解决上面这些问题，我们需要将权限控制这样的东西从我们的服务单元中抽离出去，而最适合这些逻辑的地方就是处于对外访问最前端的地方，我们需要一个更强大一些的均衡负载器的 服务网关。 服务网关是微服务架构中一个不可或缺的部分。通过服务网关统一向外系统提供REST API的过程中，除了具备服务路由、均衡负载功能之外，它还具备了权限控制等功能。Spring Cloud Netflix中的Zuul就担任了这样的一个角色，为微服务架构提供了前门保护的作用，同时将权限控制这些较重的非业务逻辑内容迁移到服务路由层面，使得服务集群主体能够具备更高的可复用性和可测试性。 Zuul包含了对请求的路由和过滤两个最主要的功能： 其中路由功能负责将外部请求转发到具体的微服务实例上，是实现外部访问统一入口的基础而过滤器功能则负责对请求的处理过程进行干预，是实现请求校验、服务聚合等功能的基础.Zuul和Eureka进行整合，将Zuul自身注册为Eureka服务治理下的应用，同时从Eureka中获得其他微服务的消息，也即以后的访问微服务都是通过Zuul跳转后获得。 ​ 注意：Zuul服务最终还是会注册进Eureka ​ 提供=代理+路由+过滤三大功能 1.简介官网：https://github.com/Netflix/zuul Zuul：维基百科 电影《捉鬼敢死队》中的怪兽，Zuul，在纽约引发了巨大骚乱。 事实上，在微服务架构中，Zuul就是守门的大Boss！一夫当关，万夫莫开！ 2.Zuul加入后的架构 不管是来自于客户端（PC或移动端）的请求，还是服务内部调用。一切对服务的请求都会经过Zuul这个网关，然后再由网关来实现 鉴权、动态路由等等操作。Zuul就是我们服务的统一入口。 ​ 3.路由基本配置（例子）3.1新建Module模块microservicecloud-zuul-gateway-99993.2 修改pom文件主要添加： &lt;!-- zuul路由网关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; 完整内容 &lt;dependencies&gt; &lt;!-- zuul路由网关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- actuator监控 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- hystrix容错--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 日常标配 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 热部署插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.3 修改application.ymlserver: port: 9999spring: application: name: microservicecloud-zuul-gatewayeureka: client: service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ instance: instance-id: microservicecloud-zuul-gateway9999 prefer-ip-address: trueinfo: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个zuul 3.4 修改主启动类ApplicationBootStart9999zuul@SpringBootApplication@EnableZuulProxypublic class ApplicationBootStart9999zuul &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart9999zuul.class, args); &#125;&#125; 3.4.1完整工程展示 3.5启动三个eureka集群 、一个服务提供类microservicecloud-provider-person-8001 、9999模块路由 3.6 测试不用路由访问：http://localhost:8001/person/get/2启用路由访问 http://localhost:9999/microservicecloud-person/person/get/2 输出都是：{“deptno”:5,”dname”:”开发部”,”db_source”:”test”} 4.路由访问映射规则上面的案例存在一个问题，那就是使用路由访问服务的时候 http://localhost:9999/microservicecloud-person/person/get/2 我们暴露了 服务名称：microservicecloud-person ，所以我们想隐藏他。 4.1 修改9999模块配置文件添加如下内容 zuul: routes: myperson.serviceId: microservicecloud-person myperson.path: /myperson/** 重要 规则说明： • zuul.routes..path=/xxx/**： 来指定映射路径。是自定义的路由名（在上面是myperson） • zuul.routes..serviceId=service-provider：来指定服务名。 而大多数情况下，我们的路由名称往往和服务名会写成一样的。因此Zuul就提供了一种简化的配置语法：zuul.routes.= 比方说上面我们关于microservicecloud-person的配置可以简化为一条： zuul: routes: microservicecloud-person: /myperson/** # 这里是映射路径# myperson.serviceId: microservicecloud-person# myperson.path: /myperson/** 省去了对服务名称的配置。 4.2 重启9999模块访问如下两个网址 （1）服务名称映射后路由访问OKhttp://localhost:9999/myperson/person/get/5 （2）未映射服务名称原路径访问OK - 默认的路由规则http://localhost:9999/microservicecloud-person/person/get/5 如果在保留第一种方式的情况下，禁止第二种方式的访问。 4.3 原真实服务名忽略-关闭默认的路由规则增你家该属性即可：ignored-services: zuul: ignored-services: microservicecloud-person #单个具体，多个可以用&quot;*&quot; ignored-services: * routes: myperson.serviceId: microservicecloud-person myperson.path: /myperson/** 重启9999zuul模块： 访问：http://localhost:9999/microservicecloud-person/person/get/5 访问失败 4.4 设置统一公共前缀增加prefix属性即可 zuul: ignored-services: microservicecloud-person #单个具体，多个可以用&quot;*&quot; ignored-services: * routes: myperson.serviceId: microservicecloud-person myperson.path: /myperson/** prefix: /sb #必须有反斜杠 http://localhost:9999/sb/myperson/person/get/5 //访问成功 http://localhost:9999/myperson/person/get/5 //不加前缀访问失败 5 .默认的路由规则在使用Zuul的过程中，上面讲述的规则（也就是上面4.1小节的规则）已经大大的简化了配置项。但是当服务较多时，配置也是比较繁琐的。因此Zuul就指定了默认的路由规则： • 默认情况下，一切服务的映射路径就是服务名本身。例如服务名为：service-provider，则默认的映射路径就 是：/service-provider/** 也就是说，刚才的映射规则我们完全不配置也是OK的，不信就试试看。 终极简化版本 zuul:# routes:# microservicecloud-person: /myperson/** # 这里是映射路径# myperson.serviceId: microservicecloud-person# myperson.path: /myperson/** 那么默认microservicecloud-person服务的映射路径是：/microservicecloud-person/** 为了不暴露服务名称，那么我么你需要关闭默认的路由规则：见4.3小节 6.过滤器Zuul作为网关的其中一个重要功能，就是实现请求的鉴权。而这个动作我们往往是通过Zuul提供的过滤器来实现的。 6.1.ZuulFilterZuulFilter是过滤器的顶级父类。在这里我们看一下其中定义的4个最重要的方法： public abstract ZuulFilter implements IZuulFilter&#123; abstract public String filterType(); abstract public int filterOrder(); boolean shouldFilter();// 来自IZuulFilter Object run() throws ZuulException;// IZuulFilter &#125; • shouldFilter：返回一个Boolean值，判断该过滤器是否需要执行。返回true执行，返回false不执行。 • run：过滤器的具体业务逻辑。 • filterType：返回字符串，代表过滤器的类型。包含以下4种： – pre：请求在被路由之前执行 – route：在路由请求时调用 – post：在route和errror过滤器之后调用 – error：处理请求时发生错误调用 • filterOrder：通过返回的int值来定义过滤器的执行顺序，数字越小优先级越高。 6.2.过滤器执行生命周期这张是Zuul官网提供的请求生命周期图，清晰的表现了一个请求在各个过滤器的执行顺序。 正常流程： • 请求到达首先会经过pre类型过滤器，而后到达route类型，进行路由，请求就到达真正的服务提供者，执行请求，返回结果后，会到达post过滤器。而后返回响应。 异常流程： • 整个过程中，pre或者route过滤器出现异常，都会直接进入error过滤器，在error处理完毕后，会将请求交给POST过滤器，最后返回给用户。 • 如果是error过滤器自己出现异常，最终也会进入POST过滤器，将最终结果返回给请求客户端。 • 如果是POST过滤器出现异常，会跳转到error过滤器，但是与pre和route不同的是，请求不会再到达POST过滤器了。 所有内置过滤器列表： 6.3.使用场景场景非常多： • 请求鉴权：一般放在pre类型，如果发现没有访问权限，直接就拦截了 • 异常处理：一般会在error类型和post类型过滤器中结合来处理。 • 服务调用时长统计：pre和post结合使用。 7.自定义过滤器接下来我们来自定义一个过滤器，模拟一个登录的校验。基本逻辑：如果请求中有access-token参数，则认为请求有效，放行。 7.1.定义过滤器类 内容： package com.kingge.filter;import com.netflix.zuul.ZuulFilter;import com.netflix.zuul.context.RequestContext;import com.netflix.zuul.exception.ZuulException;import org.apache.commons.lang.StringUtils;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;import javax.servlet.http.HttpServletRequest;@Componentpublic class LoginFilter extends ZuulFilter &#123; /** * 过滤器类型，前置过滤器 * @return */ @Override public String filterType() &#123; return &quot;pre&quot;; &#125; /** * 过滤器的执行顺序 * @return */ @Override public int filterOrder() &#123; return 1; &#125; /** * 该过滤器是否生效 * @return */ @Override public boolean shouldFilter() &#123; return true; &#125; /** * 登陆校验逻辑 * @return * @throws ZuulException */ @Override public Object run() &#123; // 获取zuul提供的上下文对象 RequestContext context = RequestContext.getCurrentContext(); // 从上下文对象中获取请求对象 HttpServletRequest request = context.getRequest(); // 获取token信息 String token = request.getParameter(&quot;access-token&quot;); // 判断 if (StringUtils.isBlank(token)) &#123; // 过滤该请求，不对其进行路由 context.setSendZuulResponse(false); // 设置响应状态码，401 context.setResponseStatusCode(401); // 设置响应信息 context.setResponseBody(&quot;&#123;\\&quot;status\\&quot;:\\&quot;401\\&quot;, \\&quot;text\\&quot;:\\&quot;request error!\\&quot;&#125;&quot;); &#125; // 校验通过，把登陆信息放入上下文信息，继续向后执行 context.set(&quot;token&quot;, token); return null; &#125;&#125; 7.2.测试没有token参数时，访问失败： 添加token参数后： 8.负载均衡和熔断Zuul中默认就已经集成了Ribbon负载均衡和Hystix熔断机制。但是所有的超时策略都是走的默认值，比如熔断超时时间只有1S，很容易就触发了。因此建议我们手动进行配置： hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 2000 # 设置hystrix的超时时间为6000ms 十一、SpringCloud Config 分布式配置中心​ 解决第四个问题，统一配置的问题 1.为什么需要配置中心 举个简单的例子，我们想要修改8001模块的服务名称的值（spring.application.name），那么我们首先得找到8001模块，然后找到配置文件，然后再进去修改。如果修改修改8003模块的配置信息，重复之前的步骤。 所以我们需要一个集中管理所有微服务配置信息的地方。 ​ 微服务意味着要将单体应用中的业务拆分成一个个子服务,每个服务的粒度相对较小,因此系统中会出现大量的服务.由于每个服务都需要必要的配置信息才能运行（application.yml）,所以一套集中式的、动态的配置管理设施是必不可少的.SpringCloud提供了ConfigServer来解决这个问题(我们每一个微服务自己带着一个application.yml,上百个配置文件的管理的问题) ​ ​ 总而言之：是为了更加方便的帮助我们集中式的管理微服务架构里面微服务的配置信息。 2. config配置中心SpringCloud Config 为微服务架构中的微服务提供集中化的外部配置支持,配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部配置，方便我们集中式的修改微服务的配置 SpringCloud Config分为服务端和客户端两部分. 服务端也称为分布式配置中心,它是一个独立的微服务应用,用来连接配置服务器并为客户端提供获取配置信息,加密/解密信息等访问接口 客户端则是通过制定的配置中心来管理应用资源,以及业务相关的配置内容,并在启动的时候从配置中心获取和加载配置信息配置服务器默认采用git来存储配置信息,这样就有助于对环境配置进行版本管理,并且可以通过git客户端工具来方便的管理和访问配置内容.（客户端可以是我们的8001,8003模块，也即是需要获取配置信息的微服务都是客户端） 功能： 1.集中管理配置文件 2.不同环境不同配置,动态化的配置更新,分环境部署比如dev/test/prod/beta/release 3.运行期间动态调整配置,不再需要在每个服务部署的机器上编写配置文件,服务会向配置中心统一拉取配置自己的信息 4.当配置发生变动时,服务不需要重启即可感知到配置的变化并应用新的配置 5.将配置信息以REST接口的形式暴露 配置中心配置文件放置的位置：与GitHub整合配置 由于SpringCloudConfig默认使用Git来存储配置文件(也有其他方式,比如支持SVN和本地文件),但最推荐的还是使用Git,而且使用的是http/https访问形式 3.SpringCloud Config服务端配置3.1 在GitHub上新建一个名为springcloud-config-server的新Repository git@github.com:JeremyKinge/springcloud-config-server.git 3.2 本地磁盘获取上述创建的仓库git命令:git clone git@github.com:JeremyKinge/springcloud-config-server.git 3.3 在上述磁盘新建配置文件并上传到git仓库（1）添加配置文件application.yml spring: profiles: active: - dev---spring: profiles: dev application: #开发环境 name: microservicecloud-config-kingge-dev---spring: profiles: test #测试环境 application: name: microservicecloud-config-kingge-test#请保存为UTF-8格式 （2）git bash执行上传命令： git add .git commit -m “新建配置文件”git push origin master （3）查看github 3.4 新建Module模块microservicecloud-config-10000它即为Cloud的配置中心模块3.5 修改10000模块pom文件和yml配置文件、启动类（1）修改pom文件 &lt;dependencies&gt; &lt;!-- springCloud Config 关键代码--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 避免Config的Git插件报错：org/eclipse/jgit/api/TransportConfigCallback --&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jgit&lt;/groupId&gt; &lt;artifactId&gt;org.eclipse.jgit&lt;/artifactId&gt; &lt;version&gt;4.10.0.201712302008-r&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 图形化监控 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 熔断 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 热部署插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 关键代码 &lt;!-- springCloud Config 关键代码--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改yml文件 server: port: 10000 spring: application: name: microservicecloud-config cloud: config: server: git: uri: git@github.com:JeremyKinge/springcloud-config-server.git #GitHub上面的git仓库名字 #username: xxxxxx #如果访问github需要密码那么填写下面三项 #password: xxxxxxx #force-pull: true （3）启动类 @SpringBootApplication@EnableConfigServerpublic class ApplicationBootStart10000config &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart10000config.class,args); &#125;&#125; 3.6 完整项目结构 3.7 测试通过Config微服务是否可以从GitHub上获取配置内容启动微服务10000http://localhost:10000/application-dev.yml 输出： http://localhost:10000/application-test.yml 输出： http://localhost:10000/application-xxx.yml(不存在的配置) 成功实现了用SpringCloud Config通过GitHub获取配置信息 3.8 配置文件读取规则 上面3.8测试中，我们采用的是第二种方式，通过10000配置中心去github获取配置文件 请求例子： /{application}-{profile}.yml http://localhost:10000/application-dev.yml http://localhost:10000/application-test.yml http://localhost:10000/application-xxx.yml(不存在的配置)/{application}/{profile}[/{label}] http://localhost:10000/application/dev/master http://localhost:10000/application/test/master http://localhost:10000/application/xxx/master/{label}/{application}-{profile}.yml http://localhost:10000/master/application-dev.yml http://localhost:10000/master/application-test.yml 4.SpringCloud Config客户端配置与测试在上面中我们已经搭建好了，配置中心的服务端。那么接下来演示一下客户端怎么取获取服务端的配置信息。 新建8004服务提供者，他的配置信息我们取自配置中心（而不是在application.yml中配置，动态获取） 4.1 新建8004 配置文件并上传到github新建microservicecloud-provider-person-config-client-8004.yml spring: profiles: active: - dev--- server: port: 8004mybatis: config-location: classpath:mybatis/mybatis.cfg.xml # mybatis配置文件所在路径 type-aliases-package: com.kingge.entity # 所有Entity别名类所在包 mapper-locations: - classpath:mybatis/mapper/**/*.xml # mapper映射文件spring: profiles: dev application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://127.0.0.1:3306/test # 数据库名称 username: root password: 123 dbcp2: min-idle: 5 # 数据库连接池的最小维持连接数 initial-size: 5 # 初始化连接数 max-total: 5 # 最大连接数 max-wait-millis: 200 # 等待连接获取的最大超时时间#eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/# http://localhost:7001/eureka #单机版本使用# defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ instance: instance-id: microservicecloud-person8001 #自定义服务实例名 prefer-ip-address: true #访问路径可以显示IP地址 lease-expiration-duration-in-seconds: 10 # 10秒即过期 lease-renewal-interval-in-seconds: 5 # 5秒一次心跳#info: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个提供查询部门人员信息的服务---server: port: 8005mybatis: config-location: classpath:mybatis/mybatis.cfg.xml # mybatis配置文件所在路径 type-aliases-package: com.kingge.entity # 所有Entity别名类所在包 mapper-locations: - classpath:mybatis/mapper/**/*.xml # mapper映射文件spring: profiles: test application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://127.0.0.1:3306/test2 # 数据库名称 username: root password: 123 dbcp2: min-idle: 5 # 数据库连接池的最小维持连接数 initial-size: 5 # 初始化连接数 max-total: 5 # 最大连接数 max-wait-millis: 200 # 等待连接获取的最大超时时间#eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/# http://localhost:7001/eureka #单机版本使用# defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ instance: instance-id: microservicecloud-person8001 #自定义服务实例名 prefer-ip-address: true #访问路径可以显示IP地址 lease-expiration-duration-in-seconds: 10 # 10秒即过期 lease-renewal-interval-in-seconds: 5 # 5秒一次心跳#info: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个提供查询部门人员信息的服务 dev和test环境的不同在于，他们的服务端口不同和访问的数据库不同 4.2 microservicecloud-provider-person-config-client-8004模块（1）修改pom文件 &lt;dependencies&gt; &lt;!-- 引入自己定义的api通用包，可以使用Person用户Entity --&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- SpringCloud Config客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- actuator监控信息完善 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 将微服务provider端注册进eureka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 关键依赖 &lt;!-- SpringCloud Config客户端 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt; 4.3 新建bootstrap.yml 系统级别配置文件applicaiton.yml是用户级的资源配置项bootstrap.yml是系统级的，优先级更加高 Spring Cloud会创建一个Bootstrap Context，作为Spring应用的Application Context的父上下文。初始化的时候，Bootstrap Context负责从外部源加载配置属性并解析配置。这两个上下文共享一个从外部获取的Environment。Bootstrap属性有高优先级，默认情况下，它们不会被本地配置覆盖。 Bootstrap context和Application Context有着不同的约定，所以新增了一个bootstrap.yml文件，保证Bootstrap Context和Application Context配置的分离。 换句话说，如果将下面内容放到application.yml中，那么项目启动会报错，因为是找不到控制层ConfigClientRest导入的属性 @Value(“${spring.application.name}”) （1）增加内容如下： spring: cloud: config: name: microservicecloud-provider-person-config-client-8004 #需要从github上读取的资源名称，注意没有yml后缀名 profile: dev #本次访问的配置项-profile值是什么，决定从github上读取什么 label: master uri: http://localhost:10000 #本微服务启动后先去找config配置中心地址，通过SpringCloudConfig获取GitHub的服务地址 （2）为了配置文件的完整性我们新建一个空的application.yml 4.4 新建一个控制层，获取配置文件的某些属性（测试）实际上如果8004服务端能够正常启动和访问，也能够说明客户端获取config配置中心配置文件成功。不过为了证实一下，所以这里通过控制层输出某些属性 （1）新建ConfigClientRest @RestControllerpublic class ConfigClientRest &#123; @Value(&quot;$&#123;spring.application.name&#125;&quot;) private String applicationName; @Value(&quot;$&#123;eureka.client.service-url.defaultZone&#125;&quot;) private String eurekaServers; @Value(&quot;$&#123;server.port&#125;&quot;) private String port; @Value(&quot;$&#123;spring.datasource.url&#125;&quot;) private String datasourceurl; @RequestMapping(&quot;/config&quot;) public String getConfig() &#123; String str = &quot;applicationName: &quot;+applicationName+&quot;\\t eurekaServers:&quot;+eurekaServers+&quot;\\t port: &quot;+port; System.out.println(&quot;******str: &quot;+ str); return &quot;applicationName: &quot;+applicationName+&quot;\\t eurekaServers:&quot;+eurekaServers+&quot;\\t port: &quot;+port + &quot;\\t datasourceurl：&quot;+datasourceurl; &#125;&#125; 后面可以通过更改 bootstrap.yml的profile属性的值，访问/config,查看服务端口号和数据库url是否改变。 4.5 新建启动类@SpringBootApplicationpublic class ApplicationBootStart8004client &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart8004client.class, args); &#125;&#125; 4.5.1 完整目录机构 4.6 启动10000配置中心和8004服务 （1） 启动Config配置中心10000微服务并自测 （2）启动8004作为Client准备访问 bootstrap.yml里面的profile值是什么，决定从github上读取什么 ​ 2.1 启动成功，说明配置文件获取成功 ​ 2.2 额外验证 假如目前是 profile: dev dev默认在github上对应的端口就是8004 http://localhost:8004/config 输出：applicationName: microservicecloud-person eurekaServers:http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ port: 8004 datasourceurl：jdbc:mysql://127.0.0.1:3306/test ​​ 假如目前是 profile: test​ test默认在github上对应的端口就是8005​ http://localhost:8005/config​ 输出：applicationName: microservicecloud-person eurekaServers:http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ port: 8005 datasourceurl：jdbc:mysql://127.0.0.1:3306/test2 5.手动刷新配置​ 上面的案例，有个缺点，那就是我们修改了github上面的配置文件（例如修改了连接数据库的地址），对应的8004模块没有刷新，也就是，他获取的连接数据库的地址还是未修改前的。 ​ 在真实的案例中，我们不可能手动关闭服务器，然后重启。所以就需要8004它能够自动刷新 需要依赖actuator组件和通过psot方式访问请求配置变动的服务器（也即是@RefreshScope注解所在的bean） （1）8004模块引入actuator依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; （2）8004启动类添加@EnableDiscoveryClient服务发现注解 @SpringBootApplication@EnableDiscoveryClientpublic class ApplicationBootStart8004client &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart8004client.class, args); &#125;&#125; （3）8004模块修改application.yml文件，暴露所有端点 management: security: enabled: false （4）8004添加controller @RestController@RefreshScopepublic class ConfigClientRest &#123; @Value(&quot;$&#123;spring.application.name&#125;&quot;) private String applicationName; @Value(&quot;$&#123;eureka.client.service-url.defaultZone&#125;&quot;) private String eurekaServers; @Value(&quot;$&#123;server.port&#125;&quot;) private String port; @Value(&quot;$&#123;spring.datasource.url&#125;&quot;) private String datasourceurl; @RequestMapping(&quot;/config&quot;) public String getConfig() &#123; String str = &quot;applicationName: &quot;+applicationName+&quot;\\t eurekaServers:&quot;+eurekaServers+&quot;\\t port: &quot;+port; System.out.println(&quot;******str: &quot;+ str); return &quot;applicationName: &quot;+applicationName+&quot;\\t eurekaServers:&quot;+eurekaServers+&quot;\\t port: &quot;+port + &quot;\\t datasourceurl：&quot;+datasourceurl; &#125;&#125; （5）启动configserver10000和8004模块 打印通过configserver从github上面获取的配置信息 此时数据库的url是：jdbc:mysql://127.0.0.1:3306/test1 （5）这个时候我们修改8004模块在github中引入的配置文件- 也就是修改microservicecloud-provider-person-config-client-8004.yml 文件。 1.修改访问数据库的url为，原先是test1 url: jdbc:mysql://127.0.0.1:3306/test2 2.修改8004的服务端口为8005，原先是8008 - 这个修改我们预测不会成功，因为服务器没有重启 server.port：8005 （6）请求：curl -X POST http://localhost:8008/refresh 手动更新8004模块的配置 我们只需要请求这个地址就可以实现配置文件的更新，而不用重启8004模块，他默认会帮我们刷新配置文件。 我们发现数据库访问地址修改成功，但是服务端口号没有成功（很明显这种做法是错误的，因为服务端口号不会轻易的变动） 注意如果执行curl -X POST http://localhost:8008/refresh 包如下错误 &#123;&quot;timestamp&quot;:1513070580796,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;Full authentication is required to access this resource.&quot;,&quot;path&quot;:&quot;/refresh&quot;&#125; 那就说明默认开启访问端口验证了。需要关闭 一、在Spring Boot1.5.x版本中通过management.security.enabled=false来暴露所有端点 二、切换SpringBoot版本为2.x 使用IDE的搜索功能，找到类ManagementServerProperties，发现Security内部类已经被删除，通过去官网查看2.0暴露端点的方式得知： 方式1： # 启用端点 envmanagement.endpoint.env.enabled=true # 暴露端点 env 配置多个,隔开management.endpoints.web.exposure.include=env 方式2： 方式1中的暴露方式需要一个一个去开启需要暴露的端点，方式2直接开启和暴露所有端点 management.endpoints.web.exposure.include=* 注意在使用Http访问端点时，需要加上默认/actuator 前缀 三、如果这三种还不行，可以尝试在8004添加security验证依赖，然后给8004模块设置访问账号和密码， 然后通过 curl -X POST http://账号:密码@localhost:8008/refresh 这种方式访问 6.自动刷新配置上面是通过手动刷新方式，缺点就是，如果我们在github上面修改了20个服务器的配置，那么我们需要手动执行20次 curl -X POST —— ，那么我们想能够缩减执行命令的次数或者说自动刷新配置。 额外知识补充1.spring cloud服务发现注解之@EnableDiscoveryClient与@EnableEurekaClient区别在使用服务发现的时候有两种注解， 一种为@EnableDiscoveryClient, 一种为@EnableEurekaClient, 用法上基本一致，下文是从stackoverflow上面找到的对这两者的解释： There are multiple implementations of &quot;Discovery Service&quot; (eureka, consul, zookeeper). @EnableDiscoveryClient lives in spring-cloud-commons and picks the implementation on the classpath. @EnableEurekaClient lives in spring-cloud-netflix and only works for eureka. If eureka is on your classpath, they are effectively the same. 意思也就是spring cloud中discovery service有许多种实现（eureka、consul、zookeeper等等） @EnableDiscoveryClient基于spring-cloud-commons；而 @EnableEurekaClient基于spring-cloud-netflix 对@EnableEurekaClient的源码如下： /** * Convenience annotation for clients to enable Eureka discovery configuration * (specifically). Use this (optionally) in case you want discovery and know for sure that * it is Eureka you want. All it does is turn on discovery and let the autoconfiguration * find the eureka classes if they are available (i.e. you need Eureka on the classpath as * well). * * @author Dave Syer * @author Spencer Gibb */@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@EnableDiscoveryClientpublic @interface EnableEurekaClient &#123;&#125; 注解@EnableEurekaClient上有@EnableDiscoveryClient注解，可以说基本就是EnableEurekaClient有@EnableDiscoveryClient的功能，另外上面的注释中提到，其实@EnableEurekaClient注解就是一种方便使用eureka的注解而已，可以说使用其他的注册中心后，都可以使用@EnableDiscoveryClient注解， 但是使用@EnableEurekaClient的情景，就是在服务采用eureka作为注册中心的时候，使用场景较为单一。 所以还是比较建议使用@EnableDiscoveryClient。 所以上面的还是建议使用@EnableDiscoveryClient替换@EnableEurekaClient 2.Ribbon和Feign这两种都是SPringcloud提供的负载均衡技术，都是客户端的软负载均衡技术。feign优化的Ribbon的服务调用方式，实现了面向接口编程的封装。","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://kingge.top/categories/springcloud/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"springcloud","slug":"springcloud","permalink":"http://kingge.top/tags/springcloud/"},{"name":"微服务架构","slug":"微服务架构","permalink":"http://kingge.top/tags/微服务架构/"}]},{"title":"浅谈dubbo和springcloud","slug":"浅谈dubbo和springcloud","date":"2019-04-27T02:21:59.000Z","updated":"2019-08-25T02:08:25.860Z","comments":true,"path":"2019/04/27/浅谈dubbo和springcloud/","link":"","permalink":"http://kingge.top/2019/04/27/浅谈dubbo和springcloud/","excerpt":"","text":"一、Dubbo负责人的采访刘军，阿里巴巴中间件高级研发工程师，主导了 Dubbo 重启维护以后的几个发版计划，所以让我们来看一下他关于Dubbo和Springcloud是否二选一，他们之间的区别的阐述。 7、目前 Dubbo 被拿来比较最多的就是 Spring Cloud ，您怎么看待二者的关系，业务上是否有所冲突？ 关于 Dubbo 和 Spring Cloud 间的关系，我们在开源中国年终盛典的 Dubbo 分享中也作了简单阐述，首先要明确的一点是 Dubbo 和 Spring Cloud 并不是完全的竞争关系，两者所解决的问题域并不一样：Dubbo 的定位始终是一款 RPC 框架，而 Spring Cloud 的目标是微服务架构下的一站式解决方案。如果非要比较的话，我觉得 Dubbo 可以类比到 Netflix OSS 技术栈，而 Spring Cloud 集成了 Netflix OSS 作为分布式服务治理解决方案，但除此之外 Spring Cloud 还提供了包括 config、stream、security、sleuth 等等分布式问题解决方案。 当前由于 RPC 协议、注册中心元数据不匹配等问题，在面临微服务基础框架选型时 Dubbo 与 Spring Cloud 是只能二选一，这也是为什么大家总是拿 Dubbo 和 Spring Cloud 做对比的原因之一。Dubbo 之后会积极寻求适配到 Spring Cloud 生态，比如作为 Spring Cloud 的二进制通信方案来发挥 Dubbo 的性能优势，或者 Dubbo 通过模块化以及对 http 的支持适配到 Spring Cloud 。 Netflix OSS 技术栈 1.Netflix Eureka 服务注册中心，提供服务的注册和发现 2.Netflix Ribbon 客户端负载均衡 3.Netflix Feign 客户端负载均衡，包装Ribbon，提供了接口式的服务调用 4.Netflix Hystrix 熔断器，负责服务的熔断和降级 5.NetFlix Hystrix dashboard 提供服务监控 6.Netflix zuul 路由网管 提供代理+路由+过滤三大功能 二、Dubbo和Springcloud比较 1.社区活跃度 https://github.com/dubbo dubbo社区https://github.com/springcloud springcloud社区 可以进去看一下他们对于技术的活跃程度曲线 2.解决问题的方向 dubbo：定位始终是一款 RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 springcloud：微服务架构，提供微服务架构的一站式服务。 3.功能对比 根据两者的解决问题的域，得到他们的功能 Dubbo Spring 服务注册中心 Zookeeper Spring Cloud Netfilx Eureka 服务调用方式 RPC REST API 服务监控 Dubbo-monitor Spring Boot Admin 断路器 不完善 Spring Cloud Netflix Hystrix 服务网关 无 Spring Cloud Netflix Zuul 分布式配置 无 Spring Cloud Config 服务跟踪 无 Spring Cloud Sleuth 消息总线 无 Spring Cloud Bus 数据流 无 Spring Cloud Stream 批量任务 无 Spring Cloud Task Dubbo的某些功能都是，通过整合其他组件实现，springcloud是通过实现Netflix oss的技术栈和原有的技术，实现的架构。 Dubbo提供了各种Filter，对于上述中“无”的要素，可以通过扩展Filter来完善。 例如 1．分布式配置：可以使用淘宝的diamond、百度的disconf来实现分布式配置管理 2．服务跟踪：可以使用京东开源的Hydra，或者扩展Filter用Zippin来做服务跟踪 3．批量任务：可以使用当当开源的Elastic-Job、tbschedule 总结：从核心要素来看，Spring Cloud 更胜一筹，在开发过程中只要整合Spring Cloud的子项目就可以顺利的完成各种组件的融合，而Dubbo缺需要通过实现各种Filter来做定制，开发成本以及技术难度略高。Dubbo更像是一个组装机，springcloud是一体机。 4.服务调用方式 Spring Cloud抛弃了RPC通讯，采用基于HTTP的REST方式。Spring Cloud牺牲了服务调用的性能，但是同时也避免了原生RPC带来的问题。REST比RPC更为灵活，不存在代码级别的强依赖，在强调快速演化的微服务环境下，显然更合适。 5.服务获取方式 dubbo通过长连接推送服务提供者地址列表给消费端，即是：Dubbo订阅Zookeeper下相应的节点，当节点的状态发生改变时，Zookeeper会立即反馈订阅的Client，实时性很高。 springcloud的eureka是 消费者端主动去eurekaServer注册中心获取数据，消费者可以配置去EurekaServer拉去服务列表的周期 dubbo支持各种通信协议，而且消费方和服务方使用长链接方式交互，通信速度上略胜Spring Cloud，如果对于系统的响应时间有严格要求，长链接更合适。 6.服务注册中心满足的CAP原则 著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性P在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。 Dubbo推荐使用zookeeper作为服务注册中心，zookeeper满足CP原则，一致性和分区容错性。springcloud的服务注册中心是Eureka，他满足的是AP原则，可用性和分区容错性。 Zookeeper如何保证CP 当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。 Eureka如何保证AP Eureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的 ，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用) 当网络稳定时，当前实例新的注册信息会被同步到其它节点中 因此， Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。 总结：那么既然保证了保证了可用性，那么数据的一致性肯定是不能够保证了，所以这个就是自我保护的机制。所以到底是AP还是CP，又或者是AC（数据库），要看业务场景来定。 而且Eureka部署集群时非常简单的，相比于dubbo部署zookeeper集群。 7.节点性质 Dubbo只有Consumer订阅Provider节点，也就是Consumer发现Provider节点信息 Eureka不区分Consumer或者Provider，两者都统称为Client，一个Client内可能同时含有Provider，Consumer，通过服务发现组件获取的是其他所有的Client节点信息，在调用时根据应用名称来筛选节点 三、总结","categories":[{"name":"dubbo","slug":"dubbo","permalink":"http://kingge.top/categories/dubbo/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"dubbo","slug":"dubbo","permalink":"http://kingge.top/tags/dubbo/"},{"name":"rpc","slug":"rpc","permalink":"http://kingge.top/tags/rpc/"}]},{"title":"docker个人总结","slug":"docker个人总结","date":"2019-02-28T13:59:59.000Z","updated":"2019-08-25T03:05:04.155Z","comments":true,"path":"2019/02/28/docker个人总结/","link":"","permalink":"http://kingge.top/2019/02/28/docker个人总结/","excerpt":"","text":"之前学习过docker，但是很浅显的使用，概念和流程各个方面总结的不够到位，下面根据旧版本的文档，重新的梳理。 一、docker出现的契机作为开发人员，我们经常会遇到一个问题，那就是环境不统一的问题。什么意思呢？自己在本地测试的项目是运行正常的，但是打包给测试或者运维人员部署使用时，经常会出现，部署报错，运行不起来，等等问题。就算是再详细的部署文档也还是会出错。 这个时候就产生了大量沟通的成本，通常产生这些问题的原因是部署的环境并不是开发人员的那一份环境，可能是jdk版本或者tomcat版本，数据库等等环境产生的问题。所以就需要我们开发人员打包一份连同环境和配置以及项目，交付给测试或者运维。这样就能够保证项目运行环境的一致性，也容易排查问题。这个就是docker的雏形 ​ 就好比我们在迁移一棵树的时候，尾部，总是会保留着一些原先的土，就是为了解决生长环境的不同额度适配问题。 Docker之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。 环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。开发人员利用 Docker 可以消除协作编码时“在我的机器上可正常工作”的问题。 ​ 总的来说，以前我们是通过提交war包的方式，那么现在是连同war运行的环境**一起打包给测试或者运维。 传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等(java为例)。而为了让这些程序可以顺利执行，开发团队也得准备完整的部署文件，让维运团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件+所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker镜像的设计，使得Docker得以打破过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间（类似于jvm的理念）的无缝接轨运作。 1.1 理念Docker是基于Go语言实现的云开源项目。 Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP（可以是一个WEB应用或数据库应用等等）及其运行环境能够做到“一次封装，到处运行”。 Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作 也就是说，我们可以把项目运行成功所需要的环境（redis，nginx，mysql）等等组件，通过编译打包的形式，打包成一个个的货仓。 项目部署到其他环境时（windowsàlinux）只需要运行这些货仓就可以安装这些环境，做到一次封装到处运行，解决了因为环境不同导致app部署或运行失败的问题 docker的logo就阐述了这一理念，部署项目的时候直接搬运已经测试成功的app运行环境。 ​ 特别是在多集群的环境下，docker的作用更显而易见（避免多次安装环境） 总的来说：解决了运行环境和配置问题软件容器（每个容器对应着一个集装箱，每个集装箱对应着项目运行所需的软件或者配置），方便做持续集成并有助于整体发布的容器虚拟化技术。 二、docker的演化2.1 虚拟机技术一个虚拟机的结构图 虚拟机（virtual machine）就是带环境安装的一种解决方案。 带环境安装的意思是：它里面模拟了一个正常的操作系统所具备的各种环境和配置（内存、处理器、硬盘。。。。） 它可以在一种操作系统里面运行另一种操作系统，比如在Windows 系统里面运行Linux 系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。 ​ 缺点： \\1. 启动很慢 \\2. 资源占用多 \\3. 冗余步骤多 所以docker在这之上就演化出了 容器虚拟化技术 2.2 容器虚拟化技术由于前面虚拟机存在这些缺点，Linux 发展出了另一种虚拟化技术：Linux 容器（Linux Containers，缩写为 LXC）。 Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。 相比虚拟机技术的系统结构图，很明显发现，公用库api模块被移除。各个app维护自己的所依赖的api模块。好处就是，节省了资源的占用。 2.3 总结不同比较了 Docker 和传统虚拟化方式的不同之处： \\1. 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程 \\2. 而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 \\3. 每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。 4. Linux虚拟机安装包可能需要4G，但是docker只需要170M。很明显这是一个很大的提升，换句话说，docker就是一个精细版的linux虚拟机 三、docker的好处一次构建、随处运行 更快速的应用交付和部署 更便捷的升级和扩缩容 更简单的系统运维 更高效的计算资源利用 四、安装和下载docker官网：http://www.docker.com docker中文网站： https://www.docker-cn.com/ Docker Hub官网: https://hub.docker.com/ Docker 分为 CE 和 EE 两大版本。 CE 即社区版（免费，支持周期 7 个月）， EE 即企业版，强调安全，付费使用，支持周期 24 个月。下面安装的是CE版本。 4.1 docker安装前提条件Docker支持以下的CentOS版本：CentOS 7 (64-bit) CentOS 6.5 (64-bit) 或更高的版本 目前，CentOS 仅发行版本中的内核支持 Docker。 Docker 运行在 CentOS 7 上，要求系统为64位、系统内核版本为 3.10 以上。 Docker 运行在 CentOS-6.5 或更高的版本的 CentOS 上，要求系统为64位、系统内核版本为 2.6.32-431 或者更高版本。 为了避免我们后面启动tomcat 容器做测试的时候，外部浏览器访问tomcat容器时，端口被拦截。这里先关闭虚拟机的防火墙 service firewalld status ；查看防火墙状态 service firewalld stop：关闭防火墙 查看自己linux 内核uname命令用于打印当前系统相关信息（内核版本号、硬件架构、主机名称和操作系统类型等）。 查看已安装的CentOS版本信息（CentOS6.8有，CentOS7无该命令） 另一种方式查询 4.2 docker安装一下安装是使用yum命令进行安装，所以linux虚拟机需要能够连接互联网 官方手册： https://docs.docker-cn.com/engine/installation/linux/docker-ce/centos/#prerequisites CentOS6.8安装Docker\\1. yum install -y epel-release 使用root用户执行改命令。 Docker使用EPEL发布，RHEL系的OS首先要确保已经持有EPEL仓库，否则先检查OS的版本，然后安装相应的EPEL包 \\2. yum install -y docker-io 发现这个命令在centos6.10 版本时，提示docker包找不到，于是花了另一命令：yum –y install docker 安装成功 \\3. 安装后的配置文件：/etc/sysconfig/docker \\4. 启动Docker后台服务：service docker start 5.docker version验证 CentOS7以上安装Docker（本人使用的版本）-推荐 下面使用仓库的方式进行安装docker-ce \\1. cat /etc/redhat-release 命令查看centos版本 \\2. yum安装gcc相关 执行以下两条命令 yum -y install gcc yum -y install gcc-c++ 3.卸载老版本（如果之前没有装过，可以忽略这一步） 注意看官网的操作手册，里面有着一段命令。 $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine \\4. 安装需要的软件包 ​ yum install -y yum-utils device-mapper-persistent-data lvm2 \\5. 设置stable镜像仓库 执行命令： ​ yum-config-manager –add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo ​ 注意这里使用的是阿里的镜像仓库，不要使用官网推荐的仓库 ​ yum-config-manager –add-repo https://download.docker.com/linux/centos/docker-ce.repo 不能使用这条命令 \\6. 更新yum软件包索引 ​ yum makecache fast \\7. 安装DOCKER CE ​ yum -y install docker-ce \\8. 启动docker ​ systemctl start docker \\9. 测试 docker version :查看docker版本 docker pull hello-world（从阿里云仓库中获取hello-world镜像） ​ docker run hello-world （要先下载hello-world镜像后才能够运行） 10.卸载 ​ 执行以下三条命令： ​ systemctl stop docker ​ yum -y remove docker-ce ​ rm -rf /var/lib/docker 4.3阿里云镜像加速因为docker官网提供的获取镜像地址（hub.docker），访问速度太过缓慢，这里改换成阿里云的镜像服务。 \\1. 登录阿里云 https://www.aliyun.com/ 进入管理中心 \\2. 搜索容器镜像服务 可以看到镜像加速器 获得加速器地址连接 \\3. 配置本机Docker运行镜像加速器 Centos6.8版本设置： vim /etc/sysconfig/docker 将获得的自己账户下的阿里云加速地址配置进 other_args=”–registry-mirror=https://你自己的账号加速信息.mirror.aliyuncs.com“ Centos7.6版本设置： sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json &lt;&lt;-‘EOF’ { “registry-mirrors”: [“https://sk6o0yc78m.mirror.aliyuncs.com“] } EOF sudo systemctl daemon-reload sudo systemctl restart docker Centos7以上的配置文件时：/etc/docker/daemon.json 4．检查配置是否生效 Centos6.8 检查命令： ​ 启动docker，执行命令 ps –ef| grep docker Centos7.6检查命令： 4.4 设置docker开机启动systemctl enable docker 五、docker组成和分析5.1 Docker组成镜像（image）Docker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 一个类可以new多个对象。 镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。 UnionFS（联合文件系统）UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 跟花卷一样，一层一层的 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 Docker镜像加载原理 Docker镜像加载原理： docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。 bootfs(boot file system)主要包含bootloader和kernel, bootloader主要是引导加载kernel, Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs (root file system) ，在bootfs之上。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。 平时我们安装进虚拟机的CentOS都是好几个G，为什么docker这里才200M？？ 对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host（宿主机）的kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别, 因此不同的发行版可以公用bootfs。 分层的镜像以我们的pull为例，在下载的过程中我们可以看到docker的镜像好像是在一层一层的在下载 有多个complete，说明有多个层次 为什么 Docker 镜像要采用这种分层结构呢最大的一个好处就是 - 共享资源 比如：有多个镜像都从相同的 base 镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像， 同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。 特点Docker镜像都是只读的 当容器启动时，一个新的可写层被加载到镜像的顶部。 这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 容器（container）鲸鱼背上的集装箱Docker 利用容器（Container）独立运行的一个或一组应用。容器是用镜像创建的运行实例。 它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 即是：容器=镜像+可读写层 仓库（repository）仓库（Repository）是集中存放镜像文件的场所。 仓库(Repository)和仓库注册服务器（Registry）是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。 最大的公开仓库是 Docker Hub(https://hub.docker.com/)， 存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云 等 总结需要正确的理解仓储/镜像/容器这几个概念: Docker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就似乎 image镜像文件。只有通过这个镜像文件才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 \\1. image 文件生成的容器实例，本身也是一个文件，称为镜像文件。 \\2. 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器 \\3. 至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。 总结：仓库存放着很多镜像，通过使用镜像可以生成多个容器 5.2 Docker运行流程 Docker 使用 C/S 结构，即客户端/服务器体系结构。 Docker 客户端与 Docker 服务器进行交互，Docker服务端负责构建、运行和分发 Docker 镜像。 Docker 客户端和服务端可以运行在一台机器上，也可以通过 RESTful 、 stock 或网络接口与远程 Docker 服务端进行通信。 执行docker run hello-world这个例子在上面我们已经使用过了，对照docker结构图我们来分析 Client：客户端就是我们的linux的命令窗口，也就是执行docker run hello-world的地方 Docker-host： docker主机，也就是执行客户端发出请求的地方，也就是我们启动的docker进程。收到一个执行hello-world容器的命令，现在本地种查找是否存在这个容器（镜像），存在则直接运行。不存在，则在Repository中取（上面我们配置了阿里云镜像仓库），pull镜像后放到本地，然后新建一个容器执行这个hello-world镜像。 Repository:：仓库，存放镜像的地方 下次执行docker run hello-world，则会从本地中拿hello-world镜像，然后新建容器执行。 完整流程如图： 六、docker常用操作命令6.1 帮助命令docker versiondocker info能够查看更详细的docker信息，比docker version命令更加详细 docker –help6.2 镜像命令6.2.1 docker images列出本地主机上的镜像 各个选项说明: REPOSITORY：表示镜像的仓库源 TAG：镜像 标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 redis，docker 将默认使用 redis:latest镜像 OPTIONS说明： -a :列出本地所有的镜像（含中间映像层） -q :只显示镜像ID。 –digests :显示镜像的摘要信息 –no-trunc :显示完整的镜像信息 6.2.2 docker search 某个XXX镜像名字需要注意，查询是从网站 https://hub.docker.com上进行查询，拉取镜像的时候是从阿里云上拉取 命令： docker search [OPTIONS] 镜像名字 OPTIONS说明： –no-trunc : 显示完整的镜像描述 -s : 列出收藏数不小于指定值的镜像。（就是下面的STARS数） –automated : 只列出 automated build类型的镜像； 6.2.3 docker pull 某个XXX镜像名字下载镜像 命令：ker pull 镜像名字[:TAG] （如果不指明标签默认下载最新版本） 6.2.4 docker rmi 某个XXX镜像名字ID删除镜像 删除单个： docker rmi -f 镜像ID （不指明标签默认删除latest） docker rmi -f 镜像名称 （不指明标签默认删除latest） 删除多个：docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除全部：ocker rmi -f $(docker images -qa) （docker images –qa 查询当前docker中所有镜像id） 6.3容器命令6.3.1有镜像才能创建容器，这是根本前提(下载一个CentOS镜像演示)docker pull centos 所以说可以把容器看做是一个简易版的 Linux 环境 6.3.2新建并启动容器docker run [OPTIONS] IMAGE [COMMAND] [ARG…] OPTIONS说明 OPTIONS说明（常用）：有些是一个减号，有些是两个减号 –name=”容器新名字”: 为容器指定一个名称； -d: 后台运行容器，并返回容器ID，也即启动守护式容器； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； -P: 随机端口映射； -p: 指定端口映射，有以下四种格式 ​ ip:hostPort:containerPort ​ ip::containerPort ​ hostPort:containerPort ​ containerPort 下面运行6.3.1中下载的centos 启动交互式容器（跟下面我们所说的启动守护式容器有区别） #使用镜像centos:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。 docker run -it centos /bin/bash 等同于 docker run -it centos 案例演示1.从Hub上下载tomcat镜像到本地并成功运行 （1）docker pull tomcat （2）docker run -it -p 8088:8080 tomcat 运行tomcat，第一个端口8088表示docker对外暴露访问内部tomcat的端口，映射内部tomcat 的8080端口，什么意思呢？ 查看运行的容器 访问tomcat成功，直接访问http://49.234.188.74:8080/ 失败，因为我们知道，tomcat 是docker运行的一个容器，所以需要docker对外暴露后才能够访问。 （3）使用-P 不指名端口的方式运行tomcat 很明显使用大P的方式启动tomcat，docker会随机分配一个对外暴露的端口 6.3.3列出当前所有正在运行的容器docker ps [OPTIONS] OPTIONS说明（常用）： -a :列出当前所有正在运行的容器+历史上运行过的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 –no-trunc :不截断输出。 查看6.3.2 运行的centos 6.3.4退出容器exit容器停止退出，销毁容器。注意，容器内的数据也会一并消失，类似java的对象，close销毁后就不会存在了。 ctrl+P+Q容器不停止退出,回到宿主机。（那么怎么回到容器呢？-请看下面补充章节的第五小点） 6.3.5启动容器docker start 容器ID或者容器名 （可以启动已经关闭的容器）（docker ps –l 查看最近运行过得容器） 6.3.6重启容器docker restart 容器ID或者容器名 6.3.7停止容器docker stop 容器ID或者容器名 6.3.8强制停止容器docker kill 容器ID或者容器名 6.3.9删除已停止的容器docker rm 容器ID 一次性删除多个容器（下面两种方式） docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm 6.3.10 迁移与备份 补充启动守护式容器 命令：docker run -d 容器名 docker run -d centos 问题：然后docker ps -a 进行查看, 会发现容器已经退出，也就是说没有刚才我们启动的容器 很重要的要说明的一点: Docker**容器后台运行,就必须有一个前台进程**. 容器运行的命令如果不是那些一直挂起的命令（比如运行**top，tail**），就是会自动退出的。 这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如 service nginx start 但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用, 这样的容器后台启动后,会立即自杀因为他觉得他没事可做了. 所以，最佳的解决方案是,将你要运行的程序以前台进程的形式运行 查看容器日志命令：docker logs -f -t –tail 容器ID 参数解析： * -t 是加入时间戳 * -f 跟随最新的日志打印 * –tail 数字 显示最后多少条 例子： 启动守护式容器（因为存在前台程序一直循环输出,那么他就不会退出） docker run -d centos /bin/sh -c “while true;do echo hello kingge;sleep 2;done” 查看容器内运行的进程docker top 容器ID 返回的信息时，上个例子中启动的守护式容器 查看容器内部细节docker inspect 容器ID 返回一个json串的描述格式 进入正在运行的容器并以命令行交互回到以ctrl+p+q的方式退出的容器中 第一种方式： 使用命令：docker exec -it 容器ID bashShell （后面必须携带bash指令） 登录操作 效果等同 例子2： 打印后台启动的centos容器的根目录的消息，我们发现，他并没有进入后台的容器，只是把容器执行的指令的结果输出到宿主。所以他的功能是比docker attach指令还要强大的 第二种方式： ​ docker attach 容器ID 两种方式的区别： attach 直接进入容器启动命令的终端，不会启动新的进程 exec 是在容器中打开新的终端，并且可以启动新的进程 从容器内拷贝文件到主机上宿主机上执行 docker cp 容器ID:容器内路径 目的主机路径 Docker镜像commit\\1. commit提交容器副本使之成为一个新的镜像 docker commit -m=“提交的描述信息” -a=“作者” 容器ID 要创建的目标镜像名:[标签名] 案例演示1.删除运行的tomcat 的文档模块，然后提交为新的镜像 这个时候再来访问tomcat容器的docs文档模块，肯定是404. \\2. 也即当前的tomcat运行实例是一个没有文档内容的容器，以它为模板commit一个没有doc的tomcat新镜像kingge/tomcatnodoc 注意这个标红框的镜像id必须是某一个正在运行的容器id 3.启动重新上传的tomcat 查看是否存在doc目录 不存在，说明这个版本就是我们亲自提交的删除文档的tomcat版本。 这个时候可以同时启动原先的tomcat版本，查看区别。 命令图例 七、Docker容器数据卷生产环境中使用Docker的过程中，往往需要对数据进行持久化，或者需要在多个容器之间进行数据共享，这必然涉及容器的数据管理操作。 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性. 卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷.( 数据卷是一个可供容器使用的特殊目录，它将主机操作系统目录直接映射进容器，类似于Linux中的mount操作) ​ 特点： 1：数据卷可在容器之间共享或重用数据 2：卷中的更改可以直接生效 3：数据卷中的更改不会包含在镜像的更新中 4：数据卷的生命周期一直持续到没有容器使用它为止 容器中管理数据主要有两种方式： \\1. 数据卷（Data Volumes）：容器内数据直接映射到本地主机环境，如何在容器内创建数据卷，并且把本地的目录或文件挂载到容器内的数据卷中。 \\2. 数据卷容器（Data Volume Containers）：使用特定容器维护数据卷。如何使用数据卷容器在容器和主机、容器和容器之间共享数据，并实现数据的备份和恢复。 7.1 创建数据卷7.1.1 第一种方式：使用命令直接添加命令： docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 ​ 就是把宿主机的某个目录关联到容器中，两者数据共通（文件夹不存在时，自动创建） 1.使用命令创建数据卷 宿主机也创建了myHostVal目录 2.校验是否数据共通 宿主机创建一个文本 查看容器是否存在hello.txt 容器存在。 反之，容器创建一个文件，宿主机也会出现同样的文件。 尖叫提示： Docker挂载数据卷的默认权限是读写（rw），用户也可以通过ro指定为只读 \\3. 极端测试 容器停止退出后（exit），宿主机创建或者修改文件，再重启容器（start），查看宿主机创建或者修改的文件是否有相应的变化。 经测试，答案是会有相应的变化。 4.查看容器的内部细节 docker inspect 容器ID 返回的json串中可以找到这样的一行描述 5.容器挂载文件夹的读写方式 docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 经过操作我们发现，宿主机创建或者修改的文件都能够同步到容器中，但是在容器中只能够查看对应的宿主机同步过来的文件，容器中不能够新增删除修改文件，只能够查看。 再次使用inspect 命令查看挂载的状态 发现可读写方式变为false，只读。 7.1.2 第二种方式：DockerFile方式添加DockerFile就是对于一个镜像的描述文件，类似于java代码编译后形成的.class文件，他是关于一个java类的描述。 1初探dockerfile结构打开docker hub，随便查看一个tomcat版本的dockerfile 打开后得到下面的代码，下面就是 这个dockerfile文件很好的阐述了tomcat镜像文件为什么这么大，而且为什么我们能够访问8080端口。 查看centos的dockerfile镜像描述文件 2.创建数据卷可在Dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷 VOLUME[“/dataVolumeContainer”,”/dataVolumeContainer2”,”/dataVolumeContainer3”] 说明： 出于可移植和分享的考虑，用-v 主机目录:容器目录这种方法不能够直接在Dockerfile中实现。 由于宿主机目录是依赖于特定宿主机的，并不能够保证在所有的宿主机上都存在这样的特定目录。（意思就是说容器中数据卷在linux01宿主机上关联的目录是myvolume1，但是如果该镜像在linux02宿主机上运行，那么容器容器启动后，可能找不到关联的myvolume1，因为你不能够保证linux02宿主机的相关目录结构是跟linux01一样的，所以使用dockerfile的方式创建数据卷的时候，单方面的指定容器中数据卷目录位置，容器启动后，会帮我们自动创建相关联的宿主机的目录） 也就是说，VOLUME命令只能够单方面的在容器中创建数据卷，不能够指明对应宿主机关联的目录（但是他会自动在宿主机创建相关联的目录） \\1. 宿主机创建dockerfile文件，依赖已经存在的centos镜像。 也就是说我们以现有的centos为某一层创建一个新的镜像（符合UnionFS） 在宿主机创建一个文件夹，存放创建的dockerfile文件 ​ 新建的dockerfile文件dc。内容 \\2. 根据dockerfile文件dc，构建新镜像 Docker images 查看现存在的镜像 启动我们创建的centos镜像 确实主动给我们创建了两个数据卷，那么他们关联的宿主机的目录是什么呢？ 使用docekr inspect命令查看 7.2数据卷容器使用特定容器维护数据卷。7.1中使用的数据卷的方式是宿主机直接映射到容器进行数据传输，但是如果我们想两个容器之间共享传递数据怎么办呢？ 就需要创建数据卷容器 7.2.1先启动一个父容器dc01以上一步新建的镜像kingge/centos为模板并运行容器dc01 在创建容器卷dataVolumeContainer2新增内容，touch hello1.txt 7.2.2 创建dc02、03继承自dc01使用–volumes-from关键命令 在dc02、03的dataVolumeContainer2目录下查看是否存在dc01创建的hello1txt，很明显是可以看到的。 7.2.3测试数据共通性1.dc02/dc03分别在dataVolumeContainer2各自新增内容，touch hello2.txt和touch hello3.txt 分别查看dc01 dc02 dc03的dataVolumeContainer2目录下是否存在hello1.txt hello2.txt hello3.txt 这三个文件，答案是：都存在这三个文件 \\2. 删除dc01，dc02修改后dc03可否访问 答案很明显是存在的，也就是说删除dc01并不会影响dc02和dc03的数据互通 结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止 八、DockerFile解析8.1 dockerfile概念Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。通过：编写Dockerfile文件-&gt; docker build -&gt; docker run，生成一个镜像文件 查看centos的dockerfile镜像描述文件 1：每条保留字指令都必须为大写字母且后面要跟随至少一个参数 2：指令按照从上到下，顺序执行 3：#表示注释 4：每条指令都会创建一个新的镜像层，并对镜像进行提交 （1）docker从基础镜像运行一个容器（from scratch） （2）执行一条指令并对容器作出修改 （3）执行类似docker commit的操作提交一个新的镜像层 （4）docker再基于刚提交的镜像运行一个新容器 （5）执行dockerfile中的下一条指令直到所有指令都执行完成 从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段， * Dockerfile是软件的原材料 * Docker镜像是软件的交付品 * Docker容器则可以认为是软件的运行态。 Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。 1 Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等; 2 Docker镜像，在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行 Docker镜像时，会真正开始提供服务; 3 Docker容器，容器是直接提供服务的。 尖叫提示：Docker Hub 中 99% 的镜像都是通过在 base 镜像中安装和配置需要的软件构建出来的 8.2 dockerfile指令（保留字指令）FROM​ 基础镜像，当前新镜像是基于哪个镜像的 MAINTAINER​ 镜像维护者的姓名和邮箱地址 RUN​ 容器构建时需要运行的命令 EXPOSE​ 当前容器对外暴露出的端口 WORKDIR指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点 ENV​ 用来在构建镜像过程中设置环境变量 ADD​ 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包 COPY类似ADD，拷贝文件和目录到镜像中。 将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置，没有解压功能 COPY src dest COPY [“src”, “dest”] VOLUME​ 容器数据卷，用于数据保存和持久化工作 CMD​ 指定一个容器启动时要运行的命令 Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换（跟ENTRYPOINT指令的区别） 举个例子，查看tomcat 的dockerfile文件，我们可以发现最后是通过cmd命令启动了tomcat。那么为了证明CMD会不会被docker run后面的参数替换，请看下面例子。 ENTRYPOINT​ 指定一个容器启动时要运行的命令 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数，但是他是以追加的形式而不是覆盖 docker run 之后的参数会被当做参数传递给 ENTRYPOINT，之后形成新的命令组合 ONBUILD当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发 小总结 8.3 案例案例一需求：修改默认的centos，修改他的落脚点（默认运行centos后容器进入的根目录）和添加vim指令、ifconfig（默认centos镜像没有安装这两个组件） \\1. 创建dockefile镜像描述文件 内容是 FROM centos MAINTAINER kingge&#x33;&#57;&#51;&#50;&#x31;&#53;&#54;&#x36;&#49;&#64;&#x71;&#113;&#x2e;&#x63;&#x6f;&#109; ENV MYPATH /usr/local WORKDIR $MYPATH RUN yum -y install vim RUN yum -y install net-tools EXPOSE 80 CMD echo $MYPATH CMD echo “success————–ok” CMD /bin/bash \\2. 根据创建的Dockerfile构建镜像 docker build -t 新镜像名字:TAG . （注意这里还有一个点，表示当前文件夹） 默认去找当前目录下名字为Dockerfile的文件构建镜像。 也可以用这个命令指定dockerfile： docker build -f /mydockerfile/Dockerfile -t kingge/mycentos:1.1 . 很明显跟原先从docker hub上拉取下来的centos多个两百多M，因为我们安装了vim和net-tools指令。 \\3. 列出镜像的变更历史 ​ docker history 镜像名（imagesid） 可以看到构建这个镜像的每一层相关的操作。 4.运行构建的镜像 可以看到容器登陆后落脚点变更为了我们设定的/usr/local，同时也支持了vim 和ifconfig命令。 案例2通过自定义一个tomcat的方式我们来使用一些dockerfile常用的指令 2.1 新建一个工作目录 存放待传输到容器中的压缩包（测试ADD命令专用）和一个文本文件（测试COPY指令专用） 2.2 根据原版centos新建Dockerfile文件 内容是： FROM centos MAINTAINER kingge&#51;&#57;&#51;&#x32;&#49;&#53;&#x36;&#x36;&#x31;&#x40;&#113;&#113;&#x2e;&#99;&#111;&#109; #把宿主机当前上下文的hello.txt拷贝到容器/usr/local/路径下 #并重命名为helloNewName.txt COPY hello.txt /usr/local/helloNewName.txt #把java与tomcat添加到容器中 ADD jdk-8u144-linux-x64.tar.gz /usr/local/ ADD apache-tomcat-9.0.21.tar.gz /usr/local/ #安装vim编辑器 RUN yum -y install vim #设置工作访问时候的WORKDIR路径，登录落脚点 ENV MYPATH /usr/local WORKDIR $MYPATH #配置java与tomcat环境变量 ENV JAVA_HOME /usr/local/jdk1.8.0_144 ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar ENV CATALINA_HOME /usr/local/apache-tomcat-9.0.21 ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.21 ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin #容器运行时监听的端口 EXPOSE 8080 #启动时运行tomcat # ENTRYPOINT [“/usr/local/apache-tomcat-9.0.21/bin/startup.sh” ] # CMD [“/usr/local/apache-tomcat-9.0.21/bin/catalina.sh”,”run”] CMD /usr/local/apache-tomcat-9.0.21/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.21/bin/logs/catalina.out 2.3 构建 构建完成 2.4 执行（RUN） docker run -d -p 9080:8080 –name myt9 -v /mydockerfile/mytomcat/tomcat9/project:/usr/local/apache-tomcat-9.0.21/webapps/project -v /mydockerfile/mytomcat/tomcat9/logs/:/usr/local/apache-tomcat-9.0.21/logs –privileged=true mytomcat921 命令的意思是：后台执行tomcat镜像，docker对外暴露8080端口，外部可以通过9080端口访问docker容器的8080端口。 –name：启动的容器重命名为myt9 -v：新建两个数据卷 –privileged=true: Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied解决办法：在挂载目录后多加一个–privileged=true参数即可 启动成功 2.5 验证1.首先验证两个tar包是否已经上传到容器并解压成功，hello.txt文件是否已经copy到容器、容器登录后落脚点是否是在我们设置的/usr/local 2.查看数据卷是否创建成功 宿主机对应数据卷创建成功 容器数据卷创建成功，project出现 3.校验数据卷 2.6 部署项目因为我们在创建数据卷的时候： /mydockerfile/mytomcat/tomcat9/project:/usr/local/apache-tomcat-9.0.21/webapps/project 宿主机的/tomcat9/project目录映射到了容器的webapps/project目录，那么可以利用数据卷的数据共通原理。在宿主机的project目录上传一个项目，然后重启容器，那么就可以实现项目的发布。 上传解压后的项目文件 很明显会自动同步到容器的webapps/project目录下 重启容器 访问项目 总结 九、常用插件安装https://www.runoob.com/docker/docker-install-mysql.html 9.1总体步骤搜索镜像、拉取镜像、查看镜像、启动镜像、停止容器、移除容器 9.2 tomcat安装9.2.1 docker hub上面查找tomcat镜像 docker search tomcat 或者直接使用浏览器登录docker hub查找也可以 9.2.2 从docker hub上拉取tomcat镜像到本地 docker pull tomcat 9.2.3 docker images查看是否有拉取到的tomcat 为什么拉取下来的tomcat有五百多M?上面文章已经做了解释（因为里面包含了jdk等等，这个也就是为什么我们可以直接运行tomcat镜像而不用配置jdk环境的原因） 9.2.4 使用tomcat镜像创建容器(也叫运行镜像) docker run -it -p 8080:8080 tomcat ​ -p 主机端口:docker容器端口 ​ -P 随机分配端口 ​ i:交互 ​ t:终端 9.3安装mysqlhttps://hub.docker.com/_/mysql 官网文档 9.3.1 docker hub上面查找mysql镜像9.3.2 从docker hub上(阿里云加速器)拉取mysql镜像到本地标签为5.6 9.3.3 使用mysql5.6镜像创建容器(也叫运行镜像) docker run -p 12345:3306 –name mysql -v /kingge/mysql/conf:/etc/mysql/conf.d -v / kingge /mysql/logs:/logs -v / kingge /mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 命令说明： -p 12345:3306：将主机的12345端口映射到docker容器的3306端口。 –name mysql：运行服务名字 -v / kingge /mysql/conf:/etc/mysql/conf.d ：将主机/ kingge /mysql录下的conf/my.cnf 挂载到容器的 /etc/mysql/conf.d -v / kingge /mysql/logs:/logs：将主机/ kingge /mysql目录下的 logs 目录挂载到容器的 /logs。 -v / kingge /mysql/data:/var/lib/mysql ：将主机/ kingge /mysql目录下的data目录挂载到容器的 /var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456：初始化 root 用户的密码。 -d mysql:5.6 : 后台程序运行mysql5.6 登录测试 docker exec -it 954efcffa04d /bin/bash 成功 外部软件连接成功 9.4 安装redis9.4.1 从docker hub上(阿里云加速器)拉取redis镜像到本地标签为3.2 9.4.2 使用redis3.2镜像创建容器(也叫运行镜像) docker run -p 6379:6379 -v /kingge/myredis/data:/data -v /kingge/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis:3.2 redis-server /usr/local/etc/redis/redis.conf –appendonly yes 这个时候可以直接连接redis了： 命令：docker exec -it 运行着Rediis服务的容器ID redis-cli 设置redis配置文件： 在主机/kingge/myredis/conf/redis.conf目录下新建redis.conf文件 vim / kingge /myredis/conf/redis.conf/redis.conf Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 。。。。省略测试持久化文件生成 十、本地镜像发布到阿里云 1.镜像生成方式（1）使用DockerFile的方式创建镜像 （2）根据运行的容器创建一个新的镜像 docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] （参见6.3章节的补充模块的Docker镜像commit） 2. 将本地镜像推送到阿里云2.1 登录阿里云，创建镜像仓库https://cr.console.aliyun.com/cn-hangzhou/instances/repositories 2.2 创建命名空间 2.3 点击镜像仓库的管理 可以获取推送镜像到阿里云仓库的地址 $ sudo docker login –username=393215661@qq.com registry.cn-hangzhou.aliyuncs.com $ sudo docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/kingge/myrepo:[镜像版本号] $ sudo docker push registry.cn-hangzhou.aliyuncs.com/kingge/myrepo:[镜像版本号] 2.4 推送镜像到阿里云首先进行登录 标记我们需要上传的镜像 开始推送 推送成功 2.5 查看是否推送成功 2.6 从阿里云下载我们推送的镜像 十一、新建本地仓库 本质就是通过一个名字为registry的镜像，构建仓库 （1）拉取私有仓库镜像 ​ docker pull registry （2）启动私有仓库容器 ​ docker run -di –name=registry -p 5000:5000 registry （3）打开浏览器 输入地址http://49.234.188.74:5000/v2/_catalog看到{&quot;repositories&quot;:[]} 表示私有仓库搭建成功并且内容为空 或者使用crul 命令查看也可以 这里有个hello-world镜像，是本人之前上传的。如果没有上传过，那么这个应该返回的是{“repositories”:[]} （4）修改daemon.json ​ vi /etc/docker/daemon.json 添加以下内容，保存退出。 {“insecure-registries”:[“49.234.188.74:5000”]} 例如 此步用于让 docker信任私有仓库地址 （5）重启docker 服务 ​ systemctl restart docker 上传镜像到本地仓库（1）标记此镜像为私有仓库的镜像 ​ docker tag hello-world 49.234.188.74:5000/ hello-world （本质就是创建一个关于hello-world的引用，镜像名字更改为hello-world 49.234.188.74:5000/hello-world ） （2）再次启动私服容器 ​ docker start registry （3）上传标记的镜像 ​ docker push 49.234.188.74:5000/ hello-world (4)查看是否上传成功 其他服务器获取上传的容器需求：192.168.1.105 服务器需要从 49.234.188.74 服务器创建的本地仓库获取上床的hello-world镜像 \\1. 192.168.1.105设置可信任仓库站点 ​ vi /etc/docker/daemon.json 添加以下内容，保存退出。 {“insecure-registries”:[“49.234.188.74:5000”]} 如果不设置这一步，那么在从49.234.188.74服务器pull镜像的时候会报以下错误 默认不支持http请求的方式获取镜像 \\2. 拉取镜像成功 十二、使用DockerMaven插件构建项目微服务部署有两种方法： （1）手动部署：首先基于源码打包生成jar包（或war包）,将jar包（或war包）上传至虚 拟机并拷贝至JDK容器。 （2）通过Maven插件自动部署。 对于数量众多的微服务，手动部署无疑是非常麻烦的做法，并且容易出错。 （1）修改宿主机的docker配置，让其可以远程访问Vi /lib/systemd/system/docker.service 其中ExecStart=后添加配置 ‐H tcp://0.0.0.0:2375 ‐H unix:///var/run/docker.sock （2）刷新配置，重启服务systemctl daemon‐reload systemctl restart docker docker start registry （这里使用的是本地仓库） （3） springboot的pom文件添加插件 最后执行：mvn clean package docker:build 即可把镜像上传到本地仓库中 上面的方式是构建 项目到本地仓库的方式。如果我们自己申请了阿里云仓库，那么可以使用下面的方式将项目推送到阿里云仓库中。 使用SpringBoot2.0+DockerFile+Maven插件构建镜像并推送到阿里云仓库 https://blog.csdn.net/haogexiang9700/article/details/88318867 问题总结1 启动mysql后使用外部数据库连接工具访问时，报错错误提示 2059 - authentication plugin ‘caching_sha2_password’” 通过查看本人启动mysql容器，mysql的版本是： 经过查询得知：出现这个问题的原因是mysql8 之前的版本中加密规则是mysql_native_password,而在mysql8之后,加密规则是caching_sha2_password, 解决问题方法是把mysql用户登录密码加密规则还原成mysql_native_password 也就是数据库访问工具还是使用mysql_native_password这样的价码规则访问数据库。 关键的位置是在：mysql数据库中的user表 解决方法： 通过命令行的方式登陆数据库 mysql -uroot -p密码 然后分别执行以下代码 use mysql; ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘123456’; ALTER USER ‘root’@’%’ IDENTIFIED WITH mysql_native_password BY ‘123456’; FLUSH PRIVILEGES; 修改完毕 修改host为localhost和%(任意客户端)的密码认证方式 官方文档对应mysql8的更新说明 https://dev.mysql.com/doc/refman/8.0/en/upgrading-from-previous-series.html 2.docker数据卷权限问题参见《持续集成和容器管理》-《额外补充》章节，启动jenkins dokcer容器时，添加数据卷权限问题。 https://www.cnblogs.com/jackluo/p/5783116.html","categories":[{"name":"docker","slug":"docker","permalink":"http://kingge.top/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://kingge.top/tags/docker/"},{"name":"容器","slug":"容器","permalink":"http://kingge.top/tags/容器/"}]},{"title":"springboot个人总结","slug":"springboot个人总结","date":"2019-01-30T14:21:59.000Z","updated":"2019-08-25T02:55:36.431Z","comments":true,"path":"2019/01/30/springboot个人总结/","link":"","permalink":"http://kingge.top/2019/01/30/springboot个人总结/","excerpt":"","text":"一．前言1.1 什么是微服务？单服务场景 开发简单，测试简单，部署简单 下面举个例子，一种单一的服务场景：所有的模块都是打包成一个war包的形式，然后部署到tomcat中。 缺点： 1) 只能采用同一种技术，很难用不同的语言或者语言不同版本开发不同模块； 2) 系统耦合性强，一旦其中一个模块有问题，整个系统就瘫痪了；一旦升级其中一个模块，整个系统就停机了； 3) 集群只能是复制整个系统，即使只是其中一个模块压力大。（可能整个订单处理，仅仅是支付模块压力过大， 按道理只需要升级支付模块，但是在单一场景里面是不能的） 1.2 微服务概念 下面是微服务（Micro-Service）架构，不同模块放到不同的进程**/**服务器上，模块之间通过网络通讯进行协作。 各个模块都部署在不同的服务器上面，模块之间可以通过http请求进行协作。例如web服务器收到请求，那么可以根据请求的业务吧请求分配到响应的处理服务器，例如短信消息服务器。 优点： 1) 可以用不同的语言或者语言不同版本开发不同模块； 2) 系统耦合性弱，其中一个模块有问题，可以通过“降级熔断”等手段来保证不停机； 3) 可以对不同模块用不同的集群策略，哪里慢集群哪里。 缺点： 1) 开发难度大，系统结构更复杂； 2) 运行效率低； （模块之间相互请求时间长等等） 详细参照微服务文档 https://martinfowler.com/articles/microservices.html#MicroservicesAndSoa 所以就衍生出了 springboot和springcloud两个框架，当然springboot并不能代表微服务的概念，springcloud才是。 因为springboot只是简化了单一web服务开发的流程，摒弃了大量的xml配置，提供了大量的自动化配置。 那么springboot跟springcloud是什么关系呢？ SpringBoot专注于快速方便的开发单个个体微服务。SpringCloud是关注全局的微服务协调整理治理框架，它将SpringBoot开发的一个个单体微服务整合并管理起来，为各个微服务之间提供，配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务。SpringBoot可以离开SpringCloud独立使用开发项目，但是SpringCloud离不开SpringBoot，属于依赖的关系. SpringBoot专注于快速、方便的开发单个微服务个体，SpringCloud关注全局的服务治理框架。 1.3 spring的演化Spring1.x 时代在Spring1.x时代，都是通过xml文件配置bean，随着项目的不断扩大，需要将xml配置分放到不同的配置文件中，需要频繁的在java类和xml配置文件中切换。 Spring2.x时代随着JDK 1.5带来的注解支持，Spring2.x可以使用注解对Bean进行申明和注入，大大的减少了xml配置文件，同时也大大简化了项目的开发。 那么，问题来了，究竟是应该使用xml还是注解呢？ 最佳实践： 1、 应用的基本配置用xml，比如：数据源、资源文件等； 2、 业务开发用注解，比如：Service中注入bean等； Spring3.x到Spring4.x从Spring3.x开始提供了Java配置方式，使用Java配置方式可以更好的理解你配置的Bean，现在我们就处于这个时代，并且Spring4.x和Spring boot都推荐使用java配置的方式。 1.4为什么要学习SpringBootjava一直被人诟病的一点就是臃肿、麻烦。当我们还在辛苦的搭建项目时，可能Python程序员已经把功能写好了，究其原因主要是两点： · 复杂的配置 项目各种配置其实是开发时的损耗， 因为在思考 Spring 特性配置和解决业务问题之间需要进行思维切换，所以写配置挤占了写应用程序逻辑的时间。 · 混乱的依赖管理 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了，你还要知道这些库的哪个版本和其他库不会有冲突，这也是件棘手的问题。并且，依赖管理也是一种损耗，添加依赖不是写应用程序代码。一旦选错了依赖的版本，随之而来的不兼容问题毫无疑问会是生产力杀手。 而SpringBoot让这一切成为过去！ 二． 什么是springboot 三．Springboot使用这里开发工具使用的是sts和idea，他们两者开发springboot 的方式没有什么区别，所以下面两种方式我都会嵌套使用。下面的工程采用springboot版本是：1.5.9。 3.1 使用maven方式手动搭建sb项目Idea maven环境配置 编码设置： 需求：浏览器发送hello请求，服务器接受请求并处理，响应Hello World字符串； 1、创建一个maven工程2、导入spring boot相关的依赖&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; ​ org.springframework.boot spring-boot-starter-parent 1.5.9.RELEASE org.springframework.boot spring-boot-starter-web 3、 编写一个主程序；启动Spring Boot应用 /* @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */ @SpringBootApplication public class HelloWorldMainApplication { public static void main(String[] args) { // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); } } /** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */@SpringBootApplicationpublic class HelloWorldMainApplication &#123; public static void main(String[] args) &#123; // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); &#125;&#125; 4、编写相关的Controller、Service@Controllerpublic class HelloController &#123; @ResponseBody @RequestMapping(&quot;/hello&quot;) public String hello()&#123; return &quot;Hello World!&quot;; &#125;&#125; @Controller public class HelloController { @ResponseBody @RequestMapping(“/hello”) public String hello(){ return “Hello World!”; } } 5、运行主程序测试直接运行HelloWorldMainApplication的main方法，启动sb程序。 在浏览器输入 127.0.0.1:8080/hello 可看到输出 Hello World! 6、打包成嵌入式web的可执行jar包&lt;!-- 这个插件，可以将应用打包成一个可执行的jar包；--&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 运行maven的 mvn package 打包命令，可以看到生成的jar包。 直接使用java -jar的命令进行执行，效果同 5 。 3.2 使用Spring Initializer快速创建Spring Boot项目-推荐3.2.1 sts编译器方式STS工具下创建CRUD的步骤（IDEA相同的步骤） 1.主菜单：File→New→Spring Starter Project。在Type中选Maven，Package选War ​ 下一步中搜索勾选Web。 ​ 点击【Finish】会创建项目，第一次创建完成后会进行maven包的下载等，需要几分钟。项目创建成功后，他会生成一个springboot的启动类。后面就是靠他来启动springboot 2.新建一个Controller 3.新建一个index.html 需要注意的是，界面要放置在 resouces – templates 目录下（不是放在我们以前的webroot下面了） 4.启动程序，访问。 可以看到，他非常的简单。不需要配置一大堆文件 3.2.2 IDEA编译器方式方式同上 IDE都支持使用Spring的项目创建向导快速创建一个Spring Boot项目； 选择我们需要的模块；向导会联网创建Spring Boot项目； 默认生成的Spring Boot项目； - 主程序已经生成好了，我们只需要我们自己的逻辑 - resources文件夹中目录结构 - static：保存所有的静态资源； js css images； - templates：保存所有的模板页面；（Spring Boot默认jar包使用嵌入式的Tomcat，默认不支持JSP页面）；可以使用模板引擎（freemarker、thymeleaf）； - application.properties：Spring Boot应用的配置文件；可以修改一些默认设置；例如修改服务器端口号 四. Springboot核心下面分析根据helloworld 源码。 4.1、POM文件&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; ​ org.springframework.boot spring-boot-starter-parent 2.1.5.RELEASE 单击进去查看，发现他的父项目是： ​ org.springframework.boot spring-boot-dependencies 2.1.5.RELEASE ../../spring-boot-dependencies &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;&lt;/parent&gt; 再单击进去发现 它里面配置了很多properties,定义了各种依赖的版本。也就是说他是用来真正管理Spring Boot应用里面的所有依赖版本，Spring Boot的版本仲裁中心；以后我们导入依赖默认是不需要写版本；（没有在dependencies里面管理的依赖自然需要声明版本号-） 4.2、web启动器&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; ​ org.springframework.boot spring-boot-starter-web spring-boot-starter-web： ​ spring-boot-starter：spring-boot场景启动器；帮我们导入了web模块正常运行所依赖的组件； Spring Boot将所有的功能场景都抽取出来，做成一个个的starters（启动器），只需要在项目里面引入这些starter相关场景的所有依赖都会导入进来。要用什么功能就导入什么场景的启动器 4.2、主程序类，主入口类 /* @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */ @SpringBootApplication public class HelloWorldMainApplication { public static void main(String[] args) { // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); } } /** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */@SpringBootApplicationpublic class HelloWorldMainApplication &#123; public static void main(String[] args) &#123; // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); &#125;&#125; 1 注解分析-@SpringBootApplication@SpringBootApplication: Spring Boot应用标注在某个类上说明这个类是SpringBoot的主配置类，SpringBoot就应该运行这个类的main方法来启动SpringBoot应用； 1．@SpringBootConfiguration @SpringBootConfiguration:Spring Boot的配置类，sb自定义的一个注解，标注在某个类上，表示这是一个Spring Boot的配置类。 点进去发现实际上他是使用spring的@Configuration来注解的，而@Configuration的上一层是@Component。 在Spring Boot项目中推荐使用@ SpringBootConfiguration替代@Configuration ​ 总的来说：这个注解的作用是把当前启动类加入到spring容器中，同时当做一个配置类来使用。 2. @EnableAutoConfiguration @EnableAutoConfiguration：开启自动配置功能，以前我们需要配置的东西，Spring Boot帮我们自动配置；例如：我们添加了spring-boot-starter-web的依赖，项目中也就会引入SpringMVC的依赖，Spring Boot就会自动配置tomcat和SpringMVC，下面我们查看这个注解具体的作用。 @AutoConfigurationPackage：自动配置包 利用Spring的底层注解@Import，给IOC容器中导入组件，导入的组件由AutoConfigurationPackages.Registrar.class来控制。 额外提示： 可以参见在网站上发布的《spring注解》文章，关于@import注解导入bean的三种方式 （1）源码分析 查看AutoConfigurationPackages的内部类Registrar，他实现了ImportBeanDefinitionRegistrar接口。通过这个接口的重载方法registerBeanDefinitions（），控制导入IOC容器的bean。 static class Registrar implements ImportBeanDefinitionRegistrar,DeterminableImports &#123; @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) &#123; 1. register(registry, new PackageImport(metadata).getPackageName()); &#125; @Override public Set&lt;Object&gt; determineImports(AnnotationMetadata metadata) &#123; return Collections.singleton(new PackageImport(metadata)); &#125;&#125; ​ static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { 1. register(registry, new PackageImport(metadata).getPackageName()); } @Override public Set determineImports(AnnotationMetadata metadata) { return Collections.singleton(new PackageImport(metadata)); } } 会去调用registerBeanDefinitions方法注册bean，紧接着registerBeanDefinitions方法调用register方法。 该方法会首先判断，IOC容器中是否存在 org.springframework.boot.autoconfigure.AutoConfigurationPackages 类，不存在则把该类注册到容器中，不存在则创建 在1处打个断点 发现获取的metadata元数据信息是HelloWorldMainApplication的。计算new PackageImport(metadata).getPackageName()的值得出的是HelloWorldMainApplication所在包的包名。Registry是IOC容器实例 总结： @AutoConfigurationPackage注解的作用是将主配置类（@SpringBootApplication标注的类）的所在包及下面所有子包里面的所有组件扫描到Spring容器，也就是说，如果controller，service等等类不在这个包下的话，是不会注册到spring容器中的，所以需要注意。 尖叫提示： 我们知道spring启动的时候会默认扫描启动类所在的包下面的所有类，注入springioc容器中，但是如果我们需要注入ioc容器的类不在启动类包下，那么我们可以通过这个@ImportResource(locations = {“classpath:beans.xml”})注解进行注入额外的类（注解加在 启动类上） 注解进行注入额外的类（注解加在 启动类上） @Import(AutoConfigurationImportSelector.class)（1）整体结构分析 还是利用了@import注解导入了AutoConfigurationImportSelector.class 类， 查看他的继承结构。 ​ 题外话： 可以看到很多组件都实现了Aware接口，关于实现Aware的作用是，我们可以在自定义组件中使用Spring底层的一些组件，例如创建一个bean时候，我们想查看IOC容器的信息或者查看当前系统的信息，那么这个时候就需要用到Aware接口，注入这些底层组件，供自定义组件调用。 可以参见在网站上发布的《spring注解》文章，关于Aware接口 关键代码 selectImports方法，返回的是需要导入到IOC容器的bean的全类名。类似 (2)详细分析 查看selectImport()，可以看到最终返回的是自动配置实体类autoConfigurationEntry. getConfigurations()方法的返回值，就是一个List 那么很明显获取自动配置实体类的代码是接下来我们所要关注的重点。 2.进入AutoConfigurationImportSelector.getAutoConfigurationEntry()方法 同理可得，关键代码是：List configurations 3.接着看getCandidateConfigurations方法里面的关键代码段。 关键代码：loadFactoryNames()方法，他的第一个参数通过debug我们可以得知，也就是getSpringFactoriesLoaderFactoryClass()方法的值是： EnableAutoConfiguration.class –&gt; org.springframework.boot.autoconfigure.EnableAutoConfiguration SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class,classLoader)**；** 深入查看loadFactoryNames方法可知。 他是通过 最终loadFactoryNames()方法的返回值是： （3）总结： @Import(AutoConfigurationImportSelector.class)注解的作用是，通过以key为 org.springframework.boot.autoconfigure.EnableAutoConfiguration，然后在META-INF/spring.factories文件中，获取key对应的value值，然后打包成一个值是全类名的List，最终返回到selectImport()方法，然后调用IOC容器把这些List里面bean，注册到IOC容器中，完成自动配置。 META-INF/spring.factories文件的位置。 J2EE**的整体整合解决方案和自动配置都在**spring-boot-autoconfigure-2.0.5.RELEASE.jar 关于自动配置：下面的5.12章节也会阐述到。 3. @ComponentScan 4.3 关闭自动配置通过上述，我们得知，Spring Boot会根据项目中的jar包依赖，自动做出配置，Spring Boot支持的自动配置如下（非常多）： 如果我们不需要Spring Boot自动配置，想关闭某一项的自动配置，该如何设置呢？ 比如：我们不想自动配置Redis，想手动配置。 当然了，其他的配置就类似了。 4.4 自定义Banner-了解即可-没什么用启动Spring Boot项目后会看到这样的图案： 这个图片其实是可以自定义的： \\1. 打开网站： http://patorjk.com/software/taag/#p=display&amp;h=3&amp;v=3&amp;f=4Max&amp;t=itcast%20Spring%20Boot 拷贝生成的字符到一个文本文件中，并且将该文件命名为banner.txt \\2. 将banner.txt拷贝到项目的resources目录中： \\3. 重新启动程序，查看效果： 如果不想看到任何的banner，也是可以将其关闭的： 4.5全局配置文件Spring Boot项目使用一个全局的配置文件application.properties或者是application.yml，在resources目录下或者类路径下的/config下，一般我们放到resources下。 1、 修改tomcat的端口为8088 重新启动应用，查看效果： 2、 修改进入DispatcherServlet的规则为：*.html 测试： 五. 配置文件SpringBoot使用一个全局的配置文件，配置文件名是固定的（名字不能更改否则无效） •application.properties （默认生成） •application.yml 而且通过观察发现 application.properties 的优先级别比 application.yml 高，同样的属性配置，**properties会覆盖yml**的配置 配置文件的作用：修改SpringBoot自动配置的默认值，SpringBoot在底层都给我们自动配置好。 5.1 YAML他是一种标记语言 5.2YAML语法 5.3配置文件值注入 5.4 @Value获取值和@ConfigurationProperties获取值比较 5.5、配置文件注入值数据校验 5.6、@PropertySource&amp;@ImportResource&amp;@Bean1. @PropertySource：加载指定的配置文件； 应用场景，我们知道application.properties是项目的整体配置文件，但是如果存在大量跟项目关系不是那么密切的配置信息，我们是可以配置到其他的配置的文件中去，避免造成application.peoperties文件的大小过大。这个时候就需要PropertySource注解进行指定。 2. @ImportResource**：** 导入Spring的配置文件，让配置文件里面的内容生效，Spring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别，想让Spring的配置文件生效，加载进来，@ImportResource标注在一个配置类上。 3.@bean Bean注解一般是配合着@Configure注解使用 5.7比较application.properties和application.yml和自定义properti的优先级别同样的属性配置 ： application.properties &gt; application.yml &gt; 自定义properties 前者会覆盖后者 5.8、配置文件占位符 5.9、Profile使用场景，针对于配置文件。例如我们在开发过程中使用的是开发环境的properties，那么生产使用的是生产的properties。那么我们怎么指定呢？ spring提供了profile功能 5.10、配置文件加载位置(Important)springboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件 –file:./config/ （在项目的的根目录下新建config） –file:./ （在项目的的根目录下） –classpath:/config/ （项目的resource目录下新建config） –classpath:/ （默认生成的application.properties是在类路径下面的） 优先级由高到底，高优先级的配置会覆盖低优先级的配置； SpringBoot会从这四个位置全部加载主配置文件；互补配置； 5.11、外部配置加载顺序 参考官方文档 5.11 bookstrap.yml其实还有一个系统级别的配置文件，这个是springcloud的在使用configserver组件的时候使用的，详情可查看后续的springcloud文章-springcloud的配置中心。 5.12、自动配置原理配置文件到底能写什么？怎么写？自动配置原理； 配置文件能配置的属性参照 1、自动配置原理1）、SpringBoot启动的时候加载主配置类，开启了自动配置功能通过这个注解@EnableAutoConfiguration 2）、@EnableAutoConfiguration作用： · 利用EnableAutoConfigurationImportSelector给容器中导入一些组件？ · 可以查看selectImports()方法的内容； · List configurations = getCandidateConfigurations(annotationMetadata, attributes);获取候选的配置 SpringFactoriesLoader.loadFactoryNames()扫描所有jar包类路径下 META-INF/spring.factories把扫描到的这些文件的内容包装成properties对象从properties中获取到EnableAutoConfiguration.class类（类名）对应的值，然后把他们添加在容器中 将 类路径下 META-INF/spring.factories 里面配置的所有**EnableAutoConfiguration**的值加入到了容器中 # Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.context.PropertyPlaceholderAu。。。。。。。。。。。。。。。。等等 每一个这样的 xxxAutoConfiguration类都是容器中的一个组件，都加入到容器中；用他们来做自动配置； 3）、每一个自动配置类进行自动配置功能； 4）、以HttpEncodingAutoConfiguration（Http编码自动配置）为例解释自动配置原理； @Configuration //表示这是一个配置类，以前编写的配置文件一样，也可以给容器中添加组件@EnableConfigurationProperties(HttpEncodingProperties.class) //启动指定类的ConfigurationProperties功能；将配置文件中对应的值和HttpEncodingProperties绑定起来；并把HttpEncodingProperties加入到ioc容器中@ConditionalOnWebApplication //Spring底层@Conditional注解（Spring注解版），根据不同的条件，如果满足指定的条件，整个配置类里面的配置就会生效； 判断当前应用是否是web应用，如果是，当前配置类生效@ConditionalOnClass(CharacterEncodingFilter.class) //判断当前项目有没有这个类CharacterEncodingFilter；SpringMVC中进行乱码解决的过滤器；@ConditionalOnProperty(prefix = &quot;spring.http.encoding&quot;, value = &quot;enabled&quot;, matchIfMissing = true) //判断配置文件中是否存在某个配置 spring.http.encoding.enabled；如果不存在，判断也是成立的//即使我们配置文件中不配置pring.http.encoding.enabled=true，也是默认生效的；public class HttpEncodingAutoConfiguration &#123; //他已经和SpringBoot的配置文件映射了 private final HttpEncodingProperties properties; //只有一个有参构造器的情况下，参数的值就会从容器中拿 public HttpEncodingAutoConfiguration(HttpEncodingProperties properties) &#123; this.properties = properties; &#125; @Bean //给容器中添加一个组件，这个组件的某些值需要从properties中获取 @ConditionalOnMissingBean(CharacterEncodingFilter.class) //判断容器没有这个组件？ public CharacterEncodingFilter characterEncodingFilter() &#123; CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(Type.RESPONSE)); return filter; &#125; 根据当前不同的条件判断，决定这个配置类是否生效？ 一但这个配置类生效；这个配置类就会给容器中添加各种组件；这些组件的属性是从对应的properties类中获取的，这些类里面的每一个属性又是和配置文件绑定的； 5）、所有在配置文件中能配置的属性都是在xxxxProperties类中封装者。配置文件能配置什么就可以参照某个功能对应的这个属性类 @ConfigurationProperties(prefix = &quot;spring.http.encoding&quot;) //从配置文件中获取指定的值和bean的属性进行绑定public class HttpEncodingProperties &#123; public static final Charset DEFAULT_CHARSET = Charset.forName(&quot;UTF-8&quot;); 精髓： 1**）、SpringBoot启动会加载大量的自动配置类** 2**）、我们看我们需要的功能有没有SpringBoot默认写好的自动配置类；** 3**）、我们再来看这个自动配置类中到底配置了哪些组件；（只要我们要用的组件有，我们就不需要再来配置了）** 4**）、给容器中自动配置类添加组件的时候，会从properties类中获取某些属性。我们就可以在配置文件中指定这些属性的值；** xxxxAutoConfigurartion：自动配置类； 给容器中添加组件 xxxxProperties:封装配置文件中相关属性； 2、细节1、@Conditional派生注解（Spring注解版原生的@Conditional作用）作用：必须是@Conditional指定的条件成立，才给容器中添加组件，配置配里面的所有内容才生效； @Conditional**扩展注解** 作用（判断是否满足当前指定条件） @ConditionalOnJava 系统的java版本是否符合要求 @ConditionalOnBean 容器中存在指定Bean； @ConditionalOnMissingBean 容器中不存在指定Bean； @ConditionalOnExpression 满足SpEL表达式指定 @ConditionalOnClass 系统中有指定的类 @ConditionalOnMissingClass 系统中没有指定的类 @ConditionalOnSingleCandidate 容器中只有一个指定的Bean，或者这个Bean是首选Bean @ConditionalOnProperty 系统中指定的属性是否有指定的值 @ConditionalOnResource 类路径下是否存在指定资源文件 @ConditionalOnWebApplication 当前是web环境 @ConditionalOnNotWebApplication 当前不是web环境 @ConditionalOnJndi JNDI存在指定项 自动配置类必须在一定的条件下才能生效； 我们怎么知道哪些自动配置类生效； ==**我们可以通过在application.properties中启用 debug=true属性；来让控制台打印自动配置报告==**，这样我们就可以很方便的知道哪些自动配置类生效； =========================AUTO-CONFIGURATION REPORT=========================Positive matches:（自动配置类启用的）----------------- DispatcherServletAutoConfiguration matched: - @ConditionalOnClass found required class &apos;org.springframework.web.servlet.DispatcherServlet&apos;; @ConditionalOnMissingClass did not find unwanted class (OnClassCondition) - @ConditionalOnWebApplication (required) found StandardServletEnvironment (OnWebApplicationCondition) Negative matches:（没有启动，没有匹配成功的自动配置类）----------------- ActiveMQAutoConfiguration: Did not match: - @ConditionalOnClass did not find required classes &apos;javax.jms.ConnectionFactory&apos;, &apos;org.apache.activemq.ActiveMQConnectionFactory&apos; (OnClassCondition) AopAutoConfiguration: Did not match: - @ConditionalOnClass did not find required classes &apos;org.aspectj.lang.annotation.Aspect&apos;, &apos;org.aspectj.lang.reflect.Advice&apos; (OnClassCondition) 六．日志七 springboot的web开发7.1 引言使用SpringBoot； 1**）、创建SpringBoot应用，选中我们需要的模块；** 2**）、SpringBoot已经默认将这些场景配置好了，只需要在配置文件中指定少量配置就可以运行起来** 3**）、自己编写业务代码；** 我们知道sb框架已经给我们自动配置了很多相关的配置，我们只需要着重于业务代码的编写，但是有些细则还是需要了解的，例如我们打包的sb程序成jar包的形式，那么我们网站的css js等等资源是放置在那个目录呢？换言说，静态资源的访问是个怎么流程呢？ 7.2、SpringBoot对静态资源的映射规则1.Web**开发的自动配置类** org.springframework.boot.autoconfigure.web.WebMvcAutoConfiguration 查看关键代码 7.2.1 查看第一层映射关系这段代码的意思是：所有/webjars/**的请求 ，都去classpath:/META-INF/resources/webjars/ 找资源。 例如请求jauery的请求 localhost:8080/webjars/jquery/3.3.1/jquery.js 1.**什么是webjars？** 以jar包的方式引入静态资源，可登陆这个网址了解http://www.webjars.org/ 也就是说，网站需要的jq等等资源你可以通过maven的方式导入到项目中。 在访问的时候只需要写webjars下面资源的名称即可 org.webjars jquery 3.3.1 &lt;!--引入jquery-webjar--&gt;在访问的时候只需要写webjars下面资源的名称即可 &lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt; &lt;/dependency&gt; 导入到sb项目后查看依赖 启动项目请求：localhost:8080/webjars/jquery/3.3.1/jquery.js。 输出jquery内容。 总结： 也就是说，如果我们需要引用网上相关的资源可以使用webjars的方式导入相关的组件，但是有个问题，那就是如果我们项目中已经存在某些**css和js，那么怎么引用到sb**项目中呢（也就是请求不满足第一层映射关系）？ 例如： 7.2.2 第二层映射关系 查看staticPathPattern的值：staticPathPattern = “/“。也就是说改代码块匹配访问当前项目任何资源的请求** 那么他是去哪里寻找资源回应请求呢？ 查看getStaticLocations()方法，通过查看发现： private static final String[] CLASSPATH_RESOURCE_LOCATIONS = &#123; &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &#125;; ​ private static final String[] CLASSPATH_RESOURCE_LOCATIONS = { “classpath:/META-INF/resources/“, “classpath:/resources/“, “classpath:/static/“, “classpath:/public/“ }; 也就是说，请求会去类路径的根路径下的这个几个目录寻找相应的资源响应。 类路径是指： 例如sbp项目中，java和resources都是属于类路径的根路径。静态资源，我们可以在resources下创建 resources目录存放，或者创建public目录存放，static**目录默认已经创建** 2.案例 我们把一些样式文件放置到static目录下 启动sb项目，访问静态资源。http://127.0.0.1:8080/asserts/img/4D74191A.jpg 访问成功。很明显不满足第一层映射关系，走的是第二层映射关系的逻辑 7.2.3 欢迎页配置 通过查看他也是从7.2.2章节中静态资源存放目录下寻找index.html。 localhost:8080/ 找index页面 7.2.4 网站图标所有的 **/favicon.ico 都是在静态资源文件下找；==（也是从7.2.2章节中静态资源存放目录下） 7.2.5 修改静态资源文件夹路径 7.3、模板引擎常用的模板引擎有JSP、Velocity、Freemarker、Thymeleaf。核心理念是，通过传入模板代码和需要替换的数据到模板引擎中，模板引擎自动转化成静态的界面显示 SpringBoot推荐的Thymeleaf，语法更简单，功能更强大； 1、引入thymeleaf&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 我这里默认导入的是3.011版本。 如果你们想要修改导入版本，那么也是可以的： Pom.xml文件中添加版本覆盖即可 2、Thymeleaf使用我们通过查看他的自动配置源码来了解一下他的加载结构： org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration 的ThymeleafProperties 属性 发下它默认加载类路径下的templates目录下的后缀为html的文件。 只要我们把HTML页面放在classpath:/templates/，thymeleaf就能自动渲染； 2.1 案例1（常规使用）（1） 在templates目录下新建ok.html界面 （2） 在controller中添加请求处理，最终是跳转到ok.html界面 （3）启动sb项目，并访问http://127.0.0.1:8080/ok 2.2 动态赋值1.修改ok.html界面 2.修改请求处理方法 ok() 3.启动sb项目，并访问http://127.0.0.1:8080/ok 总结： 这里建议给html界面加上命名空间，这样在使用thyeleaf语法时会有提示 4. Thymeleaf语法https://www.thymeleaf.org/ 官方地址 1）、th:text；改变当前元素里面的文本内容； th：任意html属性；来替换原生属性的值 2）、表达式？ 7.4、SpringMVC自动配置https://docs.spring.io/spring-boot/docs/2.1.5.RELEASE/reference/html/boot-features-developing-web-applications.html#boot-features-spring-mvc-auto-configuration 官方文档 1. Spring MVC auto-configuration通过查看官方文档查看springboot为springmvc自动配置了那些工作 逐句分析： Spring Boot 自动配置好了SpringMVC 以下是SpringBoot对SpringMVC的默认配置:关键类WebMvcAutoConfiguration 1 Inclusion of ContentNegotiatingViewResolver and BeanNameViewResolver beans. - 自动配置了ViewResolver（视图解析器：根据方法的返回值得到视图对象（View），视图对象决定如何渲染（转发或者重定向）） - ContentNegotiatingViewResolver：组合所有的视图解析器的； - 如何定制：我们可以自己给容器中添加一个视图解析器；自动的将其组合进来 2.Support for serving static resources, including support for WebJars (see below).静态资源文件夹路径,webjars 3.Static index.html support. 静态首页访问 4.Custom Favicon support (see below). favicon.ico 5.自动注册了 of Converter, GenericConverter, Formatter beans. - Converter：转换器； public String hello(User user)：类型转换使用Converter - Formatter 格式化器； 2017.12.17===Date； 关键代码： @Bean@ConditionalOnProperty(prefix = &quot;spring.mvc&quot;, name = &quot;date-format&quot;)//在文件中配置日期格式化的规则public Formatter&lt;Date&gt; dateFormatter() &#123; return new DateFormatter(this.mvcProperties.getDateFormat());//日期格式化组件&#125; ​ @Bean @ConditionalOnProperty(prefix = “spring.mvc”, name = “date-format”)//在文件中配置日期格式化的规则 public Formatter dateFormatter() { return new DateFormatter(this.mvcProperties.getDateFormat());//日期格式化组件 } 自己添加的格式化器转换器，我们只需要放在容器中即可 6**.Support for HttpMessageConverters (see below).** 7**. Automatic registration of MessageCodesResolver (see below).**定义错误代码生成规则 8**. Automatic use of a ConfigurableWebBindingInitializer bean (see below).** 2、扩展SpringMVC虽然springboot框架给我们自动配置了很多组件，但是在真实的应用场景中，肯定还需要自己实现一些组件，来扩展我们的springboot程序，例如我们需要定义一个拦截器和一个特定功能的视图解析器。那么就需要扩展了。 官方文档有段话，给我们阐述了如果扩展： 总的来说：编写一个配置类（@Configuration），是WebMvcConfigurer类型，不能标注@EnableWebMvc，即可实现扩展功能。 ​ 尖叫提示：有些文档还是使用WebMvcConfigurerAdapter。 需要注意的是：在springboot2.0版本以上，WebMvcConfigurerAdapter类已经过时，需要使用WebMvcConfigurer 接口或者WebMvcConfigurationSupport 过时原因：原因是springboot2.0以后，引用的是spring5.0，而spring5.0取消了WebMvcConfigurerAdapter 2.1 案例实现一个视图解析器，将请求是/no时，直接重定向到success界面(也就是说不用编写controller**方法**)。 访问请求：http://127.0.0.1:8080/no 重要总结总结：建议以后所有关于**mvc扩展的自定义的功能组件（视图解析器，国际化，拦截器等等），都放在某一个自实现的mvcConfig中（例如上面的MyViewConfig），方便管理，也可以实现多个扩展的webMvcConfig,**按照功能放置自定义的组件。 2.2 扩展mvc原理查看WebMvcAutoConfiguration**代码的适配器代码，可以知道关键代码是**@Import(EnableWebMvcConfiguration.class) 点进去 EnableWebMvcConfiguration.class**，发现他还是WebMvcAutoConfiguration的一个内部类。** 查看 DelegatingWebMvcConfiguration 类。 可以看到他是加载了容器中所有**WebMvcConfigurer类型的配置类，然后逐个调用。这里也可以证明了，我们自定义的扩展类仅仅只是扩展了mvc**的功能而已，并没有让其他自动配置功能失效。 3、全面接管SpringMVC全面接管也就意味着：SpringBoot**对SpringMVC的自动配置不需要了，所有都是我们自己配置，所有的SpringMVC的自动配置都会失效。** 我们需要在配置类中添加**@EnableWebMvc**即可 访问：发现提示界面找不到了。 加上注解，启动springboot项目，我们通过控制台输出的日志中我们发现少了一些filter。然后我们对于静态资源的默认访问路径等等都失效了。也就是说，访问html界面等等都行不通了。 这些代码都会失效，所以请求访问不到静态界面 3.1 原理1**）@EnableWebMvc的核心** 可以看到他的核心是导入一个类 DelegatingWebMvcConfiguration 2**）DelegatingWebMvcConfiguration类** 这个类就是我们上面2.2节查看注册各种适配器的关键代码，那么这里并没有什么代码控制 mvc**自动配置失效** 发现它继承 WebMvcConfigurationSupport ，这个类我们好像在那里看见过 2**）查看**WebMvcAutoConfiguration 让我们回过头查看mvc自动配置类 注意到一段代码：@ConditionalOnMissingBean(WebMvcConfigurationSupport.class) 意思是：当WebMvcConfigurationSupport类在容器中找不到时，执行自动配置类。 那么我们加上了@EnableWebMvc**注解，就相当于把WebMvcConfigurationSupport类注入到了容器中，所以WebMvcAutoConfiguration自动配置类失效。** 总结3**）、@EnableWebMvc将WebMvcConfigurationSupport组件导入进来；** 4**）、导入的WebMvcConfigurationSupport只是SpringMVC最基本的功能；** 7.5 默认访问首页通过查看WebMVCAutoConfiguration源代码可以知道，默认的”/”请求是会定向到类路径 下的index.html界面，那么我们可不可以手动控制他呢？ 很明显是可以的。 1.实现方式1实现一个controller，映射”/”请求 ​ 2.实现方式2 7.6 国际化1**）、编写国际化配置文件；** 2）、使用ResourceBundleMessageSource管理国际化资源文件 3）、在页面使用fmt:message取出国际化内容 步骤： 1）、编写国际化配置文件，抽取页面需要显示的国际化消息 第一个properties表示是在没有指定语言的情况下，默认的显示值 2）、SpringBoot自动配置好了管理国际化资源文件的组件； 3）、配置国际化基础名 在application.properties中设置 3）、去页面获取国际化的值； 效果：根据浏览器语言设置的信息切换了国际化； 原理国际化的核心是： 国际化Locale（区域信息对象）；LocaleResolver（获取区域信息对象）； 查看默认的区域信息解析器： 在WebMvcAutoConfiguration中 初始化了一个区域信息解析器，查看获取区域信息相关带代码 AcceptHeaderLocaleResolver ，看到这个类，我们可以猜测他是通过请求头中获取区域编码 看到 Accept-Language 说明上面我们的猜测是正确的。 也就是说，国际化的区域信息，默认是通过请求头中获取。而且需要注意的是当**spring容器中如果缺失LocaleResolver这个bean**实例，那么才会去加载默认的，区域化信息解析器。（下面的点击链接切换国际化就是很好的利用了这个注解） 点击链接切换国际化知道了上的原理，我们就可以手动的控制，切换语言环境。 1.实现一个区域信息解析器，通过获取请求中l的值，设定响应的语言环境。 因为我们自定义实现了LocalResolver，那么根据@ConditionalOnMissingBean这个注解 那么springboot默认的区域化解析器就会失效。 \\2. 实例化到spring容器中 上面的两个步骤可以整合在一起，最终实现： 在自定义的LocaleResolver中直接注入到spring容器中。注意需要指明**bean的id是localeResolver** 3.通过链接实现 http://127.0.0.1:8080/login.html?i=en_US http://127.0.0.1:8080/login.html?i=en_US 7.7 themleaf关闭缓存Thymeleaf会在第一次对模板解析之后进行缓存，极大的提高了并发处理能力，但是在开发期间模板引擎页面修改以后，要实时生效，所以我们开发阶段可以关掉缓存使用 1）、禁用模板引擎的缓存 ​ # 禁用缓存 ​ spring.thymeleaf.cache=false 2）、页面修改完成以后ctrl+f9：重新编译； 7.8 自定义拦截器 1. 第一种实现方式继承WebMvcConfigurerAdapter 1.定义一个拦截器实现HandlerInterceptor接口 2.注册到容器中 我们把扩展mvc的组件都放在自定义mvc扩展类中：MyMvcConfig 2. 第二种方式（常见）1. \\2. 然后定义配置类，注册拦截器 3.运行输出 接下来运行并查看日志： 你会发现日志中只有这些打印信息，springMVC的日志信息都没有，因为springMVC记录的log级别是debug，springboot默认是显示info以上，我们需要进行配置。 SpringBoot通过logging.level.=debug来配置日志级别，填写包名 # 设置org.springframework包的日志级别为debug logging.level.org.springframework=debug 设置完后，访问请求，查看输出 相比springmvc，springboot已经帮我们做好了静态资源的映射访问，所以不需要额外处理。 7.9 springmvc默认日期类型转化例如从前台传回一个时间字符串，后台通过date日期类型进行映射获取。那么如果前台日期格式是：2017/12/12 ，那么mvc会自动转化为date类型，因为他的默认时间转换器是以/ 进行分割的。 如果是2017-12-12、2017.12.12 这样的日期格式，那么就需要我们自己实现日期转换器 通过查看WebMvcAutoConfiguration的源码得知： 默认的日期格式是使用反斜杠来分割。 –修改默认的日期格式化： 只需要在applicatio.proeprties文件中，覆盖默认的日期格式即可 7.10 post/get请求转化为put或其他我们知道html的form表单只支持get/post两种请求，那么我们使用restful url进行请求数据的时候，那么需要使用到put请求，那么怎么转化呢？ 7.11错误处理机制1）、SpringBoot默认的错误处理机制当我们访问一个不存在的资源时，那么springboot默认帮我们跳转到一个错误界面 1.使用pc浏览器访问 2.使用其他工具访问（postman） 我们不难发现，他返回错误的形式都是不一样的，前者返回一个html界面，后者返回的是一个json字符串。为什么会有两种错误请求返回方式呢？接下来看源码就知道了。 原理： ​ 可以参照ErrorMvcAutoConfiguration；错误处理的自动配置； ​ 给容器中添加了以下四个组件 ​ 1、DefaultErrorAttributes： ​ 2、BasicErrorController：他是一个基本的错误控制类。处理默认/error请求 是一个controller，处理/error请求 这里就解释了为什么会出现，两种错误请求响应方式（**html和josn**） 那么会产生另一个问题，他是怎么知道客户端是**pc还是其他访问工具呢？究竟怎么选择返回哪种格式的错误消息??????????** 很明显是通过请求头进行区分的。 1.通过postman发出的请求，请求头是： 2.通过浏览器发出的请求，请求头是： 3、ErrorPageCustomizer： 查看getpath()， 总的来说：系统出现错误以后来到error请求进行处理；（类似我们在web.xml注册的错误页面规则-根据不同的错误码，响应不同的错误界面：使用标签） 4、DefaultErrorViewResolver： 主要解析代码： 总结​ 步骤： ​ 一但系统出现4xx或者5xx之类的错误；ErrorPageCustomizer就会生效（定制错误的响应规则）；就会来到/error请求；就会被BasicErrorController处理； ​ 1）响应页面；去哪个页面是由DefaultErrorViewResolver解析得到的； 2）、如果定制错误响应：1）、如何定制错误的页面；1**）、有模板引擎的情况下**；error/状态码; 【将错误页面命名为 错误状态码.html 放在模板引擎文件夹里面的 error文件夹下】，发生此状态码的错误就会来到对应的页面； 那么就会存在一个缺点，那就是如果我想把所有4开头的错误码都定向到一个错误界面怎么办呢？ 我们回过头查看DefaultErrorViewResolver 源代码发现，他默认注册了两个规则4xx和5xx。 我们可以使用**4xx和5xx作为错误页面的文件名来匹配这种类型的所有错误，精确优先（优先寻找精确的状态码.html**）； 页面能获取的信息（我们可以在自定义的错误界面获取到这些信息，参见DefaultErrorAttributes源码） timestamp：时间戳 status：状态码 error：错误提示 exception：异常对象 message：异常消息 errors：JSR303数据校验的错误都在这里 2）、没有模板引擎（模板引擎找不到这个错误页面），静态资源文件夹（static）下找 3）、以上都没有错误页面，就是默认来到SpringBoot默认的错误提示页面 2）、如何定制错误的json数据；1）、自定义异常处理&amp;返回定制json数据； 实现一个异常处理controller 只要是UserNotExistEception的异常都交由此方法处理，并返回自己自己定制json信息。 缺点: 永远返回json格式信息，没有html界面（7.11.1.1）。 2）、转发到/error进行自适应响应效果处理 缺点：自定义的map错误数据，没有携带过去。 3）、将我们的定制数据携带出去；出现错误以后，会来到/error请求，会被BasicErrorController处理，响应出去可以获取的数据是由getErrorAttributes得到的（是AbstractErrorController（ErrorController）规定的方法）； 1、完全来编写一个ErrorController的实现类【或者是编写AbstractErrorController的子类】，放在容器中（这种方式太过麻烦不推荐） 2、页面上能用的数据，或者是json返回能用的数据都是通过errorAttributes.getErrorAttributes得到（推荐这种方法） 容器中DefaultErrorAttributes.getErrorAttributes()；默认进行数据处理的； 自定义ErrorAttributes 八. 默认嵌套的Servlet服务器 打开springboot项目的pom文件，查看依赖。 8.1如何定制和修改Servlet容器的相关配置1、修改和server有关的配置（ServerProperties也是EmbeddedServletContainerCustomizer的实现）-第一种方式 就是我们直接在application.properties文件中的修改操作 server.port=8081 server.context-path=/crud server.tomcat.uri-encoding=UTF-8 2、编写一个EmbeddedServletContainerCustomizer：嵌入式的Servlet容器的定制器；来修改Servlet容器的配置 – 第二种方式 8.2 替换为其他嵌入式Servlet容器Springboot 完美的集成了下面这三款servlet容器，只需要做一些小改动即可切换使用。 具体实现的容器工厂类。 1. 默认使用tomcat 2. Jetty 做法其实很简单，那就是一出web模块自动装配的tomcat 组件，然后引入jetty组件即可。Jetty适合在开发长连接的项目中使用（例如聊天类的项目） 3. Undertow Undertow不支持jsp界面 8.3 嵌入式Servlet容器自动配置原理1. 源码分析EmbeddedServletContainerAutoConfiguration：嵌入式的Servlet容器自动配置 可以看到容器的切换，是通过注解来进行控制。通过容器工厂进行创建相应的容器 1）、EmbeddedServletContainerFactory（嵌入式Servlet容器工厂） 通过 getEmbeddedServletContainer()获得相应的servlet容器。 2）、EmbeddedServletContainer：（嵌入式的Servlet容器） 总的来说：通过servlet容器工厂类来创建相应的servlet容器 2.以tomcat容器工厂为例，分析流程 1.查看TomcatEmbeddedServletContainerFactory ​ 重点是：**getEmbeddedServletContainer**方法 3. ServerProperties、EmbeddedServletContainerCustomizer我们知道我们修改application.properties都是映射成ServerProperties 他实现了EmbeddedServletContainerCustomizer接口，所以衍生出了，第二种修改servlet容器的配置的方法： ​ 自定义实现EmbeddedServletContainerCustomizer： 那么这些改动是怎么样生效的呢? 容器中导入了EmbeddedServletContainerCustomizerBeanPostProcessor 步骤： 1）、SpringBoot根据导入的依赖情况，给容器中添加相应的EmbeddedServletContainerFactory【TomcatEmbeddedServletContainerFactory】 2）、容器中某个组件要创建对象就会惊动后置处理器；EmbeddedServletContainerCustomizerBeanPostProcessor； 只要是嵌入式的Servlet容器工厂，后置处理器就工作； 3）、后置处理器，从容器中获取所有的EmbeddedServletContainerCustomizer，调用定制器的定制方法 8.4 嵌入式Servlet容器启动原理什么时候创建嵌入式的Servlet容器工厂？什么时候获取嵌入式的Servlet容器并启动Tomcat； 获取嵌入式的Servlet容器工厂： 1）、SpringBoot应用启动运行run方法 2）、refreshContext(context);SpringBoot刷新IOC容器【创建IOC容器对象，并初始化容器，创建容器中的每一个组件】；如果是web应用创建AnnotationConfigEmbeddedWebApplicationContext，否则：AnnotationConfigApplicationContext 3）、refresh(context);刷新刚才创建好的**ioc**容器； 4）、 onRefresh(); web的ioc容器重写了onRefresh方法 5）、webioc容器会创建嵌入式的Servlet容器；createEmbeddedServletContainer(); 6**）、获取嵌入式的Servlet容器工厂：** EmbeddedServletContainerFactory containerFactory = getEmbeddedServletContainerFactory(); 从ioc容器中获取EmbeddedServletContainerFactory 组件；TomcatEmbeddedServletContainerFactory创建对象，后置处理器一看是这个对象，就获取所有的定制器来先定制Servlet容器的相关配置； 7）、使用容器工厂获取嵌入式的**Servlet**容器：this.embeddedServletContainer = containerFactory .getEmbeddedServletContainer(getSelfInitializer()); 8）、嵌入式的Servlet容器创建对象并启动Servlet容器； 先启动嵌入式的**Servlet容器，再将ioc**容器中剩下没有创建出的对象获取出来； ==IOC**容器启动创建嵌入式的Servlet容器**== 九． springboot注册三大组件三大组件：servlet、filter、listenner 传统的web项目，我们可以在webroot/WEB_INFO/web.xml 中配置我们三大组件，但是springboot打包方式采用jar的方式，内嵌servlet容器，那么我们应该在哪里注册这三大组件呢？ 9.1 注册servle1.首先自定义一个servler 2.通过ServletRegistrationBean注册自定义servle 注意：如果存在一个**controller同时映射了/myServlet 请求，那么就会失效（被自定义servlet所覆盖）。** 9.2 注册filter1.首先自定义一个filter 2.FilterRegistrationBean 9.3 注册listenner1.首先自定义一个listenner 2.ServletListenerRegistrationBean 总结SpringBoot帮我们自动SpringMVC的时候，自动的注册SpringMVC的前端控制器；DIspatcherServlet； DispatcherServletAutoConfiguration中： 10 使用外置的Servlet容器嵌入式Servlet容器：应用打成可执行的jar ​ 优点：简单、便携 ​ 缺点：默认不支持JSP、优化定制比较复杂（虽然可以使用定制器【ServerProperties、自定义EmbeddedServletContainerCustomizer】，自己编写嵌入式Servlet容器的创建工厂【EmbeddedServletContainerFactory】等方式修改配置，但是还是在需要通读代码的情况下才能够修改） 外置的Servlet容器：外面安装Tomcat—应用war包的方式打包 步骤1）、必须创建一个项目打包类型为war的项目；（利用idea创建好目录结构，因为默认生成的springboot项目是没有webapp和web.xml等文件，需要手动创建） 下一步 完成 很明显没有生成相应的**webapp目录，同时tomcat的作用域修改为了运行时使用，打包不使用，这个也就是为我们使用外置servlet**容器打下基础。 项目最终目录结构 2）、将嵌入式的Tomcat指定为provided； Springboot**默认已经帮我们完成** 3）、必须编写一个SpringBootServletInitializer的子类，并调用configure方法 ​ Springboot**默认已经帮我们完成** 4）、启动服务器就可以使用 可以编写一个jsp界面，访问测试 原理jar包：执行SpringBoot主类的main方法，启动ioc容器，创建嵌入式的Servlet容器 war包：启动服务器，服务器启动SpringBoot应用【SpringBootServletInitializer】，启动ioc容器 实现方式：Servlet3.0是一次Java EE规范 https://blog.csdn.net/f641385712/article/details/87474907 相关连接 规则：​ 1）、服务器启动（web应用启动）会创建当前web应用里面每一个jar包里面ServletContainerInitializer实例 ​ 2）、ServletContainerInitializer的实现放在jar包的META-INF/services文件夹下，有一个名为javax.servlet.ServletContainerInitializer的文件，内容就是ServletContainerInitializer的实现类的全类名 ​ 3）、还可以使用@HandlesTypes，在应用启动的时候加载我们感兴趣的类 流程：1）、启动Tomcat 2）、org\\springframework\\spring-web\\4.3.14.RELEASE\\spring-web-4.3.14.RELEASE.jar!\\META-INF\\services\\javax.servlet.ServletContainerInitializer： Spring的web模块里面有这个文件：org.springframework.web.SpringServletContainerInitializer 3）、SpringServletContainerInitializer将@HandlesTypes(WebApplicationInitializer.class)标注的所有这个类型的类都传入到onStartup方法的Set ​ ​ mysql ​ mysql-connector-java ​ 3 在application.properties中配置数据库和jpa的相关属性 #DB Configuration: spring.datasource.driverClassName=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=utf8 spring.datasource.username=root spring.datasource.password=root #JPA Configuration: spring.jpa.database=MySQL spring.jpa.show-sql=true spring.jpa.generate-ddl=true spring.jpa.hibernate.ddl-auto=update spring.jpa.hibernate.naming_strategy=org.hibernate.cfg.ImprovedNamingStrategy 4 创建实体配置实体 @Entity public class User { // 主键 @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; // 用户名 private String username; // 密码 private String password; // 姓名 private String name; //此处省略setter和getter方法… … } 5 编写UserRepository​ public interface UserRepository extends JpaRepository{ ​ public List findAll(); ​ } 6 编写测试类 @RunWith(SpringRunner.class) @SpringBootTest(classes=MySpringBootApplication.class) public class JpaTest { @Autowired private UserRepository userRepository; @Test public void test(){ List users = userRepository.findAll(); System.out.println(users); } } 7 控制台打印信息 注意：如果是jdk9，执行报错如下： 原因：jdk缺少相应的jar 解决方案：手动导入对应的maven坐标，如下： ​ ​ ​ javax.xml.bind ​ jaxb-api ​ 2.3.0 ​ 11.3 整合redis1 添加redis的起步依赖 org.springframework.boot spring-boot-starter-data-redis 2 配置redis的连接信息#Redis spring.redis.host=127.0.0.1 spring.redis.port=6379 3 注入RedisTemplate测试redis操作 @RunWith(SpringRunner.class) @SpringBootTest(classes = SpringbootJpaApplication.class) public class RedisTest { @Autowired private UserRepository userRepository; @Autowired private RedisTemplate redisTemplate; @Test public void test() throws JsonProcessingException { //从redis缓存中获得指定的数据 String userListData = redisTemplate.boundValueOps(“user.findAll”).get(); //如果redis中没有数据的话 if(null==userListData){ //查询数据库获得数据 List all = userRepository.findAll(); //转换成json格式字符串 ObjectMapper om = new ObjectMapper(); userListData = om.writeValueAsString(all); //将数据存储到redis中，下次在查询直接从redis中获得数据，不用在查询数据库 redisTemplate.boundValueOps(“user.findAll”).set(userListData); System.out.println(“===============从数据库获得数据===============”); }else{ System.out.println(“===============从redis缓存中获得数据===============”); } System.out.println(userListData); } } 十二、springboot运行流程回顾1.启动柜springboot程序 断点进入-step into 发现 停在了这里，也就是说他会首先创建SpringApplication对象 2. 创建SpringApplication对象创建SpringApplication对象运行run方法 继续进入方法","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://kingge.top/categories/SpringBoot/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"springboot","slug":"springboot","permalink":"http://kingge.top/tags/springboot/"},{"name":"微服务","slug":"微服务","permalink":"http://kingge.top/tags/微服务/"}]},{"title":"dubbo分布式服务框架","slug":"dubbo分布式服务框架","date":"2019-01-05T02:21:59.000Z","updated":"2019-08-25T02:24:55.615Z","comments":true,"path":"2019/01/05/dubbo分布式服务框架/","link":"","permalink":"http://kingge.top/2019/01/05/dubbo分布式服务框架/","excerpt":"","text":"Dubbo分布式服务框架** 一、分布式概念分布式的概念：某个业务逻辑的完成，拆分成几个功能/服务来实现，这些服务部署在不同的机器上。 官方解释： 分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是利用更多的机器，处理更多的数据。 首先需要明确的是，只有当单个节点的处理能力无法满足日益增长的计算、存储任务的时候，且硬件的提升（加内存、加磁盘、使用更好的CPU）高昂到得不偿失的时候，应用程序也不能进一步优化的时候，我们才需要考虑分布式系统。因为，分布式系统要解决的问题本身就是和单机系统一样的，而由于分布式系统多节点、通过网络通信的拓扑结构，会引入很多单机系统没有的问题，为了解决这些问题又会引入更多的机制、协议，带来更多的问题。 ​ 随着互联网的发展，网站应用的规模不断扩大，常规的垂直应用架构已无法应对，分布式服务架构以及流动计算架构势在必行，需一个治理系统确保架构有条不紊的演进。 1.1分布式的演化 1.1.1单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起(例如)，以减少部署节点和成本。 总的来说，一个项目就打包成一个war包，然后订单业务处理、商品等等所有业务都在这个war包里。 适用于小型网站，小型管理系统，将所有功能都部署到一个功能里，简单易用。 缺点： 1、性能扩展比较难 2、存在重复性开发的代码 3、不利于升级维护 4、 只能采用同一种技术，很难用不同的语言或者语言不同版本开发不同模块； 5、系统耦合性强，一旦其中一个模块有问题，整个系统就瘫痪了；一旦升级其中一个模块，整个系统就停机了； 6、 集群只能是复制整个系统，即使只是其中一个模块压力大。（可能整个订单处理，仅仅是支付模块压力过大， 按道理只需要升级支付模块，但是在单一场景里面是不能的） 最直观问题就是， 1.代码不够整洁，多个业务的功能都堆在一个包中，例如UserService、OrderService等等功能都放在com.kingge.service包中。当功能增多时，这个包会变得更加臃肿，代码阅读性很差。 2.服务之间依赖错综复杂不够清晰。 3.只能够通过部署集群来提高系统性能，资源浪费 1.1.2垂直应用架构​ 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率，这样就可以单独修改某个模块而不用重启或者影响其他模块，同时也可以给某个访问量剧增的模块，单独添加服务器。 ​ 通过切分业务来实现各个模块独立部署，降低了维护和部署的难度，团队各司其职更易管理，性能扩展也更方便，更有针对性。 缺点： 公用模块无法重复利用，开发性的浪费 面对突变的应用场景，可能某个模块对于web界面会频繁修改，但是模块业务功能没有变化，这样会造成单个应用频繁修改。所以需要界面+业务逻辑的实现分离。 没有处理好应用之间的交互问题，例如订单模块可能会需要查询商品模块的信息。 1.1.3分布式服务架构​ 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键（例如Dubbo）。（springcloud比他更全面，Dubbo只是相当于Spring Cloud中的Eureka模块） 分布式服务框架很好的解决了垂直应用架构的缺点，实现界面和服务的分离，实现界面和服务，以及服务与服务之间的调度。 但是存在一个问题，那就是没有一个基于访问压力的调度中心和服务注册中心，容易造成资源浪费，什么意思呢？假设用户服务部署了200台服务器，但是在某个时间段，他的访问压力很小，订单服务的访问压力剧增，服务器不够用。那么就会造成资源浪费和倾斜，存在服务器闲置或者请求量少的情况。 1.1.4流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)[ Service Oriented Architecture]是关键。 很好的解决了分布式架构的缺点。 SOA解决了服务的管理和注册，但是还是缺少服务容灾处理，网关处理以及全局配置模块等等（分别对应springcloud的hystrix、zuul、config） 1.1.5 微服务架构其实SOA架构跟微服务架构是很接近的，其实我也不是那么清晰的能够分别这种两种架构，所以这一章节暂缺。 但是他提供了比SOA架构更加细致化的服务拆分和管理，一般常用restful的方式进行数据的传输，SOA架构的传输技术是比较复杂多样的。 他的实现，见springcloud架构 1.2 RPC1.2.1什么叫RPCRPC【Remote Procedure Call】是指远程过程调用，是一种进程间通信方式，他是一种技术的思想，而不是规范。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。 1.2.2RPC基本原理 ​ 模型中多了一个stub的组件，这个是约定的接口，也就是server提供的服务。注意这里的”接口”，不是指JAVA中的interface，因为RPC是跨平台跨语言的，用JAVA写的客户端，应该能够调用用C语言提供的过程 一次完整的RPC调用流程（同步调用，异步另说）如下：1）服务消费方（client）调用以本地调用方式调用服务；2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；3）client stub找到服务地址，并将消息发送到服务端；4）server stub收到消息后进行解码；5）server stub根据解码结果调用本地的服务；6）本地服务执行并将结果返回给server stub；7）server stub将返回结果打包成消息并发送至消费方；8）client stub接收到消息，并进行解码；9）服务消费方得到最终结果。 RPC两个核心模块：通讯，序列化。 也就是说决定RPC连接效率的核心因素了，服务之间通讯的速度，以及消息序列化和反序列化的速度（这个就是为什么hadoop会采用自己的反序列化机制而不是java的反序列化，Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效） 常用的RPC框架：dubbo，gRPC，thrift。HSF 二、Dubbo概念和理解2.1 概念官网： [http://dubbo.apache.org/]{} Apache Dubbo 是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。即是：提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案 Dubbo是阿里巴巴公司开源的一个高性能优秀的服务框架，后来Dubbo 进入 Apache 孵化器。 Dubbo 采用全 Spring 配置方式，透明化接入应用，对应用没有任何 API 侵入，只需用 Spring 加载 Dubbo 的配置即可，Dubbo 基于 Spring 的 Schema 扩展 进行加载。 如果不想使用 Spring 配置，可以通过 API 的方式 进行调用。 官方文档 http://dubbo.apache.org/zh-cn/docs/user/quick-start.html 2.2 dubbo结构 服务提供者（Provider）：暴露服务的服务提供方，服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者（Consumer）: 调用远程服务的服务消费方，服务消费者在启动时，向注册中心订阅自己所需的服务，服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 注册中心（Registry）：注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者 监控中心（Monitor）：服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心 调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法（不是硬件级别的负载均衡），选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 2.3 dubbo架构Dubbo 架构具有以下几个特点，分别是连通性、健壮性、伸缩性、以及向未来架构的升级性。 连通性 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 服务提供者向注册中心注册其提供的服务，并汇报调用时间到监控中心，此时间不包含网络开销 服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心，此时间包含网络开销 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 伸缩性 注册中心为对等集群，可动态增加机器部署实例，所有客户端将自动发现新的注册中心 服务提供者无状态，可动态增加机器部署实例，注册中心将推送新的服务提供者信息给消费者 升级性 当服务集群规模进一步扩大，带动IT治理结构进一步升级，需要实现动态部署，进行流动计算，现有分布式服务架构不会带来阻力。下图是未来可能的一种架构： 节点角色说明 节点 角色说明 Deployer 自动部署服务的本地代理 Repository 仓库用于存储服务应用发布包 Scheduler 调度中心基于访问压力自动增减服务提供者 Admin 统一管理控制台 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 三、Dubbo环境搭建3.1 windows下搭建dubbo环境3.1.1 安装zookeeperDubbo推荐使用zookeeper作为注册中心 1.登录zookeeper官网下载zookeeper[https://archive.apache.org/dist/zookeeper/zookeeper-3.4.13/]{.underline} 这里以zookeeper-3.4.13为例 2.下载后解压修改zoo.cfg配置文件将conf下的zoo_sample.cfg复制一份改名为zoo.cfg即可。 注意几个重要位置： dataDir=./ 临时数据存储的目录（可写相对路径） clientPort=2181 zookeeper的端口号 修改完成后启动zookeeper 在zookeeeper目录下新建zooData目录保存数据 3. 运行zkServer.cmd启动成功后，使用zkCli.cmd测试 ls /：列出zookeeper根目录下保存的所有节点 create -e /kingge 123：创建一个临时的kingge节点，值为123 get / kingge：获取/ kingge节点的值 3.1.2安装dubbo-admin管理控制台dubbo本身并不是一个服务软件。它其实就是一个jar包能够帮你的java程序连接到zookeeper，并利用zookeeper消费、提供服务。所以你不用在Linux上启动什么dubbo服务。 但是为了让用户更好的管理监控众多的dubbo服务，官方提供了一个可视化的监控程序，不过这个监控即使不装也不影响使用。 1、下载dubbo-admin[https://github.com/apache/incubator-dubbo-ops]{.underline} 下载下来后解压 2、进入目录，修改dubbo-admin配置修改 src\\main\\resources\\application.properties 指定zookeeper地址 3、打包dubbo-admin（maven环境已经配置好）mvn clean package -Dmaven.test.skip=true 4、运行dubbo-adminjava -jar dubbo-admin-0.0.1-SNAPSHOT.jar （springboot方式启动项目） 注意：【有可能控制台看着启动了，但是网页打不开，需要在控制台按下ctrl+c即可】 默认使用root/root 登陆 3.2 linux下搭建dubbo环境3.2.1安装zookeeper1、安装jdk1、下载jdk[http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html]{.underline} 不要使用wget命令获取jdk链接，这是默认不同意，导致下载来的jdk压缩内容错误 2、上传到服务器并解压 3、设置环境变量/usr/local/java/jdk1.8.0_171 文件末尾加入下面配置 export JAVA_HOME=/usr/local/java/jdk1.8.0_171export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 4、使环境变量生效&amp;测试JDK 2、安装zookeeper1、下载zookeeper网址 [https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/]{.underline} wget [https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz]{.underline} 2、解压 3、移动到指定位置并改名为zookeeper 3、开机启动zookeeper（可选）1）-复制如下脚本 #!/bin/bash#chkconfig:2345 20 90#description:zookeeper#processname:zookeeperZK_PATH=/usr/local/zookeeperexport JAVA_HOME=/usr/local/java/jdk1.8.0_171case $1 in start) sh $ZK_PATH/bin/zkServer.sh start;; stop) sh $ZK_PATH/bin/zkServer.sh stop;; status) sh $ZK_PATH/bin/zkServer.sh status;; restart) sh $ZK_PATH/bin/zkServer.sh restart;; *) echo &quot;require start|stop|status|restart&quot; ;;esac 2）-把脚本注册为Service 3）-增加权限 4、配置zookeeper1、初始化zookeeper配置文件拷贝/usr/local/zookeeper/conf/zoo_sample.cfg 到同一个目录下改个名字叫zoo.cfg 2、启动zookeeper 3.2.2安装dubbo-admin管理控制台1、安装Tomcat8（旧版dubbo-admin是war，新版是jar不需要安装Tomcat）1、下载Tomcat8并解压[https://tomcat.apache.org/download-80.cgi]{.underline} wget[http://mirrors.shu.edu.cn/apache/tomcat/tomcat-8/v8.5.32/bin/apache-tomcat-8.5.32.tar.gz]{.underline} 提供两种访问方式 2、解压移动到指定位置 3、开机启动tomcat8 复制如下脚本#!/bin/bash#chkconfig:2345 21 90#description:apache-tomcat-8#processname:apache-tomcat-8CATALANA_HOME=/opt/apache-tomcat-8.5.32export JAVA_HOME=/opt/java/jdk1.8.0_171case $1 instart) echo &quot;Starting Tomcat...&quot; $CATALANA_HOME/bin/startup.sh ;;stop) echo &quot;Stopping Tomcat...&quot; $CATALANA_HOME/bin/shutdown.sh ;;restart) echo &quot;Stopping Tomcat...&quot; $CATALANA_HOME/bin/shutdown.sh sleep 2 echo echo &quot;Starting Tomcat...&quot; $CATALANA_HOME/bin/startup.sh ;;*) echo &quot;Usage: tomcat &#123;start|stop|restart&#125;&quot; ;; esac 4、注册服务&amp;添加权限 5、启动服务&amp;访问tomcat测试 2、安装dubbo-admindubbo本身并不是一个服务软件。它其实就是一个jar包能够帮你的java程序连接到zookeeper，并利用zookeeper消费、提供服务。所以你不用在Linux上启动什么dubbo服务。 但是为了让用户更好的管理监控众多的dubbo服务，官方提供了一个可视化的监控程序，不过这个监控即使不装也不影响使用。 1、下载dubbo-admin[https://github.com/apache/incubator-dubbo-ops]{.underline} 2、进入目录，修改dubbo-admin配置修改 src\\main\\resources\\application.properties 指定zookeeper地址 3、打包dubbo-adminmvn clean package -Dmaven.test.skip=true 4、运行dubbo-adminjava -jar dubbo-admin-0.0.1-SNAPSHOT.jar 默认使用root/root 登陆 3.3 测试dubbo（dubbo-hello）[http://dubbo.apache.org/zh-cn/docs/user/quick-start.html]{.underline} 官方例子 4.1）、提出需求某个电商系统，订单服务需要调用用户服务获取某个用户的所有地址； 我们现在 需要创建两个服务模块进行测试 模块 功能 订单服务web模块 创建订单等 用户服务service模块 查询用户地址等 测试预期结果： 订单服务web模块在A服务器，用户服务模块在B服务器，A可以远程调用B的功能。 4.2）、工程架构根据 dubbo《服务化最佳实践》 http://dubbo.apache.org/zh-cn/docs/user/best-practice.html 1、分包（就是抽取公共的接口或者实体类到一个工程）​ 建议将服务接口，服务模型，服务异常等均放在 API 包中，因为服务模型及异常也是 API 的一部分，同时，这样做也符合分包原则：重用发布等价原则(REP)，共同重用原则(CRP)。 ​ 如果需要，也可以考虑在 API 包中放置一份 spring 的引用配置，这样使用方，只需在 spring 加载过程中引用此配置即可，配置建议放在模块的包目录下，以免冲突，如：com/alibaba/china/xxx/dubbo-reference.xml。 ​ 就是为了避免重复书写某些代码，抽取公共部分代码形成一个模块（例如下面的common-interface），其他模块需要用到时，添加依赖即可（pom文件） 2、粒度​ 服务接口尽可能大粒度，每个服务方法应代表一个功能，而不是某功能的一个步骤，否则将面临分布式事务问题，Dubbo 暂未提供分布式事务支持。服务接口建议以业务场景为单位划分，并对相近业务做抽象，防止接口数量爆炸。不建议使用过于抽象的通用接口，如：Map query(Map)，这样的接口没有明确语义，会给后期维护带来不便。 4.3）、创建模块1、common-interface：公共接口层（model，service，exception…）（符合分包理念）作用：定义公共接口，也可以导入公共依赖，保存服务提供者和服务消费者的接口 好处就是，可以让服务生产者和消费者同时依赖这个公共接口层maven项目，避免书写重复代码。 2、user-module：用户模块（对用户接口的实现，服务提供者） pom.xml (引用公共接口层) &lt;dependency&gt; &lt;groupId&gt;com.kingge.common&lt;/groupId&gt; &lt;artifactId&gt;common-interface&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 用户接口实现 这里为了快速演示，就做了一些虚拟数据，不会去连接数据库。 到时候订单模块只需要调用即可 4、order-module：订单模块（调用用户模块）1. pom.xml (引用公共接口层) &lt;dependency&gt; &lt;groupId&gt;com.kingge.common&lt;/groupId&gt; &lt;artifactId&gt;common-interface&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 2. 订单接口实现 现在这样是无法进行调用的。我们order-module引入了common-interface，但是common-interface公用模块里的用户接口的具体实现在user-module，我们并没有引入user-module模块，所以上诉代码调用肯定是失败的。而且user-module模块可能还在别的服务器中（不在同一个进程中）。 ​ 所以我们需要把user-module模块的用户接口的实现类暴露出去，供order-module模块使用。 接下来使用dubbo来实现这样的功能。 4.4）、使用dubbo改造1、改造user-module作为服务提供者（1）引入dubbo （pom.xml引入依赖） &lt;!-- 引入dubbo --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 由于我们使用zookeeper作为注册中心，所以需要操作zookeeperdubbo 2.6以前的版本引入zkclient操作zookeeper dubbo 2.6及以后的版本引入curator操作zookeeper下面两个zk客户端根据dubbo版本2选1即可--&gt; &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt; &lt;/dependency&gt; &lt;!-- curator-framework --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt; &lt;/dependency&gt; （2）配置提供者（新建provider.xml文件） 内容如下： &lt;!-- 1、指定当前服务/应用的名字（同样的服务名字相同，不要和别的服务同名） --&gt; &lt;dubbo:application name=&quot;user-module&quot;&gt;&lt;/dubbo:application&gt;&lt;!-- 2、指定注册中心的位置 --&gt;&lt;!-- &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot;&gt;&lt;/dubbo:registry&gt; 这种方式也可以 --&gt; &lt;dubbo:registry protocol=&quot;zookeeper&quot; address=&quot;127.0.0.1:2181&quot;&gt;&lt;/dubbo:registry&gt; &lt;!--使用dubbo协议，将服务暴露在20880端口 consumer和provider连接的协议，协议由提供方指定，消费方被动接受 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt;&lt;!-- 指定需要暴露的服务 --&gt;&lt;dubbo:service interface=&quot;com.kingge.common.service.UserService&quot; ref=&quot;userServiceImpl&quot; &gt;&lt;/dubbo:service&gt; &lt;!-- 服务的实现 --&gt;&lt;bean id=&quot;userServiceImpl&quot; class=&quot;com.kingge.user.service.impl.UserServiceImpl&quot;&gt;&lt;/bean&gt; dubbo支持多种传输协议 （3）启动服务 public class MainApplication &#123; public static void main(String[] args) throws IOException &#123; ClassPathXmlApplicationContext ioc = new ClassPathXmlApplicationContext(&quot;provider.xml&quot;); ioc.start(); System.in.read(); &#125;&#125; 运行后，打开我们之前配置的dubbo-admin 2、改造order-module作为服务消费者（1）引入dubbo （pom.xml引入依赖） &lt;!-- 引入dubbo --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 由于我们使用zookeeper作为注册中心，所以需要操作zookeeperdubbo 2.6以前的版本引入zkclient操作zookeeper dubbo 2.6及以后的版本引入curator操作zookeeper下面两个zk客户端根据dubbo版本2选1即可--&gt; &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt; &lt;/dependency&gt; &lt;!-- curator-framework --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt; &lt;/dependency&gt; （2）配置消费者信息 新建consumer.xml文件 &lt;!—配置包扫描 --&gt;&lt;context:component-scan base-package=&quot;com.kingge.order.service.impl&quot;&gt;&lt;/context:component-scan&gt;&lt;!-- 应用名 --&gt; &lt;dubbo:application name=&quot;order-module&quot;&gt;&lt;/dubbo:application&gt;&lt;!-- 指定注册中心地址 --&gt; &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot; /&gt;&lt;!-- 生成远程服务代理，可以和本地bean一样使用userService --&gt;&lt;dubbo:reference interface=&quot;com.kingge.common.service.UserService&quot; id=&quot;userService&quot; &gt;&lt;/dubbo:reference&gt; （3）修改OrderServiceImpl （4）创建启动类，启动consumer 3、测试调用调用成功 查看dubbo admin 3.4 配置dubbo监控中心3.4.1 dubbo-admin图形化的服务管理页面；安装时需要指定注册中心地址，即可从注册中心中获取到所有的提供者/消费者进行配置管理 安装和使用方式在上面我们已经说过了，这里就不在阐述。 3.4.2 dubbo-monitor-simple简单的监控中心； 1、下载 dubbo-opshttps://github.com/apache/incubator-dubbo-ops 2、修改配置指定注册中心地址进入 dubbo-monitor-simple\\src\\main\\resources\\conf 修改 dubbo.properties文件 3、打包dubbo-monitor-simplemvn clean package -Dmaven.test.skip=true 打包完成后，打开target目录，解压下面的tar.gz包 他跟3.4.1 的dubbo-admin不一样，不是一个springboot项目，不能够直接使用java –jar命令执行。 打开解压后的目录 Conf目录存放就是我们的dubbo.properties assembly.bin目录存放运行dubbo-monitor-simple 的可执行文件 双击 start.bat 执行 4.在项目中使用simple监控中心 以3.3的例子为例： 分别在provider.xml和consumer.xml中添加以下代码 &lt;dubbo:monitor protocol=&quot;registry&quot;&gt;&lt;/dubbo:monitor&gt; &lt;!-- &lt;dubbo:monitor address=&quot;127.0.0.1:7070&quot;&gt;&lt;/dubbo:monitor&gt;--&gt; 然后重新运行项目，再查看 四、dubbo整合springboot改造4.3章节普通的maven项目为springboot项目，关键代码没有变化，主要是一些配置和依赖发生了变化 关于idea怎么创建多模块项目这里就不阐述了，详情可参见一下网址 [https://blog.csdn.net/tiantangdizhibuxiang/article/details/81130297]{.underline} https://www.cnblogs.com/zjfjava/p/9696086.html 4.1 新建boot-common-interface：公共接口层（model，service，exception…）（符合分包理念） 一路next完成。 复制4.3.1 章节common-interface的相关代码到此工程 4.2 新建boot-user-module：用户模块4.2.1 新建模块 一路next创建即可。 4.2.2 代码实现4.2.2同理复制章节4.3.2 user-module项目代码到此项目并修改pom.xml文件 添加一下依赖 &lt;!--引用公用模块--&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge&lt;/groupId&gt; &lt;artifactId&gt;boot-common-interface&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!--引入dubbo starter--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; 需要注意的是： 注意springboot的starter版本跟dubbo版本适配： 因为本人使用的是springboot2.1.6版本，故使用0.2.0版本dubbo starter 4.2.3修改application.proeprties文件dubbo.application.name=boot-user-moduledubbo.registry.protocol=zookeeperdubbo.registry.address=127.0.0.1:2181#开启包扫描，可替代 @EnableDubbo 注解#dubbo.scan.base-packages=com.kingge.orderdubbo.protocol.name=dubbodubbo.protocol.port=20880dubbo.monitor.protocol=registry application.name就是服务名，不能跟别的dubbo提供端重复 registry.protocol 是指定注册中心协议 registry.address 是注册中心的地址加端口号 protocol.name 是分布式固定是dubbo,不要改。 dubbo.scan.base-packages 需要暴露的接口的实现类所在的包（跟启动类在同一个包或者子包下都不需要配置这个属性的值，因为springboot启动类会扫描注入。） dubbo.monitor.protocol 开启simple检测中心 4.2.4 配置需要暴露的服务（通过注解方式） 4.2.5 boot启动类开启dubbo功能 4.2.6 完整项目结构和启动服务提供者 4.3 新建boot-order-module：订单模块4.3.1 因为我们打算通过web的方式访问controller，再去调用暴露的接口，所以需要引入web依赖 一路next即可 4.3.2 代码实现4.3.2 同理复制章节4.3.3 order-module项目代码到此项目并修改pom.xml文件 添加一下依赖 &lt;!--引用公用模块--&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge&lt;/groupId&gt; &lt;artifactId&gt;boot-common-interface&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!--引入dubbo starter--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; 4.3.3 修改application.proeprties文件dubbo.application.name=boot-order-module dubbo.registry.address= zookeeper://127.0.0.1:2181 dubbo.monitor.protocol=registry 4.3.4 service使用@reference调用暴露接口 4.3.5 实现一个controller，接受请求 4.3.6 boot启动类开启dubbo功能 4.3.7 启动工程，并访问请求 服务调用成功 4.4 补充整合dubbo的三种方式：官网 [http://dubbo.apache.org/zh-cn/docs/user/configuration/api.html]{.underline} * SpringBoot与dubbo整合的三种方式： * 1）、导入dubbo-starter，在application.properties配置属性，使用\\@Service【暴露服务】使用@Reference【引用服务】使用\\@EnableDubbo，开启dubbo注解功能（上面例子我们使用的就是这种方式） * 2）、保留dubbo xml配置文件 * 导入dubbo-starter，使用\\@ImportResource导入dubbo的配置文件即可 * 3）、使用注解API的方式： * 将每一个组件手动创建到容器中,让dubbo来扫描其他的组件 第二种整合方式解决： 如果我们需要在boot-user-module服务提供者配置方法级别的超时时间，那么有没有响应的注解配置呢？ 答案是没有的，那么就需要我们使用xml的方式进行配置。 1. 首先去掉@Service、@Reference、@EnableDubbo等注解 2. @ImportResource(locations=\\”classpath:provider.xml\\”) 使用 ImportResource注解导入外部xml文件 五、Dubbo配置[http://dubbo.apache.org/zh-cn/docs/user/configuration/annotation.html]{.underline} JVM 启动 -D 参数优先，这样可以使用户在部署和启动时进行参数重写，比如在启动时需改变协议的端口。 XML（springboot项目中对应的是application.properties文件） 次之，如果在 XML 中有配置，则 dubbo.properties 和代码中的相应配置项无效。 使用代码设置的方式优先级排在第三。 Properties 最后，相当于缺省值，只有 XML 没有配置时，dubbo.properties 的相应配置项才会生效，通常用于共享公共配置，比如应用名。 六、Dubbo官方提供的例子（功能）[http://dubbo.apache.org/zh-cn/docs/user/demos/preflight-check.html]{.underline} 6.1启动时检查 6.2 服务超时调用和重试次数1. timeout属性 2. retries 属性 默认重试2次，也就是说一共会调用提供者服务三次（第一次调用不计次数） 如果在注册中心，提供者有三个（a1、a2、a3），那么消费者(b1)在重试获取服务的时候，这三个都会可能去调用。 b1请求a1服务超时，发现注册中心存在相同的服务a2、a3那么b1会去请求a2或者a3，当然也有可能再次请求a1 那么什么时候使用这个字段。建议在“幂等”的业务场景下使用，不要在非幂等的场景下使用。 幂等：就是提供者提供的服务，调用多次跟调用一次的起到的作用是一致的（例如对数据库的delete某条数据的操作） 非幂等：单次调用和多次调用的结果是不一样的（数据库的insert操作） 设置为0，表示不进行重试，直接报异常 6.3标签属性配置优先级 6.4多版本当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。 可以按照以下的步骤进行版本迁移： 1.在低压力时间段，先升级一半提供者为新版本 2.再将所有消费者升级为新版本 3.然后将剩下的一半提供者升级为新版本 使用场景：新版接口需要替代旧版接口时。 Version=”*“表示任何调用新旧版本 举个例子： 使用第四章节的例子： 1.boot-user-module模块，添加一个新的UserService接口实现类，作为新接口的实现 给两个接口添加版本标识（标识新旧接口） 2.修改boot-order-module模块消费者消费接口版本 通过version属性实现新旧接口的调用 6.5本地存根 6.6 更多用法参见官网的事例七、Dubbo高可用7.1、zookeeper宕机与dubbo直连现象：zookeeper注册中心宕机，还可以消费dubbo暴露的服务。 原因： 健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 高可用：通过设计，减少系统不能提供服务的时间； 直连dubbo：因为我们知道zookeeper注册中心保存的信息主要是消息提供者的位置，那么我们消费者可以通过url的方式直接访问消息提供者提供的服务地址也是可以的。 7.2、集群下dubbo负载均衡配置在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。 负载均衡策略 http://dubbo.apache.org/zh-cn/docs/user/demos/loadbalance.html 1.Random LoadBalance 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 可以在Dubbo-admin设置某个服务的权重 2.RoundRobin LoadBalance 轮循，按公约后的权重设置轮循比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 3.LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 在消费服务的时候，总是查询上一个服务处理请求处理最快的那一台服务器 很明显请求会发到一号服务器，因为他处理最快。每次都记录请求处理的时间。 4.ConsistentHash LoadBalance 一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。算法参见：http://en.wikipedia.org/wiki/Consistent\\_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 \\ 缺省用 160 份虚拟节点，如果要修改，请配置 \\ 7.3、整合hystrix，服务熔断与降级处理1、服务降级什么是服务降级？ 当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作。 可以通过服务降级功能临时屏蔽某个出错的非关键服务，并定义降级后的返回策略。 向注册中心写入动态配置覆盖规则： RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf(&quot;zookeeper://10.20.153.10:2181&quot;));registry.register(URL.valueOf(&quot;override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;mock=force:return+null&quot;)); 其中： mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 相当于在dubbo-admin中设置如下： 还可以改为 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 2、集群容错[http://dubbo.apache.org/zh-cn/docs/user/demos/fault-tolerent-strategy.html]{.underline} 在集群调用失败时，Dubbo 提供了多种容错方案，缺省为 failover 重试。 集群容错模式 Failover Cluster失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=&quot;2&quot; 来设置重试次数(不含第一次)。重试次数配置如下：&lt;dubbo:service retries=&quot;2&quot; /&gt;或&lt;dubbo:reference retries=&quot;2&quot; /&gt;或&lt;dubbo:reference&gt; &lt;dubbo:method name=&quot;findFoo&quot; retries=&quot;2&quot; /&gt;&lt;/dubbo:reference&gt;Failfast Cluster快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。Failsafe Cluster失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。Failback Cluster失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。Forking Cluster并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=&quot;2&quot; 来设置最大并行数。Broadcast Cluster广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。集群模式配置按照以下示例在服务提供方和消费方配置集群模式&lt;dubbo:service cluster=&quot;failsafe&quot; /&gt;或&lt;dubbo:reference cluster=&quot;failsafe&quot; /&gt; 3、整合hystrixHystrix 旨在通过控制那些访问远程系统、服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。Hystrix具备拥有回退机制和断路器功能的线程和信号隔离，请求缓存和请求打包，以及监控和配置等功能 1、配置spring-cloud-starter-netflix-hystrixspring boot官方提供了对hystrix的集成，直接在pom.xml里加入依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;version&gt;1.4.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; 然后在Application类上增加\\@EnableHystrix来启用hystrix starter： @SpringBootApplication@EnableHystrixpublic class ProviderApplication &#123; 2、配置Provider端在Dubbo的Provider上增加\\@HystrixCommand配置，这样子调用就会经过Hystrix代理。 @com.alibaba.dubbo.config.annotation.Service (version = &quot;1.0.0&quot;)public class HelloServiceImpl implements HelloService &#123; @HystrixCommand(commandProperties = &#123; @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;), @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value = &quot;2000&quot;) &#125;) @Override public String sayHello(String name) &#123; // System.out.println(&quot;async provider received: &quot; + name); // return &quot;annotation: hello, &quot; + name; throw new RuntimeException(&quot;Exception to show hystrix enabled.&quot;); &#125;&#125; 3、配置Consumer端对于Consumer端，则可以增加一层method调用，并在method上配置@HystrixCommand。当调用出错时，会走到fallbackMethod = \\”reliable\\”的调用里。 @Reference(version = &quot;1.0.0&quot;)private HelloService demoService;@HystrixCommand(fallbackMethod = &quot;reliable&quot;)public String doSayHello(String name) &#123; return demoService.sayHello(name);&#125;public String reliable(String name) &#123; return &quot;hystrix fallback value&quot;;&#125; 八、Dubbo原理1、RPC原理 一次完整的RPC调用流程（同步调用，异步另说）如下： 1）服务消费方（client）调用以本地调用方式调用服务； 2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； 3）client stub找到服务地址，并将消息发送到服务端； 4）server stub收到消息后进行解码； 5）server stub根据解码结果调用本地的服务； 6）本地服务执行并将结果返回给server stub； 7）server stub将返回结果打包成消息并发送至消费方； 8）client stub接收到消息，并进行解码； 9）服务消费方得到最终结果。RPC框架的目标就是要2~8这些步骤都封装起来，这些细节对用户来说是透明的，不可见的。 2、netty通信原理Netty是一个异步事件驱动的网络应用程序框架， 用于快速开发可维护的高性能协议服务器和客户端。它极大地简化并简化了TCP和UDP套接字服务器等网络编程。 BIO：(Blocking IO) NIO (Non-Blocking IO) Selector 一般称 为选择器 ，也可以翻译为 多路复用器， Connect（连接就绪）、Accept（接受就绪）、Read（读就绪）、Write（写就绪） Netty基本原理： 3、dubbo原理1、dubbo原理 -框架设计[http://dubbo.apache.org/zh-cn/docs/dev/design.html]{.underline} config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 2、dubbo原理 -启动解析、加载配置信息我们知道spring加载配置文件，是通过BeanDefinitionparser这个接口的实现类进行绑定的 调用parse方法，解析标签 根据xml文件的每一行进行处理解析 不同的标签处理逻辑是不一样的 那么每一个标签对应的类是哪一个。这个是哪里定义的呢？ 通过DubboNamespaceHandler来定义标签对应的解析类 3、dubbo原理 -服务暴露通过上面的配置文件解析，我们知道服务相关的信息是通过解析后存放在ServiceBean中 public class ServiceBean&lt;T&gt; extends ServiceConfig&lt;T&gt; implements InitializingBean, DisposableBean, ApplicationContextAware, ApplicationListener&lt;ContextRefreshedEvent&gt;, BeanNameAware &#123; 关键的两个接口 InitializingBean Spring的接口,当组件创建完对象之后, 组件属性设置完成，会调用InitializingBean中的afterPropertiesSet方法 ApplicationListener应用监听器 监听IOC容器的刷新事件.当IOC容器中，所有对象都创建完成会回调onApplicationEvent方法 设置了延迟暴露，dubbo在Spring实例化bean（initializeBean）的时候会对实现了InitializingBean的类进行回调，回调方法是afterPropertySet() 没有设置延迟或者延迟为-1，dubbo会在Spring实例化完bean之后，在刷新容器最后一步发布ContextRefreshEvent事件的时候，通知实现了ApplicationListener的类进行回调onApplicationEvent 总的调用接口如下： 4、dubbo原理 -服务引用 5、dubbo原理 -服务调用","categories":[{"name":"dubbo","slug":"dubbo","permalink":"http://kingge.top/categories/dubbo/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"dubbo","slug":"dubbo","permalink":"http://kingge.top/tags/dubbo/"},{"name":"rpc","slug":"rpc","permalink":"http://kingge.top/tags/rpc/"}]},{"title":"hadoop在使用中的常用优化手段","slug":"hadoop在使用中的常用优化手段","date":"2018-05-14T13:59:59.000Z","updated":"2019-08-25T02:20:55.395Z","comments":true,"path":"2018/05/14/hadoop在使用中的常用优化手段/","link":"","permalink":"http://kingge.top/2018/05/14/hadoop在使用中的常用优化手段/","excerpt":"","text":"一、前言我们知道影响MapReduce运算的因素很多，主要是机器性能、网络、磁盘读写速度、I/O 操作等等有关。 机器的问题属于外部因素，那么下面主要是介绍关于IO操作引发的性能问题： 主要是有几个以下方面 （1）数据倾斜 - 重点 （2）map和reduce数设置不合理 （3）map运行时间太长，导致reduce等待过久 （4）小文件过多 - 重点 （5）大量的不可分块的超大文件 （6）spill次数过多 （7）merge次数过多。 ​ MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。 下面想讲解小文件的处理方式： 1.1 HDFS小文件优化​ HDFS上每个文件都要在namenode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大是的索引速度变慢。 解决方案1）Hadoop Archive: 是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了namenode的内存使用。 2）Sequence file： sequence file由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。 3）CombineFileInputFormat： CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。（之前hadoop相关的章节讲解道，可以翻翻看看） 4）开启JVM重用 对于大量小文件Job，可以开启JVM重用会减少45%运行时间。 JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他map。 具体设置：mapreduce.job.jvm.numtasks值在10-20之间。 1.2 分阶段优化数据输入阶段 （1）合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致mr运行较慢。 （2）采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 数据传输阶段1）采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器。 2）使用SequenceFile二进制文件。 进入Map阶段 1）减少溢写（spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘IO。 2）减少合并（merge）次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。 3）在map之后，不影响业务逻辑前提下，先进行combine处理，减少 I/O。 进入Reduce阶段 暂无 数据倾斜 暂无总结 常用参数哟花 暂无","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop优化","slug":"hadoop优化","permalink":"http://kingge.top/tags/hadoop优化/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(十二)-数据压缩","slug":"hadoop大数据-十二-数据压缩","date":"2018-03-20T14:59:59.000Z","updated":"2019-08-01T13:41:44.559Z","comments":true,"path":"2018/03/20/hadoop大数据-十二-数据压缩/","link":"","permalink":"http://kingge.top/2018/03/20/hadoop大数据-十二-数据压缩/","excerpt":"","text":"4.1 概述压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在Hadoop下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下，I/O操作和网络数据传输要花大量的时间。还有，Shuffle与Merge过程同样也面临着巨大的I/O压力。 ​ 鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。 ​ 如果磁盘I/O和网络带宽影响了MapReduce作业性能，在任意MapReduce阶段启用压缩都可以改善端到端处理时间并减少I/O和网络流量。 压缩Mapreduce的一种优化策略：通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。 注意：压缩特性运用得当能提高性能，但运用不当也可能降低性能。 基本原则： （1）运算密集型的job，少用压缩 （2）IO密集型的job，多用压缩 4.2 MR支持的压缩编码 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFAULT 是，直接使用 DEFAULT .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFAULT .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 4.3 压缩方式选择4.3.1 Gzip压缩优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。 4.3.2 Bzip2压缩优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。 缺点：压缩/解压速度慢；不支持native。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 4.3.3 Lzo压缩优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。 4.3.4 Snappy压缩优点：高速压缩速度和合理的压缩率。 缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装； 应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。 4.4 压缩位置选择​ 压缩可以在MapReduce作用的任意阶段启用。 4.5 压缩配置参数要在Hadoop中启用压缩，可以配置如下参数： 参数 默认值 阶段 建议 io.compression.codecs （在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress（在mapred-site.xml中配置） false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec mapper输出 使用LZO或snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置） false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress. DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置） RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 4.6 压缩实战4.6.1 数据流的压缩和解压缩​ CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据。要想对正在被写入一个输出流的数据进行压缩，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流。相反，要想对从输入流读取而来的数据进行解压缩，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据。 测试一下如下压缩方式： DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec package com.kingge.mapreduce.compress;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.CompressionCodecFactory;import org.apache.hadoop.io.compress.CompressionInputStream;import org.apache.hadoop.io.compress.CompressionOutputStream;import org.apache.hadoop.util.ReflectionUtils;public class TestCompress &#123; public static void main(String[] args) throws Exception &#123; compress(\"e:/hello.txt\",\"org.apache.hadoop.io.compress.BZip2Codec\");// decompress(\"e:/hello.txt.bz2\"); &#125; // 压缩 private static void compress(String filename, String method) throws Exception &#123; // 1 获取输入流 FileInputStream fis = new FileInputStream(new File(filename)); Class codecClass = Class.forName(method); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); // 2 获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename +codec.getDefaultExtension())); CompressionOutputStream cos = codec.createOutputStream(fos); // 3 流的对拷 IOUtils.copyBytes(fis, cos, 1024*1024*5, false); // 4 关闭资源 fis.close(); cos.close(); fos.close(); &#125; // 解压缩 private static void decompress(String filename) throws FileNotFoundException, IOException &#123; // 0 校验是否能解压缩 CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration()); CompressionCodec codec = factory.getCodec(new Path(filename)); if (codec == null) &#123; System.out.println(\"cannot find codec for file \" + filename); return; &#125; // 1 获取输入流 CompressionInputStream cis = codec.createInputStream(new FileInputStream(new File(filename))); // 2 获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename + \".decoded\")); // 3 流的对拷 IOUtils.copyBytes(cis, fos, 1024*1024*5, false); // 4 关闭资源 cis.close(); fos.close(); &#125;&#125; 4.6.2 Map输出端采用压缩​ 即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置： 1）给大家提供的hadoop源码支持的压缩格式有：BZip2Codec 、DefaultCodec package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.BZip2Codec; import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); // 开启map端输出压缩 configuration.setBoolean(&quot;mapreduce.map.output.compress&quot;, true); // 设置map端输出压缩方式 configuration.setClass(&quot;mapreduce.map.output.compress.codec&quot;, BZip2Codec.class, CompressionCodec.class); Job job = Job.getInstance(configuration); job.setJarByClass(WordCountDriver.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 1 : 0); &#125;&#125; 2）Mapper保持不变 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割 String[] words = line.split(&quot; &quot;); // 3 循环写出 for(String word:words)&#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; 3）Reducer保持不变 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; // 1 汇总 for(IntWritable value:values)&#123; count += value.get(); &#125; // 2 输出 context.write(key, new IntWritable(count)); &#125;&#125; 7.10.3 Reduce输出端采用压缩基于wordcount案例处理 1）修改驱动 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.BZip2Codec;import org.apache.hadoop.io.compress.DefaultCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.io.compress.Lz4Codec;import org.apache.hadoop.io.compress.SnappyCodec;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); job.setJarByClass(WordCountDriver.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 设置reduce端输出压缩开启 FileOutputFormat.setCompressOutput(job, true); // 设置压缩的方式 FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); // FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); // FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); boolean result = job.waitForCompletion(true); System.exit(result?1:0); &#125;&#125; 2）Mapper和Reducer保持不变（详见4.6.2）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(十一)-Mapreduce框架原理","slug":"hadoop大数据-十一-Mapreduce框架原理","date":"2018-03-18T10:59:59.000Z","updated":"2019-06-17T13:45:43.508Z","comments":true,"path":"2018/03/18/hadoop大数据-十一-Mapreduce框架原理/","link":"","permalink":"http://kingge.top/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/","excerpt":"","text":"三 MapReduce框架原理3.1 MapReduce工作流程1）流程示意图 2.Submit()方法包含在这里面– 然后接着是切片处理数据（128M为一片）。很明显图例200M的文件需要切成两片处理。分配两个map进行计算操作 3.正式提交任务到yarn上，包含一些job的相关信息。 4．MrAppMaster进行资源调度。根据片块数分配相应数量的MapTask（这里分配两个MapTask） 5.然后MapTask根据InputFormat去读取文本数据。一行一行的经过Mapper程序的map()方法进行计算操作，最后输出到分区中，并有序的存储。 6.等到所有MapTask计算完毕后。启动MrAppMaster启动相对应分区数量的reduce数量进行统计操作。最后生成多个分区对应的统计文件。输出。 2）流程详解 上面的流程是整个mapreduce最全工作流程，但是shuffle过程只是从第7步开始到第16步结束，具体shuffle过程详解，如下： 1）maptask收集我们的map()方法输出的kv对，放到内存缓冲区中 2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 3）多个溢出文件会被合并成大的溢出文件 4）在溢出过程中，及合并的过程中，都要调用partitioner进行分区和针对key进行排序 5）reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据 6）reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序） 7）合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法） 3）注意 Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。 缓冲区的大小可以通过参数调整，参数：io.sort.mb 默认100M。 3.2 InputFormat数据输入3.2.1 Job提交流程和切片源码详解1）job提交流程源码详解 waitForCompletion()submit();// 1建立连接-主要的工作是建立集群环境，以便运行Job任务。同时会根据Configuration配置信息来辨别当前job是需要在本地LocalRunner上运行还是在真实的yarn上运行。 connect(); // 1）创建提交job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交jobsubmitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2）获取jobid ，并创建job路径 JobID jobId = submitClient.getNewJobID(); // 3）拷贝jar包到集群 – 如果是在本地运行那么就不需要提交jar包，但是如果是在远程服务器上运行，那么就需要提交jar包，防止找不到copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir);// 4）计算切片，生成切片规划文件-默认是切一片，会去读取配置文件，获取自定义的最小切片数。切片数最大值也是有一个默认值，最大值是Long.MAX_VALUEwriteSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);// 5）向Stag路径写xml配置文件writeConf(conf, submitJobFile); conf.writeXml(out);// 6）提交job,返回提交状态status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); 2）FileInputFormat源码解析(input.getSplits(job)) （1）找到你数据存储的目录。 ​ （2）开始遍历处理（规划切片）目录下的每一个文件 ​ （3）遍历第一个文件ss.txt ​ a）获取文件大小fs.sizeOf(ss.txt); ​ b）计算切片大小computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M ​ c）默认情况下，切片大小=blocksize ​ d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） ​ e）将切片信息写到一个切片规划文件中 ​ f）整个切片的核心过程在getSplit()方法中完成。 ​ g）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。 ​ h）注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分。 ​ （4）提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。 23.2.2 FileInputFormat切片机制1）FileInputFormat中默认的切片机制： （1）简单地按照文件的内容长度进行切片 （2）切片大小，默认等于block大小 （3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片(他会遍历输入目录里面的文件，一个一个处理，debug查看FileInputFormat的getSplits方法可知) 比如待处理数据有两个文件： file1.txt 320M file2.txt 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下： file1.txt.split1-- 0~128file1.txt.split2-- 128~256file1.txt.split3-- 256~320file2.txt.split1-- 0~10M 2）FileInputFormat切片大小的参数配置 通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize)); 切片主要由这几个值来运算决定 mapreduce.input.fileinputformat.split.minsize=1 默认值为1 mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue 因此，默认情况下，切片大小=blocksize。 maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。 minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 3）获取切片信息API // 根据文件类型获取切片信息FileSplit inputSplit = (FileSplit) context.getInputSplit();// 获取切片的文件名称String name = inputSplit.getPath().getName(); 3.2.3 CombineTextInputFormat切片机制1）关于大量小文件的优化策略1）默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。 2）优化策略​ （1）最好的办法，在数据处理系统的最前端（预处理/采集），将小文件先合并成大文件，再上传到HDFS做后续分析。 ​ （2）补救措施：如果已经是大量小文件在HDFS中了，可以使用另一种InputFormat来做切片（CombineTextInputFormat），它的切片逻辑跟TextFileInputFormat不同：它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个maptask。 ​ （3）优先满足最小切片大小，不超过最大切片大小 ​ CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m ​ CombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m ​ 举例：0.5m+1m+0.3m+5m=2m + 4.8m=2m + 4m + 0.8m ​ 0.5+1+0.3 = 1.8没有满足最小切片大小，所以向5借0.2M,最后合并成2+4.8，但是4.8大于最大切片数，所以拆成4+0.8 ，所以这个四个小文件最后合并成三个文件 3）具体实现步骤注意CombineTextInputFormat的jar包是： // 如果不设置InputFormat,它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class)CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4mCombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m 4）案例​ 大量小文件的切片优化（CombineTextInputFormat）。 4.1 数据准备准备5个小文件（这里准备五个txt文本） 4.2 我们依旧使用我们上一个章节使用的统计文本中单词出现个数的代码代码详见 《hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节案例》 先不进行任何的改造操作，直接用着五个小文件当做输入，运行后查看日志。 （1）不做任何处理，运行需求1中的wordcount程序，观察切片个数为5 （2）在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为1 // 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4mCombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m 3.2.4 InputFormat接口实现类MapReduce任务的输入文件一般是存储在HDFS里面。输入的文件格式包括：基于行的日志文件、二进制格式文件等。这些文件一般会很大，达到数十GB，甚至更大。那么MapReduce是如何读取这些数据的呢？下面我们首先学习InputFormat接口。 InputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。 1）TextInputFormat TextInputFormat是默认的InputFormat。每条记录是一行输入。键是LongWritable类型，存储该行在整个文件中的字节偏移量。值是这行的内容，不包括任何行终止符（换行符和回车符）。 以下是一个示例，比如，一个分片包含了如下4条文本记录。 Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 每条记录表示为以下键/值对： (0,Rich learning form)(19,Intelligent learning engine)(47,Learning more convenient)(72,From the real demand for more close to the enterprise) 很明显，键并不是行号。一般情况下，很难取得行号，因为文件按字节而不是按行切分为分片。 2）KeyValueTextInputFormat 每一行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “ “);来设定分隔符。默认分隔符是tab（\\t）。 以下是一个示例，输入是一个包含4条记录的分片。其中——&gt;表示一个（水平方向的）制表符。 line1 ——&gt;Rich learning formline2 ——&gt;Intelligent learning engineline3 ——&gt;Learning more convenientline4 ——&gt;From the real demand for more close to the enterprise 每条记录表示为以下键/值对： (line1,Rich learning form)(line2,Intelligent learning engine)(line3,Learning more convenient)(line4,From the real demand for more close to the enterprise) 此时的键是每行排在制表符之前的Text序列。 3）NLineInputFormat 如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数/N=切片数，如果不整除，切片数=商+1。 以下是一个示例，仍然以上面的4行输入为例。 Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 例如，如果N是2，则每个输入分片包含两行。开启2个maptask。 (0,Rich learning form) (19,Intelligent learning engine) 另一个 mapper 则收到后两行： (47,Learning more convenient) (72,From the real demand for more close to the enterprise) ​ 这里的键和值与TextInputFormat生成的一样。 3.2.5 自定义InputFormat1）概述（1）自定义一个类继承FileInputFormat。 （2）改写RecordReader，实现一次读取一个完整文件封装为KV。 （3）在输出时使用SequenceFileOutPutFormat输出合并文件。 2）案例​ 无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。将多个小文件合并成一个文件SequenceFile，SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。 小文件的优化无非以下几种方式： （1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS （2）在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并 （3）在mapreduce处理时，可采用CombineTextInputFormat提高效率 2.1 数据准备准备三个文本文件。 aa.txt:包含以下内容yongpeng weidong weinansanfeng luozong xiaomingbb.txt:包含以下内容longlong fanfanmazong kailun yuhang yixinlonglong fanfanmazong kailun yuhang yixincc.txt:包含以下内容shuaige changmo zhenqiang dongli lingu xuanxuan 最终预期文件格式： part-r-00000 2.2 代码实现使用自定义InputFormat的方式，处理输入小文件的问题。 （1）自定义一个类继承FileInputFormat （2）改写RecordReader，实现一次读取一个完整文件封装为KV （3）在输出时使用SequenceFileOutPutFormat输出合并文件 （1）自定义InputFromatpackage com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;// 定义类继承FileInputFormatpublic class WholeFileInputformat extends FileInputFormat&lt;NullWritable, BytesWritable&gt;&#123; @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; （2）自定义RecordReader package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class WholeRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt;&#123; private Configuration configuration; private FileSplit split; private boolean processed = false; private BytesWritable value = new BytesWritable(); @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.split = (FileSplit)split; configuration = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; // 1 定义缓存区 byte[] contents = new byte[(int)split.getLength()]; FileSystem fs = null; FSDataInputStream fis = null; try &#123; // 2 获取文件系统 Path path = split.getPath(); fs = path.getFileSystem(configuration); // 3 读取数据 fis = fs.open(path); // 4 读取文件内容 IOUtils.readFully(fis, contents, 0, contents.length); // 5 输出文件内容 value.set(contents, 0, contents.length); &#125; catch (Exception e) &#123; &#125;finally &#123; IOUtils.closeStream(fis); &#125; processed = true; return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return processed? 1:0; &#125; @Override public void close() throws IOException &#123; &#125;&#125; （3）SequenceFileMapper处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class SequenceFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;&#123; Text k = new Text(); @Override protected void setup(Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 1 获取文件切片信息 FileSplit inputSplit = (FileSplit) context.getInputSplit(); // 2 获取切片名称 String name = inputSplit.getPath().toString(); // 3 设置key的输出 k.set(name); &#125; @Override protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(k, value); &#125;&#125; （4）SequenceFileReducer处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, values.iterator().next()); &#125;&#125; （5）SequenceFileDriver处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;public class SequenceFileDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[] &#123; &quot;e:/input/inputinputformat&quot;, &quot;e:/output1&quot; &#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SequenceFileDriver.class); job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 设置输入的inputFormat job.setInputFormatClass(WholeFileInputformat.class); // 设置输出的outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.3 MapTask工作机制3.3.1 并行度决定机制1）问题引出 maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度。那么，mapTask并行任务是否越多越好呢？ 2）MapTask并行度决定机制 ​ 一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。 3.3.2 MapTask工作机制 ​ （1）Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。 ​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。 ​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner—调用用户自定义getPartition方法），并写入一个环形内存缓冲区中。 ​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 ​ 溢写阶段详情： ​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 ​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 ​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 ​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 ​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 ​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 https://blog.csdn.net/qq_41455420/article/details/79288764 好的总结： 3.4 Shuffle机制3.4.1 Shuffle机制Mapreduce确保每个reducer的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reducer）称为shuffle。 3.4.2 Partition分区 分区的行为在每一次的map操作都会调用一或者多次 0）问题引出：要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区） 默认只输出到一个分区，也就是结果输出到一个文件 1）默认partition分区 public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; ​ 默认分区是根据key的hashCode对reduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。（numReduceTasks默认是1，也就是说，默认返回0，也就是只创建一个分区，所以是part-r-00000） 2）自定义Partitioner步骤 ​ （1）自定义类继承Partitioner，重写getPartition()方法 public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123;// 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; ​ （2）在job驱动中，设置自定义partitioner： ​ job.setPartitionerClass(CustomPartitioner.class); ​ （3）自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce task ​ job.setNumReduceTasks(5); 3）注意： 如果reduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx； 如果1&lt;reduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception； 如果reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000； ​ 例如：假设自定义分区数为5，则 （1）job.setNumReduceTasks(1);会正常运行，只不过会产生一个输出文件 （2）job.setNumReduceTasks(2);会报错 （3）job.setNumReduceTasks(6);大于5，程序会正常运行，会产生空文件 4）案例4.1 案例1​ 将统计结果按照手机归属地不同省份输出到不同文件中（分区） 1）数据准备 phone.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 2）分析 （1）Mapreduce中会将map输出的kv对，按照相同key分组，然后分发给不同的reducetask。默认的分发规则为：根据key的hashcode%reducetask数来分发 （2）如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner 自定义一个CustomPartitioner继承抽象类：Partitioner （3）在job驱动中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class) 3）在&lt;hadoop大数据(十)-Mapreduce基础 章节的2.6.2 案例&gt;的基础上，增加一个分区类 package com.kingge.mapreduce.flowsum;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;//他的key和value就是map输出的kvpublic class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 在驱动函数中增加自定义数据分区设置和reduce task设置 package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 8 指定自定义数据分区 job.setPartitionerClass(ProvincePartitioner.class); // 9 同时指定相应数量的reduce task job.setNumReduceTasks(5); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 4.2 案例2​ 把单词按照ASCII码奇偶分区（Partitioner），结合&lt;hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节–统计一堆文件中单词出现的个数&gt; 只需要在此代码的基础上，添加自定义分区 package com.kingge.mapreduce.wordcount;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class WordCountPartitioner extends Partitioner&lt;Text, IntWritable&gt;&#123; @Override public int getPartition(Text key, IntWritable value, int numPartitions) &#123; // 1 获取单词key String firWord = key.toString().substring(0, 1); char[] charArray = firWord.toCharArray(); int result = charArray[0]; // int result = key.toString().charAt(0); // 2 根据奇数偶数分区 if (result % 2 == 0) &#123; return 0; &#125;else &#123; return 1; &#125; &#125;&#125; 在驱动类中配置加载分区，设置reducetask个数 job.setPartitionerClass(WordCountPartitioner.class);job.setNumReduceTasks(2);//想分多少个区，这里必须开多少个reduce，否则默认只会生成一个分区，那么自定义分区失效 5）总结l 结果输出文件，跟分区数量和reduce数量有关系 l getPartition方法是在map调用之后才会进入，而且是每一次map可能会调用多次getPartition。为什么说是多次调用分区方法呢？我们知道每一次进入map方法都是一行数据（例如 hello.txt的第一行hello kingge），那么经过分割后生成两个单词，调用两次**context.write（）所以为了确定这两个单词所属那个分区，那么就需要调用两次getPartition。也就说在这个例子中，一次map调用处理完后需要调用两次getPartition。（即：context.write（）内部会进行分区） l 如果job.setNumReduceTasks(1)（也就是保持默认值），那么就是生成一个分区，不会进入自定义的分区方法。Redeucetask必须大于1，自定义分区方法才会生效。 3.4.3 WritableComparable排序排序是MapReduce框架中最重要的操作之一。Map Task和Reduce Task均会对数据（按照key）进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 ​ 对于Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行一次合并，以将这些文件合并成一个大的有序文件。 ​ 对于Reduce Task，它从每个Map Task上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上，否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task统一对内存和磁盘上的所有数据进行一次合并。 每个阶段的默认排序 1）排序的分类：​ （1）部分排序： MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。例如输出文件到五个分区，那么部分排序能够保证各个五个分区的数据都是有序的。 ​ （2）全排序： 如何用Hadoop产生一个全局排序的文件？最简单的方法是使用一个分区，那么这个分区里面的数据全局都是排序的。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。 ​ 替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为上述文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。这种方式可以达到全排序的功能 （3）辅助排序：（GroupingComparator分组） ​ Mapreduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。 ​ （4）二次排序： ​ 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 2）自定义排序WritableComparable（1）原理分析 bean对象实现WritableComparable接口重写compareTo方法，就可以实现排序 @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 3）案例3.1 案例1在&lt;hadoop大数据(十)-Mapreduce基础 章节的2.6.2 案例&gt;输出结果的基础上增加一个新的需求 根据2.6.2 案例输出的结果：再次对总流量进行排序 1）数据准备 phone.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 2）分析 ​ （1）把程序分两步走，第一步正常统计总流量，第二步再把结果进行排序 ​ （2）context.write(总流量，手机号) ​ （3）FlowBean实现WritableComparable接口重写compareTo方法 @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 3）代码实现 （1）FlowBean对象在在需求2.6.2基础上增加了比较功能（compareTo） package com.kingge.mapreduce.sort;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; private long upFlow; private long downFlow; private long sumFlow; // 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public void set(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; /** * 序列化方法 * @param out * @throws IOException */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; /** * 反序列化方法 注意反序列化的顺序和序列化的顺序完全一致 * @param in * @throws IOException */ @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong(); &#125; @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; &#125; @Override public int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1; &#125;&#125; （2）编写mapper package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;&#123; FlowBean bean = new FlowBean(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 String phoneNbr = fields[0]; long upFlow = Long.parseLong(fields[1]); long downFlow = Long.parseLong(fields[2]); bean.set(upFlow, downFlow); v.set(phoneNbr); // 4 输出 context.write(bean, v); &#125;&#125; （3）编写reducer package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt;&#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; // 循环输出，避免总流量相同情况 for (Text text : values) &#123; context.write(text, key); &#125; &#125;&#125; （4）编写driver package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowCountSortDriver &#123; public static void main(String[] args) throws ClassNotFoundException, IOException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCountSortDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.2 案例2改造案例1的需求 ​ 要求每个省份手机号输出的文件中按照总流量内部排序。（部分排序） 2）做法 在案例1的基础上增加自定义分区类即可。 package com.kingge.mapreduce.sort;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; &#123; @Override public int getPartition(FlowBean key, Text value, int numPartitions) &#123; // 1 获取手机号码前三位 String preNum = value.toString().substring(0, 3); int partition = 4; // 2 根据手机号归属地设置分区 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; （2）在驱动类中添加分区类 // 加载自定义分区类job.setPartitionerClass(FlowSortPartitioner.class);// 设置Reducetask个数 job.setNumReduceTasks(5); 3.4.4 GroupingComparator分组（辅助排序）1）对reduce阶段的数据根据某一个或几个字段进行分组。 2）案例 ​ 求出每一个订单中最贵的商品（GroupingComparator） 1）需求 有如下订单数据 订单id 商品id 成交金额 0000001 Pdt_01 222.8 0000001 Pdt_06 25.8 0000002 Pdt_03 522.8 0000002 Pdt_04 122.4 0000002 Pdt_05 722.4 0000003 Pdt_01 222.8 0000003 Pdt_02 33.8 现在需要求出每一个订单中最贵的商品。 2）输入数据 goods.txt 0000001 Pdt_01 222.80000002 Pdt_06 722.40000001 Pdt_05 25.80000003 Pdt_01 222.80000003 Pdt_01 33.80000002 Pdt_03 522.80000002 Pdt_04 122.4 输出数据预期： ​ 3 222.8 2 722.4 1 222.8 3）分析 （1）利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce。 （2）在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值。 4）代码实现 （1）定义订单信息OrderBean package com.kingge.mapreduce.order;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int order_id; // 订单id号 private double price; // 价格 public OrderBean() &#123; super(); &#125; public OrderBean(int order_id, double price) &#123; super(); this.order_id = order_id; this.price = price; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; order_id = in.readInt(); price = in.readDouble(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + price; &#125; public int getOrder_id() &#123; return order_id; &#125; public void setOrder_id(int order_id) &#123; this.order_id = order_id; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; // 二次排序 @Override public int compareTo(OrderBean o) &#123; int result; if (order_id &gt; o.getOrder_id()) &#123; result = 1; &#125; else if (order_id &lt; o.getOrder_id()) &#123; result = -1; &#125; else &#123; // 价格倒序排序 result = price &gt; o.getPrice() ? -1 : 1; &#125; return result; &#125;&#125; （2）编写OrderSortMapper package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; OrderBean k = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 k.setOrder_id(Integer.parseInt(fields[0])); k.setPrice(Double.parseDouble(fields[2])); // 4 写出 context.write(k, NullWritable.get()); &#125;&#125; （3）编写OrderSortPartitioner package com.kingge.mapreduce.order;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Partitioner;public class OrderPartitioner extends Partitioner&lt;OrderBean, NullWritable&gt; &#123; @Override public int getPartition(OrderBean key, NullWritable value, int numReduceTasks) &#123; return (key.getOrder_id() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; （4）编写OrderSortGroupingComparator package com.kingge.mapreduce.order;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; //可以查看super的源代码，true是必须要传的，否则汇报空指针，因为我们在下面的compare方法中使用了强转的操作，那么如果不注明比较的bean的类型，那么就会有问题。 super(OrderBean.class, true); &#125; @SuppressWarnings(&quot;rawtypes&quot;) @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123; result = -1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125; （5）编写OrderSortReducer package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125;&#125; （6）编写OrderSortDriver package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class OrderDriver &#123; public static void main(String[] args) throws Exception, IOException &#123; // 1 获取配置信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 设置jar包加载路径 job.setJarByClass(OrderDriver.class); // 3 加载map/reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4 设置map输出数据key和value类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5 设置最终输出数据的key和value类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); // 6 设置输入数据和输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 10 设置reduce端的分组 job.setGroupingComparatorClass(OrderGroupingComparator.class); // 7 设置分区 job.setPartitionerClass(OrderPartitioner.class); // 8 设置reduce个数 job.setNumReduceTasks(3); // 9 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;//如果不使用GroupingComparator方法，那么就无法实现功能，因为我们知道进入reduce的数据，他们key一定是一样的。那么上面的OrderBean作为key很明显是不一样的，就算order_id相同，但是他们的price不相同。那么GroupingComparator就可以帮我们做到，假设某个值是相同的，那么他就认为整个key是相同的。那么OrderBean作为key就可以分组处理也就是说，我们通过在GroupingComparator方法中指明了，相同key的规则，那么就可以实现进入reduce的数据的分组情况尖叫提示： Map阶段结束后，马上进入GroupingComparator方法，进行判断key的逻辑。每判断一次完后，就调用reduce一次。循环此操作直到数据统计结束。 在进入GroupingComparator之前，map阶段输出的数据，已经按照订单分区，分区内的价格也已经按照大到小排序。 3.4.5 Combiner合并1）combiner是MR程序中Mapper和Reducer之外的一种组件。 2）combiner组件的父类就是Reducer。 3）combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行; Reducer是接收全局所有Mapper的输出结果； 4）combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。 5）combiner能够应用的前提是不能影响最终的业务逻辑，而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来。 Mapper3 5 7 -&gt;(3+5+7)/3=5 2 6 -&gt;(2+6)/2=4Reducer(3+5+7+2+6)/5=23/5 不等于 (5+4)/2=9/2 很明显，combiner不适合做求平均值这样的操作。他适合做汇总这样的业务场景。 6）自定义Combiner实现步骤： （1）自定义一个combiner继承Reducer，重写reduce方法 public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1 汇总操作 int count = 0; for(IntWritable v :values)&#123; count = v.get(); &#125; // 2 写出 context.write(key, new IntWritable(count)); &#125;&#125; （2）在job驱动类中设置： job.setCombinerClass(WordcountCombiner.class); 7）案例 ​ 前提：结合&lt;hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节–统计一堆文件中单词出现的个数&gt; 代码 数据输入也是同上 ​ 需求：统计过程中对每一个maptask的输出进行局部汇总，以减小网络传输量即采用Combiner功能。 方案一 1）增加一个WordcountCombiner类继承Reducer package com.kingge.mr.combiner;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1 汇总 int count = 0; for(IntWritable v :values)&#123; count += v.get(); &#125; // 2 写出 context.write(key, new IntWritable(count)); &#125;&#125; // 9 指定需要使用combiner，以及用哪个类作为combiner的逻辑 job.setCombinerClass(WordcountCombiner.class); // 9 指定需要使用combiner，以及用哪个类作为combiner的逻辑job.setCombinerClass(WordcountCombiner.class); 方案二 1）将WordcountReducer作为combiner在WordcountDriver驱动类中指定 // 指定需要使用combiner，以及用哪个类作为combiner的逻辑job.setCombinerClass(WordcountReducer.class); 运行程序 总结自定义Combiner的调用时机：是在MapTask阶段的split溢写阶段，需要写入到磁盘的之前进行。将有相同 key 的 key/value 对的 value 加起来，减少溢写到磁盘的数据量。调用完后进入**reduce**方法 ​ 3.5 ReduceTask工作机制1）设置ReduceTask并行度（个数） reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置： //默认值是1，手动设置为4 job.setNumReduceTasks(4); 2）注意 （1）reducetask=0 ，表示没有reduce阶段，输出文件个数和map个数一致。 ​ 例子7.1.1 job.setNumReduceTasks(0); 输出 ​ 生成一个分区，但是分区内的单词没有汇总 ​ （2）reducetask默认值就是1，所以输出文件个数为一个。 （3）如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜（也就是说，相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况） （4）reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask。 （5）具体多少个reducetask，需要根据集群性能而定。 （6）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在maptask的源码中，执行分区的前提是先判断reduceNum个数是否大于1。不大于1肯定不执行。 3）实验：测试reducetask多少合适。 （1）实验环境：1个master节点，16个slave节点：CPU:8GHZ，内存: 2G （2）实验结论： ​ 表1 改变reduce task （数据量为1GB） Map task =16 Reduce task 1 5 10 15 16 20 25 30 45 60 总时间 892 146 110 92 88 100 128 101 145 104 4）ReduceTask工作机制 ​ （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 ​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 ​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。 ​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 3.6 OutputFormat数据输出3.6.1 OutputFormat接口实现类 OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。 1）文本输出TextOutputFormat ​ 默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串。 2）SequenceFileOutputFormat SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。 3）自定义OutputFormat ​ 根据用户需求，自定义实现输出。 3.6.2 自定义OutputFormat为了实现控制最终文件的输出路径，可以自定义OutputFormat。 要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现。 1）自定义OutputFormat步骤（1）自定义一个类继承FileOutputFormat。 （2）改写recordwriter，具体改写输出数据的方法write()。 2）案例​ 修改日志内容及自定义日志输出路径（自定义OutputFormat）。 1）需求 ​ 过滤输入的log日志中是否包含kingge ​ （1）包含kingge的网站输出到e:/kingge.log ​ （2）不包含kingge的网站输出到e:/other.log 2）输入数据（pp.txt） http://www.baidu.comhttp://www.google.comhttp://cn.bing.comhttp://www.kingge.comhttp://www.sohu.comhttp://www.sina.comhttp://www.sin2a.comhttp://www.sin2desa.comhttp://www.sindsafa.com 输出预期： kingge.log文件包含： http://www.kingge.com other.log文件包含： http://cn.bing.comhttp://www.baidu.comhttp://www.google.comhttp://www.sin2a.comhttp://www.sin2desa.comhttp://www.sina.comhttp://www.sindsafa.comhttp://www.sohu.com 3）代码实现： （1）自定义一个outputformat package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; // 创建一个RecordWriter return new FilterRecordWriter(job); &#125;&#125; （2）具体的写数据RecordWriter package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream kinggeOut = null; FSDataOutputStream otherOut = null; public FilterRecordWriter(TaskAttemptContext job) &#123; // 1 获取文件系统 FileSystem fs; try &#123; fs = FileSystem.get(job.getConfiguration()); // 2 创建输出文件路径 Path kinggePath = new Path(&quot;e:/kingge.log&quot;); Path otherPath = new Path(&quot;e:/other.log&quot;); // 3 创建输出流 kinggeOut = fs.create(kinggePath); otherOut = fs.create(otherPath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断是否包含“kingge”输出到不同文件 if (key.toString().contains(&quot;kingge&quot;)) &#123; kinggeOut.write(key.toString().getBytes()); &#125; else &#123; otherOut.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; // 关闭资源 if (kinggeOut != null) &#123; kinggeOut.close(); &#125; if (otherOut != null) &#123; otherOut.close(); &#125; &#125;&#125; （3）编写FilterMapper package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FilterMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); k.set(line); // 3 写出 context.write(k, NullWritable.get()); &#125;&#125; （4）编写FilterReducer package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FilterReducer extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; String k = key.toString(); k = k + &quot;\\r\\n&quot;; context.write(new Text(k), NullWritable.get()); &#125;&#125; （5）编写FilterDriver package com.kingge.mapreduce.outputformat;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FilterDriver &#123; public static void main(String[] args) throws Exception &#123;args = new String[] &#123; &quot;e:/input/inputoutputformat&quot;, &quot;e:/output2&quot; &#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FilterDriver.class); job.setMapperClass(FilterMapper.class); job.setReducerClass(FilterReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要将自定义的输出格式组件设置到job中 job.setOutputFormatClass(FilterOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); // 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat // 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.7 Join多种应用3.7.1 Reduce join1）原理： Map端的主要工作：为来自不同表(文件)的key/value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。 Reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行合并就ok了。 2）该方法的缺点 这种方式的缺点很明显就是会造成map和reduce端也就是shuffle阶段出现大量的数据传输，效率很低。 3）案例​ reduce端表合并（数据倾斜） 通过将关联条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reducetask，在reduce中进行数据的串联。 1）代码实现 ​ 1.1 创建商品和订合并后的bean类 package com.kingge.mapreduce.table;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class TableBean implements Writable &#123; private String order_id; // 订单id private String p_id; // 产品id private int amount; // 产品数量 private String pname; // 产品名称 private String flag;// 表的标记 public TableBean() &#123; super(); &#125; public TableBean(String order_id, String p_id, int amount, String pname, String flag) &#123; super(); this.order_id = order_id; this.p_id = p_id; this.amount = amount; this.pname = pname; this.flag = flag; &#125; public String getFlag() &#123; return flag; &#125; public void setFlag(String flag) &#123; this.flag = flag; &#125; public String getOrder_id() &#123; return order_id; &#125; public void setOrder_id(String order_id) &#123; this.order_id = order_id; &#125; public String getP_id() &#123; return p_id; &#125; public void setP_id(String p_id) &#123; this.p_id = p_id; &#125; public int getAmount() &#123; return amount; &#125; public void setAmount(int amount) &#123; this.amount = amount; &#125; public String getPname() &#123; return pname; &#125; public void setPname(String pname) &#123; this.pname = pname; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(order_id); out.writeUTF(p_id); out.writeInt(amount); out.writeUTF(pname); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.order_id = in.readUTF(); this.p_id = in.readUTF(); this.amount = in.readInt(); this.pname = in.readUTF(); this.flag = in.readUTF(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + pname + &quot;\\t&quot; + amount + &quot;\\t&quot; ; &#125;&#125; 2）编写TableMapper程序 package com.kingge.mapreduce.table;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt;&#123; TableBean bean = new TableBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取输入文件类型 FileSplit split = (FileSplit) context.getInputSplit(); String name = split.getPath().getName(); // 2 获取输入数据 String line = value.toString(); // 3 不同文件分别处理 if (name.startsWith(&quot;order&quot;)) &#123;// 订单表处理 // 3.1 切割 String[] fields = line.split(&quot;\\t&quot;); // 3.2 封装bean对象 bean.setOrder_id(fields[0]); bean.setP_id(fields[1]); bean.setAmount(Integer.parseInt(fields[2])); bean.setPname(&quot;&quot;); bean.setFlag(&quot;0&quot;); k.set(fields[1]); &#125;else &#123;// 产品表处理 // 3.3 切割 String[] fields = line.split(&quot;\\t&quot;); // 3.4 封装bean对象 bean.setP_id(fields[0]); bean.setPname(fields[1]); bean.setFlag(&quot;1&quot;); bean.setAmount(0); bean.setOrder_id(&quot;&quot;); k.set(fields[0]); &#125; // 4 写出 context.write(k, bean); &#125;&#125; 3）编写TableReducer程序 package com.kingge.mapreduce.table;import java.io.IOException;import java.util.ArrayList;import org.apache.commons.beanutils.BeanUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 1准备存储订单的集合 ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;(); // 2 准备bean对象 TableBean pdBean = new TableBean(); for (TableBean bean : values) &#123; if (&quot;0&quot;.equals(bean.getFlag())) &#123;// 订单表 // 拷贝传递过来的每条订单数据到集合中 TableBean orderBean = new TableBean(); try &#123; BeanUtils.copyProperties(orderBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; orderBeans.add(orderBean); &#125; else &#123;// 产品表 try &#123; // 拷贝传递过来的产品表到内存中 BeanUtils.copyProperties(pdBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 3 表的拼接 for(TableBean bean:orderBeans)&#123; bean.setPname (pdBean.getPname()); // 4 数据写出去 context.write(bean, NullWritable.get()); &#125; &#125;&#125; 4）编写TableDriver程序 package com.kingge.mapreduce.table;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 指定本程序的jar包所在的本地路径 job.setJarByClass(TableDriver.class); // 3 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 4 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 5 指定最终输出的数据的kv类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 6 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3）运行程序查看结果 1001 小米 1 1001 小米 1 1002 华为 2 1002 华为 2 1003 格力 3 1003 格力 3 缺点：这种方式中，合并的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜 解决方案： map端实现数据合并 3.7.2 Map join（Distributedcache分布式缓存）1）使用场景：一张表十分小、一张表很大。 2）解决方案 在map端缓存多张表，提前处理业务逻辑，这样增加map端业务，减少reduce端数据的压力，尽可能的减少数据倾斜。 3）具体办法：采用distributedcache ​ （1）在mapper的setup阶段，将文件读取到缓存集合中。 ​ （2）在驱动函数中加载缓存。 job.addCacheFile(new URI(“file:/e:/mapjoincache/pd.txt”));// 缓存普通文件到task运行节点 4）案例：​ map端表合并（Distributedcache） - 结合上个案例代码（3.7.1 3 案例） 1）分析 适用于关联表中有小表的情形； 可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行合并并输出最终结果，可以大大提高合并操作的并发度，加快处理速度。 2）实操案例 （1）先在驱动模块中添加缓存文件 package test;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class DistributedCacheDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取job信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置加载jar包路径 job.setJarByClass(DistributedCacheDriver.class); // 3 关联map job.setMapperClass(DistributedCacheMapper.class); // 4 设置最终输出数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5 设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 加载缓存数据 job.addCacheFile(new URI(&quot;file:///e:/inputcache/pd.txt&quot;)); // 7 map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); // 8 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; （2）读取缓存的文件数据 package test;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(); @Override protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 1 获取缓存的文件 BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;pd.txt&quot;),&quot;UTF-8&quot;)); String line; while(StringUtils.isNotEmpty(line = reader.readLine()))&#123; // 2 切割 String[] fields = line.split(&quot;\\t&quot;); // 3 缓存数据到集合 pdMap.put(fields[0], fields[1]); &#125; // 4 关流 reader.close(); &#125; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 获取产品id String pId = fields[1]; // 4 获取商品名称 String pdName = pdMap.get(pId); // 5 拼接 k.set(line + &quot;\\t&quot;+ pdName); // 6 写出 context.write(k, NullWritable.get()); &#125;&#125; 3.8 数据清洗（ETL）1）概述 在运行核心业务Mapreduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行mapper程序，不需要运行reduce程序。 2）案例日志清洗（数据清洗）。 简单解析版1）需求： 去除日志中字段长度小于等于11的日志。 2）输入数据 里面的内容就是我们平时网站输出的日志。例如： 194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;101.226.68.137 - - [18/Sep/2013:06:49:45 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;60.208.6.156 - - [18/Sep/2013:06:49:48 +0000] &quot;GET /wp-content/uploads/2013/07/rcassandra.png HTTP/1.0&quot; 200 185524 &quot;http://cos.name/category/software/packages/&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] &quot;GET /images/my.jpg HTTP/1.1&quot; 200 19939 &quot;http://www.angularjs.cn/A00n&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;222.68.172.190 - - [18/Sep/2013:06:50:08 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot; 3）实现代码： （1）编写LogMapper package com.kingge.mapreduce.weblog;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取1行数据 String line = value.toString(); // 2 解析日志 boolean result = parseLog(line,context); // 3 日志不合法退出 if (!result) &#123; return; &#125; // 4 设置key k.set(line); // 5 写出数据 context.write(k, NullWritable.get()); &#125; // 2 解析日志 private boolean parseLog(String line, Context context) &#123; // 1 截取 String[] fields = line.split(&quot; &quot;); // 2 日志长度大于11的为合法 if (fields.length &gt; 11) &#123; // 系统计数器 context.getCounter(&quot;map&quot;, &quot;true&quot;).increment(1); return true; &#125;else &#123; context.getCounter(&quot;map&quot;, &quot;false&quot;).increment(1); return false; &#125; &#125;&#125; （2）编写LogDriver package com.kingge.mapreduce.weblog;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class LogDriver &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; &quot;e:/input/inputlog&quot;, &quot;e:/output1&quot; &#125;; // 1 获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 加载jar包 job.setJarByClass(LogDriver.class); // 3 关联map job.setMapperClass(LogMapper.class); // 4 设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置reducetask个数为0 job.setNumReduceTasks(0); // 5 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 提交 job.waitForCompletion(true); &#125;&#125; 复杂解析版1）需求： 对web访问日志中的各字段识别切分 去除日志中不合法的记录 根据统计需求，生成各类访问请求过滤数据 2）输入数据 输入同上一个案例 3）实现代码： （1）定义一个bean，用来记录日志数据中的各数据字段 package com.kingge.mapreduce.log;public class LogBean &#123; private String remote_addr;// 记录客户端的ip地址 private String remote_user;// 记录客户端用户名称,忽略属性&quot;-&quot; private String time_local;// 记录访问时间与时区 private String request;// 记录请求的url与http协议 private String status;// 记录请求状态；成功是200 private String body_bytes_sent;// 记录发送给客户端文件主体内容大小 private String http_referer;// 用来记录从那个页面链接访问过来的 private String http_user_agent;// 记录客户浏览器的相关信息 private boolean valid = true;// 判断数据是否合法 public String getRemote_addr() &#123; return remote_addr; &#125; public void setRemote_addr(String remote_addr) &#123; this.remote_addr = remote_addr; &#125; public String getRemote_user() &#123; return remote_user; &#125; public void setRemote_user(String remote_user) &#123; this.remote_user = remote_user; &#125; public String getTime_local() &#123; return time_local; &#125; public void setTime_local(String time_local) &#123; this.time_local = time_local; &#125; public String getRequest() &#123; return request; &#125; public void setRequest(String request) &#123; this.request = request; &#125; public String getStatus() &#123; return status; &#125; public void setStatus(String status) &#123; this.status = status; &#125; public String getBody_bytes_sent() &#123; return body_bytes_sent; &#125; public void setBody_bytes_sent(String body_bytes_sent) &#123; this.body_bytes_sent = body_bytes_sent; &#125; public String getHttp_referer() &#123; return http_referer; &#125; public void setHttp_referer(String http_referer) &#123; this.http_referer = http_referer; &#125; public String getHttp_user_agent() &#123; return http_user_agent; &#125; public void setHttp_user_agent(String http_user_agent) &#123; this.http_user_agent = http_user_agent; &#125; public boolean isValid() &#123; return valid; &#125; public void setValid(boolean valid) &#123; this.valid = valid; &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append(this.valid); sb.append(&quot;\\001&quot;).append(this.remote_addr); sb.append(&quot;\\001&quot;).append(this.remote_user); sb.append(&quot;\\001&quot;).append(this.time_local); sb.append(&quot;\\001&quot;).append(this.request); sb.append(&quot;\\001&quot;).append(this.status); sb.append(&quot;\\001&quot;).append(this.body_bytes_sent); sb.append(&quot;\\001&quot;).append(this.http_referer); sb.append(&quot;\\001&quot;).append(this.http_user_agent); return sb.toString(); &#125;&#125; （2）编写LogMapper程序 package com.kingge.mapreduce.log;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取1行 String line = value.toString(); // 2 解析日志是否合法 LogBean bean = pressLog(line); if (!bean.isValid()) &#123; return; &#125; k.set(bean.toString()); // 3 输出 context.write(k, NullWritable.get()); &#125; // 解析日志 private LogBean pressLog(String line) &#123; LogBean logBean = new LogBean(); // 1 截取 String[] fields = line.split(&quot; &quot;); if (fields.length &gt; 11) &#123; // 2封装数据 logBean.setRemote_addr(fields[0]); logBean.setRemote_user(fields[1]); logBean.setTime_local(fields[3].substring(1)); logBean.setRequest(fields[6]); logBean.setStatus(fields[8]); logBean.setBody_bytes_sent(fields[9]); logBean.setHttp_referer(fields[10]); if (fields.length &gt; 12) &#123; logBean.setHttp_user_agent(fields[11] + &quot; &quot;+ fields[12]); &#125;else &#123; logBean.setHttp_user_agent(fields[11]); &#125; // 大于400，HTTP错误 if (Integer.parseInt(logBean.getStatus()) &gt;= 400) &#123; logBean.setValid(false); &#125; &#125;else &#123; logBean.setValid(false); &#125; return logBean; &#125;&#125; （3）编写LogDriver程序 package com.kingge.mapreduce.log;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class LogDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 加载jar包 job.setJarByClass(LogDriver.class); // 3 关联map job.setMapperClass(LogMapper.class); // 4 设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 提交 job.waitForCompletion(true); &#125;&#125; 3.9 计数器应用​ Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。 1）API ​ （1）采用枚举的方式统计计数 enum MyCounter{MALFORORMED,NORMAL} //对枚举定义的自定义计数器加1 context.getCounter(MyCounter.MALFORORMED).increment(1); （2）采用计数器组、计数器名称的方式统计 context.getCounter(“counterGroup”, “countera”).increment(1); ​ 组名和计数器名称随便起，但最好有意义。 ​ （3）计数结果在程序运行后的控制台上查看。 2）案例 ​ 数据清洗的两个案例 3.10 MapReduce开发总结在编写mapreduce程序时，需要考虑的几个方面： 1）输入数据接口：InputFormat 默认使用的实现类是：TextInputFormat TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。 KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\\t）。 NlineInputFormat按照指定的行数N来划分切片。 CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。 用户还可以自定义InputFormat。 2）逻辑处理接口：Mapper 用户根据业务需求实现其中三个方法：map() setup() cleanup () 3）Partitioner分区 ​ 有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces ​ 如果业务上有特别的需求，可以自定义分区。 4）Comparable排序 ​ 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。 ​ 部分排序：对最终输出的每一个文件进行内部排序。 ​ 全排序：对所有数据进行排序，通常只有一个Reduce。 ​ 二次排序：排序的条件有两个。 5）Combiner合并 Combiner合并可以提高程序执行效率，减少io传输。但是使用时必须不能影响原有的业务处理结果。 6）reduce端分组：Groupingcomparator ​ reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数。 ​ 利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑。 ​ 自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果。然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）。这样，我们要取的最大值就是reduce()方法中传进来key。 7）逻辑处理接口：Reducer ​ 用户根据业务需求实现其中三个方法：reduce() setup() cleanup () 8）输出数据接口：OutputFormat ​ 默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对向目标文本文件中输出为一行。 SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。 用户还可以自定义OutputFormat。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(十)-Mapreduce基础","slug":"hadoop大数据-十-Mapreduce基础","date":"2018-03-16T11:59:59.000Z","updated":"2019-06-17T12:46:35.921Z","comments":true,"path":"2018/03/16/hadoop大数据-十-Mapreduce基础/","link":"","permalink":"http://kingge.top/2018/03/16/hadoop大数据-十-Mapreduce基础/","excerpt":"","text":"一 MapReduce入门1.1 MapReduce定义Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。 Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 1.2 MapReduce优缺点1.2.1 优点1**）MapReduce 易于编程。**它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 2**）良好的扩展性。**当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 3**）高容错性。**MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。 4**）适合PB**级以上海量数据的离线处理（他跟其他的分布式运行框架不同，例如spark等等）。这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。 1.2.2 缺点MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。 1）实时计算。MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 2）流式计算。流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 3）DAG（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 1.3 MapReduce核心思想 下面根据一个小小的案例来体现 mapreduce的运转流程。 根据块大小（128M）进行分片运算，每个maptask负责处理自己所属的块数据，把每个单词出现个数计算统计然后放到hashmap（实际上是放到磁盘上）中，key是单词，value是单词出现次数。 1）分布式的运算程序往往需要分成至少2个阶段。（map阶段和reduce阶段） 2）第一个阶段的maptask并发实例，完全并行运行，互不相干。 3）第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。 4）MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。 1.4 MapReduce进程一个完整的mapreduce程序在分布式运行时有三类实例进程： 1）MrAppMaster：负责整个程序的过程调度及状态协调。 2）MapTask：负责map阶段的整个数据处理流程。 3）ReduceTask：负责reduce阶段的整个数据处理流程。 1.5 MapReduce编程规范用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) 1）Mapper阶段​ （1）用户自定义的Mapper要继承自己的父类 ​ （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） ​ （3）Mapper中的业务逻辑写在map()方法中 ​ （4）Mapper的输出数据是KV对的形式（KV的类型可自定义） ​ （5）map()方法（maptask进程）对每一个调用一次 2）Reducer阶段​ （1）用户自定义的Reducer要继承自己的父类 ​ （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV ​ （3）Reducer的业务逻辑写在reduce()方法中 ​ （4）Reducetask进程对每一组相同k的组调用一次reduce()方法 3）Driver阶段整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象 4）案例​ 统计一堆文件中单词出现的个数（WordCount案例）。 在一堆给定的文本文件中统计输出每一个单词出现的总次数 1.数据准备 anly.text 包涵一下数据。hello worldkingge kinggehadoop sparkhello worldkingge kinggehadoop sparkhello worldhadoop spark 2.按照mapreduce编程规范，分别编写Mapper，Reducer，Driver。 简单案例分析 3.书写java代码（1）编写mapper类package com.kingge.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;//四个参数：前两个是map的输入参数类型，后两个数输出参数类型//很明显，执行一个map，数据的key值是long类型代表着数据所属的行号，那么value值就是string类型，对应Hadoop的序列化类型是text.//输出的结果是，每个单词对应的个数。那么输出的key应该是Text,代表单词,value应该是Int类型，代表这个单词的个数public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行-因为map是一行一行进行处理的 String line = value.toString(); // 2 切割 String[] words = line.split(&quot; &quot;); // 3 输出 for (String word : words) &#123; k.set(word); context.write(k, v); &#125; &#125;&#125; （2）编写reducer类package com.kingge.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; value, Context context) throws IOException, InterruptedException &#123; // 1 累加求和 int sum = 0; for (IntWritable count : value) &#123; sum += count.get(); &#125; // 2 输出 context.write(key, new IntWritable(sum)); &#125;&#125;执行到reduce阶段，那么经过map的计算和排序，最终会形成了一组一组的相同key的KV键值对（key group）。然后相同组的会进行reduce统计。一组接着一组进行计算。并不是所有组都通过reduce。//例如假设最终返回的KV值是：//hello 1//hello 1//word 1//word 1 那么 前两个hello为一组，经过reduce运算，然后返回，同时word为一组也经过统计返回。这两组并不会都由同一个reduce处理 （3）编写驱动类package com.kingge.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 设置map和Reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置Reduce输出 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 4）集群上测试 （1）将程序打成jar包，然后拷贝到hadoop集群中。 （2）启动hadoop集群 （3）执行wordcount程序 [kingge@hadoop102 software]$ hadoop jar wc.jar com.kingge.wordcount.WordcountDriver /user/kingge/input /user/kingge/output1 5）本地测试 （1）在windows环境上配置HADOOP_HOME环境变量。 （2）在eclipse上运行程序 （3）注意：如果eclipse打印不出日志，在控制台上只显示 1.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). 2.log4j:WARN Please initialize the log4j system properly. 3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 需要在项目的src目录下，新建一个文件，命名为“log4j.properties”，在文件中填入 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 经过debug发现，只有当map处理完所有数据，才会进入reduce，map处理数据是一行一行进行处理的，每一行数据的处理都会经过一次map方法，直到所有数据处理完毕。Map处理完所有数据后，会排序所有的key，进行分组。然后一组一组的经过reduce，进行统计操作。直到所有组统计完毕，然后输出数据。 二 Hadoop序列化2.1 为什么要序列化？​ 一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。 2.2 什么是序列化？序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。 反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。 2.3 为什么不用Java的序列化？​ Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效。 2.4 为什么序列化对Hadoop很重要？​ 因为Hadoop在集群之间进行通讯或者RPC调用的时候，需要序列化，而且要求序列化要快，且体积要小，占用带宽要小。所以必须理解Hadoop的序列化机制。 ​ 序列化和反序列化在分布式数据处理领域经常出现：进程通信和永久存储。然而Hadoop中各个节点的通信是通过远程调用（RPC）实现的，那么RPC序列化要求具有以下特点： 1）紧凑：紧凑的格式能让我们充分利用网络带宽，而带宽是数据中心最稀缺的资源 2）快速：进程通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本的； 3）可扩展：协议为了满足新的需求变化，所以控制客户端和服务器过程中，需要直接引进相应的协议，这些是新协议，原序列化方式能支持新的协议报文； 4）互操作：能支持不同语言写的客户端和服务端进行交互； 2.5 常用数据序列化类型常用的数据类型对应的hadoop数据序列化类型 Java**类型** Hadoop Writable**类型** boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable string Text map MapWritable array ArrayWritable 2.6 自定义bean对象实现序列化接口（Writable）1）自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项。 （1）必须实现Writable接口 （2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造 ​ public FlowBean() { super(); } （3）重写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; （4）重写反序列化方法 ​ @Overridepublic void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong();&#125; （5）注意反序列化的顺序和序列化的顺序完全一致 （6）要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。 （7）如果需要将自定义的bean放在key中传输，则还需要实现WritableComparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序。 ​ 《自定义的bean放在key中传输》是什么意思呢？因为我们知道map操作中输入数据的存储结构是-key-value的形式.上面的例子中统计文本单词数，那么文本文件中每一行的文本的序号就是key（0,1,2,3）每一行的文本，就是value的值。Map操作完后输出的数据结构也是key-value的形式。而且输出的数据会根据key排序，以便reduce处理。那么怎么排序在hadoop中有一个默认规则（如果key是2.5中的常用数据类型），如果使我们自定义的序列化数据类型作为key。那么默认排序规则就会失效，那么就需要我们制定一个排序规则就需要覆盖compareTo方法。** ​ @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 2）案例​ 每一个手机号耗费的总上行流量、下行流量、总流量（序列化）。 2.1 数据准备pd.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13560436666 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 输入数据格式： 输出数据格式 2.2 分析基本思路： Map阶段： （1）读取一行数据，切分字段 （2）抽取手机号、上行流量、下行流量 （3）以手机号为key，bean对象为value输出，即context.write(手机号,bean); Reduce阶段： （1）累加上行流量和下行流量得到总流量。 （2）实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输 （3）MR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key 所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable。然后重写key的compareTo方法。 2.3 编写mapreduce程序（1）编写流量统计的bean对象 package com.kingge.mapreduce.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;// 1 实现writable接口public class FlowBean implements Writable&#123; private long upFlow ; private long downFlow; private long sumFlow; //2 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; //3 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; //4 反序列化方法 //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 6 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125; （2）编写mapper package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); v.set(downFlow, upFlow); // 4 写出 context.write(new Text(phoneNum), new FlowBean(upFlow, downFlow)); &#125;&#125; （3）编写reducer package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_downFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sum_upFlow += flowBean.getSumFlow(); sum_downFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow); // 3 写出 context.write(key, resultBean); &#125;&#125; （4）编写驱动 package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(九)-yarn","slug":"hadoop大数据-九-yarn","date":"2018-03-14T14:59:59.000Z","updated":"2019-08-01T13:33:46.678Z","comments":true,"path":"2018/03/14/hadoop大数据-九-yarn/","link":"","permalink":"http://kingge.top/2018/03/14/hadoop大数据-九-yarn/","excerpt":"","text":"5.1 Hadoop1.x和Hadoop2.x架构区别在Hadoop1.x时代，Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。 ResourceManagement 资源管理 JobScheduling/JobMonitoring 任务调度监控 在Hadoop2.x时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。这样就能够各司其职 ResourceManger ApplicationMaster ​ 需要注意的是，在Yarn中我们把job的概念换成了application，因为在新的Hadoop2.x中，运行的应用不只是MapReduce了，还有可能是其它应用如一个DAG（有向无环图Directed Acyclic Graph，例如storm应用）。Yarn的另一个目标就是拓展Hadoop，使得它不仅仅可以支持MapReduce计算，还能很方便的管理诸如Hive、Hbase、Pig、Spark/Shark等应用。这种新的架构设计能够使得各种类型的应用运行在Hadoop上面，并通过Yarn从系统层面进行统一的管理，也就是说，有了Yarn，各种应用就可以互不干扰的运行在同一个Hadoop系统中，共享整个集群资源。 5.2 Yarn概述Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 5.3 Yarn基本架构​ YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。 5.4 Yarn工作机制1）Yarn运行机制 2）工作机制详解 ​ （0）Mr程序提交到客户端所在的节点。 ​ （1）Yarnrunner向Resourcemanager申请一个Application。 ​ （2）rm将该应用程序的资源路径返回给yarnrunner。 ​ （3）该程序将运行所需资源提交到HDFS上。 ​ （4）程序资源提交完毕后，申请运行mrAppMaster。 ​ （5）RM将用户的请求初始化成一个task。 ​ （6）其中一个NodeManager领取到task任务。 ​ （7）该NodeManager创建容器Container，并产生MRAppmaster。 ​ （8）Container从HDFS上拷贝资源到本地。 ​ （9）MRAppmaster向RM 申请运行maptask资源。 ​ （10）RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 ​ （11）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。 （12）MrAppMaster等待所有maptask运行完毕后，向RM申请容器，运行reduce task。 ​ （13）reduce task向maptask获取相应分区的数据。 ​ （14）程序运行完毕后，MR会向RM申请注销自己。 5.5 作业提交全过程1）作业提交过程之YARN 作业提交全过程详解 （1）作业提交第0步：client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。 第1步：client向RM申请一个作业id。 第2步：RM给client返回该job资源的提交路径和作业id。 第3步：client提交jar包、切片信息和配置文件到指定的资源提交路径。 第4步：client提交完资源后，向RM申请运行MrAppMaster。 （2）作业初始化第5步：当RM收到client的请求后，将该job添加到容量调度器中。 第6步：某一个空闲的NM领取到该job。 第7步：该NM创建Container，并产生MRAppmaster。 第8步：下载client提交的资源到本地。 （3）任务分配第9步：MrAppMaster向RM申请运行多个maptask任务资源。 第10步：RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 （4）任务运行第11步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。 第12步：MrAppMaster等待所有maptask运行完毕后，向RM申请容器，运行reduce task。 第13步：reduce task向maptask获取相应分区的数据。 第14步：程序运行完毕后，MR会向RM申请注销自己。 （5）进度和状态更新YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。 （6）作业完成除了向应用管理器请求作业进度外, 客户端每5分钟都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。 2）作业提交过程之MapReduce 3）作业提交过程之读数据 4）作业提交过程之写数据 5.6 资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2默认的资源调度器是Capacity Scheduler。 具体设置详见：yarn-default.xml文件 The class to use as the resource scheduler. yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler &lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 1）先进先出调度器（FIFO） 2）容量调度器（Capacity Scheduler） 3）公平调度器（Fair Scheduler） 5.7 任务的推测执行1）作业完成时间取决于最慢的任务完成时间 一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。 典型案例：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？ 2）推测执行机制： 发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。 3）执行推测任务的前提条件 （1）每个task只能有一个备份任务； （2）当前job已完成的task必须不小于0.05（5%） （3）开启推测执行参数设置。Hadoop2.7.2 mapred-site.xml文件中默认是打开的。 &lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; mapreduce.map.speculative true If true, then multiple instances of some map tasks may be executed in parallel. mapreduce.reduce.speculative true If true, then multiple instances of some reduce tasks may be executed in parallel. 4）不能启用推测执行机制情况 （1）任务间存在严重的负载倾斜； （2）特殊任务，比如任务向数据库中写数据。 5）算法原理：","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"yarn","slug":"yarn","permalink":"http://kingge.top/tags/yarn/"}]},{"title":"hadoop大数据(八)-namenode和resourcemanager高可用","slug":"hadoop大数据-八-namenode和resourcemanager高可用","date":"2018-03-12T12:59:59.000Z","updated":"2019-06-10T13:20:52.956Z","comments":true,"path":"2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/","link":"","permalink":"http://kingge.top/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/","excerpt":"","text":"HDFS 高可用高可用概述1）所谓HA（high available），即高可用（7*24小时不中断服务）。 2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。 3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。 4）NameNode主要在以下两个方面影响HDFS集群 ​ NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启 ​ NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用 HDFS HA功能通过配置Active/Standby两个nameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。 HDFS-HA工作机制1）通过双namenode消除单点故障 HDFS-HA工作要点1）元数据管理方式需要改变： 内存中各自保存一份元数据； Edits日志只有Active状态的namenode节点可以做写操作； 两个namenode都可以读取edits； 共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）； 2）需要一个状态管理功能模块 实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split（脑裂）现象的发生。 脑裂：集群中存在两台active状态的namenode。 3）必须保证两个NameNode之间能够ssh无密码登录。 4）隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务 怎么能够保证两台namenode，有一台是active另一台是standby，而不出现脑裂现象呢？ 假想一：两台namenode进行通信，周期请求对面，告知自己状态。在一定的条件下可以实现高可用，但是存在如下问题：1.两台namenode直接通信，如果namenode1（active）处理client请求时，没空响应namenode2那么nn2等待了一段时间，就认为nn1已经碟机，那么nn2启动（切换为active）。这个时候集群出现脑裂现象。2.nn1和nn2 因为网络问题，可能存在一定的延迟，无法实时的切换（nn1碟机，切换到nn2的时候，可能会等待一两分钟）。 假想二：使用zookeeper记录namenode 的状态。也会出现上面的问题。如果网络出现问题，nn1（active）无法正确汇报自己的状态到zookeeper，那么nn2启动，也会出现脑裂问题。 假想三：zkfailover，一个namenode的内部进程（解决网络交互问题） HDFS-HA自动故障转移工作机制前面学习了使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能： 1）故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。 2）现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。 ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责： 1）健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。 2）ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。 3）基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为active状态。 ​ zookeeper服务端 HDFS-HA集群配置环境准备1）修改IP 2）修改主机名及主机名和IP地址的映射 3）关闭防火墙 4）ssh免密登录 5）安装JDK，配置环境变量等 规划集群hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager NodeManager NodeManager NodeManager 配置Zookeeper集群0）集群规划 在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 1）解压安装 （1）解压zookeeper安装包到/opt/module/目录下 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ （2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData ​ mkdir -p zkData （3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg ​ mv zoo_sample.cfg zoo.cfg 2）配置zoo.cfg文件 ​ （1）具体配置 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 增加如下配置 ​ #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 （2）配置参数解读 Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器； B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 3）集群操作 （1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件 ​ touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 （2）编辑myid文件 ​ vi myid ​ 在文件中添加与server对应的编号：如2 （3）拷贝配置好的zookeeper到其他机器上 ​ scp -r zookeeper-3.4.10/ root@hadoop103.kingge.com:/opt/app/ ​ scp -r zookeeper-3.4.10/ root@hadoop104.kingge.com:/opt/app/ ​ 并分别修改myid文件中内容为3、4 （4）分别启动zookeeper ​ [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start （5）查看状态 [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower 配置HDFS-HA集群（手动故障转移配置） 为什么说是手动，因为假设某一台namenode 出现了问题，并不会自动的切换另一台namenode为active状态，需要我们手动切换 1）官方地址：http://hadoop.apache.org/ 2）在opt目录下创建一个ha文件夹 mkdir ha 3）将/opt/app/下的 hadoop-2.7.2拷贝到/opt/ha目录下 cp -r hadoop-2.7.2/ /opt/ha/ 4）配置hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 5）配置core-site.xml fs.defaultFS hdfs://mycluster //任意名字，代表着整个namenode集群，至于调用那个namenode，他会自己分配 hadoop.tmp.dir /opt/ha/hadoop-2.7.2/data/tmp &lt;configuration&gt;&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; //任意名字，代表着整个namenode集群，至于调用那个namenode，他会自己分配 &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6）配置hdfs-site.xml（因为有了热备namenode，那么就可以把secondary-namenode关闭，功能重复） &lt;configuration&gt; &lt;!—可以配置文件块备份数--&gt; &lt;!-- 完全分布式集群名称 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; //这个名字必须与上面的mycluster一致 &lt;/property&gt; &lt;!-- 集群中NameNode节点都有哪些 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode元数据（edit.log）在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 防止脑裂--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/kingge/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 声明journalnode服务器存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/jn&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭权限检查--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; &lt;!—可以配置文件块备份数–&gt; dfs.nameservices mycluster //这个名字必须与上面的mycluster一致 dfs.ha.namenodes.mycluster nn1,nn2 dfs.namenode.rpc-address.mycluster.nn1 hadoop102:9000 dfs.namenode.rpc-address.mycluster.nn2 hadoop103:9000 dfs.namenode.http-address.mycluster.nn1 hadoop102:50070 dfs.namenode.http-address.mycluster.nn2 hadoop103:50070 dfs.namenode.shared.edits.dir qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster dfs.ha.fencing.methods sshfence dfs.ha.fencing.ssh.private-key-files /home/atguigu/.ssh/id_rsa dfs.journalnode.edits.dir /opt/ha/hadoop-2.7.2/data/jn dfs.permissions.enable false dfs.client.failover.proxy.provider.mycluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider 7）可以关闭secondary namenode和关闭原先的namenode 的http访问模式 8）拷贝配置好的hadoop环境到其他节点 启动HDFS-HA集群1）在各个JournalNode节点上，输入以下命令启动journalnode服务：（在这里是hadoop102、hadoop103.、hadoop104 三台服务器都需要执行下面命令） ​ sbin/hadoop-daemon.sh start journalnode 2）在[nn1]上，对其进行格式化，并启动： ​ bin/hdfs namenode –format // ​ sbin/hadoop-daemon.sh start namenode //开启active namenode 3）在[nn2]上，同步nn1的元数据信息： ​ bin/hdfs namenode -bootstrapStandby 4）启动[nn2]：启动备用namenode ​ sbin/hadoop-daemon.sh start namenode 5）查看web页面显示 6）在[nn1]上，启动所有datanode ​ sbin/hadoop-daemons.sh start datanode 7）将[nn1]切换为Active ​ bin/hdfs haadmin -transitionToActive nn1 8）查看是否Active ​ bin/hdfs haadmin -getServiceState nn1 9）尝试kill 掉nn1的namenode Kill 7575 查看nn1和nn2的namenode 状态， 你会发现nn1挂掉后，nn2不还是standby状态，没有自动切换为active，需要手动切换为Active 配置HDFS-HA自动故障转移（上面的是手动故障转移，就是需要手动启动某个namenode为active）1）具体配置 ​ （1）在hdfs-site.xml中增加 &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; dfs.ha.automatic-failover.enabled true ​ （2）在core-site.xml文件中增加 &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;&lt;/property&gt; ha.zookeeper.quorum hadoop102:2181,hadoop103:2181,hadoop104:2181 2）启动 ​ （1）关闭所有HDFS服务： ​ sbin/stop-dfs.sh ​ （2）启动Zookeeper集群： ​ bin/zkServer.sh start ​ （3）初始化HA在Zookeeper中状态： ​ bin/hdfs zkfc -formatZK ​ （4）启动HDFS服务： ​ sbin/start-dfs.sh ​ （5）在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode ​ sbin/hadoop-daemin.sh start zkfc 3）验证 ​ （1）将Active NameNode进程kill ​ kill -9 namenode的进程id ​ （2）将Active NameNode机器断开网络 ​ service network stop YARN-高可用配置YARN-HA工作机制1）官方文档： http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html 2）YARN-HA工作机制 配置YARN-HA集群（也就是开两台resourcemanager）0）环境准备 （1）修改IP （2）修改主机名及主机名和IP地址的映射 （3）关闭防火墙 （4）ssh免密登录 （5）安装JDK，配置环境变量等 ​ （6）配置Zookeeper集群 1）规划集群 hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager ResourceManager NodeManager NodeManager NodeManager 2）具体配置 （1）yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--启用resourcemanager ha--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--声明两台resourcemanager的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster-yarn1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop102&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper集群的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复 – 当resourcemanager碟机后自动重启--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; yarn.nodemanager.aux-services mapreduce_shuffle yarn.resourcemanager.ha.enabled true yarn.resourcemanager.cluster-id cluster-yarn1 yarn.resourcemanager.ha.rm-ids rm1,rm2 yarn.resourcemanager.hostname.rm1 hadoop102 yarn.resourcemanager.hostname.rm2 hadoop103 yarn.resourcemanager.zk-address hadoop102:2181,hadoop103:2181,hadoop104:2181 yarn.resourcemanager.recovery.enabled true yarn.resourcemanager.store.class org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore ​ （2）同步更新其他节点的配置信息 3）启动hdfs （1）在各个JournalNode节点上，输入以下命令启动journalnode服务： ​ sbin/hadoop-daemon.sh start journalnode （2）在[nn1]上，对其进行格式化，并启动： ​ bin/hdfs namenode -format ​ sbin/hadoop-daemon.sh start namenode （3）在[nn2]上，同步nn1的元数据信息： ​ bin/hdfs namenode -bootstrapStandby （4）启动[nn2]： ​ sbin/hadoop-daemon.sh start namenode （5）启动所有datanode ​ sbin/hadoop-daemons.sh start datanode （6）将[nn1]切换为Active ​ bin/hdfs haadmin -transitionToActive nn1 4）启动yarn （1）在hadoop102中执行： sbin/start-yarn.sh （2）在hadoop103中执行： sbin/yarn-daemon.sh start resourcemanager （3）查看服务状态 bin/yarn rmadmin -getServiceState rm1 HDFS Federation架构设计1） NameNode架构的局限性 （1）Namespace（命名空间）的限制 由于NameNode在内存中存储所有的元数据（metadata），因此单个namenode所能存储的对象（文件+块）数目受到namenode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个datanode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个datanode从4T增长到36T，集群的尺寸增长到8000个datanode。存储的需求从12PB增长到大于100PB。 （2）隔离问题 由于HDFS仅有一个namenode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。 ​ （3）性能的瓶颈 ​ 由于是单个namenode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个namenode的吞吐量。 2）HDFS Federation架构设计 能不能有多个NameNode NameNode NameNode NameNode 元数据 元数据 元数据 Log machine 电商数据/话单数据 3）HDFS Federation应用思考 不同应用可以使用不同NameNode进行数据管理 ​ 图片业务、爬虫业务、日志审计业务 Hadoop生态系统中，不同的框架使用不同的namenode进行管理namespace。（隔离性）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"},{"name":"hadoop高可用","slug":"hadoop高可用","permalink":"http://kingge.top/tags/hadoop高可用/"}]},{"title":"hadoop大数据(七)-HDFS的Namenode和Datanode","slug":"hadoop大数据-七-HDFS的Namenode和Datanode","date":"2018-03-10T07:38:59.000Z","updated":"2019-06-10T12:56:33.609Z","comments":true,"path":"2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/","link":"","permalink":"http://kingge.top/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/","excerpt":"","text":"五 NameNode工作机制5.1 NameNode&amp;Secondary NameNode工作机制 1）第一阶段：namenode启动（1）第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存（初始化系统为上一次退出时的最新状态）。 （2）客户端对元数据进行增删改的请求 （3）namenode记录操作日志，更新滚动日志。 （4）namenode在内存中对数据进行增删改查 对于namenode而言最新的操作日志是 edits.in.progress(正在执行的日志) 2）第二阶段：Secondary NameNode工作 核心工作：检查是否需要合并namenode的编辑日志和镜像文件（checkpoint） ​ （1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。定时时间默认1小时，edits默认一百万次 ​ （2）Secondary NameNode请求执行checkpoint。（是否需要合并两个文件） ​ （3）namenode滚动正在写的edits日志（edits.in.progress） ​ （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode ​ （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 ​ （6）生成新的镜像文件fsimage.chkpoint ​ （7）拷贝fsimage.chkpoint到namenode ​ （8）namenode将fsimage.chkpoint重新命名成fsimage 也就是说，secondarynamenode的主要作用是帮助namenode分担他的压力，主要是帮助namenode合并镜像和操作日志，合并后，推给namenode。 总结正如上面所分析的，Hadoop文件系统会出现编辑日志（edits）不断增长的情况，尽管在NameNode运行期间不会对文件系统造成影响，但是如果NameNode重新启动，它将会花费大量的时间运行编辑日志中的每个操作，在此期间也就是我们前面所说的安全模式下，文件系统是不可用的。为了解决上述问题，Hadoop会运行一个Secondary NameNode进程，它的任务就是为原NameNode内存中的文件系统元数据产生检查点。其实说白了，就是辅助NameNode来处理fsimage文件与edits文件的一个进程。它从NameNode中复制fsimage与edits到临时目录并定期合并成一个新的fsimage并且删除原来的编辑日志edits。具体 步骤如下：（1）Secondary NameNode首先请求原NameNode进行edits的滚动，这样会产生一个新的编辑日志文件edits来保存对文件系统的操作（例如：上传新文件，删除文件，修改文件）。（2）Secondary NameNode通过Http方式读取原NameNode中的fsimage及edits。（3）Secondary NameNode将fsimage及edits进行合并产生新的fsimage（4）Secondary NameNode通过Http方式将新生成的fsimage发送到原来的NameNode中（5）原NameNode用新生成的fsimage替换掉旧的fsimage文件，新生成的edits文件也就是（1）生成的滚动编辑日志文件替换掉之前的edits文件 3）web端访问SecondaryNameNode​ （1）启动集群 ​ （2）浏览器中输入：http://hadoop102:50090/status.html ​ （3）查看SecondaryNameNode信息 4）chkpoint检查时间参数设置（1）通常情况下，SecondaryNameNode每隔一小时执行一次。 ​ [hdfs-default.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt; （2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt;&lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt;&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;&lt;/property&gt; 5.2 镜像文件和编辑日志文件1）概念​ namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件 edits_0000000000000000000 fsimage_0000000000000000000.md5 seen_txid VERSION （1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。 （2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。 （3）seentxid文件保存的是一个数字，就是最后一个edits的数字（最后一次操作的序号） （4）每次Namenode启动的时候都会将fsimage文件读入内存，并从00001开始到seen_txid中记录的数字依次执行每个edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成Namenode启动的时候就将fsimage和edits文件进行了合并。 2）oiv查看fsimage文件（1）查看oiv和oev命令 [kingge@hadoop102 current]$ hdfs oiv apply the offline fsimage viewer to an fsimage oev apply the offline edits viewer to an edits file （2）基本语法 hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径 （3）案例实操 [kingge@hadoop102 current]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current [kingge@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml [kingge@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml 将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。 ​ 总结 查看XML你会发现，里面存储了HDFS中文件或者文件夹的创建日期，权限，名称等等元数据信息。但是并没有存储文件保存的位置，也就是：并没有发现文件存储的DataNode节点信息信息那么当客户端请求读数据的时候，namenode是怎么返回数据所在块信息呢？原来他会一直跟datanode进行交互，获取数据所在块信息。 3）oev查看edits文件edits包括两类，edits_XXX,edits_inprogress_XXX edits_XXX：保存文件系统的操作，查看方式见下面语法&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;-&lt;EDITS&gt;&lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt;-&lt;RECORD&gt;&lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt;-&lt;DATA&gt;&lt;TXID&gt;7170&lt;/TXID&gt;&lt;/DATA&gt;&lt;/RECORD&gt;-&lt;RECORD&gt;&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;-&lt;DATA&gt;&lt;TXID&gt;7171&lt;/TXID&gt;&lt;LENGTH&gt;0&lt;/LENGTH&gt;&lt;INODEID&gt;17787&lt;/INODEID&gt;&lt;PATH&gt;/user/zpx/a.txt&lt;/PATH&gt;&lt;REPLICATION&gt;3&lt;/REPLICATION&gt;&lt;MTIME&gt;1489118864779&lt;/MTIME&gt;&lt;ATIME&gt;1489118864779&lt;/ATIME&gt;&lt;BLOCKSIZE&gt;数据的大小&lt;/BLOCKSIZE&gt;&lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_1295720148_1&lt;/CLIENT_NAME&gt;&lt;CLIENT_MACHINE&gt;192.168.231.1&lt;/CLIENT_MACHINE&gt;&lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt;-&lt;PERMISSION_STATUS&gt;&lt;USERNAME&gt;Administrator&lt;/USERNAME&gt;&lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt;&lt;MODE&gt;420&lt;/MODE&gt;&lt;/PERMISSION_STATUS&gt;&lt;RPC_CLIENTID&gt;0c9a5af9-26a8-45d9-8754-cd0e7e47f65b&lt;/RPC_CLIENTID&gt;&lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt;&lt;/DATA&gt;&lt;/RECORD&gt;&lt;/EDITS&gt;对Hdfs文件系统的每一个操作都保存在了edits文件中，每一个操作都是事务，有事务id——&lt;TXID&gt;7171&lt;/TXID&gt;，还有当前操作做了什么&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;，副本数，以及大小edits_inprogress_XXX：正在使用的过程，当前正在向前滚动。查看方式见下面语法 （1）基本语法 hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 （2）案例实操 [kingge@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml [kingge@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml 将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。 总结 你会发现，edits_inprogress，记录的是当前客户端请求执行的操作（增量记录当前操作） 5.3 滚动编辑日志正常情况HDFS文件系统有更新操作时，就会滚动编辑日志。也可以用命令强制滚动编辑日志。 1）滚动编辑日志（前提必须启动集群） [kingge@hadoop102 current]$ hdfs dfsadmin -rollEdits 2）镜像文件什么时候产生 Namenode启动时加载镜像文件和编辑日志 5.4 Namenode版本号1）查看namenode版本号 在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current这个目录下查看VERSION namespaceID=1933630176 clusterID=CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175 cTime=0 storageType=NAME_NODE blockpoolID=BP-97847618-192.168.10.102-1493726072779 layoutVersion=-63 2）namenode版本号具体解释 （1） namespaceID在HDFS上，会有多个Namenode，所以不同Namenode的namespaceID是不同的，分别管理一组blockpoolID。 （2）clusterID集群id，全局唯一 （3）cTime属性标记了namenode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 （4）storageType属性说明该存储目录包含的是namenode的数据结构。 （5）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。 （6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 5.5 SecondaryNameNode目录结构Secondary NameNode用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。 也即是说存在两种情况secondarynamenode会向namenode请求合并镜像文件和日志文件。（1）当上次请求时间已经间隔了一个小时后，会去请求（2）当操作数（edits，操作日志数）到达一百万次时，会去请求那么他怎么知道操作次数到达一百万次呢？答案是，一分钟请求namenode一次，查询操作次数是否到达一百万次。注意，这个检查操作数的时间设置最好不要跟 一致，不然他会默认执行第一种场景（间隔一个小时） 在/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/current这个目录中查看SecondaryNameNode目录结构。 edits_0000000000000000001-0000000000000000002 fsimage_0000000000000000002 fsimage_0000000000000000002.md5 VERSION SecondaryNameNode的namesecondary/current目录和主namenode的current目录的布局相同。 好处：在主namenode**发生故障时（假设没有及时备份数据），可以从SecondaryNameNode**恢复数据。 根据secondarynamenode恢复namenode方法一：将SecondaryNameNode中数据拷贝到namenode存储数据的目录； 方法二：使用-importCheckpoint选项启动namenode守护进程，从而将SecondaryNameNode中数据拷贝到namenode目录中。 1）案例实操（一）： 模拟namenode故障，并采用方法一，恢复namenode数据 （1）kill -9 namenode进程 （2）删除namenode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* （3）拷贝SecondaryNameNode中数据到原namenode存储数据目录 ​ [kingge@hadoop102 hadoop-2.7.2]$ scp -R /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* /opt/module/hadoop-2.7.2/data/tmp/dfs/name/ （4）重新启动namenode [kingge@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 2）案例实操（二）： 模拟namenode故障，并采用方法二，恢复namenode数据 （0）修改hdfs-site.xml中的 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;# 120秒checkpoint一次&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt;# namenode镜像文件和操作日志存放目录 （1）kill -9 namenode进程 （2）删除namenode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* （3）如果SecondaryNameNode不和Namenode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到Namenode存储数据的平级目录。Scp命令拷贝过来 [kingge@hadoop102 dfs]$ pwd/opt/module/hadoop-2.7.2/data/tmp/dfs[kingge@hadoop102 dfs]$ lsdata name namesecondary （4）导入检查点数据（等待一会ctrl+c结束掉） [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint （5）启动namenode [kingge@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode （6）如果提示文件锁了，可以删除in_use.lock ​ [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/in_use.lock 5.5.5 设置checkpoint检查时间默认的checkpoint period是1个小时。可以去hdfs-site.xml中修改 5.6 集群安全模式操作1）概述 Namenode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的fsimage文件和一个空的编辑日志。此时，namenode开始监听datanode请求。但是此刻，namenode运行在安全模式，即namenode的文件系统对于客户端来说是只读的。（可以解释为什么在namenode启动的时候，我们put数据到hdfs会提示，安全模式错误）因为这个时候namenode和datanode还没有联通对方，需要等待连通后，安全模式自动关闭，然后就可以上传文件了 系统中的数据块的位置并不是由namenode维护的，而是以块列表的形式存储在datanode中。在系统的正常操作期间，namenode会在内存中保留所有块位置的映射信息。在安全模式下，各个datanode会向namenode发送最新的块列表信息，namenode了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”，namenode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以namenode不会进入安全模式。 2）基本语法 集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。 （1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） （2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） （3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） （4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态） 3）案例 ​ 模拟等待安全模式 ​ 1）先进入安全模式 [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter ​ 2）执行下面的脚本 编辑一个脚本 #!/bin/bashbin/hdfs dfsadmin -safemode waitbin/hdfs dfs -put ~/hello.txt /root/hello.txt ​ 3）再打开一个窗口，执行 [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave 5.7 Namenode多目录配置1）namenode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。 2）具体配置如下： ​ hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 2.停止集群，删除数据文件 -- rm -rf data/logs (集群里有多少台服务器就删除多少台)3.格式化namenode4.调用xsync 分发脚本到各个集群 5.启动集群6.查看设置的本地目录name1、name2 你会发现里面的数据一模一样 测试NameNode场景：关闭namenode（stop-dfs.sh），关闭yarn（stop-yarn.sh），删除hadoop目录下的data目录和log目录。 1.格式化namenode – bin/hdfs namenode -format 2.启动hdfs和yarn 六 DataNode工作机制6.1 DataNode工作机制 1）一个数据块在datanode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 2）DataNode启动后向namenode注册，通过后，周期性（1小时）的向namenode上报所有的块信息。 3）心跳是每3秒一次，心跳返回结果带有namenode给该datanode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个datanode的心跳，则认为该节点不可用。 4）集群运行中可以安全加入和退出一些机器（在不关闭集群的情况下服役和退役服务器） 6.2 数据完整性1）当DataNode读取block的时候，它会计算checksum 2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。 3）client读取其他DataNode上的block。 4）datanode在其文件创建后周期验证checksum 6.3 掉线时限参数设置datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： ​ timeout = 2 dfs.namenode.heartbeat.recheck-interval + 10 dfs.heartbeat.interval。 ​ 而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。 ​ 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 &lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt; dfs.heartbeat.interval &lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 6.4 DataNode的目录结构和namenode不同的是，datanode的存储目录是初始阶段自动创建的，不需要额外格式化。 1）在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current这个目录下查看版本号 [kingge@hadoop102 current]$ cat VERSION storageID=DS-1b998a1d-71a3-43d5-82dc-c0ff3294921b clusterID=CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175 cTime=0 datanodeUuid=970b2daf-63b8-4e17-a514-d81741392165 storageType=DATA_NODE layoutVersion=-56 2）具体解释 ​ （1）storageID：存储id号 ​ （2）clusterID集群id，全局唯一 ​ （3）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 ​ （4）datanodeUuid：datanode的唯一识别码 ​ （5）storageType：存储类型 ​ （6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 3）在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-97847618-192.168.10.102-1493726072779/current这个目录下查看该数据块的版本号 [kingge@hadoop102 current]$ cat VERSION #Mon May 08 16:30:19 CST 2017 namespaceID=1933630176 cTime=0 blockpoolID=BP-97847618-192.168.10.102-1493726072779 layoutVersion=-56 4）具体解释 （1）namespaceID：是datanode首次访问namenode的时候从namenode处获取的storageID对每个datanode来说是唯一的（但对于单个datanode中所有存储目录来说则是相同的），namenode可用这个属性来区分不同datanode。 （2）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 （3）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。 （4）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 6.5 服役新数据节点0）需求： 随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。 1）环境准备 ​ （1）克隆一台虚拟机 ​ （2）修改ip地址和主机名称 ​ （3）修改xcall和xsync文件，增加新`增节点的同步ssh ​ （4）删除原来HDFS文件系统留存的文件 ​ /opt/module/hadoop-2.7.2/data 和 /opt/module/hadoop-2.7.2/log目录 2）服役新节点具体步骤（下面的操作建议在namenode所在节点进行操作） ​ （1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件 [kingge@hadoop105 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop105 hadoop]$ touch dfs.hosts （名字任意） [kingge@hadoop105 hadoop]$ vi dfs.hosts 添加如下主机名称（包含新服役的节点） hadoop102 hadoop103 hadoop104 hadoop105 ​ （2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性 &lt;property&gt;&lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; ​ （3）刷新namenode [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful ​ （4）更新resourcemanager节点 [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 操作完后，打开**hdfs文件系统，发现已经服役了一个新的data**节点 ​ （5）在namenode的slaves文件中增加新主机名称 ​ 增加105 不需要分发 hadoop102 hadoop103 hadoop104 hadoop105 ​ （6）单独命令启动新的数据节点和节点管理器 [kingge@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-datanode-hadoop105.out [kingge@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-kingge-nodemanager-hadoop105.out ​ （7）在web浏览器上检查是否ok 3）如果数据不均衡，可以用命令实现集群的再平衡 ​ [kingge@hadoop102 sbin]$ ./start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 6.6 退役旧数据节点1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件 ​ [kingge@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop102 hadoop]$ touch dfs.hosts.exclude [kingge@hadoop102 hadoop]$ vi dfs.hosts.exclude 添加如下主机名称（要退役的节点） hadoop105 2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 &lt;property&gt;&lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 3）刷新namenode、刷新resourcemanager [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 4）检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点。 5）等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役。· [kingge@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode stopping datanode [kingge@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager stopping nodemanager 6）从include文件中删除退役节点，再运行刷新节点的命令 ​ （1）从namenode的dfs.hosts文件中删除退役节点hadoop105 hadoop102 hadoop103 hadoop104 ​ （2）刷新namenode，刷新resourcemanager [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 7）从namenode的slave文件中删除退役节点hadoop105 hadoop102 hadoop103 hadoop104 8）如果数据不均衡，可以用命令实现集群的再平衡 [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 6.7 Datanode多目录配置1）datanode也可以配置成多个目录，每个目录存储的数据不一样，即是上传一个文本，那么文本存储在data，但是data2什么都没有（跟namenode多目录区别）。即：数据不是副本。 2）具体配置如下： ​ hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; 七 HDFS其他功能7.1 集群间数据拷贝1）scp实现两个远程主机之间的文件复制 ​ scp -r hello.txt root@hadoop103:/user/kingge/hello.txt // 推 push ​ scp -r root@hadoop103:/user/kingge/hello.txt hello.txt // 拉 pull ​ scp -r root@hadoop103:/user/kingge/hello.txt root@hadoop104:/user/kingge //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。 2）采用discp命令实现两个hadoop集群之间的递归数据复制 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp hdfs://haoop102:9000/user/kingge/hello.txt hdfs://hadoop103:9000/user/kingge/hello.txt 7.2 Hadoop存档1）理论概述 每个文件均按块存储，每个块的元数据存储在namenode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽namenode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。 Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少namenode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件可以用作MapReduce的输入。 2）案例实操 （1）需要启动yarn进程 ​ [kingge@hadoop102 hadoop-2.7.2]$ start-yarn.sh （2）归档文件 ​ 归档成一个叫做xxx.har的文件夹，该文件夹下有相应的数据文件。Xx.har目录是一个整体，该目录看成是一个归档文件即可。 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p /user/kingge /user/my （3）查看归档 ​ [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///myhar.har （4）解归档文件 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har /* /user/kingge 7.3 快照管理快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 1）基本语法 ​ （1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能） ​ （2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用） ​ （3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照） ​ （4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照） ​ （5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照） ​ （6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有可快照目录） ​ （7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处） ​ （8）hdfs dfs -deleteSnapshot （功能描述：删除快照） 2）案例实操 ​ （1）开启/禁用指定目录的快照功能 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/kingge/data [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/kingge/data ​ （2）对目录创建快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/kingge/data 通过web访问hdfs://hadoop102:9000/user/kingge/data/.snapshot/s…..// 快照和源文件使用相同数据块 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/kingge/data/.snapshot/ ​ （3）指定名称创建快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/kingge/data miao170508 ​ （4）重命名快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/kingge/data/ miao170508 kingge170508 ​ （5）列出当前用户所有可快照目录 [kingge@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir ​ （6）比较两个快照目录的不同之处 [kingge@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/kingge/data/ . .snapshot/kingge170508 ​ （7）恢复快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/kingge/input/.snapshot/s20170708-134303.027 /user 7.4 回收站1）默认回收站 默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。 默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。 要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。 2）启用回收站 修改core-site.xml，配置垃圾回收时间为1分钟。 &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 3）查看回收站 回收站在集群中的；路径：/user/kingge/.Trash/…. 4）修改访问垃圾回收站用户名称(如果不修改为想要查看该回收站的用户的名称，那么该用户试图进入回收站时会提示权限问题) ​ 进入垃圾回收站用户名称，默认是dr.who，修改为kingge用户 ​ [core-site.xml] &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;kingge&lt;/value&gt;&lt;/property&gt; 5）通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 Trash trash = New Trash(conf); trash.moveToTrash(path); 6）恢复回收站数据 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/kingge/.Trash/Current/user/kingge/input /user/kingge/input 7）清空回收站（他并不是真正删除文件，而是生成一个当前时间戳的文件夹然后把回收站里面的文件都放到这个文件夹里面） [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -expunge","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(六)-HDFS的读写数据流程","slug":"hadoop大数据-六-HDFS的读写数据流程","date":"2018-03-08T14:38:59.000Z","updated":"2019-06-09T04:48:50.181Z","comments":true,"path":"2018/03/08/hadoop大数据-六-HDFS的读写数据流程/","link":"","permalink":"http://kingge.top/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/","excerpt":"","text":"四 HDFS的数据流4.1 HDFS写数据流程4.1.1 剖析文件写入 1）客户端通过Distributed FileSystem模块向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。（存在覆盖，不存在创建） 2）namenode返回是否可以上传。 3）客户端请求第一个 block上传到哪几个datanode服务器上。 4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。（根据配置文件中指定的备份数量及机架感知原理进行文件分配） 5）客户端通过FSDataOutputStream模块请求dn1上传数据（建立RPC请求），dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。-应答成功，开始传输数据 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet（默认 64K）为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 128M他并不是一次性写入dn1，而是分包的形式，dn1接收到一个包，然后保存在自己所在服务器的本地缓存中。然后再写入自己磁盘（7_blk_1）的同时，传输给dn2，以此类推。直到传输完整个128M。第一块传输完毕。 8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器。（重复执行3-7步）。 4.1.2 网络拓扑概念​ 在本地网络中，两个节点被称为“彼此近邻”是什么意思？在海量数据处理中，其主要限制因素是节点之间数据的传输速率——带宽很稀缺。这里的想法是将两个节点间的带宽作为距离的衡量标准。 ​ 节点距离：两个节点到达最近的共同祖先的距离总和。 例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 Distance(/d1/r1/n1, /d1/r1/n1)=0（同一节点上的进程） Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点） Distance(/d1/r1/n1, /d1/r3/n2)=4（同一数据中心不同机架上的节点） Distance(/d1/r1/n1, /d2/r4/n2)=6（不同数据中心的节点） 大家算一算每两个节点之间的距离。 4.1.3 机架感知（副本节点选择） 一份数据如果存在三个副本，那么副本存放服务器的选择，应该采取怎么样的策略 1）官方ip地址： http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication 2）低版本Hadoop副本节点选择 第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于不相同机架的随机节点上。 第三个副本和第二个副本位于相同机架，节点随机。 3）Hadoop2.7.2副本节点选择 ​ 第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。 ​ 第二个副本和第一个副本位于相同机架，随机节点。 ​ 第三个副本位于不同机架，随机节点。 为什么第一个副本选择在客户端所在的节点，因为这样client**请求数据的时候，可以做到更快的响应，优先读取当前节点副本信息（设计网络拓扑概念），距离客户端越近的节点，数据传输速率越快** 4.2 HDFS读数据流程 1）客户端通过Distributed FileSystem向namenode请求下载文件，namenode通过查询元数据，找到文件块所在的datanode地址。 2）挑选一台datanode（就近原则，然后随机）服务器，请求读取数据。 3）datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验）。 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 4.3 一致性模型1）debug调试如下代码 @Test public void writeFile() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); fs = FileSystem.get(configuration); // 2 创建文件输出流 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”); FSDataOutputStream fos = fs.create(path); // 3 写数据 fos.write(“hello”.getBytes()); // 4 一致性刷新 fos.hflush(); fos.close(); } @Test public void writeFile() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); fs = FileSystem.get(configuration); // 2 创建文件输出流 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;); FSDataOutputStream fos = fs.create(path); // 3 写数据 fos.write(&quot;hello&quot;.getBytes()); // 4 一致性刷新 fos.hflush(); fos.close(); &#125; 2）总结 写入数据时，如果希望数据被其他client立即可见，调用如下方法 FsDataOutputStream. hflush (); //清理客户端缓冲区数据，被其他client立即可见 因为传统的流操作，只有在关闭流的时候，才会去执行flush**操作，那么可能在关闭流之前发生错误，导致数据没有存储到对应的节点。为了避免这种问题，可以手动显式的执行flush**写入磁盘操作。（IOUtils. copyBytes 内部已经实现刷新机制，不需要手动刷新）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(五)-HDFS概念和基本操作","slug":"hadoop大数据-五-HDFS概念和基本操作","date":"2018-03-06T12:31:59.000Z","updated":"2019-06-09T04:01:11.154Z","comments":true,"path":"2018/03/06/hadoop大数据-五-HDFS概念和基本操作/","link":"","permalink":"http://kingge.top/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/","excerpt":"","text":"一 HDFS简单介绍1.1 HDFS产生背景随着数据量越来越大，在一个操作系统管辖的范围内存不下了，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。 1.2 HDFS概念HDFS**，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的**，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS的设计适合一次写入，多次读出的场景，且不支持文件的修改（也是可以修改的，但是不建议）。适合用来做数据分析，并不适合用来做网盘应用。 1.3 HDFS 优缺点1.3.1 优点1）高容错性 ​ （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。 ​ （2）某一个副本丢失以后，它可以自动恢复，这是由 HDFS 内部机制实现的，我们不必关心。 2）适合大数据处理 ​ （1）数据规模：能够处理数据规模达到 GB、TB、甚至PB级别的数据。（因为他会切块存储，所以可以存储大文件-**参见HDFS**写数据流程） ​ （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 3）流式数据访问**（一点一点的处理数据，而不是一次性读取整个数据，这样会极大消耗内存）** ​ （1）一次写入，多次读取，不能修改，只能追加。 ​ （2）它能保证数据的一致性。 4）可构建在廉价机器上 ​ （1）它通过多副本机制，提高可靠性。 ​ （2）它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。 1.3.2 缺点1）不适合低延时数据访问 ​ （1）比如毫秒级的来存储数据，这是不行的，它做不到。 ​ （2）它适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况下是不行的，比如毫秒级以内读取数据，这样它是很难做到的。 2）无法高效的对大量小文件进行存储（HDFS默认的最基本的存储单位是128M的数据块。） ​ （1）存储大量小文件;)的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。 ​ （2）小文件存储的寻道时间会超过读取时间，它违反了HDFS的设计目标。 3）并发写入、文件随机修改 ​ （1）一个文件只能有一个写，不允许多个线程同时写。 ​ （2）仅支持数据 append（追加），不支持文件的随机修改。 1.4 HDFS架构HDFS的架构图 ​ 这种架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分。 1）Client：就是客户端。 ​ （1）文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的Block，然后进行存储。 ​ （2）与NameNode交互，获取文件的位置信息。 ​ （3）与DataNode交互，读取或者写入数据。 ​ （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS。 ​ （5）Client可以通过一些命令来访问HDFS。 2）NameNode：就是master，它是一个主管、管理者。 ​ （1）管理HDFS的名称空间。 ​ （2）管理数据块（Block）;)映射信息 ​ （3）配置副本策略;) ​ （4）处理客户端读写请求。 3） DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 ​ （1）存储实际的数据块。 ​ （2）执行数据块的读/写操作。 4） Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 ​ （1）辅助NameNode，分担其工作量。 ​ （2）定期合并fsimage和fsedits;)，并推送给NameNode。 ​ （3）在紧急情况下，可辅助恢复NameNode。 1.5 HDFS 文件块大小HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。 HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。 如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。 块的大小： 10ms*100*100M/s = 100M 块大小取决于磁盘传输速率 二 HFDS命令行操作操作HDFS的命令其实有三个： ​ Hadoop fs：使用面最广，可以操作任何文件系统。 ​ hadoop dfs与hdfs dfs：只能操作HDFS文件系统相关（包括与Local FS间的操作），前者已经Deprecated，一般使用后者。 推荐使用：hadoop fs 1）基本语法bin/hadoop fs 具体命令 2）参数大全​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs [-appendToFile … ] [-cat [-ignoreCrc] …] [-checksum …] [-chgrp [-R] GROUP PATH…] [-chmod [-R] PATH…] [-chown [-R] [OWNER][:[GROUP]] PATH…] [-copyFromLocal [-f] [-p] … ] [-copyToLocal [-p] [-ignoreCrc] [-crc] … ] [-count [-q] …] [-cp [-f] [-p] … ] [-createSnapshot []] [-deleteSnapshot ] [-df [-h] [ …]] [-du [-s] [-h] …] [-expunge] [-get [-p] [-ignoreCrc] [-crc] … ] [-getfacl [-R] ] [-getmerge [-nl] ] [-help [cmd …]] [-ls [-d] [-h] [-R] [ …]] [-mkdir [-p] …] [-moveFromLocal … ] [-moveToLocal ] [-mv … ] [-put [-f] [-p] … ] [-renameSnapshot ] [-rm [-f] [-r|-R] [-skipTrash] …] [-rmdir [–ignore-fail-on-non-empty] …] [-setfacl [-R] [{-b|-k} {-m|-x } ]|[–set ]] [-setrep [-R] [-w] …] [-stat [format] …] [-tail [-f] ] [-test -[defsz] ] [-text [-ignoreCrc] …] [-touchz …] [-usage [cmd …]] [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] &lt;path&gt; ...] [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] &lt;path&gt; ...] [-expunge] [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] &lt;file&gt;] [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]] 3）常用命令 （0）启动Hadoop集群（方便后续的测试） ​ [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -help rm ​ （2）-ls: 显示目录信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -ls / （3）-mkdir：在hdfs上创建目录 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /user/kingge/test （4）-moveFromLocal从本地剪切粘贴到hdfs [kingge@hadoop102 hadoop-2.7.2]$ touch jinlian.txt [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./jinlian.txt /user/kingge/test （5）–appendToFile ：追加一个文件到已经存在的文件末尾 [kingge@hadoop102 hadoop-2.7.2]$ touch ximen.txt [kingge@hadoop102 hadoop-2.7.2]$ vi ximen.txt 输入 wo ai jinlian [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile ximen.txt /user/kingge/test/jinlian.txt （6）-cat ：显示文件内容 （7）-tail：显示一个文件的末尾 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /user/kingge/test/jinlian.txt （8）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /hello.txt [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -chown someuser:somegrp /hello.txt （9）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /user/kingge/test （10）-copyToLocal：从hdfs拷贝到本地 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /user/kingge/test/jinlian.txt ./jinlian.txt （11）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /user/kingge/test/jinlian.txt /jinlian2.txt （12）-mv：在hdfs目录中移动文件 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /jinlian2.txt /user/kingge/test/ （13）-get：等同于copyToLocal，就是从hdfs下载文件到本地 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -get /user/kingge/test/jinlian2.txt ./ （14）-getmerge ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,… [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/kingge/test/* ./zaiyiqi.txt （15）-put：等同于copyFromLocal [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/kingge/test/ （16）-rm：删除文件或文件夹 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/kingge/test/jinlian2.txt （17）-rmdir：删除空目录 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test （18）-df ：统计文件系统的可用空间信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -df -h / （19）-du统计文件夹的大小信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/kingge/test 2.7 K /user/kingge/test ###查看文件夹下文件总大小 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/kingge/test 1.3 K /user/kingge/test/README.txt 15 /user/kingge/test/jinlian.txt 1.4 K /user/kingge/test/zaiyiqi.txt ##查看文件夹下各个文件大小 （20）-setrep：设置hdfs中文件的副本数量 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 2 /user/kingge/test/jinlian.txt 这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。 三 HDFS客户端操作-eclipse3.1 Eclipse环境准备注意：以下操作，是在hadoop集群中中进行的，也就是说，需要先启动linux的hadoop集群 3.1.1 jar包准备1）解压hadoop-2.7.2.tar.gz到非中文目录 2）进入share文件夹，查找所有jar包，并把jar包拷贝到_lib文件夹下 3）在全部jar包中查找sources.jar，并剪切到_source文件夹。 4）在全部jar包中查找tests.jar，并剪切到_test文件夹。 3.1.2 Eclipse准备1）配置HADOOP_HOME环境变量 2）采用Hadoop编译后的bin 、lib两个文件夹（如果不生效，重新启动eclipse） 3）创建第一个java工程 4）导入 编译后目录的jar包（可以在文章下方回复我，我私信给你们） public class HdfsClientDemo1 { public static void main(String[] args) throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 configuration.set(“fs.defaultFS”, “hdfs://hadoop102:9000”); FileSystem fileSystem = FileSystem.get(configuration); // 直接配置访问集群的路径和访问集群的用户名称 // FileSystem fileSystem = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 把本地文件上传到文件系统中 fileSystem.copyFromLocalFile(new Path(“f:/hello.txt”), new Path(“/hello1.copy.txt”)); // 3 关闭资源 fileSystem.close(); System.out.println(“over”); } } public class HdfsClientDemo1 &#123; public static void main(String[] args) throws Exception &#123; // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 - 如果不配置，默认使用的是本地的hadoop运行环境。 configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); FileSystem fileSystem = FileSystem.get(configuration); // 直接配置访问集群的路径和访问集群的用户名称// FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 把本地文件上传到文件系统中 fileSystem.copyFromLocalFile(new Path(&quot;f:/hello.txt&quot;), new Path(&quot;/hello1.copy.txt&quot;)); // 3 关闭资源 fileSystem.close(); System.out.println(&quot;over&quot;); &#125;&#125; 4）执行程序 运行时需要配置用户名称 客户端去操作hdfs时，是有一个用户身份的。默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=kingge，kingge为用户名称。 3.2 通过API操作HDFS3.2.1 HDFS获取文件系统1）详细代码 ​ @Test public void initHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); // 2 获取文件系统 FileSystem fs = FileSystem.get(configuration); // 3 打印文件系统 System.out.println(fs.toString()); } @Testpublic void initHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); // 2 获取文件系统 FileSystem fs = FileSystem.get(configuration); // 3 打印文件系统 System.out.println(fs.toString());&#125; 3.2.2 HDFS文件上传 @Test public void putFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration configuration = new Configuration(); // 2 设置参数 // 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 3 创建要上传文件所在的本地路径 Path src = new Path(&quot;e:/hello.txt&quot;); // 4 创建要上传到hdfs的目标路径 Path dst = new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;); // 5 拷贝文件 fs.copyFromLocalFile(src, dst); fs.close(); &#125; ​ @Test public void putFileToHDFS() throws Exception{ // 1 创建配置信息对象 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration configuration = new Configuration(); // 2 设置参数 // 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 configuration.set(“dfs.replication”, “2”); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 3 创建要上传文件所在的本地路径 Path src = new Path(“e:/hello.txt”); // 4 创建要上传到hdfs的目标路径 Path dst = new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”); // 5 拷贝文件 fs.copyFromLocalFile(src, dst); fs.close(); } 2）将core-site.xml拷贝到项目的根目录下 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; fs.defaultFS hdfs://hadoop102:9000 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）将hdfs-site.xml拷贝到项目的根目录下 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; dfs.replication 1 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 4）测试参数优先级 参数优先级： 1**、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3**、然后是服务器的默认配置 3.2.3 HDFS文件下载 @Test public void getFileFromHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // fs.copyToLocalFile(new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“d:/hello.txt”)); // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件效验 // 2 下载文件 fs.copyToLocalFile(false, new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“e:/hellocopy.txt”), true); fs.close(); } @Testpublic void getFileFromHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // fs.copyToLocalFile(new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;d:/hello.txt&quot;)); // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件效验 // 2 下载文件 fs.copyToLocalFile(false, new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;e:/hellocopy.txt&quot;), true); fs.close(); &#125; 3.2.4 HDFS目录创建​ @Test public void mkdirAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 创建目录 fs.mkdirs(new Path(“hdfs://hadoop102:9000/user/atguigu/output”)); } @Testpublic void mkdirAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 创建目录 fs.mkdirs(new Path(&quot;hdfs://hadoop102:9000/user/kingge/output&quot;));&#125; 3.2.5 HDFS文件夹删除@Testpublic void deleteAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 删除文件夹 ，如果是非空文件夹，参数2是否递归删除，true递归 fs.delete(new Path(&quot;hdfs://hadoop102:9000/user/kingge/output&quot;), true);&#125; ​ @Test public void deleteAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 删除文件夹 ，如果是非空文件夹，参数2是否递归删除，true递归 fs.delete(new Path(“hdfs://hadoop102:9000/user/atguigu/output”), true); } 3.2.6 HDFS文件名更改​ @Test public void renameAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 重命名文件或文件夹 fs.rename(new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“hdfs://hadoop102:9000/user/atguigu/hellonihao.txt”)); } @Testpublic void renameAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 重命名文件或文件夹 fs.rename(new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;hdfs://hadoop102:9000/user/kingge/hellonihao.txt&quot;));&#125; 3.2.7 HDFS文件详情查看查看文件名称、权限、长度、块信息-不是文件夹，是文件 @Test public void readListFiles() throws Exception { // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 思考：为什么返回迭代器，而不是List之类的容器 RemoteIterator listFiles = fs.listFiles(new Path(“/“), true); while (listFiles.hasNext()) { LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) { System.out.println(“block-offset:” + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) { System.out.println(host); } } System.out.println(“————–李冰冰的分割线————–”); } } @Testpublic void readListFiles() throws Exception &#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 思考：为什么返回迭代器，而不是List之类的容器 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) &#123; System.out.println(&quot;block-offset:&quot; + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(&quot;--------------李冰冰的分割线--------------&quot;); &#125; &#125; 3.2.8 HDFS文件和文件夹判断 不支持递归，只能查询当前某个目录下的文件或者或者文件夹，然后判断 @Testpublic void findAtHDFS() throws Exception, IllegalArgumentException, IOException&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 获取查询路径下的文件状态信息 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); // 3 遍历所有文件状态 for (FileStatus status : listStatus) &#123; if (status.isFile()) &#123; System.out.println(&quot;f--&quot; + status.getPath().getName()); &#125; else &#123; System.out.println(&quot;d--&quot; + status.getPath().getName()); &#125; &#125;&#125; @Test public void findAtHDFS() throws Exception, IllegalArgumentException, IOException{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 获取查询路径下的文件状态信息 FileStatus[] listStatus = fs.listStatus(new Path(“/“)); // 3 遍历所有文件状态 for (FileStatus status : listStatus) { if (status.isFile()) { System.out.println(“f–” + status.getPath().getName()); } else { System.out.println(“d–” + status.getPath().getName()); } } } 3.3 通过IO流操作HDFS3.3.1 HDFS文件上传​ @Test public void putFileToHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 创建输入流 FileInputStream inStream = new FileInputStream(new File(“e:/hello.txt”)); // 3 获取输出路径 String putFileName = “hdfs://hadoop102:9000/user/atguigu/hello1.txt”; Path writePath = new Path(putFileName); // 4 创建输出流 FSDataOutputStream outStream = fs.create(writePath); // 5 流对接 try{ IOUtils.copyBytes(inStream, outStream, 4096, false); }catch(Exception e){ e.printStackTrace(); }finally{ IOUtils.closeStream(inStream); IOUtils.closeStream(outStream); } } @Testpublic void putFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 创建输入流 FileInputStream inStream = new FileInputStream(new File(&quot;e:/hello.txt&quot;)); // 3 获取输出路径 String putFileName = &quot;hdfs://hadoop102:9000/user/kingge/hello1.txt&quot;; Path writePath = new Path(putFileName); // 4 创建输出流 FSDataOutputStream outStream = fs.create(writePath); // 5 流对接 try&#123; IOUtils.copyBytes(inStream, outStream, 4096, false); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; IOUtils.closeStream(inStream); IOUtils.closeStream(outStream); &#125;&#125; 3.3.2 HDFS文件下载​ @Test public void getFileToHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 获取读取文件路径 String filename = “hdfs://hadoop102:9000/user/atguigu/hello1.txt”; // 3 创建读取path Path readPath = new Path(filename); // 4 创建输入流 FSDataInputStream inStream = fs.open(readPath); // 5 流对接输出到控制台 try{ IOUtils.copyBytes(inStream, System.out, 4096, false); }catch(Exception e){ e.printStackTrace(); }finally{ IOUtils.closeStream(inStream); } } @Testpublic void getFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 获取读取文件路径 String filename = &quot;hdfs://hadoop102:9000/user/kingge/hello1.txt&quot;; // 3 创建读取path Path readPath = new Path(filename); // 4 创建输入流 FSDataInputStream inStream = fs.open(readPath); // 5 流对接输出到控制台 try&#123; IOUtils.copyBytes(inStream, System.out, 4096, false); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; IOUtils.closeStream(inStream); &#125;&#125; 3.3.3 定位文件读取1）下载第一块 @Test // 定位下载第一块内容 public void readFileSeek1() throws Exception { // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”), configuration, “atguigu”); // 2 获取输入流路径 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/tmp/hadoop-2.7.2.tar.gz”); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(“e:/hadoop-2.7.2.tar.gz.part1”); // 5 流对接 byte[] buf = new byte[1024]; for (int i = 0; i &lt; 128 1024; i++) { fis.read(buf); fos.write(buf); } // 6 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream*(fos); } @Test// 定位下载第一块内容public void readFileSeek1() throws Exception &#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kingge&quot;); // 2 获取输入流路径 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/tmp/hadoop-2.7.2.tar.gz&quot;); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;); // 5 流对接 byte[] buf = new byte[1024]; for (int i = 0; i &lt; 128 * 1024; i++) &#123; fis.read(buf); fos.write(buf); &#125; // 6 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos); &#125; 2）下载第二块 ​ @Test // 定位下载第二块内容 public void readFileSeek2() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”), configuration, “atguigu”); // 2 获取输入流路径 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/tmp/hadoop-2.7.2.tar.gz”); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(“e:/hadoop-2.7.2.tar.gz.part2”); // 5 定位偏移量（第二块的首位） fis.seek(1024 1024 128); // 6 流对接 IOUtils.copyBytes(fis, fos, 1024); // 7 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } @Test// 定位下载第二块内容public void readFileSeek2() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kingge&quot;); // 2 获取输入流路径 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/tmp/hadoop-2.7.2.tar.gz&quot;); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;); // 5 定位偏移量（第二块的首位） fis.seek(1024 * 1024 * 128); // 6 流对接 IOUtils.copyBytes(fis, fos, 1024); // 7 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos);&#125; 3）合并文件 在window命令窗口中执行 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 解压，发现，就是我们上传的压缩文件。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(四)-Hadoop编译源码","slug":"hadoop大数据-四-Hadoop编译源码","date":"2018-02-28T11:31:59.000Z","updated":"2019-06-09T02:46:53.773Z","comments":true,"path":"2018/02/28/hadoop大数据-四-Hadoop编译源码/","link":"","permalink":"http://kingge.top/2018/02/28/hadoop大数据-四-Hadoop编译源码/","excerpt":"","text":"五 Hadoop编译源码5.1 前期准备工作1）CentOS联网 配置CentOS能连接外网。Linux虚拟机ping www.baidu.com 是畅通的 注意：采用root角色编译，减少文件夹权限出现问题 2）jar包准备(hadoop源码、JDK7 、 maven、 ant 、protobuf) （1）hadoop-2.7.2-src.tar.gz （2）jdk-7u79-linux-x64.gz （3）apache-ant-1.9.9-bin.tar.gz （4）apache-maven-3.0.5-bin.tar.gz （5）protobuf-2.5.0.tar.gz 5.2 jar包安装0）注意：所有操作必须在root用户下完成 1）JDK解压、配置环境变量 JAVA_HOME和PATH，验证java-version(如下都需要验证是否配置成功) [root@hadoop101 software] # tar -zxf jdk-7u79-linux-x64.gz -C /opt/module/ [root@hadoop101 software]# vi /etc/profile #JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：java -version 2）Maven解压、配置 MAVEN_HOME和PATH。 [root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile #MAVEN_HOME export MAVEN_HOME=/opt/module/apache-maven-3.0.5 export PATH=$PATH:$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn -version 3）ant解压、配置 ANT _HOME和PATH。 [root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-ant-1.9.9]# vi /etc/profile #ANT_HOME export ANT_HOME=/opt/module/apache-ant-1.9.9 export PATH=$PATH:$ANT_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：ant -version 4）安装 glibc-headers 和 g++ 命令如下: [root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers [root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++ 5）安装make和cmake [root@hadoop101 apache-ant-1.9.9]# yum install make [root@hadoop101 apache-ant-1.9.9]# yum install cmake 6）解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0 然后相继执行命令： [root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/ [root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/ [root@hadoop101 protobuf-2.5.0]#./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make check [root@hadoop101 protobuf-2.5.0]# make install [root@hadoop101 protobuf-2.5.0]# ldconfig [root@hadoop101 hadoop-dist]# vi /etc/profile #LD_LIBRARY_PATH export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0 export PATH=$PATH:$LD_LIBRARY_PATH [root@hadoop101 software]#source /etc/profile 验证命令：protoc –version 7）安装openssl库 [root@hadoop101 software]#yum install openssl-devel 8）安装 ncurses-devel库： [root@hadoop101 software]#yum install ncurses-devel 到此，编译工具安装基本完成。 5.3 编译源码1）解压源码到/opt/tools目录 [root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/ 2）进入到hadoop源码主目录 [root@hadoop101 hadoop-2.7.2-src]# pwd /opt/hadoop-2.7.2-src 3）通过maven执行编译命令 [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 等待时间30分钟左右，最终成功是全部SUCCESS。 4）成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下。 [root@hadoop101 target]# pwd /opt/hadoop-2.7.2-src/hadoop-dist/target 5.4 常见的问题及解决方案1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） 2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐 2.7.0版本的问题汇总帖子 http://www.tuicool.com/articles/IBn63qf","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(三)-Hadoop三种部署和运行方式","slug":"hadoop大数据-三-Hadoop三种部署和运行方式","date":"2018-02-26T15:21:59.000Z","updated":"2019-06-09T02:46:10.040Z","comments":true,"path":"2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/","link":"","permalink":"http://kingge.top/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/","excerpt":"","text":"四 Hadoop运行模式1）官方网址 （1）官方网站： http://hadoop.apache.org/ （2）各个版本归档库地址 https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/ （3）hadoop2.7.2版本详情介绍 http://hadoop.apache.org/docs/r2.7.2/ 2）Hadoop运行模式 （1）本地模式（默认模式）： 不需要启用单独进程，直接可以运行，测试和开发时使用。 （2）伪分布式模式： 等同于完全分布式，只有一个节点。 （3）完全分布式模式： 多个节点一起运行。 4.1 本地运行Hadoop 案例4.1.1 官方grep案例1）创建在hadoop-2.7.2文件下面创建一个input文件夹 [kingge@hadoop101 hadoop-2.7.2]$mkdir input 2）将hadoop的xml配置文件复制到input [kingge@hadoop101 hadoop-2.7.2]$cp etc/hadoop/*.xml input 3）执行share目录下的mapreduce程序 [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output ‘dfs[a-z.]+’ 4）查看输出结果 [kingge@hadoop101 hadoop-2.7.2]$ cat output/* 4.1.2 官方wordcount案例1）创建在hadoop-2.7.2文件下面创建一个wcinput文件夹 [kingge@hadoop101 hadoop-2.7.2]$mkdir wcinput 2）在wcinput文件下创建一个wc.input文件 [kingge@hadoop101 hadoop-2.7.2]$cd wcinput [kingge@hadoop101 wcinput]$touch wc.input 3）编辑wc.input文件 [kingge@hadoop101 wcinput]$vim wc.input在文件中输入如下内容hadoop yarnhadoop mapreduce kinggekingge保存退出：：wq 4）回到hadoop目录/opt/module/hadoop-2.7.2 5）执行程序： [kingge@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput 6）查看结果： [kingge@hadoop101 hadoop-2.7.2]$cat wcoutput/part-r-00000 kingge 2 hadoop 2 mapreduce 1 yarn 1 4.1.3 总结第一个案例：统计input文件里面的文件，文件内容包含’dfs[a-z.]+’ 规则的文字，筛选出来。 第二个案例：统计wc.input 文件中单词出现的个数，特别注意，第二次运行之前必须删除已有的结果输出目录（wcoutput）（rm -rf wcoutput/），否则执行wordcount指令就会报错，提示文件已经存在 —- 操作设计的文件都是存储在linux 的文件系统中。本地操作，不支持联网操作 4.2 伪分布式运行Hadoop案例4.2.1 启动HDFS并运行MapReduce程序1）分析： ​ （1）准备1台客户机 ​ （2）安装jdk ​ （3）配置环境变量 ​ （4）安装hadoop ​ （5）配置环境变量 ​ （6）配置集群 ​ （7）启动、测试集群增、删、查 ​ （8）执行wordcount案例 2）执行步骤 需要配置hadoop文件如下 （1）配置集群 （a）配置：hadoop-env.sh ​ 1.Linux系统中获取jdk的安装路径： [kingge@hadoop100 hadoop-2.7.2]$ cd etc/hadoop/ [root@ hadoop101 ~]# echo $JAVA_HOME /opt/module/jdk1.8.0_144 ​ 2.修改Jhadoop-env.sh的JAVA_HOME 路径： export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置：core-site.xml &lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; （c）配置：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; （2）启动集群（a）格式化namenode（第一次启动时格式化，以后就不要总格式化） ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs namenode –format ​ 会在上面配置配置的存储目录生成 这两个文件 （b）启动namenode ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode ​ c）启动datanode ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode （3）查看集群​ （a）查看是否启动成功 [kingge@hadoop101 hadoop-2.7.2]$ jps 13586 NameNode 13668 DataNode 13786 Jps ​ （b）查看产生的log日志 当前目录：/opt/module/hadoop-2.7.2/logs [kingge@hadoop101 logs]$ ls hadoop-kingge-datanode-hadoop.kingge.com.log hadoop-kingge-datanode-hadoop.kingge.com.out hadoop-kingge-namenode-hadoop.kingge.com.log hadoop-kingge-namenode-hadoop.kingge.com.out SecurityAuth-root.audit [kingge@hadoop101 logs]# cat hadoop-kingge-datanode-hadoop101.log ​ （c）web端查看HDFS文件系统 ​ http://192.168.1.101:50070/dfshealth.html#tab-overview ​ 注意：如果不能查看，看如下帖子处理 http://www.cnblogs.com/zlslch/p/6604189.html （4）操作集群​ （a）在hdfs文件系统上创建一个input文件夹 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/kingge/input ​ （b）将测试文件内容上传到文件系统上 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -put wcinput/wc.input /user/kingge/input/ ​ （c）查看上传的文件是否正确 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -ls /user/kingge/input/ [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/ input/wc.input ​ （d）运行mapreduce程序(所有数据在HDFS上) ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input/ /user/kingge/output ​ （e）查看输出结果 命令行查看： [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/output/* 浏览器查看 ​ （f）将测试文件内容下载到本地 [kingge@hadoop101 hadoop-2.7.2]$ hadoop fs -get /user/kingge/ output/part-r-00000 ./wcoutput/ （g）删除输出结果 [kingge@hadoop101 hadoop-2.7.2]$ hdfs dfs -rmr /user/kingge/output 4.2.2 YARN上运行MapReduce 程序1）分析：​ （1）准备1台客户机 ​ （2）安装jdk ​ （3）配置环境变量 ​ （4）安装hadoop ​ （5）配置环境变量 ​ （6）配置集群yarn上运行 ​ （7）启动、测试集群增、删、查 ​ （8）在yarn上执行wordcount案例 2）执行步骤（1）配置集群 ​（a）配置yarn-env.sh ​ 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置yarn-site.xml &lt;!-- reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt; ​ （c）配置：mapred-env.sh ​ 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ （d）配置： (对mapred-site.xml.template重新命名为) mapred-site.xml [kingge@hadoop101 hadoop]$ mv mapred-site.xml.template mapred-site.xml [kingge@hadoop101 hadoop]$ vi mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; ​ （2）启动集群 （a）启动resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager （b）启动nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager ​ （3）集群操作 （a）yarn的浏览器页面查看 http://192.168.1.101:8088/cluster ​ （b）删除文件系统上的output文件 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/kingge/output ​ （c）执行mapreduce程序 ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input /user/kingge/output ​ （d）查看运行结果 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/output/* 4.2.3 配置临时文件存储路径1）停止进程 [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop namenode 2）修改hadoop.tmp.dir ​ [core-site.xml] &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 3）将/opt/module/hadoop-2.7.2路径中的logs文件夹删除掉 [kingge@hadoop101 hadoop-2.7.2]$ rm -rf logs/ 4）进入到tmp目录将tmp目录中hadoop-kingge目录删除掉 [kingge@hadoop101 tmp]$ rm -rf hadoop-kingge/ 5）格式化NameNode ​ [kingge@hadoop101 hadoop-2.7.2]$ hadoop namenode -format 6）启动所有进程 ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager ​ 7）查看/opt/module/hadoop-2.7.2/data/tmp这个目录下的内容。 4.2.4 配置历史服务器​ 1）配置mapred-site.xml [kingge@hadoop101 hadoop]$ vi mapred-site.xml &lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt; ​ 2）查看启动历史服务器文件目录： [kingge@hadoop101 hadoop-2.7.2]$ ls sbin/ | grep mr mr-jobhistory-daemon.sh ​ 3）启动历史服务器 [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver ​ 4）查看历史服务器是否启动 ​ [kingge@hadoop101 hadoop-2.7.2]$ jps ​ 5）查看jobhistory http://192.168.1.101:19888/jobhistory 4.2.5 配置日志的聚集日志聚集概念：应用运行完成以后，将日志信息上传到HDFS系统上。 开启日志聚集功能步骤： （1）配置yarn-site.xml [kingge@hadoop101 hadoop]$ vi yarn-site.xml &lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; （2）关闭nodemanager 、resourcemanager和historymanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh stop historyserver （3）启动nodemanager 、resourcemanager和historymanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver （4）删除hdfs上已经存在的hdfs文件 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/kingge/output （5）执行wordcount程序 [kingge@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input /user/kingge/output （6）查看日志 http://192.168.1.101:19888/jobhistory 4.2.6 配置文件说明Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。 （1）默认配置文件：存放在hadoop相应的jar包中 [core-default.xml] ​ hadoop-common-2.7.2.jar/ core-default.xml ​ [hdfs-default.xml] hadoop-hdfs-2.7.2.jar/ hdfs-default.xml ​ [yarn-default.xml] hadoop-yarn-common-2.7.2.jar/ yarn-default.xml ​ [core-default.xml] hadoop-mapreduce-client-core-2.7.2.jar/ core-default.xml ​ （2）自定义配置文件：存放在$HADOOP_HOME/etc/hadoop ​ core-site.xml ​ hdfs-site.xml ​ yarn-site.xml ​ mapred-site.xml 4.2.7 总结相比于本地运行模式，伪分布式模式支持互联网操作，不过集群的副本是1（不配置的话默认是3，详情查看hdfs-default.xml的dfs.replication属性）。 4.3 完全分布式部署Hadoop分析： ​ 1）准备3台客户机（关闭防火墙、静态ip、主机名称） ​ 2）安装jdk ​ 3）配置环境变量 ​ 4）安装hadoop ​ 5）配置环境变量 ​ 6）安装ssh ​ 7）配置集群 ​ 8）启动测试集群 4.3.1 虚拟机准备详见3.2-3.3章。 Hadoop运行环境搭建3.2-3.3 4.3.2 主机名设置Hadoop运行环境搭建3.4 4.3.3 scp1）scp可以实现服务器与服务器之间的数据拷贝。 操作一：hadoop101 主动推送数据到hadoop102 操作二：hadoop102主动从hadoop101获取数据到本地 操作三：hadoop101 控制将hadoop102的数据拷贝到hadoop103 ​ 接收方一般使用root**用户接受，因为有些文件夹的权限只有root用户才有，为保证传输成功，双方最好都切换到root用户** 2）案例实操 （1）将hadoop101中/opt/module和/opt/software文件拷贝到hadoop102、hadoop103和hadoop104上。 [root@hadoop101 /]# scp -r /opt/module/ root@hadoop102:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop102:/opt [root@hadoop101 /]# scp -r /opt/module/ root@hadoop103:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop103:/opt [root@hadoop101 /]# scp -r /opt/module/ root@hadoop104:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop105:/opt （2）将hadoop101服务器上的/etc/profile文件拷贝到hadoop102上。 [root@hadoop102 opt]# scp root@hadoop101:/etc/profile /etc/profile 例子1 [root@hadoop102 opt]# scp -r root@192.168.1.101:/opt/module/ /opt/–例子二 ​ （3）实现两台远程机器之间的文件传输（hadoop103主机文件拷贝到hadoop104主机上） ​ [kingge@hadoop102 test]$ scp kingge@hadoop103:/opt/test/haha kingge@hadoop104:/opt/test/ 注意：如果传递环境变量配置文件后需要source /etc/profile 一下，让其生效。同时可能需要修改一下文件的权限或者文件所属（chmod chown） 4.3.4 SSH无密码登录 针对执行ssh命令的 无密码操作 1）配置ssh （1）基本语法 ssh 另一台电脑的ip地址 （2）ssh连接时出现Host key verification failed的解决方法 [root@hadoop102 opt]# ssh 192.168.1.103 The authenticity of host ‘192.168.1.103 (192.168.1.103)’ can’t be established. RSA key fingerprint is cf:1e:de:d7:d0:4c:2d:98:60:b4:fd:ae:b1:2d:ad:06. Are you sure you want to continue connecting (yes/no)? Host key verification failed. （3）解决方案如下：直接输入yes 2）无密钥配置 （1）进入到我的home目录 ​ [kingge@hadoop102 opt]$ cd ~/.ssh （2）生成公钥和私钥： [kingge@hadoop102 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） （3）将公钥拷贝到要免密登录的目标机器上 [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop102 （给自己授权免密登录） [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop103 [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop104 查看103或者104的~/.ssh，发现多了authorized_keys 个文件 需求： A服务器需要访问B服务器，不需要输入密码 3）.ssh文件夹下的文件功能解释 ​ （1）~/.ssh/known_hosts ：记录ssh访问过计算机的公钥(public key) ​ （2）id_rsa ：生成的私钥 ​ （3）id_rsa.pub ：生成的公钥 ​ （4）authorized_keys ：存放授权过得无秘登录服务器公钥 注意： 如果你授权的免密登录用户，被切换了，那么还是需要输入密码才能够登录。 例子：hadoop100服务器的kingge用户生成了密匙，然后发给你了hadoop101，那么100服务器就可以免密登录101服务器，但是假设100服务器su root（切换为了root用户），那么当执行ssh hadoop101 操作时，就需要输入101服务器密码，而不能免密登录，所以要想在root用户下也能免密登录101服务器，就需要在root用户下重新走一遍免密登录流程 4.3.5 rsyncrsync远程同步工具，主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync**和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp**是把所有文件都复制过去。 （1）查看rsync使用说明 man rsync | more ​ （2）基本语法 rsync -rvl $pdir/$fname $user@hadoop$host:$pdir ​ 命令 命令参数 要拷贝的文件路径/名称 目的用户@主机:目的路径 ​ 选项 -r 递归 -v 显示复制过程 -l 拷贝符号连接 ​ （3）案例实操 ​ 把本机/opt/tmp目录同步到hadoop103服务器的root用户下的/opt/tmp目录 [kingge@hadoop102 opt]$ rsync -rvl /opt/tmp root@hadoop103:/opt/ 4.3.6 编写集群分发脚本xsync 场景：分布式系统中假设有6900**台服务器，那么假设我们需要同步配置信息，我们不可能一台台的执行scp/rsync 命令，效率极低，那怎么办呢？** 1）需求分析：循环复制文件到所有节点的相同目录下。 ​ （1）原始拷贝： rsync -rvl /opt/module root@hadoop103:/opt/ ​ （2）期望脚本： xsync 要同步的文件名称 ​ （3）在/usr/local/bin这个目录下存放的脚本，可以在系统任何地方直接执行。（就是在执行xsync命令时可以不用输入/usr/local/bin这样前缀） 2）案例实操： （1）在/usr/local/bin目录下创建xsync文件，文件内容如下： [root@hadoop102 bin]# touch xsync [root@hadoop102 bin]# vi xsync #!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do #echo $pdir/$fname $user@hadoop$host:$pdir echo --------------- hadoop$host ---------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone （2）修改脚本 xsync 具有执行权限 [root@hadoop102 bin]# chmod 777 xsync [root@hadoop102 bin]# chown kingge:kingge -R xsync （3）调用脚本形式：xsync 文件名称 [kingge@hadoop102 opt]$ xsync tmp/ 4.3.7 编写集群操作脚本xcall1）需求分析：在所有主机上同时执行相同的命令 xcall +命令 2）具体实现 （1）在/usr/local/bin目录下创建xcall文件，文件内容如下： [root@hadoop102 bin]# touch xcall [root@hadoop102 bin]# vi xcall #!/bin/bashpcount=$#if((pcount==0));then echo no args; exit;fiecho -------------localhost----------$@for((host=101; host&lt;=108; host++)); do echo ----------hadoop$host--------- ssh hadoop$host $@done （2）修改脚本xcall具有执行权限 ​ [root@hadoop102 bin]# chmod 777 xcall [root@hadoop102 bin]# chown kingge:kingge xcall （3）调用脚本形式： xcall 操作命令 [root@hadoop102 ~]# xcall rm -rf /opt/tmp/ 4.3.8 配置集群1）集群部署规划 hadoop102 hadoop103 hadoop104 HDFS NameNode DataNode DataNode SecondaryNameNode DataNode YARN NodeManager ResourceManager NodeManager NodeManager 集群规划的原则：NameNode/SecondaryNameNode/ ResourceManager必须要单独占据一个服务器(或者这三个不能在同一个节点上运行)，因为他是相当于目录，请求他的次数最大，所以不能跟其他插件部署在一起，不能跟NameNode抢占资源，因为他不能挂掉。Datanode就没有这些限制，挂掉也无所谓。 但是下面的例子中NameNode和DataNode都部署在hadoop102这个节点，因为我们是属于测试搭建环境下，所以无所谓，但是生产环境下必须按照规则 2）配置文件​ （1）core-site.xml [kingge@hadoop102 hadoop]$ vi core-site.xml &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; ​ （2）Hdfs ​ 2.1 hadoop-env.sh [kingge@hadoop102 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ 2.2 hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;# 如果不配置。默认是跟namenode同个位置 &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ​ 2.3 Slaves ​ 配置文件里面不能存在多余的空格或者换行 [kingge@hadoop102 hadoop]$ vi slaves hadoop102 hadoop103 hadoop104 ​ （3）yarn ​ yarn-env.sh [kingge@hadoop102 hadoop]$ vi yarn-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ yarn-site.xml ​ [kingge@hadoop102 hadoop]$ vi yarn-site.xml &lt;configuration&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ​ （4）mapreduce ​ mapred-env.sh [kingge@hadoop102 hadoop]$ vi mapred-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ mapred-site.xml [kingge@hadoop102 hadoop]$ vi mapred-site.xml &lt;configuration&gt; &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）在集群上分发以上所有文件[kingge@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/slaves 4）查看文件分发情况 [kingge@hadoop102 hadoop]$ xcall cat /opt/module/hadoop-2.7.2/etc/hadoop/slaves 4.3.9 集群启动及测试1）启动集群 ​ 清空之前启动namenode的数据 ​ Rm –rf data/ log/ ​ Data就是在core-site.xml中配置的文件存储目录，log就是hadoop的log目录。 ​ （0）如果集群是第一次启动，需要格式化namenode ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -format （1）启动HDFS： [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh –这一步跟我们之前的单个启动不一样（sbin/hadoop-daemon.sh start namenode**），这个操作是启动整个集群的namenode**和datanode—那么就需要配置ssh 无密码登录 [kingge@hadoop102 hadoop-2.7.2]$ jps 4166 NameNode 4482 Jps 4263 DataNode [kingge@hadoop103 hadoop-2.7.2]$ jps 3218 DataNode 3288 Jps [kingge@hadoop104 hadoop-2.7.2]$ jps 3221 DataNode 3283 SecondaryNameNode 3364 Jps （2）启动yarn （启动namemanager 和 resourceManager） [kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh 注意：Namenode和ResourceManger如果不是同一台机器，不能在NameNode上启动 yarn，应该在ResouceManager所在的机器上启动yarn。 因为我们在这个集群中ResourceManager是在hadoop103上面的，所以在103上启动 2）集群基本测试 （1）上传文件到集群 ​ 1.上传小文件 ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/kingge/input ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -put etc/hadoop/wc.input /user/kingge/input 上传完后 hadoop103和hadoop104 上面也会同步副本（即是，也会存在上面的/user/kingge/tep/conf 和 *-site.xml 这些东西） 查看HDFS系统的文件结构 我们可以看到他是存储在了块0，而且有三个副本101/102/103 ​ 接下来我们再上传一个大文件看看 ​ 2.上传大文件 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -put /opt/software/hadoop-2.7.2.tar.gz /user/kingge/input 为什么他这里会分为两块存储呢？因为默认块大小是128，当超过这个大小时就需要分块存储 （2）上传文件后查看文件存放在什么位置 ​ 文件存储路径 ​ [kingge@hadoop102 subdir0]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-938951106-192.168.10.107-1495462844069/current/finalized/subdir0/subdir0 ​ 查看文件内容(wc.input) [kingge@hadoop102 subdir0]$ cat blk_1073741825 hadoop kingge kingge 然后26 27 就是那个大文件，分为了两块 （3）拼接 -rw-rw-r–. 1 kingge kingge 134217728 5月 23 16:01 blk_1073741836 -rw-rw-r–. 1 kingge kingge 1048583 5月 23 16:01 blk_1073741836_1012.meta -rw-rw-r–. 1 kingge kingge 63439959 5月 23 16:01 blk_1073741837 -rw-rw-r–. 1 kingge kingge 495635 5月 23 16:01 blk_1073741837_1013.meta [kingge@hadoop102 subdir0]$ cat blk_1073741836&gt;&gt;tmp.file [kingge@hadoop102 subdir0]$ cat blk_1073741837&gt;&gt;tmp.file [kingge@hadoop102 subdir0]$ tar -zxvf tmp.file 已解压发现就是我们上传那个tar**大文件的解压版本，说明这两个文件就是存放着大文件** （4）下载 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -get /user/kingge/input/hadoop-2.7.2.tar.gz 3）性能测试集群 ​ 写海量数据 ​ 读海量数据 4.3.10 Hadoop启动停止方式1）各个服务组件逐一启动 ​ （1）分别启动hdfs组件 ​ hadoop-daemon.sh start|stop namenode|datanode|secondarynamenode ​ （2）启动yarn ​ yarn-daemon.sh start|stop resourcemanager|nodemanager 2）各个模块分开启动（配置ssh是前提）常用 ​ （1）整体启动/停止hdfs ​ start-dfs.sh ​ stop-dfs.sh ​ （2）整体启动/停止yarn ​ start-yarn.sh ​ stop-yarn.sh 3）全部启动（不建议使用） ​ start-all.sh ​ stop-all.sh 4.3.11 集群时间同步时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 配置时间同步实操： 1）时间服务器配置（必须root用户） （1）检查ntp是否安装 [root@hadoop102 桌面]# rpm -qa|grep ntp ntp-4.2.6p5-10.el6.centos.x86_64 fontpackages-filesystem-1.41-1.1.el6.noarch ntpdate-4.2.6p5-10.el6.centos.x86_64 （2）修改ntp配置文件 [root@hadoop102 桌面]# vi /etc/ntp.conf 修改内容如下 a）修改1 #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap为 restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap ​ b）修改2 注释时间服务器 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst为 #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst ​ c）添加3 自己的时间服务器 server 127.127.1.0 fudge 127.127.1.0 stratum 10 （3）修改/etc/sysconfig/ntpd 文件 [root@hadoop102 桌面]# vim /etc/sysconfig/ntpd 增加内容如下 SYNC_HWCLOCK=yes ​ （4）重新启动ntpd [root@hadoop102 桌面]# service ntpd status ntpd 已停 [root@hadoop102 桌面]# service ntpd start 正在启动 ntpd： [确定] ​ （5）执行： ​ [root@hadoop102 桌面]# chkconfig ntpd on 2）其他机器配置（必须root用户） ​ （1）在其他机器配置10分钟与时间服务器同步一次 ​ [root@hadoop103 hadoop-2.7.2]# crontab -e ​ 编写脚本 ​ /10 * /usr/sbin/ntpdate hadoop102 ​ （2）修改任意机器时间 ​ [root@hadoop103 hadoop]# date -s “2017-9-11 11:11:11” ​ （3）十分钟后查看机器是否与时间服务器同步 ​ [root@hadoop103 hadoop]# date 4.3.12 配置集群常见问题1）防火墙没关闭、或者没有启动yarn INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032 2）主机名称配置错误 3）ip地址配置错误 4）ssh没有配置好 5）root用户和kingge两个用户启动集群不统一 6）配置文件修改不细心 7）未编译源码 Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 17/05/22 15:38:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032 8）datanode不被namenode识别问题 Namenode在format初始化的时候会形成两个标识，blockPoolId和clusterId。新的datanode加入时，会获取这两个标识作为自己工作目录中的标识。 一旦namenode重新format后，namenode的身份标识已变，而datanode如果依然持有原来的id，就不会被namenode识别。 解决办法，删除datanode节点中的数据后，再次重新格式化namenode。 9）不识别主机名称 java.net.UnknownHostException: hadoop102: hadoop102 at java.net.InetAddress.getLocalHost(InetAddress.java:1475) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:146) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) 解决办法： （1）在/etc/hosts文件中添加192.168.1.102 hadoop102 ​ （2）主机名称不要起hadoop hadoop000等特殊名称 10）datanode和namenode进程同时只能工作一个。 11）执行命令 不生效，粘贴word中命令时，遇到-和长–没区分开。导致命令失效 解决办法：尽量不要粘贴word中代码。 12）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。 13）jps不生效。 原因：全局变量hadoop java没有生效，需要source /etc/profile文件。 14）8088端口连接不上 [kingge@hadoop102 桌面]$ cat /etc/hosts 注释掉如下代码 #127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 #::1 hadoop102","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(二)-运行环境搭建","slug":"hadoop大数据-二-运行环境搭建","date":"2018-02-24T02:21:59.000Z","updated":"2019-06-07T09:12:38.271Z","comments":true,"path":"2018/02/24/hadoop大数据-二-运行环境搭建/","link":"","permalink":"http://kingge.top/2018/02/24/hadoop大数据-二-运行环境搭建/","excerpt":"","text":"三 Hadoop运行环境搭建3.0 前置准备需要linux相关的知识，和安装虚拟机。所以还不了解的请看：Linux基础 3.1 虚拟机网络模式设置为NAT ​ 最后，重新启动系统。 ​ [root@hadoop101 ~]# sync ​ [root@hadoop101 ~]# reboot 3.2 克隆虚拟机1）克隆虚拟机 2）启动虚拟机 3.3 修改为静态ip1）在终端命令窗口中输入[root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules 进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址 2）修改IP地址[root@hadoop101 /]# vim /etc/sysconfig/network-scripts/ifcfg-eth0 需要修改的内容有5项： IPADDR=192.168.1.101 GATEWAY=192.168.1.2 ONBOOT=yes BOOTPROTO=static DNS1=192.168.1.2 ​ （1）修改前 ​ （2）修改后 ：wq 保存退出 3）执行[root@hadoop101 /]# service network restart 4）如果报错，reboot，重启虚拟机。​ [root@hadoop101 /]# reboot 3.4 修改主机名1）修改linux的hosts文件（1）进入Linux系统查看本机的主机名。通过hostname命令查看。 [root@hadoop100 /]# hostname hadoop100 （2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件。 [root@hadoop100~]# vi /etc/sysconfig/network 修改文件中主机名称 NETWORKING=yes NETWORKING_IPV6=no HOSTNAME= hadoop101 注意：主机名称不要有“_”下划线 （3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。 （4）保存退出。 （5）打开/etc/hosts [root@hadoop100 ~]# vim /etc/hosts 添加如下内容 192.168.1.100 hadoop100 192.168.1.101 hadoop101 192.168.1.102 hadoop102 192.168.1.103 hadoop103 192.168.1.104 hadoop104 192.168.1.105 hadoop105 192.168.1.106 hadoop106 192.168.1.107 hadoop107 192.168.1.108 hadoop108 192.168.1.109 hadoop109 192.168.1.110 hadoop110 （6）并重启设备，重启后，查看主机名，已经修改成功 2）修改window7的hosts文件(可以不改)只不过是方便于在windows服务器中，使用域名的方式访问hadopp相关的组件。 ​ （1）进入C:\\Windows\\System32\\drivers\\etc路径 ​ （2）打开hosts文件并添加如下内容 192.168.1.100 hadoop100 192.168.1.101 hadoop101 192.168.1.102 hadoop102 192.168.1.103 hadoop103 192.168.1.104 hadoop104 192.168.1.105 hadoop105 192.168.1.106 hadoop106 192.168.1.107 hadoop107 192.168.1.108 hadoop108 192.168.1.109 hadoop109 192.168.1.110 hadoop110 3.5 关闭防火墙1）查看防火墙开机启动状态 [root@hadoop101 ~]# chkconfig iptables –list 2）关闭防火墙 [root@hadoop101 ~]# chkconfig iptables off 3.6 在opt目录下创建文件1）创建kingge用户​ 在root用户里面执行如下操作 [root@hadoop101 opt]# adduser atguigu [root@hadoop101 opt]# passwd atguigu 更改用户 test 的密码 。 新的 密码： 无效的密码： 它没有包含足够的不同字符 无效的密码： 是回文 重新输入新的 密码： passwd： 所有的身份验证令牌已经成功更新。 2）设置kingge用户具有root权限修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： [root@hadoop101 kingge]# vi /etc/sudoers ## Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) ALL 修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。 3）在/opt目录下创建文件夹（1）在root用户下创建module、software文件夹 [root@hadoop101 opt]# mkdir module [root@hadoop101 opt]# mkdir software （2）修改module、software文件夹的所有者 [root@hadoop101 opt]# chown kingge:kingge module [root@hadoop101 opt]# chown kingge:kingge sofrware [root@hadoop101 opt]# ls -al 总用量 16 drwxr-xr-x. 6 root root 4096 4月 24 09:07 . dr-xr-xr-x. 23 root root 4096 4月 24 08:52 .. drwxr-xr-x. 4 kingge kingge 4096 4月 23 16:26 module drwxr-xr-x. 2 kingge kingge 4096 4月 23 16:25 software 3.7 安装jdk1）卸载现有jdk（1）查询是否安装java软件： [root@hadoop101 opt]# rpm -qa|grep java （2）如果安装的版本低于1.7，卸载该jdk： [root@hadoop101 opt]# rpm -e 软件包 2）复制文件用SecureCRT工具将jdk、Hadoop-2.7.2.tar.gz导入到opt目录下面的software文件夹下面 3）在linux系统下的opt目录中查看软件包是否导入成功。[root@hadoop101opt]# cd software/ [root@hadoop101software]# ls hadoop-2.7.2.tar.gz jdk-8u144-linux-x64.tar.gz 4）解压jdk到/opt/module目录下​ [root@hadoop101software]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/ 5）配置jdk环境变量​ （1）先获取jdk路径： [root@hadoop101 jdk1.8.0_144]# pwd /opt/module/jdk1.8.0_144 ​ （2）打开/etc/profile文件： [root@hadoop101 jdk1.8.0_144]# vi /etc/profile ​ 在profie文件末尾添加jdk路径： ​ ##JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin ​ （3）保存后退出： :wq ​ （4）让修改后的文件生效： [root@hadoop101 jdk1.8.0_144]# source /etc/profile ​ （5）重启（如果java -version可以用就不用重启）： [root@hadoop101 jdk1.8.0_144]# sync ​ [root@hadoop101 jdk1.8.0_144]# reboot 6）测试jdk安装成功[root@hadoop101 jdk1.8.0_144]# java -version java version “1.8.0_144” 3.8 安装Hadoop 这里使用的是已经编译过后的hadoop源码，官网下载的是非编译过后的，需要编译。如何编译参见。 Hadoop编译源码 1）进入到Hadoop安装包路径下： [root@hadoop101 ~]# cd /opt/software/ 2）解压安装文件到/opt/module下面 [root@hadoop101 software]# tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/ 3）查看是否解压成功 [root@hadoop101 software]# ls /opt/module/ hadoop-2.7.2 4）在/opt/module/hadoop-2.7.2/etc/hadoop路径下配置hadoop-env.sh （1）Linux系统中获取jdk的安装路径： [root@hadoop101 jdk1.8.0_144]# echo $JAVA_HOME /opt/module/jdk1.8.0_144 （2）修改hadoop-env.sh文件中JAVA_HOME 路径： [root@hadoop101 hadoop]# vi hadoop-env.sh 修改JAVA_HOME如下 export JAVA_HOME=/opt/module/jdk1.8.0_144 5）将hadoop添加到环境变量 （1）获取hadoop安装路径：[root@ hadoop101 hadoop-2.7.2]# pwd/opt/module/hadoop-2.7.2 （2）打开/etc/profile文件：[root@ hadoop101 hadoop-2.7.2]# vi /etc/profile 在profie文件末尾添加jdk路径：（shitf+g）\\##HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin （3）保存后退出：:wq （4）让修改后的文件生效：[root@ hadoop101 hadoop-2.7.2]# source /etc/profile（5）重启(如果hadoop命令不能用再重启)： [root@ hadoop101 hadoop-2.7.2]# sync [root@ hadoop101 hadoop-2.7.2]# reboot 6）修改/opt目录下的所有文件所有者为kingge ​ [root@hadoop101 opt]# chown kingge:kingge -R /opt/ 7）切换到kingge用户 ​ [root@hadoop101 opt]# su kingge","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(一)-理论知识了解","slug":"hadoop大数据-一-理论知识了解","date":"2018-02-20T16:31:59.000Z","updated":"2019-06-07T08:31:03.225Z","comments":true,"path":"2018/02/21/hadoop大数据-一-理论知识了解/","link":"","permalink":"http://kingge.top/2018/02/21/hadoop大数据-一-理论知识了解/","excerpt":"","text":"前言首先本人要先声明的是，这一系列的大数据总结，只是对于整个大数据生态的一个稍微深入的总结。并非是非常深入的，但是能够满足大部分人的需求。 一 大数据概念大数据的概念： 指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 这个概念的解释来源于百度百科，这样的解释太过空洞，那么就用更浅显易懂的解释是： 解决海量数据的存储和海量数据的分析计算 1.2 大数据的特点 图片来源于网上 1.3 大数据应用场景 4.给顾客推荐访问过的商品，我们使用淘宝之类的，当我们浏览某个商品之后，下次进来时，他会默认给你推荐你上次浏览过的商品。 第八：人工智能，目前最火的的风口 二 Hadoop框架2.1 Hadoop是什么1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构 2）主要解决，海量数据的存储和海量数据的分析计算问题（很明显只是一句废话，哈哈哈）。 3）广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈 2.2 Hadoop发展历史1）Lucene–Doug Cutting开创的开源软件，用java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎 2）2001年年底成为apache基金会的一个子项目 3）对于大数量的场景，Lucene面对与Google同样的困难 4）学习和模仿Google解决这些问题的办法 ：微型版Nutch 5）可以说Google是hadoop的思想之源(Google在大数据方面的三篇论文) ​ GFS —&gt;HDFS ​ Map-Reduce —&gt;MR ​ BigTable —&gt;Hbase 6）2003-2004年，Google公开了部分GFS和Mapreduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和Mapreduce机制，使Nutch性能飙升 7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。2006 年 3 月份，Map-Reduce和Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中 8）名字来源于Doug Cutting儿子的玩具大象 9）Hadoop就此诞生并迅速发展，标志这云计算时代来临 2.3 Hadoop三大发行版本Hadoop 三大发行版本: Apache、Cloudera、Hortonworks。 Apache版本最原始（最基础）的版本，对于入门学习最好。 Cloudera在大型互联网企业中用的较多。（因为他解决了hadoop各个版本和其他框架的兼容问题） Hortonworks文档较好。 1）Apache Hadoop 官网地址：http://hadoop.apache.org/releases.html 下载地址：https://archive.apache.org/dist/hadoop/common/ 2）Cloudera Hadoop 官网地址：https://www.cloudera.com/downloads/cdh/5-10-0.html 下载地址：http://archive-primary.cloudera.com/cdh5/cdh/5/ （1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。 （2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support （3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。 （4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。 （5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。 3）Hortonworks Hadoop 官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform （1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。 （2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。 （3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。 （4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。 （5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 （6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的microsoft Windows平台上本地运行。定价以集群为基础，每10个节点每年为12500美元。 2.4 Hadoop的优势1）高可靠性：因为Hadoop假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理。2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度（根据文件快开启相应数量的map任务处理，reduce的并行数量是可控的）。4）高容错性：自动保存多份副本数据，并且能够自动将失败的任务重新分配。 2.5 Hadoop组成2.5.0 四个组成1）Hadoop HDFS：一个高可靠、高吞吐量的分布式文件系统。2）Hadoop MapReduce：一个分布式的离线并行计算框架。3）Hadoop YARN：作业调度与集群资源管理的框架。4）Hadoop Common：支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作）。 2.5.1 HDFS 组成： namenode：存储文件元数据。例如文件名称，文件目录结构，文件属性（生成时间、副本数，文件权限），以及每个文件的快列表和快所在的datanode。hdfs的文件目录维护中心。datanode：真正存储文件的单位，也就是统称的块。secondary namenode：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS的元数据的快照。帮助namenode合并镜像文件和操作日志，合并完成后同步给namenode，减少namenode的处理任务的压力。他不能够替代namenode，只能够做备份（方便namenode意外挂掉后，能够恢复数据） 2.5.2 YARN1）ResourceManager(rm)：处理客户端请求、启动/监控ApplicationMaster、监控NodeManager、资源分配与调度； 2）NodeManager(nm)：单个节点上的资源管理、处理来自ResourceManager的命令、处理来自ApplicationMaster的命令； 3）ApplicationMaster：数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错。 4）Container：对任务运行环境的抽象，封装了CPU、内存等多维资源以及环境变量、启动命令等任务运行相关的信息。 2.5.3 MapReduce主要是获取HDFS存储的数据，通过拆分块的形式进行数据获取分析处理，最后输出自己想要的结果。 MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据 2）Reduce阶段对Map结果进行汇总 上图简单的阐明了map和reduce的两个过程或者作用，虽然不够严谨，但是足以提供一个大概的认知，map过程是一个蔬菜到制成食物前的准备工作，reduce将准备好的材料合并进而制作出食物的过程。 2.6 大数据整个结构 图片来源于网上 图中涉及的技术名词解释如下：1）Sqoop：sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 一般是将关系型数据库的数据导入到hive或者HDFS中，目的就是为了分析这些数据，因为我们知道关系型数据库当数据量变得很大时，通过sql去统计查询，那么就会卡死。就是因为整个架构他们本省就是不支持的，他们通常进行的是扫表操作。2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息（3）支持通过Kafka服务器和消费机集群来分区消息。（4）支持Hadoop并行数据加载。4）Storm：Storm为分布式实时计算提供了一组通用原语，可被用于“流处理”之中，实时处理消息并更新数据库。这是管理队列及工作者集群的另一种方式。 Storm也可被用于“连续计算”（continuous computation），对数据流做连续查询，在计算时就将结果以流的形式输出给用户。5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。Oozie协调作业就是通过时间（频率）和有效数据触发当前的Oozie工作流程。7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。8）Hive：hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。11）Mahout:Apache Mahout是个可扩展的机器学习和数据挖掘库，当前Mahout支持主要的4个用例：推荐挖掘：搜集用户动作并以此给用户推荐可能喜欢的事物。聚集：收集文件并进行相关文件分组。分类：从现有的分类文档中学习，寻找文档中的相似特征，并为无标签的文档进行正确的归类。频繁项集挖掘：将一组项分组，并识别哪些个别项会经常一起出现。12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 好了到这里我们已经了解了hadoop整个生态相关的概念和组成，以及架构。那么接下来让我们进行hadoop环境的搭建。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"关于2018年的计划","slug":"关于2018年的计划","date":"2018-02-18T16:00:00.000Z","updated":"2019-06-04T16:28:14.626Z","comments":true,"path":"2018/02/19/关于2018年的计划/","link":"","permalink":"http://kingge.top/2018/02/19/关于2018年的计划/","excerpt":"","text":"因为公司业务的转移和相关产品的开发，再加上大数据的火热，所以本人决定更新一些hadoop生态链相关的文章。所以敬请期待吧，哈哈哈哈哈。 Comming Soon！！！","categories":[{"name":"新年计划","slug":"新年计划","permalink":"http://kingge.top/categories/新年计划/"}],"tags":[{"name":"新年计划","slug":"新年计划","permalink":"http://kingge.top/tags/新年计划/"}]},{"title":"zookeeper知识学习","slug":"zookeeper知识学习","date":"2018-02-02T11:12:44.000Z","updated":"2019-06-02T13:24:38.033Z","comments":true,"path":"2018/02/02/zookeeper知识学习/","link":"","permalink":"http://kingge.top/2018/02/02/zookeeper知识学习/","excerpt":"","text":"引言 最近公司开发saas模式的企业应用软件服务，所以下面是我个人使用和总结（掺杂了大数据相关的总结） 一、正文0.1 下载1）官网首页： https://zookeeper.apache.org/ 2）下载截图 1.1 概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 1.2 模型构造和特点 1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。 2）Leader负责进行投票的发起和决议，更新系统状态。 3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票。 4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。（例如现在zookeeper集群现在有四台，那么挂掉两台后就不能正常工作了。假设初始时只有三台，那么最多也是挂掉两台后就不能工作了。也就是说，部署三台和部署四台的效用其实是一样的，所以一般都是部署奇数台zookeeper，节省资源） 5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。 6）更新请求顺序进行（全局数据一致性的提现），来自同一个client的更新请求按其发送顺序依次执行。 7）数据更新原子性，一次数据更新要么成功，要么失败。 8）实时性，在一定时间范围内（数据一致性的更新会有延迟），client能读到最新数据。 9）一次性监听（缺点）-这个缺点我们在后面可以用代码解决。 1.3 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。每个节点的存储的数据量，也决定了他的应用场景并不是存储大量的数据。下面的1.4章节会阐述到他的作用-应用场景 1.4 应用场景如果某个需求：当某个节点发生变化，通知其他关注这个节点的其他节点。那么就可以使用**zookeeper** 项目中常用到分布式锁和配置管理 1.4.1 统一命名服务 意思就是：我们通常使用域名来访问某个网站，但是域名对应的ip我们是不需要关注的为了系统的容错性，我们访问Baidu的这个请求是会随机寻找一个正常运行的服务器去处理。那么我们就可以使用zookeeper来进行管理。管理可以访问到Baidu这个网址的ip列表。客户端每次请求百度时，只需要去请求这个zookeeper获取可访问的ip即可。实现动态ip的上下线管理 1.4.2 统一配置管理 1.4.3 统一集群管理集群管理结构图如下所示。 1.4.4 服务器节点动态上下线 1.4.5 软负载均衡 控制某个服务器的访问数，达到资源合理分配 1.4.6 分布式锁（主要原理是同一路径下的节点名称不能重复，不能重复创建）有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。（在下面的3.2章节会讲到zookeeper顺序节点的相关内容） 好的博客： https://my.oschina.net/aidelingyu/blog/1600979 https://www.jianshu.com/p/5d12a01018e1 1.4.7 队列管理两种类型的队列： 1、 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 2、队列按照 FIFO 方式进行入队和出队操作。 第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。 二 Zookeeper安装2.1 本地模式安装部署1）安装前准备： （1）安装jdk （2）通过cshell工具拷贝zookeeper到linux系统下 （3）修改tar包权限 [kingge@hadoop102 software]$ chmod u+x zookeeper-3.4.10.tar.gz （4）解压到指定目录 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 2）配置修改 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； ​ 进入zoo.cfg文件：vim zoo.cfg ​ 修改dataDir路径为 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹 ​ [kingge@hadoop102 zookeeper-3.4.10]$ mkdir zkData 3）操作zookeeper （1）启动zookeeper [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start （2）查看进程是否启动 ​ [kingge@hadoop102 zookeeper-3.4.10]$ jps 4020 Jps 4001 QuorumPeerMain （3）查看状态： [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: standalone （4）启动客户端： [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkCli.sh （5）退出客户端： [zk: localhost:2181(CONNECTED) 0] quit （6）停止zookeeper [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh stop 2.2 配置参数解读解读zoo.cfg文件中参数含义 1）tickTime=2000：通信心跳数，Zookeeper服务器心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) 2）initLimit=10：Leader和Follower初始通信时限（10* tickTime – 也就是不要超过二十秒） 集群中的follower跟随者服务器与leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 投票选举新leader的初始化时间 Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 Leader允许Follower在initLimit时间内完成这个工作。 3）syncLimit=5：Leader和Follower同步通信时限（5* tickTime – 也就是不要超过十秒） 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。（默认超过十秒，leader就认为follwer已经碟机） 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 4）dataDir：数据文件目录+数据持久化路径 保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。 5）clientPort=2181：客户端连接端口 监听客户端连接的端口 2.3 分布式模式下的安装详见第四章节 三 Zookeeper内部原理3.1 选举机制1）半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。所以zookeeper**适合装在奇数台机器上**。 2）Zookeeper虽然在配置文件中并没有指定master和slave。但是，zookeeper工作时，是有一个节点为leader，其他则为follower，Leader是通过内部的选举机制临时产生的。 3）以一个简单的例子来说明整个选举的过程。 假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。 （1）服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是LOOKING状态。 （2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3 5/2=2.5 向上取整3)，所以服务器1、2还是继续保持LOOKING状态。 （3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的leader。 （4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。 （5）服务器5启动，同4一样当小弟。 3.2 节点类型1）Znode有两种类型： 短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 2）Znode有四种形式的目录节点（默认是persistent ） （1）持久化目录节点（PERSISTENT） ​ 客户端与zookeeper断开连接后，该节点依旧存在。 （2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） ​ 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。（保证创建的节点名称不会重复） （3）临时目录节点（EPHEMERAL） 客户端与zookeeper断开连接后，该节点被删除。 （4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 3）创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 4）在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序（应用场景1.4.6 分布式锁 第二种方式） 3.3 stat结构体1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid 每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。 事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。 2）ctime - znode被创建的毫秒数(从1970年开始) 3）mzxid - znode最后更新的zxid 4）mtime - znode最后修改的毫秒数(从1970年开始) 5）pZxid-znode最后更新的子节点zxid 6）cversion - znode子节点变化号，znode子节点修改次数 7）dataversion - znode数据变化号 8）aclVersion - znode访问控制列表的变化号 9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。 10）dataLength- znode的数据长度 11）numChildren - znode子节点数量 3.4 监听器原理 3.5 写数据流程 1.收到请求，先找到leader节点 2.广播请求给其他follower 3.哥哥follower写入数据，写入成功后，通知leader写入成功。（半数以上follower写入成功即为写入数据成功） 4.leader通知最初收到客户请求的server，数据写入成功，该server通知客户端写入数据成功 四 Zookeeper实战4.1 分布式安装部署0）集群规划 在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 1）解压安装 （1）解压zookeeper安装包到/opt/module/目录下 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ （2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData ​ mkdir -p zkData （3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg ​ mv zoo_sample.cfg zoo.cfg 2）配置zoo.cfg文件 ​ （1）具体配置 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 增加如下配置 ​ #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 （2）配置参数解读 Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器；（必须唯一） B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 3）集群操作 （1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件 ​ touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 （2）编辑myid文件 ​ vi myid ​ 在文件中添加与server对应的编号：如2 （3）拷贝配置好的zookeeper到其他机器上（可以用shell脚本进行分发数据） ​ scp -r zookeeper-3.4.10/ root@hadoop103.kingge.com:/opt/app/ ​ scp -r zookeeper-3.4.10/ root@hadoop104.kingge.com:/opt/app/ ​ 并分别修改myid文件中内容为3、4 （4）分别启动zookeeper ​ [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start （5）查看状态 [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower 分析：当第二个zookeeper启动时，因为2 &gt; 3/2=1.5，所以他被投票为了Leader，所以其他的节点就是follwer 4.2 客户端命令行操作 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建 -s 含有序列 -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 1）启动客户端（随便连接那个zookeeper都可以，因为他们内容都是一样的，下面连接的是103服务器） [kingge@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh 2）显示所有操作命令 [zk: localhost:2181(CONNECTED) 1] help 3）查看当前znode中所包含的内容 [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper] 4）查看当前节点数据并能看到更新次数等数据 [zk: localhost:2181(CONNECTED) 1] ls2 / [zookeeper] cZxid = 0x0 ctime = Thu Jan 01 08:00:00 CST 1970 mZxid = 0x0 mtime = Thu Jan 01 08:00:00 CST 1970 pZxid = 0x0 cversion = -1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 5）创建普通节点（注意创建节点时需要写入一些数据，否则创建不成功-例如create /app1 这样的话创建是没有效果的） [zk: localhost:2181(CONNECTED) 2] create /app1 “hello app1” Created /app1 [zk: localhost:2181(CONNECTED) 4] create /app1/server101 “192.168.1.101” Created /app1/server101 1.不支持递归创建节点，比如你要创建/app1/a,如果app1不存在，你就不能创建a( KeeperException.NoNode)。2.不可以再ephemeral类型的节点下创建子节点(KeeperException.NoChildrenForEphemerals)。（因为他本身是临时节点）3.如果指定的节点已经存在，会触发KeeperException.NodeExists 异常,当然了对于sequential类型的，不会抛出这个异常。（有编号类型的节点名称会自动递增）4.数据内容不能超过1M,否则将抛出KeeperException异常。 6）获得节点的值 [zk: localhost:2181(CONNECTED) 6] get /app1 hello app1 cZxid = 0x20000000a ctime = Mon Jul 17 16:08:35 CST 2017 mZxid = 0x20000000a mtime = Mon Jul 17 16:08:35 CST 2017 pZxid = 0x20000000b cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 10 numChildren = 1 [zk: localhost:2181(CONNECTED) 8] get /app1/server101 192.168.1.101 cZxid = 0x20000000b ctime = Mon Jul 17 16:11:04 CST 2017 mZxid = 0x20000000b mtime = Mon Jul 17 16:11:04 CST 2017 pZxid = 0x20000000b cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 13 numChildren = 0 7）创建短暂节点 [zk: localhost:2181(CONNECTED) 9] create -e /app-emphemeral 8888 （1）在当前客户端是能查看到的 [zk: localhost:2181(CONNECTED) 10] ls / [app1, app-emphemeral, zookeeper] （2）退出当前客户端然后再重启客户端 ​ [zk: localhost:2181(CONNECTED) 12] quit [kingge@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh （3）再次查看根目录下短暂节点已经删除 ​ [zk: localhost:2181(CONNECTED) 0] ls / [app1, zookeeper] 8）创建带序号的节点 ​ （1）先创建一个普通的根节点app2 ​ [zk: localhost:2181(CONNECTED) 11] create /app2 “app2” ​ （2）创建带序号的节点 ​ [zk: localhost:2181(CONNECTED) 13] create -s /app2/aa 888 Created /app2/aa0000000000 [zk: localhost:2181(CONNECTED) 14] create -s /app2/bb 888 Created /app2/bb0000000001 [zk: localhost:2181(CONNECTED) 15] create -s /app2/cc 888 Created /app2/cc0000000002 如果原节点下有1个节点，则再排序时从1开始，以此类推。 [zk: localhost:2181(CONNECTED) 16] create -s /app1/aa 888 Created /app1/aa0000000001 9）修改节点数据值 [zk: localhost:2181(CONNECTED) 2] set /app1 999 10）节点的值变化监听（一次性触发器）（Watch**的通知事件是从服务器发送给客户端的，是异步的**） ​ （1）在104主机上注册监听/app1节点数据变化（） 需要注意的是，注册一次监听，只能够响应一次，如果/app1节点的数据修改了两次，那么只显示第一次监听的信息，第二次不会有任何响应，想要得到响应，需要再次监听 [zk: localhost:2181(CONNECTED) 26] get /app1 watch ​ （2）在103主机上修改/app1节点的数据 [zk: localhost:2181(CONNECTED) 5] set /app1 777 ​ （3）观察104主机收到数据变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1 11）节点的子节点变化监听（路径变化） ​ （1）在104主机上注册监听/app1节点的子节点变化 [zk: localhost:2181(CONNECTED) 1] ls /app1 watch [aa0000000001, server101] ​ （2）在103主机/app1节点上创建子节点 [zk: localhost:2181(CONNECTED) 6] create /app1/bb 666 Created /app1/bb ​ （3）观察104主机收到子节点变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 12）删除节点 [zk: localhost:2181(CONNECTED) 4] delete /app1/bb 13）递归删除节点 [zk: localhost:2181(CONNECTED) 7] rmr /app2 14）查看节点状态 [zk: localhost:2181(CONNECTED) 12] stat /app1 cZxid = 0x20000000a ctime = Mon Jul 17 16:08:35 CST 2017 mZxid = 0x200000018 mtime = Mon Jul 17 16:54:38 CST 2017 pZxid = 0x20000001c cversion = 4 dataVersion = 2 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 3 numChildren = 2 15）exists 节点 这个函数很特殊，因为他可以监听一个尚未存在的节点，这是getData，getChildren不能做到的。exists可以监听一个节点的生命周期：从无到有，节点数据的变化，从有到无。 在传递给exists的watcher里，当path指定的节点被成功创建后，watcher会收到NodeCreated事件通知。当path所指定的节点的数据内容发送了改变后，wather会受到NodeDataChanged事件通知。 这里最需要注意的就是，exists可以监听一个未存在的节点，这是他与getData，getChildren本质的区别。 注意看上面的代码，其实我们已经实现了多次监听，解决了zookeeper单次监听的缺点。关键代码，我们在监听器里面，又再次声明了一次监听---zkClient.exists(&quot;eclipse&quot;,true) 16） getData 16） getChildren 4.3 API应用4.3.1 Eclipse环境搭建1）创建一个工程 2）解压zookeeper-3.4.10.tar.gz文件 3）拷贝zookeeper-3.4.10.jar、jline-0.9.94.jar、log4j-1.2.16.jar、netty-3.10.5.Final.jar、slf4j-api-1.6.1.jar、slf4j-log4j12-1.6.1.jar到工程的lib目录。并build一下，导入工程。 4）拷贝log4j.properties文件到项目根目录 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 4.3.2 创建ZooKeeper客户端private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void init() throws Exception &#123;//创建zookeeper连接的时候同时注册一个全局的默认的事件监听器 – // event.getType() 永远为null默认监听到None事件// //默认监听也可以使用register方法注册 //zkClient.register(watcherDefault); zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（用户的业务逻辑） System.out.println(event.getType() + &quot;--&quot; + event.getPath()); // 再次启动监听 - 解决zookeeper单次监听的缺点 try &#123; zkClient.getChildren(&quot;/&quot;, true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125;这里的watcher是该客户端总的监听方法，任何操作都会执行，而且是可以多次执行，并非单次。 4.3.3 创建子节点// 创建子节点@Testpublic void create() throws Exception &#123; // 数据的增删改查 // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型 String nodeCreated = zkClient.create(&quot;/eclipse&quot;, &quot;hello zk&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT);&#125; 4.3.4 获取子节点并监听// 获取子节点 @Test public void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren(&quot;/&quot;, true); for (String child : children) &#123; System.out.println(child); &#125; // 延时阻塞 Thread.sleep(Long.MAX_VALUE); &#125; 4.3.5 判断znode是否存在// 判断znode是否存在 @Test public void exist() throws Exception &#123; Stat stat = zkClient.exists(&quot;/eclipse&quot;, false); System.out.println(stat == null ? &quot;not exist&quot; : &quot;exist&quot;); &#125; 4.3.6 事件类型对照表 本表总结：exits和getData设置数据监视，而getChildren设置子节点监视 4.3.7 实现永久监听（伪）我们知道zookeeper的监听是一次性监听（on-time-trriger） 详情可查看 4.3.2代码 和 4.2 的15）exists 节点 4.4 案例总结1）需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线 2）需求分析 3）具体实现： （0）现在集群上创建/servers节点 [zk: localhost:2181(CONNECTED) 10] create /servers “servers” Created /servers （1）服务器端代码 package com.kingge.zkcase;import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.ZooDefs.Ids;public class DistributeServer &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException&#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); &#125; // 注册服务器 public void registServer(String hostname) throws Exception&#123; String create = zk.create(parentNode + \"/server\", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +\" is noline \"+ create); &#125; // 业务功能 public void business(String hostname) throws Exception&#123; System.out.println(hostname+\" is working ...\"); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 利用zk连接注册服务器信息 server.registServer(args[0]); // 启动业务功能 server.business(args[0]); &#125;&#125; （2）客户端代码 package com.kingge.zkcase;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = &quot;/servers&quot;; private volatile ArrayList&lt;String&gt; serversList = new ArrayList&lt;&gt;(); // 创建到zk的客户端连接 public void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 再次启动监听 try &#123; getServerList(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // public void getServerList() throws Exception &#123; // 获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); for (String child : children) &#123; byte[] data = zk.getData(parentNode + &quot;/&quot; + child, false, null); servers.add(new String(data)); &#125; // 把servers赋值给成员serverList，已提供给各业务线程使用 serversList = servers; System.out.println(serversList); &#125; // 业务功能 public void business() throws Exception &#123; System.out.println(&quot;client is working ...&quot;);Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 业务进程启动 client.business(); &#125;&#125; 4.5 zookeeper核心原理（事件） https://blog.csdn.net/yinwenjie/article/details/47685077 五 好的总结网站\\1. https://blog.csdn.net/liu857279611/article/details/70495413 \\2. https://www.jianshu.com/p/a1d7826073e6 \\3. https://blog.csdn.net/yinwenjie/article/details/47685077 \\4. https://www.jianshu.com/p/5d12a01018e1","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://kingge.top/categories/zookeeper/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"hadoop，linux","slug":"hadoop，linux","permalink":"http://kingge.top/tags/hadoop，linux/"}]},{"title":"聊聊分布式事务，再说说解决方案-cap","slug":"聊聊分布式事务，再说说解决方案-cap","date":"2017-10-18T09:57:58.000Z","updated":"2017-10-18T10:00:42.654Z","comments":true,"path":"2017/10/18/聊聊分布式事务，再说说解决方案-cap/","link":"","permalink":"http://kingge.top/2017/10/18/聊聊分布式事务，再说说解决方案-cap/","excerpt":"","text":"数据库事务 在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会经常使用到。但是即使如此，可能对于一些细节问题，很多人仍然不清楚。比如很多人都知道数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。但是再往下比如问到隔离性指的是什么的时候可能就不知道了，或者是知道隔离性是什么但是再问到数据库实现隔离的都有哪些级别，或者是每个级别他们有什么区别的时候可能就不知道了。 本文并不打算介绍这些数据库事务的这些东西，有兴趣可以搜索一下相关资料。不过有一个知识点我们需要了解，就是假如数据库在提交事务的时候突然断电，那么它是怎么样恢复的呢？ 为什么要提到这个知识点呢？ 因为分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，所以我们在做分布式系统的时候，最先考虑的就是这种情况。这些异常可能有 机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常等等… 我们接着说本地事务数据库断电的这种情况，它是怎么保证数据一致性的呢？我们使用SQL Server来举例，我们知道我们在使用 SQL Server 数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多。数据库进行任何写入操作的时候都是要先写日志的，同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。 接着，我们就说一下分布式事务。 分布式理论 当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况了，而在这种ACID的集群环境下，再想保证集群的ACID几乎是很难达到，或者即使能达到那么效率和性能会大幅下降，最为关键的是再很难扩展新的分区了，这个时候如果再追求集群的ACID会导致我们的系统变得很差，这时我们就需要引入一个新的理论原则来适应这种集群的情况，就是 CAP 原则或者叫CAP定理，那么CAP定理指的是什么呢？ CAP定理 CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性： 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成 具体地讲在分布式系统中，在任何数据库设计中，一个Web应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。 这个定理在迄今为止的分布式系统中都是适用的！ 为什么这么说呢？ 转载链接描述的很到位：http://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html","categories":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"数据库","slug":"数据库","permalink":"http://kingge.top/tags/数据库/"}]},{"title":"数据库中的undo和redo日志","slug":"数据库中的undo和redo日志","date":"2017-10-18T09:52:46.000Z","updated":"2017-10-18T09:55:01.852Z","comments":true,"path":"2017/10/18/数据库中的undo和redo日志/","link":"","permalink":"http://kingge.top/2017/10/18/数据库中的undo和redo日志/","excerpt":"","text":"转载好的博客解释1： http://blog.csdn.net/kobejayandy/article/details/50885693 转载好的博客解释2： http://www.cnblogs.com/Bozh/archive/2013/03/18/2966494.html","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"数据库","slug":"数据库","permalink":"http://kingge.top/tags/数据库/"}]},{"title":"vSphere与Workstation虚拟机交互的几种方法","slug":"vSphere与Workstation虚拟机交互的几种方法","date":"2017-10-18T08:11:28.000Z","updated":"2017-10-18T08:14:18.402Z","comments":true,"path":"2017/10/18/vSphere与Workstation虚拟机交互的几种方法/","link":"","permalink":"http://kingge.top/2017/10/18/vSphere与Workstation虚拟机交互的几种方法/","excerpt":"","text":"参见转载链接： http://wangchunhai.blog.51cto.com/225186/1884052","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"centos","slug":"centos","permalink":"http://kingge.top/tags/centos/"},{"name":"vmware","slug":"vmware","permalink":"http://kingge.top/tags/vmware/"}]},{"title":"查看虚拟机里的Centos7的IP","slug":"查看虚拟机里的Centos7的IP","date":"2017-10-18T07:48:43.000Z","updated":"2017-10-18T08:09:04.483Z","comments":true,"path":"2017/10/18/查看虚拟机里的Centos7的IP/","link":"","permalink":"http://kingge.top/2017/10/18/查看虚拟机里的Centos7的IP/","excerpt":"","text":"登录虚拟机 输入用户名和密码（用户名一般是root） 查看ip 指令 ip addr 指令： 查看当前虚拟机ip 我们发现ens32 没有 inet 这个属性，没有出现ip，那么说明在设置的时候没有开启，需要先去设置。 当前位置：[root@localhost ~]# pwd/root[root@localhost ~]# 接着来查看ens32网卡的配置： vi /etc/sysconfig/network-scripts/ifcfg-ens32 注意vi后面加空格. etc 文件夹的位置在于 [root@localhost ~]# cd ..[root@localhost /]# lsbin dev home lib64 mnt proc run srv tmp varboot etc lib media opt root sbin sys usr 查看 ifcfg-ens32 的内容 从配置清单中可以发现 CentOS 7 默认是不启动网卡的（ONBOOT=no）。 把这一项改为YES（ONBOOT=yes） – (按 i 进入编辑模式 ，修改完，按 esc退出编辑模式，然后 按 ctrl + shift + : 输入 wq 完成编辑) 然后重启网络服务： sudo service network restart 然后我们再输入 ip addr 命令 使用第三方工具登录 这里是用的是 xshell，你也可以用winscp（这个一般是用来传文件的） 然后点击连接，输入用户名和密码，便可以进入命令界面","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"centos","slug":"centos","permalink":"http://kingge.top/tags/centos/"},{"name":"vmware","slug":"vmware","permalink":"http://kingge.top/tags/vmware/"}]},{"title":"activity工作流框架——数据库表结构说明","slug":"activity工作流框架——数据库表结构说明","date":"2017-10-12T06:54:33.000Z","updated":"2017-10-12T07:08:08.890Z","comments":true,"path":"2017/10/12/activity工作流框架——数据库表结构说明/","link":"","permalink":"http://kingge.top/2017/10/12/activity工作流框架——数据库表结构说明/","excerpt":"","text":"本文转载于： http://www.jianshu.com/p/f9fd1cc02eae activity一共23张表 表的命名第一部分都是以 ACT_开头的。 表的命名第二部分是一个两个字符用例表的标识 act_ge_*： ‘ge’代表general（一般）。普通数据，各种情况都使用的数据。 act_gebytearray：二进制数据表，用来保存部署文件的大文本数据1.ID:资源文件编号，自增长2.REVINT:版本号3.NAME:资源文件名称4.DEPLOYMENTID:来自于父表act_redeployment的主键5.BYTES:大文本类型，存储文本字节流 act_geproperty：属性数据表，存储这整个流程引擎级别的数据。在初始化表结构时，会默认插入三条记录。1.NAME:属性名称2.VALUE_:属性值3.REV_INT:版本号 act_hi_*： hi’代表 history（历史）。就是这些表包含着历史的相关数据，如结束的流程实例、变量、任务、等等。 act_hiactinst：历史节点表1.ID : 标识2.PROC_DEFID :流程定义id3.PROC_INSTID : 流程实例id4.EXECUTIONID : 执行实例5.ACTID : 节点id6.ACTNAME : 节点名称7.ACTTYPE : 节点类型8.ASSIGNEE_ : 节点任务分配人9.STARTTIME : 开始时间10.ENDTIME : 结束时间11.DURATION : 经过时长 act_hi_attachment：历史附件表 act_hicomment：历史意见表1.ID :标识2.TYPE : 意见记录类型 为comment 时 为处理意见3.TIME : 记录时间4.USERID :5.TASKID ： 对应任务的id6.PROC_INSTID : 对应的流程实例的id7.ACTION ： 为AddComment 时为处理意见8.MESSAGE : 处理意见9.FULLMSG : act_hidetail：历史详情表，启动流程或者在任务complete之后,记录历史流程变量1.ID : 标识2.TYPE_ : variableUpdate 和 formProperty 两种值3.PROC_INSTID : 对应流程实例id4.EXECUTIONID : 对应执行实例id5.TASKID : 对应任务id6.ACT_INSTID : 对应节点id7.NAME : 历史流程变量名称，或者表单属性的名称8.VARTYPE : 定义类型9.REV : 版本10.TIME : 导入时间11.BYTEARRAYID12.DOUBLE : 如果定义的变量或者表单属性的类型为double，他的值存在这里13.LONG : 如果定义的变量或者表单属性的类型为LONG ,他的值存在这里14.TEXT : 如果定义的变量或者表单属性的类型为string，值存在这里15.TEXT2: act_hi_identitylink：历史流程人员表 act_hiprocinst： 历史流程实例表1.ID : 唯一标识2.PROC_INSTID : 流程ＩＤ3.BUSINESSKEY : 业务编号4.PROC_DEFID ： 流程定义id5.STARTTIME : 流程开始时间6.ENT_TIME : 结束时间7.DURATION : 流程经过时间8.START_USERID : 开启流程用户id9.START_ACTID : 开始节点10.END_ACTID： 结束节点11.SUPER_PROCESS_INSTANCEID : 父流程流程id12.DELETEREASON : 从运行中任务表中删除原因 act_hitaskinst： 历史任务实例表1.ID ： 标识2.PROC_DEFID ： 流程定义id3.TASK_DEFKEY : 任务定义id4.PROC_INSTID : 流程实例ｉｄ5.EXECUTIONID : 执行实例id6.PARENT_TASKID : 父任务id7.NAME : 任务名称8.DESCRIPTION : 说明9.OWNER : 拥有人（发起人）10.ASSIGNEE : 分配到任务的人11.START_TIME : 开始任务时间12.ENDTIME : 结束任务时间13.DURATION_ : 时长14.DELETEREASON :从运行时任务表中删除的原因15.PRIORITY_ : 紧急程度16.DUEDATE : act_hi_varinst：历史变量表 act_id_*： id’代表 identity（身份）。这些表包含着标识的信息，如用户、用户组、等等。 act_idgroup:用户组信息表，用来存储用户组信息。1.ID：用户组名2.REVINT:版本号3.NAME:用户组描述信息4.TYPE_:用户组类型 act_id_info：用户扩展信息表 act_id_membership：用户与用户组对应信息表，用来保存用户的分组信息1.USERID:用户名2.GROUPID:用户组名 act_iduser：用户信息表1.ID:用户名2.REVINT:版本号3.FIRST:用户名称4.LAST:用户姓氏5.EMAIL:邮箱6.PWD_:密码 act_re_*： ’re’代表 repository（仓库）。带此前缀的表包含的是静态信息，如，流程定义、流程的资源（图片、规则，等）。 act_redeployment:部署信息表,用来存储部署时需要持久化保存下来的信息1.ID:部署编号，自增长2.NAME_:部署包的名称3.DEPLOYTIME:部署时间 act_re_model 流程设计模型部署表 act_reprocdef:业务流程定义数据表1.ID:流程ID，由“流程编号：流程版本号：自增长ID”组成2.CATEGORY:流程命名空间（该编号就是流程文件targetNamespace的属性值）3.NAME:流程名称（该编号就是流程文件process元素的name属性值）4.KEY:流程编号（该编号就是流程文件process元素的id属性值）5.VERSION:流程版本号（由程序控制，新增即为1，修改后依次加1来完成的）6.DEPLOYMENTID:部署编号7.RESOURCENAME:资源文件名称8.DGRM_RESOURCENAME:图片资源文件名称9.HAS_START_FROMKEY:是否有Start From Key 注：此表和ACT_RE_DEPLOYMENT是多对一的关系，即，一个部署的bar包里可能包含多个流程定义文件，每个流程定义文件都会有一条记录在ACT_REPROCDEF表内，每个流程定义的数据，都会对于ACT_GE_BYTEARRAY表内的一个资源文件和PNG图片文件。和ACT_GE_BYTEARRAY的关联是通过程序用ACT_GE_BYTEARRAY.NAME与ACT_REPROCDEF.NAME完成的，在数据库表结构中没有体现。 act_ru_*： ’ru’代表 runtime（运行时）。就是这个运行时的表存储着流程变量、用户任务、变量、作业，等中的运行时的数据。 activiti 只存储流程实例执行期间的运行时数据，当流程实例结束时，将删除这些记录。这就使这些运行时的表保持 的小且快。 act_ru_event_subscr act_ruexecution：运行时流程执行实例表1.ID：主键，这个主键有可能和PROC_INSTID相同，相同的情况表示这条记录为主实例记录。2.REV_：版本，表示数据库表更新次数。3.PROC_INSTID：流程实例编号，一个流程实例不管有多少条分支实例，这个ID都是一致的。4.BUSINESSKEY：业务编号，业务主键，主流程才会使用业务主键，另外这个业务主键字段在表中有唯一约束。5.PARENTID：找到该执行实例的父级，最终会找到整个流程的执行实例6.PROC_DEFID：流程定义ID7.SUPEREXEC： 引用的执行模板，这个如果存在表示这个实例记录为一个外部子流程记录，对应主流程的主键ID。8.ACTID： 节点id，表示流程运行到哪个节点9.ISACTIVE： 是否活动流程实例10.ISCONCURRENT：是否并发。上图同步节点后为并发，如果是并发多实例也是为1。11.ISSCOPE： 主实例为1，子实例为0。12.TENANTID : 这个字段表示租户ID。可以应对多租户的设计。13.IS_EVENT_SCOPE: 没有使用到事件的情况下，一般都为0。14.SUSPENSIONSTATE：是否暂停。 act_ruidentitylink：运行时流程人员表，主要存储任务节点与参与者的相关信息1.ID： 标识2.REV_： 版本3.GROUPID： 组织id4.TYPE_： 类型5.USERID： 用户id6.TASKID： 任务id act_ru_job act_rutask：运行时任务节点表1.ID：2.REV_：3.EXECUTIONID： 执行实例的id4.PROC_INSTID： 流程实例的id5.PROC_DEFID： 流程定义的id,对应act_reprocdef 的id6.NAME_： 任务名称，对应 task 的name7.PARENT_TASKID : 对应父任务8.DESCRIPTION_：9.TASK_DEFKEY： task 的id10.OWNER : 发起人11.ASSIGNEE： 分配到任务的人12.DELEGATION : 委托人13.PRIORITY： 紧急程度14.CREATETIME： 发起时间15.DUETIME：审批时长 act_ruvariable：运行时流程变量数据表1.ID：标识2.REV：版本号3.TYPE：数据类型4.NAME_：变量名5.EXECUTIONID： 执行实例id6.PROC_INSTID： 流程实例id7.TASKID： 任务id8.BYTEARRAYID：9.DOUBLE：若数据类型为double ,保存数据在此列10.LONG： 若数据类型为Long保存数据到此列11.TEXT： string 保存到此列12.TEXT2： 结论及总结: 流程文件部署主要涉及到3个表，分别是：ACT_GE_BYTEARRAY、ACT_RE_DEPLOYMENT、ACT_RE_PROCDEF。主要完成“部署包”–&gt;“流程定义文件”–&gt;“所有包内文件”的解析部署关系。从表结构中可以看出，流程定义的元素需要每次从数据库加载并解析，因为流程定义的元素没有转化成数据库表来完成，当然流程元素解析后是放在缓存中的，具体的还需要后面详细研究。 流程定义中的java类文件不保存在数据库里 。 组织机构的管理相对较弱，如果要纳入单点登录体系内还需要改造完成，具体改造方法有待研究。 运行时对象的执行与数据库记录之间的关系需要继续研究 历史数据的保存及作用需要继续研究。","categories":[{"name":"activity","slug":"activity","permalink":"http://kingge.top/categories/activity/"}],"tags":[{"name":"activity","slug":"activity","permalink":"http://kingge.top/tags/activity/"},{"name":"工作流","slug":"工作流","permalink":"http://kingge.top/tags/工作流/"}]},{"title":"关于web.xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序","slug":"关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序","date":"2017-10-10T08:22:44.000Z","updated":"2017-10-10T09:25:05.991Z","comments":true,"path":"2017/10/10/关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序/","link":"","permalink":"http://kingge.top/2017/10/10/关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序/","excerpt":"","text":"前言 今天跑一个web项目，想做一些初始化工作，于是使用Filter来实现，但是发现ServletContextListener，Servlet也是能够实现的。但是肯定会有先后顺序执行的问题，那么接下来探讨这个问题。 作者规则：为了节省部分人的时间，先说结论。结论就是标题的顺序：ServletContext - ServletContextListener- Filter、Servlet web加载 启动一个WEB项目的时候，WEB容器会去读取它的配置文件web.xml。 加载产生Servlet上下文实例，ServletContext 这个web项目的所有部分都将共享这个上下文。容器将转换为键值对，并交给servletContext。L例如我们在使用spring的时候，会配置applicationContext.xml &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; 依次加载Servlet的事件监听器 - ServletContextListener 并依次调用public void contextInitialized(ServletContextEvent sce)方法。加载和调用多个Listener的顺序由在web.xml中配置的依次顺序决定的。 &lt;listener&gt; &lt;listener-class&gt;com.wlx.core.application.ApplicaltionListener&lt;/listener-class&gt;&lt;/listener&gt;&lt;listener&gt; &lt;listener-class&gt;com.wlx.core.application.ApplicaltionListener2&lt;/listener-class&gt;&lt;/listener&gt;先执行 ApplicaltionListener的contextInitialized方法后执行ApplicaltionListener2的contextInitialized方法 我们可以通过这个方法做一些初始化工作：例如初始化数据库连接池，初始化redis，启动定时器服务，启动线程池做一些socket通讯服务等等工作。 然后在contextDestroyed方法关闭这些服务即可。 .依次加载Servlet的过滤器-Filter 并依次调用public void init(FilterConfig filterConfig) throws ServletException;方法加载和调用多个filter的顺序由在web.xml中配置的依次顺序决定的。 &lt;filter&gt; &lt;filter-name&gt;appFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.wlx.core.application.AppFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;appFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 依次加载Servlet Load-on-startup元素在web应用启动的时候指定了servlet被加载的顺序，它的值必须是一个整数。如果它的值是一个负整数或是这个元素不存在，那么容器会在该servlet被调用的时候(例如下面代码访问-/servlet/UploadFile 为后缀的时候才会去初始化init，并不会在项目启动时候访问init)，加载这个servlet。如果值是正整数或零，容器在配置的时候就加载并初始化这个servlet，容器必须保证值小的先被加载。如果值相等，容器可以自动选择先加载谁。 在servlet的配置当中，&lt;load-on-startup&gt;5&lt;/load-on-startup&gt;的含义是：标记容器是否在启动的时候就加载这个servlet。当值为0或者大于0时，表示容器在应用启动时就加载这个servlet；当是一个负数时或者没有指定时，则指示容器在该servlet被选择时才加载。正数的值越小，启动该servlet的优先级越高。 项目启动时会去调用 UploadFile的init方法&lt;servlet&gt; &lt;servlet-name&gt;UploadFile&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wlx.core.application.servlet.UploadFile&lt;/servlet-class&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;UploadFile&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/UploadFile&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 项目启动时不会去调用 EServlet的init方法，访问匹配规则的网址时才会去调用init，而且只调用一次 &lt;servlet&gt; &lt;servlet-name&gt;EServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wlx.core.application.servlet.EServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;EServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/EServlet&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 总结 以上是Web容器在启动时加载的顺序，启动加载只会加载一次。web.xml 的加载顺序是：ServletContext-&gt; context-param -&gt;listener -&gt; filter -&gt; servlet. 扩展知识-请求执行循序 在上面中我们总结web加载的执行顺序，那么一个请求的执行循序呢？实际上就是一个责任链模式的问题 依次执行过滤器filter的方法public void doFilter(ServletRequest request, ServletResponse response,FilterChain chain)，这个方法应用了责任链模式，当在该方法中使用chain.doFilter(request, response);则这个过滤器就调用下一个过滤器，直到过滤器链条完成调用，进入Servlet处理，这个时候doFilter并未执行完成，仅仅在servlet之前进行一连串的过滤处理。 进入相应Servlet并调用public void service(ServletRequest req, ServletResponse res)方法，或者说是GET和POST方法。public void doGet(HttpServletRequest request, HttpServletResponse respose)进行请求响应的业务处理。 Servlet处理完成后，执行chain.doFilter(request, response);执行其他过滤器链条的后置过滤处理，然后执行自己的后置处理。 以上Filter和Servlet的执行顺序有点像Spring AOP 的前置通知和后置通知与业务方法关系。在Filter的doFilter方法中的chain.doFilter(request, response);之前做的业务逻辑就像前置通知，之后的逻辑像后置通知。业务方法是Sevlet中的public void service(ServletRequest req, ServletResponse res)方法。并且可以由多个有序的过滤链条进行Servlet的过滤。 Filter的过滤请求的Servlet的范围与配置有关,Filter在每次访问Servlet时都会拦截过滤。 代码例子： public class MyFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;执行MyFilter init&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;执行MyFilter doFilter&quot;); System.out.println(&quot;执行MyFilter doFilter before&quot;); chain.doFilter(request, response); System.out.println(&quot;执行MyFilter doFilter after&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;执行MyFilter destroy&quot;); &#125;&#125;-------------------------------------------------------------public class MyFilter1 implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;执行MyFilter1 init&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;执行MyFilter1 doFilter &quot;); System.out.println(&quot;执行MyFilter1 doFilter before&quot;); chain.doFilter(request, response); System.out.println(&quot;执行MyFilter1 doFilter after&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;执行MyFilter1 destroy&quot;); &#125;&#125;------------------------------------------------------------------------public class MyServlet1 extends HttpServlet &#123; private static final long serialVersionUID = 1L; public void init() throws ServletException &#123; System.out.println(&quot;执行Servlet1 init()&quot;); &#125; public void destroy() &#123; System.out.println(&quot;执行Servlet1 destroy()&quot;); &#125; public void doGet(HttpServletRequest request, HttpServletResponse respose) throws ServletException, IOException &#123; System.out.println(&quot;执行Servlet1 service&quot;); &#125;&#125; 省略在web.xml中的配置 输出： 执行MyFilter doFilter执行MyFilter doFilter before执行MyFilter1 doFilter执行MyFilter1 doFilter before执行Servlet service执行MyFilter1 doFilter after执行MyFilter doFilter after","categories":[{"name":"javaweb","slug":"javaweb","permalink":"http://kingge.top/categories/javaweb/"}],"tags":[{"name":"javaweb","slug":"javaweb","permalink":"http://kingge.top/tags/javaweb/"},{"name":"web.xml","slug":"web-xml","permalink":"http://kingge.top/tags/web-xml/"}]},{"title":"软技能-代码之外的生存指南-把自己当做一个企业去思考","slug":"软技能-代码之外的生存指南-把自己当做一个企业去思考","date":"2017-10-09T00:46:26.000Z","updated":"2017-10-09T00:51:47.087Z","comments":true,"path":"2017/10/09/软技能-代码之外的生存指南-把自己当做一个企业去思考/","link":"","permalink":"http://kingge.top/2017/10/09/软技能-代码之外的生存指南-把自己当做一个企业去思考/","excerpt":"","text":"《软技能》—— 把自己当做一个企业去思考","categories":[{"name":"读书系统","slug":"读书系统","permalink":"http://kingge.top/categories/读书系统/"}],"tags":[{"name":"软技能","slug":"软技能","permalink":"http://kingge.top/tags/软技能/"},{"name":"代码之外的生存指南","slug":"代码之外的生存指南","permalink":"http://kingge.top/tags/代码之外的生存指南/"}]},{"title":"java到底是值传递还是引用传递","slug":"java到底是值传递还是引用传递","date":"2017-09-26T07:13:17.000Z","updated":"2017-09-26T07:56:21.045Z","comments":true,"path":"2017/09/26/java到底是值传递还是引用传递/","link":"","permalink":"http://kingge.top/2017/09/26/java到底是值传递还是引用传递/","excerpt":"","text":"引言 我们先给本文定下基调，java是值传递 有一种说法，引用传递实际上也就是值传递。这个说法很有意思，实际上这种说法也是有道理的，传递引用，这个引用实际上就是一个地址，也即是一个值。 什么是值传递和引用传递 首先，不要纠结于 Pass By Value 和 Pass By Reference 的字面上的意义，否则很容易陷入所谓的“一切传引用其实本质上是传值”这种并不能解决问题无意义论战中。更何况，要想知道Java到底是传值还是传引用，起码你要先知道传值和传引用含义。 一：搞清楚 基本类型 和 引用类型的不同之处 int num = 10;String str = &quot;hello&quot;; num是基本类型，值就直接保存在变量中。而str是引用类型，变量中保存的只是实际对象的地址。一般称这种变量为”引用”，引用指向实际对象，实际对象中保存着内容。 二：搞清楚赋值运算符（=）的作用 num = 20;str = &quot;java&quot;; 对于基本类型 num ，赋值运算符会直接改变变量的值，原来的值被覆盖掉。对于引用类型 str，赋值运算符会改变引用中所保存的地址，原来的地址被覆盖掉。但是原来的对象不会被改变（重要）。 例子 参数传递基本上就是赋值操作 第一个例子：基本类型void foo(int value) &#123; value = 100;&#125;foo(num); // num 没有被改变第二个例子：没有提供改变自身方法的引用类型void foo(String text) &#123; text = &quot;windows&quot;;&#125;foo(str); // str 也没有被改变第三个例子：提供了改变自身方法的引用类型StringBuilder sb = new StringBuilder(&quot;iphone&quot;);void foo(StringBuilder builder) &#123; builder.append(&quot;4&quot;);&#125;foo(sb); // sb 被改变了，变成了&quot;iphone4&quot;。第四个例子：提供了改变自身方法的引用类型，但是不使用，而是使用赋值运算符。StringBuilder sb = new StringBuilder(&quot;iphone&quot;);void foo(StringBuilder builder) &#123; builder = new StringBuilder(&quot;ipad&quot;);&#125;foo(sb); // sb 没有被改变，还是 &quot;iphone&quot;。 重点理解为什么，第三个例子和第四个例子结果不同？ 例子5 public class Employee &#123; public int age;&#125;public class Main &#123; public static void changeEmployee(Employee employee3) &#123; employee3 = new Employee(); // flag 1 employee3.age = 1000; &#125; public static void main(String[] args) &#123; Employee employee = new Employee(); employee.age = 100; changeEmployee(employee); System.out.println(employee.age); &#125;&#125;输出： 100如果把 flag 1 位置代码注释，那么程序结果输出1000---原因同上 总结 = 号的理解是最重要的，他是一个动词，可能会引起左边变量值的改变 java中方法参数传递方式是按值传递。 如果参数是基本类型，传递的是基本类型的字面量值的拷贝。也就是你我没有半毛钱关系 如果参数是引用类型，传递的是该参量所引用的对象在堆中地址值的拷贝。你我可能存在关系 = 是赋值操作（任何包含=的如+=、-=、 /=等等，都内含了赋值操作）。不再是你以前理解的数学含义了，而+ - /和 = 在java中更不是一个级别，换句话说， = 是一个动作，一个可以改变内存状态的操作，一个可以改变变量的符号，而+ - /却不会。这里的赋值操作其实是包含了两个意思：1、放弃了原有的值或引用；2、得到了 = 右侧变量的值或引用。Java中对 = 的理解很重要啊！！可惜好多人忽略了，或者理解了却没深思过。 对于基本数据类型变量，= 操作是完整地复制了变量的值。换句话说，“=之后，你我已无关联”；至于基本数据类型，就不在这科普了。 对于非基本数据类型变量，= 操作是复制了变量的引用。换句话说，“嘿，= 左侧的变量，你丫别给我瞎动！咱俩现在是一根绳上的蚂蚱，除非你再被 = 一次放弃现有的引用！！上面说了 = 是一个动作，所以我把 = 当作动词用啦！！”。而非基本数据类型变量你基本上可以参数本身是变量 参数传递本质就是一种 = 操作。参数是变量，所有我们对变量的操作、变量能有的行为，参数都有。所以把C语言里参数是传值啊、传指针啊的那套理论全忘掉，参数传递就是 = 操作。","categories":[{"name":"java","slug":"java","permalink":"http://kingge.top/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java深入理解","slug":"java深入理解","permalink":"http://kingge.top/tags/java深入理解/"}]},{"title":"继承之上溯造型和下溯造型","slug":"继承之上溯造型和下溯造型","date":"2017-09-12T03:26:59.000Z","updated":"2017-09-12T07:01:12.151Z","comments":true,"path":"2017/09/12/继承之上溯造型和下溯造型/","link":"","permalink":"http://kingge.top/2017/09/12/继承之上溯造型和下溯造型/","excerpt":"","text":"前言 我们在平时的开发编码中，都会用到上溯造型和下溯造型，只是我们并不知道他的官方叫法而已， 上溯造型跟继承和多态，以及动态绑定的关系很密切 ，关于这几个概念后面会有涉及到他们的概念。 继承和合成 继承：它的本质就是为了使得代码复用（可以基于已经存在的类构造一个新类。继承已经存在的类就可以复用这些类的方法和域。在此基础上，可以添加新的方法和域，从而扩充了类的功能。） 合成：在新类里创建原有的对象称为合成。这种方式可以重复利用现有的代码而不更改它的形式。 -----继承关键字extends表明新类派生于一个已经存在的类。已存在的类称为父类或基类，新类称为子类或派生类。例如:class Dog extends Animal &#123;&#125;类Dog继承了Animal，Animal类称为父类或基类，Dog类称为子类或派生类。---合成合成比较简单，就是在一个类中创建一个已经存在的类。class Dog &#123; Animal animal;&#125; 上溯造型 这个术语缘于继承关系图的传统画法：将基类至于顶部，而向下发展的就是派生类(子类)，发送给父类的消息亦可发给衍生类，父类包含子类。假设把子类赋值给父类，这个过程就称之为上溯造型— 这个时候只能够调用父类父类的方法，子类特有的方法不能够调用，子类变窄 //父类abstract class Animal &#123; public abstract void speak(); public void eat()&#123; &#125; &#125;//子类特有方法interface DoorGod &#123; void guard(); &#125; //Dog 子类和 Cat 子类class Cat extends Animal &#123; @Override public void eat() &#123; try &#123; Thread.sleep( 1000 ); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // super .eat(); System.out.println(&quot;cat eat&quot;); &#125; @Override public void speak() &#123; System.out.println( &quot; 喵喵 &quot; ); &#125; &#125; class Dog extends Animal implements DoorGod&#123; @Override public void speak() &#123; System.out.println( &quot; 汪汪 &quot; ); &#125; public void guard() &#123; while ( true )&#123; System.out.println( &quot; 汪汪 &quot; ); &#125; &#125; &#125; //测试方法public class TestShangSu&#123; public static void upcasting(Animal animal)&#123; animal.speak(); animal.eat(); &#125; @Test public void test1()&#123; Animal dog1 = new Dog(); upcasting(dog1); Animal cat = new Cat(); upcasting(cat); &#125; &#125;//输出 汪汪 喵喵 cat eat 这个时候为什么输出是：子类覆盖父类的方法，而不是父类的方法，这个涉及到动态绑定。后面再讲 由于upcasting(Animal animal)方法的参数是 Animal类型的，因此如果传入的参数是 Animal的子类，传入的参数就会被转换成父类Animal类型，这样你创建的Dog对象能使用的方法只是Animal中的签名方法；也就是说，在上溯的过程中，Dog的接口变窄了，它本身的一些方法（例如实现了 DoorGod的guard方法）就不可见了。如果你想使用Dog中存在而Animal中不存在的方法（比如guard方法），编译时不能通过的。由此可见，上溯造型是安全的类型转换。 如果Dog在上溯造型过程中想使用 DoorGod的guard方法，那么需要配合下溯造型和安全检查，来进行强制转换，讲Animal 下溯为 Dog类型。 注意的是：下溯是不安全的，由父类转化为子类，所以需要加上判断。 下溯造型 将基类转化为衍生类，不安全的操作，可能会引发ClassCastException。 上面的例子只需要加上这一层判断即可 public static void upcasting(Animal animal)&#123; if( animal instanceof Dog )&#123;//下溯造型判断 Dog dog = (Dog) animal; dog.guard(); &#125; animal.speak(); animal.eat(); &#125; 我们在使用注解实现请求方法的登录控制 登录拦截器里面有段关键代码使用的就是下溯造型 为什么使用上溯和下溯造型 上面的例子我们发现，关键的代码是upcasting方法，为什么在调用upcasting方法时要有意忽略调用它的对象类型呢？如果让upcasting方法简单地获取Dog句柄似乎更加直观易懂，但是那样会使衍生自Animal类的每一个新类都要实现专属自己的upcasting方法：例如Cat会实现一个重复的upcasting(Cat cat )这样的方法。 实现多态的好处和代码复利用。 动态绑定 在上面的upcasting方法，测试例子输出的是子类的方法，而非是父类的方法，但是我们使用的是父类去调用这些方法，为什么输出不是父类的呢？ upcasting它接收的是Animal句柄，当执行speak和eat方法时时，它是如何知道Animal句柄指向的是一个Dog对象而不是Cat对象呢？编译器是无从得知的，这涉及到接下来要说明的绑定问题。 Java实现了一种方法调用机制，可在运行期间判断对象的类型，然后调用相应的方法，这种在运行期间进行，以对象的类型为基础的绑定称为动态绑定。除非一个方法被声明为final，Java中的所有方法都是动态绑定的。 静态方法的绑定 他跟普通的方法不同，子类和父类方法都是静态的，子类如果去掉父类编译会错误 package Test;class Person &#123; static void eat() &#123; System.out.println(&quot;Person.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Person.speak()&quot;); &#125;&#125;class Boy extends Person &#123; static void eat() &#123; System.out.println(&quot;Boy.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Boy.speak()&quot;); &#125;&#125;class Girl extends Person &#123; static void eat() &#123; System.out.println(&quot;Girl.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Girl.speak()&quot;); &#125;&#125;public class Persons &#123; public static Person randPerson() &#123; switch ((int)(Math.random() * 2)) &#123; default: case 0: return new Boy(); case 1: return new Girl(); &#125; &#125; public static void main(String[] args) &#123; Person[] p = new Person[4]; for (int i = 0; i &lt; p.length; i++) &#123; p[i] = randPerson(); // 随机生成Boy或Girl &#125; for (int i = 0; i &lt; p.length; i++) &#123; p[i].eat(); &#125; &#125;&#125;//输出Person.eat()Person.eat()Person.eat()Person.eat() 对于静态方法而言，不管父类引用指向的什么子类对象，调用的都是父类的方法。 总结 上溯造型和动态绑定实际上就是多态的体现，下溯造型是为了解决因为上溯而导致衍生类功能变小的问题，继承则是上溯和下溯以及动态编译的基础。","categories":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"继承","slug":"继承","permalink":"http://kingge.top/tags/继承/"},{"name":"多态","slug":"多态","permalink":"http://kingge.top/tags/多态/"}]},{"title":"注解实现请求方法的登录控制","slug":"注解实现请求方法的登录控制","date":"2017-09-06T02:37:08.000Z","updated":"2017-09-06T03:56:00.492Z","comments":true,"path":"2017/09/06/注解实现请求方法的登录控制/","link":"","permalink":"http://kingge.top/2017/09/06/注解实现请求方法的登录控制/","excerpt":"","text":"前言 之前一直使用的是，拦截器来统一验证当前用户是否登录，通过验证cookie或者session里面的是否存在已经登录标识来完成登录逻辑判断。但是会发现，这个很麻烦，而且有很多配置需要配置，例如免验证URL等等配置，无法实现可拔插式方法级别的控制。 public class RequestInterceptor extends HandlerInterceptorAdapter &#123; public String[] allowUrls;//配置不拦截的资源，所以在代码里面来排除. public void setAllowUrls(String[] allowUrls) &#123; this.allowUrls = allowUrls; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; // TODO Auto-generated method stub &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // TODO Auto-generated method stub request.setCharacterEncoding(&quot;UTF8&quot;); HttpSession session=request.getSession();//获取登录的SESSION String sessionid=request.getSession().getId();//获取登录的SESSIONID String requestPath=request.getServletPath();//获取客户请求页面 //先过滤掉不需要判断SESSION的请求 for(String url : allowUrls) &#123; if(requestPath.contains(url)) &#123; return true; &#125; &#125; Object attribute = request.getSession().getAttribute(&quot;sys_user&quot;); if( attribute == null )&#123; response.sendRedirect(&quot;/index.jsp&quot;); &#125; return true; &#125; 大体上是这样的，通过allowUrls来控制免登录url（上面的代码其实可以使用配置文件的方式来配置allowUrls的值，可以不通过setAllowUrls的方式来赋值，但是为了方面扩展就加入了。） 这里会面临一个问题，那就是如果网站网页多的话，那么allowUrls的值会变得很庞大，可能会缺漏。所以下面讲解本人用到的解决方式—-注解 和 spring配置方式（跟数组形式没有什么区别） spring 配置方式path 对所有的请求拦截使用/**，对某个模块下的请求拦截使用：/myPath/*&lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot; /&gt; &lt;bean class=&quot;com.kingge.oa.user.LoginInterceptor&quot; /&gt;&lt;/mvc:interceptor&gt; 或者 &lt;!-- 拦截是否登录 &lt;mvc:interceptor&gt; 需拦截的地址 二级目录 &lt;mvc:mapping path=&quot;/*/*&quot;/&gt; &lt;bean class=&quot;com.jk.ssm.interceptor.RequestInterceptor&quot; &gt; &lt;property name=&quot;allowUrls&quot;&gt; //回去调用拦截器的 setAllowUrls 方法 &lt;list&gt; 如果请求中包含以下路径，则不进行拦截 &lt;value&gt;/account/login.html&lt;/value&gt; &lt;value&gt;/captcha/image.html&lt;/value&gt; &lt;value&gt;/register/register.html&lt;/value&gt; &lt;value&gt;/error/400.html&lt;/value&gt; &lt;value&gt;/error/404.html&lt;/value&gt; &lt;value&gt;/error/500.html&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/mvc:interceptor&gt; 使用注解关于注解 官方说辞：JDK5开始，java增加了对元数据(MetaData)的支持，怎么支持？答：通过Annotation(注解）来实现。Annotation提供了为程序元素设置元数据的方法。元数据：描述数据的数据。 个人理解：首先什么是元数据，元数据就是对一类事物的统称，他不仅限于某个事物的描述。例如我们有ABC三个系统，分别使用oracle，mysql，db2，都有登录功能，他们的用户表字段名称是不一样的。那么有个需求，我想把A系统的用户数据pour到B系统中，那么进行映射操作？这个时候就需要一个描述用户数据的一个统一标识（元数据）这样我们就可以先把，A系统数据映射到元数据，然后再从元数据取数据映射到B系统中。 粗俗的理解，元数据就是一个类的属性，但是他所具备的职能的而应用范围，跟真正意义上类的属性数不一样的。传统的类的属性他只描述这个类，元数据可以描述多个具有共性的类。 再举个例子，我们现在常用的数据中心（DC）就是使用了元数据来作为数据传输的媒介。 元数据作用：：Annotation就像代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取。读取到了程序元素的元数据，就可以执行相应的处理。通过注解，程序开发人员可以在不改变原有逻辑的情况下，在源代码文件中嵌入一些补充信息。代码分析工具、开发工具和部署工具可以通过解析这些注解获取到这些补充信息，从而进行验证或者进行部署等。 到java8为止一共提供了五个 注解 unchecked异常：运行时异常。是RuntimeException的子类，不需要在代码中显式地捕获unchecked异常做处理。Java异常 @SafeVarargs (java7新增）：java7的“堆污染”警告与@SafeVarargs堆污染：把一个不带泛型的对象赋给一个带泛型的变量是，就会发生堆污染。例如：下面代码引起堆污染，会给出警告List l2 = new ArrayList&lt;Number&gt;();List&lt;String&gt; ls = l2;3中方式去掉这个警告 3种方式去掉这个警告：使用注解@SafeVarargs修饰引发该警告的方法或构造器。使用@SuppressWarnings(“unchecked”) 修饰。使用编译器参数命令：-Xlint:varargs @Functionlnterface （java8新增）：修饰函数式接口使用该注解修饰的接口必须是函数式接口，不然编译会出错。那么什么是函数式接口？答：如果接口中只有一个抽象方法（可以包含多个默认方法或static方法），就是函数式接口。 五个基本元注解 元注解：描述注解的注解（概念跟元数据类似）。 java提供了6个元注解（Meta Annotation)，在java.lang.annotation中。其中5个用于修饰其他的Annonation定义。而@Repeatable专门用于定义Java8新增的重复注解。所以要定义注解必须使用到5个元注解来定义( 五个注解用法 详情百度 ) @Inherited @Documented @Retention（英文：保留） @Target ( 目标) 自定义注解 参见下面，例子或者白度，具体就不阐述了。 使用注解解决登录问题定义一个枚举类 作用： 是否进行验证权限（因为后期可能会增加权限判断注解，而且是否登录也可以说是权限判断的一种，所以这里的枚举类的作用就是保存是否进行权限判断信息） public enum Action&#123; Normal(&quot;0&quot;,&quot;执行权限验证&quot;), Skip(&quot;1&quot;, &quot;跳过权限验证&quot;); private final String key; private final String desc; private Action(String key, String desc) &#123; this.key = key; this.desc = desc; &#125; //省略get set方法 定义登录和权限注解 Login属性是action ，属性类型是Action（上面的枚举类） @Target(ElementType.METHOD)@Documented@Retention(RetentionPolicy.RUNTIME)public @interface Login&#123; Action action() default Action.Normal;&#125; @Target(&#123;java.lang.annotation.ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Permission&#123; String value() default &quot;&quot;; // 这里我是保存一个权限代码，例如赋值为4000，表示当前用户的必须具备4000的权限才能够访问方法 Action action() default Action.Normal;&#125; 拦截器public class LoginInterceptor extends HandlerInterceptorAdapter&#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception&#123; if(handler instanceof HandlerMethod)&#123; //是否为请求方法 HandlerMethod handlerMethod = (HandlerMethod) handler; Login login = handlerMethod.getMethodAnnotation(Login.class);//当前请求方法是否添加了Login注解 if( login != null &amp;&amp; &quot;0&quot;.equals(login.action().getKey()) )&#123;//判断属性的值是否是0-表示需要进行登录验证 Object attribute = request.getSession().getAttribute(&quot;sys_user&quot;); if( attribute == null )&#123; response.sendRedirect(&quot;/index.jsp&quot;); &#125; &#125; return true; &#125; return true; &#125;&#125; 在spring中配置拦截器&lt;mvc:interceptors&gt;&lt;bean class=&quot;com.kingge.oa.user.LoginInterceptor&quot;&gt;&lt;/bean&gt;&lt;/mvc:interceptors&gt; 给请求方法添加权限控制 @Login(action=Action.Skip) //不需要进行登录校验 @Permission(value=&quot;4000&quot;,action=Action.normal)//需要进行权限号为4000的权限校验 @RequestMapping(&quot;/list&quot;) public String list(Model model,HttpServletRequest request) &#123; request.getSession().setAttribute(&quot;sys_user&quot;, &quot;denglule&quot;); List&lt;User&gt; userList = userService.findAllObjects(); System.out.println( userList ); model.addAttribute(&quot;userList&quot;,userList ); return &quot;list&quot;; &#125; @Login(action=Action.Normal)//添加操作，需要校验是否登录 @RequestMapping(value=&quot;/add&quot;, method=RequestMethod.POST) public String add( User user ) &#123; System.out.println( user ); userService.insert(user); return &quot;forward:/user/list&quot;; &#125;","categories":[{"name":"java","slug":"java","permalink":"http://kingge.top/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java注解","slug":"java注解","permalink":"http://kingge.top/tags/java注解/"},{"name":"登录控制","slug":"登录控制","permalink":"http://kingge.top/tags/登录控制/"}]},{"title":"java8新特性","slug":"java8新特性","date":"2017-08-29T04:27:16.000Z","updated":"2017-09-01T02:08:20.226Z","comments":true,"path":"2017/08/29/java8新特性/","link":"","permalink":"http://kingge.top/2017/08/29/java8新特性/","excerpt":"","text":"Java 8可谓是自Java 5以来最具革命性的版本了，她在语言、编译器、类库、开发工具以及Java虚拟机等方面都带来了不少新特性。我们来一一回顾一下这些特性。 一、Lambda表达式 Lambda表达式可以说是Java 8最大的卖点，她将函数式编程引入了Java。Lambda允许把函数作为一个方法的参数，或者把代码看成数据。Lambda 是一个匿名函数。 一个Lambda表达式可以由用逗号分隔的参数列表、–&gt;符号与函数体三部分表示。例如： 例子1 需求： 比较TreeSet中数据，按小到大输出 使用匿名内部类实现一个排序功能 //采用匿名内部类的方式-实现比较器 Comparator&lt;Integer&gt; comparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return Integer.compare(o1, o2);//关键代码 &#125; &#125;;//传入比较器 TreeSet&lt;Integer&gt; tree2 = new TreeSet&lt;&gt;(comparator ); tree2.add(12); tree2.add(-12); tree2.add(100);System.out.println(tree2) //输出 -12 12 100 我们不难发现上面的代码存在一个问题：其实关键代码只有第七行，其他代码都是冗余的 使用Lambda表达式实现同样功能 //使用Lambda表达式，抽取关键代码，减少代码量Comparator&lt;Integer&gt; comparator2 = (x, y) -&gt; Integer.compare(x, y); //关键代码 TreeSet&lt;Integer&gt; tree = new TreeSet&lt;&gt;(comparator2 ); tree.add(12); tree.add(-12); tree.add(100); tree.forEach(System.out::println);//代替System.out.println 代码瞬间就变得很简短，你可能觉得这个有什么，没什么感觉。那么我们在进入第二个例子 例子2 需求：1.获取公司中年龄小于 35 的员工信息2.获取公司中工资大于 5000 的员工信息。。。。。。 前期准备实现一个Employee类,有四个属性 private int id;private String name;private int age;private double salary;忽略get/set方法和构造器 初始化一个List： List&lt;Employee&gt; emps = Arrays.asList( new Employee(101, &quot;张三&quot;, 18, 9999.99), new Employee(102, &quot;李四&quot;, 59, 6666.66), new Employee(103, &quot;王五&quot;, 28, 3333.33), new Employee(104, &quot;赵六&quot;, 8, 7777.77), new Employee(105, &quot;田七&quot;, 38, 5555.55)); 常规方法实现实现两个方法，然后传入需要过滤的源数据，返回过滤后的结果集 //需求：获取公司中年龄小于 35 的员工信息public List&lt;Employee&gt; filterEmployeeAge(List&lt;Employee&gt; emps)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee emp : emps) &#123; if(emp.getAge() &lt;= 35)&#123;//比较代码 list.add(emp); &#125; &#125; return list;&#125;//需求：获取公司中工资大于 5000 的员工信息public List&lt;Employee&gt; filterEmployeeSalary(List&lt;Employee&gt; emps)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee emp : emps) &#123; if(emp.getSalary() &gt;= 5000)&#123;//比较代码 list.add(emp); &#125; &#125; return list;&#125; 我们不难发现上面的代码存在一个问题：那就是两个方法除了比较部分不同，其他逻辑是一样的，存在大量冗余，假设有新的需求（例如求得求得名字姓王的员工）那么就需要再创建一个 filterEmployee**方法对应新的需求。 使用策略设计模式实现 提供父借口 和 两个 实现类（两个需求对应的逻辑实现类） // 父接口 @FunctionalInterfacepublic interface MyPredicate&lt;T&gt; &#123; public boolean test(T t); &#125;//需求1 实现类-年龄小于35public class FilterEmployeeForAge implements MyPredicate&lt;Employee&gt;&#123; @Override public boolean test(Employee t) &#123; return t.getAge() &lt;= 35; &#125;&#125;//需求1 实现类-工资大于5000public class FilterEmployeeForSalary implements MyPredicate&lt;Employee&gt; &#123; @Override public boolean test(Employee t) &#123; return t.getSalary() &gt;= 5000; &#125;&#125; 测试代码 // 通用过滤方法 public List&lt;Employee&gt; filterEmployee(List&lt;Employee&gt; emps, MyPredicate&lt;Employee&gt; mp)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee employee : emps) &#123; if(mp.test(employee))&#123; list.add(employee); &#125; &#125; return list; &#125; @Test public void test4()&#123; //传入实现年龄过滤的实现类 List&lt;Employee&gt; list = filterEmployee(emps, new FilterEmployeeForAge()); for (Employee employee : list) &#123; System.out.println(employee); &#125; System.out.println(&quot;------------------------------------------&quot;); List&lt;Employee&gt; list2 = filterEmployee(emps, new FilterEmployeeForSalary()); for (Employee employee : list2) &#123; System.out.println(employee); &#125; &#125; 使用策略模式比上一个的好处是：代码很清晰，便于维护，新的需求我们只需要再实现对应的需求实现类即可，然后传入MyPredicate```接口即可。缺点是：需要实现对应的需求类然后实现``` MyPredicate&lt;T&gt;```接口### **匿名内部类**这种方法类似于例子1中的 Comparator这个接口的实现```JAVA//直接使用 MyPredicate&lt;Employee&gt;接口，不去实现对应的需求类（上面的FilterEmployeeForSalary 和 FilterEmployeeForAge ） @Test public void test5()&#123; List&lt;Employee&gt; list = filterEmployee(emps, new MyPredicate&lt;Employee&gt;() &#123; @Override public boolean test(Employee t) &#123; return t.getId() &lt;= 103; &#125; &#125;); for (Employee employee : list) &#123; System.out.println(employee); &#125; &#125; 我们不难发现上面的代码存在一个问题：跟例子1一样，存在大量的冗余。 Lambda 表达式实现前期准备public List&lt;Employee&gt; filterEmployee(List&lt;Employee&gt; emps, MyPredicate&lt;Employee&gt; mp)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee employee : emps) &#123; if(mp.test(employee))&#123; list.add(employee); &#125; &#125; return list;&#125; @Testpublic void test6()&#123; List&lt;Employee&gt; list = filterEmployee(emps, (e) -&gt; e.getAge() &lt;= 35); list.forEach(System.out::println); System.out.println(\"------------------------------------------\"); List&lt;Employee&gt; list2 = filterEmployee(emps, (e) -&gt; e.getSalary() &gt;= 5000); list2.forEach(System.out::println);&#125; 我们不难发现上面的代码存在一个问题：这个代码，是不是已经非常简短了，感觉已经是终极的最简代码。但是实际上还有更简短的代码（使用stream api）缺点：太过依赖 MyPredicate 这个接口，假设这个接口不存在，该怎么办呢？（我们这里仅仅是做个假设） 终极实现方式：Stream API@Testpublic void test7()&#123; emps.stream() .filter((e) -&gt; e.getAge() &lt;= 35) .forEach(System.out::println); System.out.println(&quot;----------------------------------------------&quot;); emps.stream() .filter((e) -&gt; e.getSalary() &gt;= 5000) .forEach(System.out::println); System.out.println(&quot;----------------------------------------------&quot;); // 可以使用map 指定输出那个属性的值，代替普通的便利输出 emps.stream() .map(Employee::getName) .limit(3)// 输出前三个 .sorted()//排序 .forEach(System.out::println); &#125; 输出 Employee [id=101, name=张三, age=18, salary=9999.99]Employee [id=103, name=王五, age=28, salary=3333.33]Employee [id=104, name=赵六, age=8, salary=7777.77]----------------------------------------------Employee [id=101, name=张三, age=18, salary=9999.99]Employee [id=102, name=李四, age=59, salary=6666.66]Employee [id=104, name=赵六, age=8, salary=7777.77]Employee [id=105, name=田七, age=38, salary=5555.55]----------------------------------------------张三李四王五 我们不难发现上面的代码存在一个问题：这个代码，是非常潇洒，舒服的，不依赖我们上面所说的接口。 函数式接口 为了使现有函数更好的支持Lambda表达式，Java 8引入了函数式接口的概念。函数式接口就是只有一个方法的普通接口。java.lang.Runnable与java.util.concurrent.Callable是函数式接口最典型的例子。为此，Java 8增加了一种特殊的注解@FunctionalInterface： –也就是说：这个接口里面只能够存在一个接口方法，多个就会报错 例子：@FunctionalInterfacepublic interface Functional &#123; void method();&#125; 认识Lambda表达式概念 一、Lambda 表达式的基础语法：Java8中引入了一个新的操作符 “-&gt;” 该操作符称为箭头操作符或 Lambda 操作符 箭头操作符将 Lambda 表达式拆分成两部分： 左侧：Lambda 表达式的参数列表 右侧：Lambda 表达式中所需执行的功能， 即 Lambda 体 上面的例子：List list = filterEmployee(emps, (e) -&gt; e.getAge() &lt;= 35); 第二个参数他会去找 MyPredicate&lt;T&gt; 接口里面的 public boolean test(T t);test方法，lambda表达式左边的(e) 对应的是test方法的入参, ambda表达式右边的e.getAge() &lt;= 35 对应得是test方法的实现 那么你可能会有疑问，假设MyPredicate接口里面有很多个接口方法，那么他会去调用那个呢？他怎么知道去找test方法呢？ 引入了：@FunctionalInterface这个函数式接口的概念，解决了这个问题。 * 语法格式一：无参数，无返回值 * () -&gt; System.out.println(&quot;Hello Lambda!&quot;); &gt; 例如 Runnable接口的 run方法就是无参数无返回值： @Test public void test1()&#123; int num = 0;//jdk 1.7 前，我们知道匿名内部引用局部变量必须声明为final //但jdk1.8，它默认给我们添加了final，不用显示声明。 Runnable r = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Hello World!&quot; + num); //这里如果改为 num++是会报错的，因为他本质上是一个final &#125; &#125;; r.run(); System.out.println(&quot;-------------------------------&quot;); Runnable r1 = () -&gt; System.out.println(&quot;Hello Lambda!&quot;); r1.run(); &#125;这两个是等效的 * * 语法格式二：有一个参数，并且无返回值* (x) -&gt; System.out.println(x)* 例子：Consumer这个类jdk自带--有参数无返回值@Testpublic void test2()&#123; Consumer&lt;String&gt; con = x -&gt; System.out.println(x); con.accept(&quot;我是你泽精哥！&quot;);&#125; * 语法格式三：若只有一个参数，小括号可以省略不写* x -&gt; System.out.println(x)* * 语法格式四：有两个以上的参数，有返回值，并且 Lambda 体中有多条语句* Comparator&lt;Integer&gt; com = (x, y) -&gt; &#123;* System.out.println(&quot;函数式接口&quot;);* return Integer.compare(x, y);* &#125;; * 语法格式五：若 Lambda 体中只有一条语句， return 和 大括号都可以省略不写* Comparator&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x, y);* * 语法格式六：Lambda 表达式的参数列表的数据类型可以省略不写，因为JVM编译器通过上下文推断出，数据类型，即“类型推断”* (Integer x, Integer y) -&gt; Integer.compare(x, y); 类型推断 : jdk1.8后，添加了这个功能String[] strs = {“aaa”, “bbb”, “ccc”} ; 它自动会转换里面的数据为String类型的数据改为： String[] strs;strs = &#123;&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;&#125;;//会报错--因为这样无法进行类型推断 类型推断例子2 public void show(Map&lt;String, Integer&gt; map)&#123;&#125;//方法 show(new HashMap&lt;&gt;());//调用方法我们发现在调用方法的时候入参我们并没有明确声明类型，但是在jdk1.8中是可以编译通过的。这里也是运用了类型推断（注意：jdk1.7中编译会失败） 热身例子一 //函数是接口@FunctionalInterfacepublic interface MyFun &#123; public Integer getValue(Integer num);&#125;//测试 //需求：对一个数进行运算 @Test public void test6()&#123; Integer num = operation(100, (x) -&gt; x * x); System.out.println(num); System.out.println(operation(200, (y) -&gt; y + 200)); &#125; public Integer operation(Integer num, MyFun mf)&#123; return mf.getValue(num); &#125; 热身例子二//函数接口 @FunctionalInterface //约束当前接口只能有一个方法public interface CalcLong&lt;K,T&gt;&#123; // public K getMultiply(T t, T tt); K getMultiply(T t, T tt);&#125;//需求：求得两个数的和 String result = getMuyl(10L,10L,(e,ee)-&gt;&#123; System.out.println(e+ &quot; &quot; + ee); return e+ee+&quot;&quot;; &#125;); System.out.println(result); public String getMuyl(Long l,Long ll,CalcLong&lt;String,Long&gt; mf)&#123; return mf.getMultiply(l, ll); &#125; 看到这里可能会有疑惑？我靠，使用lambda表达式还得声明一个函数接口，这么麻烦。实际上，java内部已经帮我们实现了很多个接口供我们使用，不需要重新自己定义，除非有特别操作。 java8内置四大函数式接口 为了解决接口需要自定义问题 /* * Java8 内置的四大核心函数式接口 * * Consumer&lt;T&gt; : 消费型接口 * void accept(T t); * * Supplier&lt;T&gt; : 供给型接口 * T get(); * * Function&lt;T, R&gt; : 函数型接口 * R apply(T t); * * Predicate&lt;T&gt; : 断言型接口 * boolean test(T t); * */ 例子消费型接口//Consumer&lt;T&gt; 消费型接口 :@Testpublic void test1()&#123; String p; happy(10000, (m) -&gt; System.out.println(&quot;桑拿，每次消费：&quot; + m + &quot;元&quot;));&#125; public void happy(double money, Consumer&lt;Double&gt; con)&#123; con.accept(money);&#125; Supplier 供给型接口 //Supplier&lt;T&gt; 供给型接口 :@Testpublic void test2()&#123; List&lt;Integer&gt; numList = getNumList(10, () -&gt; (int)(Math.random() * 100)); for (Integer num : numList) &#123; System.out.println(num); &#125;&#125;//需求：产生指定个数的整数，并放入集合中public List&lt;Integer&gt; getNumList(int num, Supplier&lt;Integer&gt; sup)&#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; num; i++) &#123; Integer n = sup.get(); list.add(n); &#125; return list;&#125; Function 函数型接口 //Function&lt;T, R&gt; 函数型接口：@Testpublic void test3()&#123; String newStr = strHandler(&quot;\\t\\t\\t 去除前后空格 &quot;, (str) -&gt; str.trim()); System.out.println(newStr); String subStr = strHandler(&quot;截取字符串你知不知道&quot;, (str) -&gt; str.substring(2, 5)); System.out.println(subStr);&#125;//需求：用于处理字符串public String strHandler(String str, Function&lt;String, String&gt; fun)&#123; return fun.apply(str);&#125; Predicate 断言型接口 //Predicate&lt;T&gt; 断言型接口：@Testpublic void test4()&#123; List&lt;String&gt; list = Arrays.asList(&quot;Hello&quot;, &quot;atguigu&quot;, &quot;Lambda&quot;, &quot;www&quot;, &quot;ok&quot;); List&lt;String&gt; strList = filterStr(list, (s) -&gt; s.length() &gt; 3); for (String str : strList) &#123; System.out.println(str); &#125;&#125;//需求：将满足条件的字符串，放入集合中public List&lt;String&gt; filterStr(List&lt;String&gt; list, Predicate&lt;String&gt; pre)&#123; List&lt;String&gt; strList = new ArrayList&lt;&gt;(); for (String str : list) &#123; if(pre.test(str))&#123; strList.add(str); &#125; &#125; return strList;&#125; 四大内置函数衍生的子函数 二、接口的默认方法与静态方法 我们可以在接口中定义默认方法，使用default关键字，并提供默认的实现。所有实现这个接口的类都会接受默认方法的实现，除非子类提供的自己的实现。例如：public interface DefaultFunctionInterface &#123; default String defaultFunction() &#123; return &quot;default function&quot;; &#125;&#125; 我们还可以在接口中定义静态方法，使用static关键字，也可以提供实现。例如：public interface StaticFunctionInterface &#123; static String staticFunction() &#123; return &quot;static function&quot;; &#125;&#125; 接口的默认方法和静态方法的引入，其实可以认为引入了C＋＋中抽象类的理念，以后我们再也不用在每个实现类中都写重复的代码了。 三、方法引用 通常与Lambda表达式联合使用，可以直接引用已有Java类或对象的方法。一般有四种不同的方法引用： 构造器引用 构造器引用。语法是Class::new，构造器的参数列表，需要与函数式接口中参数列表保持一致！也就是说，决定Class::new调用那一个构造器得是：接口函数的方法的参数 //构造器引用@Testpublic void test7()&#123; // Supplier 的接口方法 T get(); --所以调用无参构造器 Supplier&lt;Employee&gt; fun0 = Employee::new; //Function 的接口方法 R apply(T t);-调用一个参数构造器 Function&lt;String, Employee&gt; fun = Employee::new; //BiFunction 的接口方法 R apply(T t, U u); -调用二参构造器 BiFunction&lt;String, Integer, Employee&gt; fun2 = Employee::new;&#125; 对象静态方法引用（类名::静态方法） 静态方法引用。语法是Class::static_method，要求接受一个Class类型的参数； //类名 :: 静态方法名//max和compare 都是静态方法@Testpublic void test4()&#123; Comparator&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x, y); System.out.println(&quot;-------------------------------------&quot;); Comparator&lt;Integer&gt; com2 = Integer::compare; BiFunction&lt;Double, Double, Double&gt; fun = (x, y) -&gt; Math.max(x, y); System.out.println(fun.apply(1.5, 22.2)); System.out.println(&quot;------------------------------------&quot;); BiFunction&lt;Double, Double, Double&gt; fun2 = Math::max; System.out.println(fun2.apply(1.2, 1.5));&#125; 对象实例方法引用（对象引用::实例方法名） 特定类的任意对象方法引用。它的语法是Class::method。要求方法是没有参数的； //对象的引用 :: 实例方法名@Testpublic void test2()&#123; Employee emp = new Employee(101, &quot;张三&quot;, 18, 9999.99); Supplier&lt;String&gt; sup = () -&gt; emp.getName(); System.out.println(sup.get()); System.out.println(&quot;----------------------------------&quot;); Supplier&lt;String&gt; sup2 = emp::getName; System.out.println(sup2.get());&#125; 类名实例方法引用(类名::实例方法名) 我们知道一般是有对象才能够引用实例方法，但是有种特殊情况是可以直接使用类名引用实例方法若Lambda 的参数列表的第一个参数，是实例方法的调用者，第二个参数(或无参)是实例方法的参数时，格式： ClassName::MethodName //类名 :: 实例方法名//按照常规是String st = new String(&quot;123&quot;); st::equals,//对象调用实例方法，但是下面因为符合第四种引用的规则，//所以可以使用类名调用实例方法@Testpublic void test5()&#123;//第一个参数为实例方法调用者，第二个参数为为实例方法参数 BiPredicate&lt;String, String&gt; bp = (x, y) -&gt; x.equals(y); System.out.println(bp.test(&quot;abcde&quot;, &quot;abcde&quot;)); System.out.println(&quot;-------------------------------------&quot;); BiPredicate&lt;String, String&gt; bp2 = String::equals;System.out.println(bp2.test(&quot;abc&quot;, &quot;abc&quot;)); System.out.println(&quot;---------------------------------------&quot;); //第一个参数为实例方法调用者，第二个参数为空Function&lt;Employee, String&gt; fun = (e) -&gt; e.show();System.out.println(fun.apply(new Employee()));System.out.println(&quot;--------------------------------------&quot;); Function&lt;Employee, String&gt; fun2 = Employee::show; System.out.println(fun2.apply(new Employee())); &#125; 注意： ①方法体所引用的方法的参数列表与返回值类型，需要与函数式接口中抽象方法的参数列表和返回值类型保持一致！ ②若Lambda 的参数列表的第一个参数，是实例方法的调用者，第二个参数(或无参)是实例方法的参数时，格式：ClassName::MethodName (针对于第四种方法引用) 数组引用（类型[] :: new）//数组引用@Testpublic void test8()&#123; Function&lt;Integer, String[]&gt; fun = (args) -&gt; new String[args]; String[] strs = fun.apply(10); System.out.println(strs.length); System.out.println(&quot;--------------------------&quot;); Function&lt;Integer, Employee[]&gt; fun2 = Employee[] :: new; Employee[] emps = fun2.apply(20); System.out.println(emps.length);&#125; 四、重复注解在Java 5中使用注解有一个限制，即相同的注解在同一位置只能声明一次。Java 8引入重复注解，这样相同的注解在同一地方也可以声明多次。重复注解机制本身需要用@Repeatable注解。Java 8在编译器层做了优化，相同注解会以集合的方式保存，因此底层的原理并没有变化。 五、扩展注解的支持Java 8扩展了注解的上下文，几乎可以为任何东西添加注解，包括局部变量、泛型类、父类与接口的实现，连方法的异常也能添加注解。 六、OptionalJava 8引入Optional类来防止空指针异常，Optional类最先是由Google的Guava项目引入的。Optional类实际上是个容器：它可以保存类型T的值，或者保存null。使用Optional类我们就不用显式进行空指针检查了。 七、Stream前言 Stream API是把真正的函数式编程风格引入到Java中。其实简单来说可以把Stream理解为MapReduce，当然Google的MapReduce的灵感也是来自函数式编程。她其实是一连串支持连续、并行聚集操作的元素。从语法上看，也很像linux的管道、或者链式编程，代码写起来简洁明了，非常酷帅！ 八、Date/Time API (JSR 310)Java 8新的Date-Time API (JSR 310)受Joda-Time的影响，提供了新的java.time包，可以用来替代 java.util.Date和java.util.Calendar。一般会用到Clock、LocaleDate、LocalTime、LocaleDateTime、ZonedDateTime、Duration这些类，对于时间日期的改进还是非常不错的。 九、JavaScript引擎NashornNashorn允许在JVM上开发运行JavaScript应用，允许Java与JavaScript相互调用。 十、Base64在Java 8中，Base64编码成为了Java类库的标准。Base64类同时还提供了对URL、MIME友好的编码器与解码器。 除了这十大新特性之外，还有另外的一些新特性： 更好的类型推测机制：Java 8在类型推测方面有了很大的提高，这就使代码更整洁，不需要太多的强制类型转换了。 编译器优化：Java 8将方法的参数名加入了字节码中，这样在运行时通过反射就能获取到参数名，只需要在编译时使用-parameters参数。 并行（parallel）数组：支持对数组进行并行处理，主要是parallelSort()方法，它可以在多核机器上极大提高数组排序的速度。 并发（Concurrency）：在新增Stream机制与Lambda的基础之上，加入了一些新方法来支持聚集操作。 Nashorn引擎jjs：基于Nashorn引擎的命令行工具。它接受一些JavaScript源代码为参数，并且执行这些源代码。 类依赖分析器jdeps：可以显示Java类的包级别或类级别的依赖。 JVM的PermGen空间被移除：取代它的是Metaspace（JEP 122），元空间直接采用的是物理空间，也即是我们电脑的内存，电脑内存多大，元空间就有多大。","categories":[{"name":"Java8","slug":"Java8","permalink":"http://kingge.top/categories/Java8/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java8","slug":"java8","permalink":"http://kingge.top/tags/java8/"},{"name":"java8新特性","slug":"java8新特性","permalink":"http://kingge.top/tags/java8新特性/"}]},{"title":"哈希表冲突解决方式之开放地址法和链地址法","slug":"哈希表冲突解决方式之开放地址法和链地址法","date":"2017-08-29T03:07:51.000Z","updated":"2017-08-29T03:56:03.815Z","comments":true,"path":"2017/08/29/哈希表冲突解决方式之开放地址法和链地址法/","link":"","permalink":"http://kingge.top/2017/08/29/哈希表冲突解决方式之开放地址法和链地址法/","excerpt":"","text":"基本定义 散列技术是在记录的存储位置和它的关键字之间建立一个确定的对应关系 f，使得每个关键字key对应一个存储位置f(key)。 这种对应关系f称为散列或哈希函数 采用上述思想将数据存储在一块连续的存储空间中，这块连续的存储空间称为散列或哈希表 关键字对应的存储位置称为散列地址 如果碰到两个不同的关键字key1≠key2，但却有相同的f(key1)=f(key2)，这种现象称为冲突， 并把key1和key2 称为这个散列函数的同义词（synonym） 散列函数构造方法好的散列函数参考如下两个原则： 计算简单 散列地址分布均匀 最常用的方法是除留余数法，对于散列表长度为m的散列函数是 f(key)=key mod p (p≤m) 处理散列冲突 开放地址法 开放地址法就是一旦发生冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列表总能找到，并存入。开放地址法又分为线性探测法，二次探测法和随机探测法。 链地址法 将所有同义词的关键字存储在同一个单链表中，称这个单链表为同义词子表，在散列表中只存储同义词子表的头指针。只要有冲突，就在同义词的子表中增加结点。(java中的HashMap就是采用这种方法) 开放地址法","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://kingge.top/categories/数据结构/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"http://kingge.top/tags/数据结构/"},{"name":"哈希表","slug":"哈希表","permalink":"http://kingge.top/tags/哈希表/"},{"name":"哈希解决冲突","slug":"哈希解决冲突","permalink":"http://kingge.top/tags/哈希解决冲突/"}]},{"title":"java8之Hashmap","slug":"java8之Hashmap","date":"2017-08-28T14:27:16.000Z","updated":"2017-08-29T08:08:38.273Z","comments":true,"path":"2017/08/28/java8之Hashmap/","link":"","permalink":"http://kingge.top/2017/08/28/java8之Hashmap/","excerpt":"","text":"Java8-HashMap变化 数据的存储结构从：数组+链表 演变为了 数组+链表+红黑树 Map 家庭族谱 HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 总结对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 HashMap简介 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 我们知道HashMap的数据存储结构就是：数组加上链表。通过对于key的值做hash运算，获得对应的值找到对应的数组下标，然后再存储值。存储值的过程中可能当前数组已经存在值（这个称之为冲突） 然后再生成一个链表存储冲突的值。 HashCode() 和 Hash() 方法实现得足够好，能够尽可能地减少冲突的产生，那么对 HashMap 的操作几乎等价于对数组的随机访问操作，具有很好的性能。但是，如果 HashCode() 或者 Hash() 方法实现较差，在大量冲突产生的情况下，HashMap 事实上就退化为几个链表，对 HashMap 的操作等价于遍历链表，此时性能很差。 解决冲突的方法：开放地址法和链地址法 HashMap特点 允许null为key 输出无序 如果想要输出有序，那以使用继承他的LinkedHashMap，元素输出顺序跟输入顺序一致,他提供了一个节点保存输入的元素的顺序。想要对元素的值进行排序 推荐TreeMap（因为他继承了SortedMap) 非线程安全 数组+链表存储方式 Java8特性 HashMap是数组+链表+红黑树 存储算法： map.put(&quot;kingge&quot;,&quot;shuai&quot;)系统将调用kingge”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 好的hash算法和扩容机制是解决冲突和高效存取的命题 HashMap 重要的几个属性int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; Node[] table(Hash桶)始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 分析HashMap的put方法put方法图解，详情可以去看源码 public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 为什么说HashMap是线程不安全的 个人觉得有两个表现，如果还有其他的希望大家补充，或者以后等楼主源码研究透了再补充 表现一 我们知道当插入数据超过了threshold(threshold=length * Load factor),那么就会扩容，扩容会去调用resize和transfer方法，这个时候原先hash桶里面的所有数据都会重新计算，对应的位置–称之为rehash，这个成本很大 最根本的原因是出现时死循环-也就是在死锁问题出现在了transfer方法上面,而且是因为在扩容转换的过程中采用的是链表的头插法的形式进行插入数据。例如原来在数组arr[0]的位置又链表1–&gt;2–&gt;3 那么扩容后，采用头插法就变成了arr[0]：3–&gt;2–&gt;1 为什么采用头插法，因为时间复杂度为O(1)，想象一下尾插法，那么需要遍历找到最尾元素然后插入时间复杂度是O(n) 具体源码分析参见：http://www.importnew.com/22011.html 表现二多个线程同时操作一个hashmap就可能出现不安全的情况：比如A B两个线程(A线程获数据 B线程存数据) 同时操作myHashMap1.B线程执行存放数据modelHashMap.put(“1”,”2”);2.A线程执行get获取数据modelHashMap.get(“1”)A线程获取的值本来应该是2，但是如果A线程在刚到达获取的动作还没执行的时候，线程执行的机会又跳到线程B，此时线程B又对modelHashMap赋值 如：modelHashMap.put(“1”,”3”);然后线程虚拟机又执行线程A，A取到的值为3，这样map中第一个存放的值 就会丢失。。。。。—原子性 解决HashMap非线程安全其实上面我已经有提过了： 三个方法： Hashtable替换HashMap Collections.synchronizedMap将HashMap包装起来 private Map map = Collections.synchronizedMap(new HashMap());替换private HashMap map = new HashMap(); ConcurrentHashMap替换HashMap private ConcurrentHashMap map = new ConcurrentHashMap();替换private HashMap map = new HashMap(); 好的博文http://blog.csdn.net/lyg468088/article/details/49464121","categories":[{"name":"Java8","slug":"Java8","permalink":"http://kingge.top/categories/Java8/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java8","slug":"java8","permalink":"http://kingge.top/tags/java8/"},{"name":"java8新特性","slug":"java8新特性","permalink":"http://kingge.top/tags/java8新特性/"}]},{"title":"程序员未来规划","slug":"程序员未来规划","date":"2017-08-28T02:00:36.000Z","updated":"2017-08-28T02:06:13.437Z","comments":true,"path":"2017/08/28/程序员未来规划/","link":"","permalink":"http://kingge.top/2017/08/28/程序员未来规划/","excerpt":"","text":"http://www.jianshu.com/p/9d29a441ee17?utm_source=desktop&amp;utm_medium=timeline","categories":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/categories/心情/"}],"tags":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/tags/心情/"},{"name":"大龄程序员","slug":"大龄程序员","permalink":"http://kingge.top/tags/大龄程序员/"},{"name":"规划","slug":"规划","permalink":"http://kingge.top/tags/规划/"}]},{"title":"java之ClassLoader源码分析","slug":"java之ClassLoader源码分析","date":"2017-08-24T06:36:35.000Z","updated":"2017-08-24T08:36:55.128Z","comments":true,"path":"2017/08/24/java之ClassLoader源码分析/","link":"","permalink":"http://kingge.top/2017/08/24/java之ClassLoader源码分析/","excerpt":"","text":"类加载器ClassLoader的含义 不论多么简单的java程序，都是由一个或者多个java文件组成，java内部实现了程序所需要的功能逻辑，类之间可能还存在着依赖关系。当程序运行的时候，类加载器会把一部分类编译为class后加载到内存中，这样程序才能够调用里面的方法并运行。 类之间如果存在依赖关系，那么类加载会去帮你加载相关的类到内存中，这样才能够完成调用。如果找不到相关的类，那么他就会抛出我们在开发经常见到的异常：ClassNotFoundException Java中的所有类，必须被装载到jvm中才能运行，这个装载工作是由jvm中的类装载器完成的，类装载器所做的工作实质是把类文件从硬盘读取到内存中，JVM在加载类的时候，都是通过ClassLoader的loadClass（）方法来加载class的,与此同时在loadClass中存在着三种加载策略，loadClass使用双亲委派模式。 所以Classloader就是用来动态加载Class文件到内存当中用的。 Java默认提供的三个ClassLoader1.Bootstrap ClassLoader 称为启动类加载器，是Java类加载层次中最顶层的类加载器，负责加载JDK中的核心类库，预设上它负责搜寻JRE所在目录的classes或lib目录下（实际上是由系统参数sun.boot.class.path指定）。如：rt.jar、resources.jar、charsets.jar等，可通过如下程序获得该类加载器从哪些地方加载了相关的jar或class文件： URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i &lt; urls.length; i++) &#123; System.out.println(urls[i].toExternalForm()); &#125; &gt; 这两种输出的内容都是一样的。 System.out.println(System.getProperty(\"sun.boot.class.path\")); 输出 file:/F:/JDK/jdk1.8/jre/lib/resources.jarfile:/F:/JDK/jdk1.8/jre/lib/rt.jarfile:/F:/JDK/jdk1.8/jre/lib/sunrsasign.jarfile:/F:/JDK/jdk1.8/jre/lib/jsse.jarfile:/F:/JDK/jdk1.8/jre/lib/jce.jarfile:/F:/JDK/jdk1.8/jre/lib/charsets.jarfile:/F:/JDK/jdk1.8/jre/lib/jfr.jarfile:/F:/JDK/jdk1.8/jre/classesF:\\JDK\\jdk1.8\\jre\\lib\\resources.jar;F:\\JDK\\jdk1.8\\jre\\lib\\rt.jar;F:\\JDK\\jdk1.8\\jre\\lib\\sunrsasign.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jsse.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jce.jar;F:\\JDK\\jdk1.8\\jre\\lib\\charsets.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jfr.jar;F:\\JDK\\jdk1.8\\jre\\classes 2.Extension ClassLoader 称为扩展类加载器，负责加载Java的扩展类库，默认加载JAVA_HOME/jre/lib/ext/目下的所有jar（实际上是由系统参数java.ext.dirs指定） 3.App ClassLoader 称为系统类加载器，负责加载应用程序classpath目录下的所有jar和class文件（由系统参数java.class.path指定） 总结 Extension ClassLoader和App ClassLoader 这两个类加载器实际上都是继承了ClassLoader类，但是Bootstrap ClassLoader不继承自ClassLoader，因为它不是一个普通的Java类，底层由C++编写，已嵌入到了JVM内核当中，当JVM启动后，Bootstrap ClassLoader也随着启动，负责加载完核心类库后，并构造Extension ClassLoader和App ClassLoader类加载器，也就是说： Bootstrap Loader会在JVM启动之后载入，之后它会载入ExtClassLoader并将ExtClassLoader的parent设为Bootstrap Loader，然后BootstrapLoader再加载AppClassLoader，并将AppClassLoader的parent设定为 ExtClassLoader。 ClassLoader加载类的原理双亲委托加载模式 我们知道除了顶级的 Bootstrap Loader他的parent属性为null之外，其他的两个或者自定义的类加载器都是存在parent 的。 当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader 进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象 为什么要使用双亲委托这种模型呢 java是一门具有很高的安全性的语言，使用这种加载策略的目的是为了：防止重载，父类如果已经找到了需要的类并加载到了内存，那么子类加载器就不需要再去加载该类。安全性问题。两个原因 JVM在搜索类的时候，又是如何判定两个class是相同的 类名是否相同 否由同一个类加载器实例加载 看一个例子public class TestClassLoader&#123; public static void main(String[] args) &#123; ClassLoader loader = TestClassLoader.class.getClassLoader(); //获得加载ClassLoaderTest.class这个类的类加载器 while(loader != null) &#123; System.out.println(loader); loader = loader.getParent(); //获得父类加载器的引用 &#125; System.out.println(loader); &#125;&#125; 输出 sun.misc.Launcher$AppClassLoader@2a139a55sun.misc.Launcher$ExtClassLoader@7852e922null 结论 第一行结果说明：ClassLoaderTest的类加载器是AppClassLoader。 第二行结果说明：AppClassLoader的类加器是ExtClassLoader，即parent=ExtClassLoader。 第三行结果说明：ExtClassLoader的类加器是Bootstrap ClassLoader，因为Bootstrap ClassLoader不是一个普通的Java类，所以ExtClassLoader的parent=null，所以第三行的打印结果为null就是这个原因。 测试2 将ClassLoaderTest.class打包成ClassLoaderTest.jar，放到Extension ClassLoader的加载目录下（JAVA_HOME/jre/lib/ext），然后重新运行这个程序，得到的结果会是什么样呢？ 输出 sun.misc.Launcher$ExtClassLoader@7852e922null 结果分析： 为什么第一行的结果是ExtClassLoader呢？ 因为ClassLoader的委托模型机制，当我们要用ClassLoaderTest.class这个类的时候，AppClassLoader在试图加载之前，先委托给Bootstrcp ClassLoader，Bootstracp ClassLoader发现自己没找到，它就告诉ExtClassLoader，兄弟，我这里没有这个类，你去加载看看，然后Extension ClassLoader拿着这个类去它指定的类路径（JAVA_HOME/jre/lib/ext）试图加载，唉，它发现在ClassLoaderTest.jar这样一个文件中包含ClassLoaderTest.class这样的一个文件，然后它把找到的这个类加载到内存当中，并生成这个类的Class实例对象，最后把这个实例返回。所以ClassLoaderTest.class的类加载器是ExtClassLoader。 第二行的结果为null，是因为ExtClassLoader的父类加载器是Bootstrap ClassLoader。 测试3: 用Bootstrcp ClassLoader来加载ClassLoaderTest.class，有两种方式：1、在jvm中添加-Xbootclasspath参数，指定Bootstrcp ClassLoader加载类的路径，并追加我们自已的jar（ClassTestLoader.jar）2、将class文件放到JAVA_HOME/jre/classes/目录下（上面有提到）(将ClassLoaderTest.jar解压后，放到JAVA_HOME/jre/classes目录下，如下图所示：)提示：jre目录下默认没有classes目录，需要自己手动创建一个提供者：Java团长 自定义ClassLoader前言 实现自定义类加载的目的是，假设我们的类他不是存在特定的位置，可能是某个磁盘或者某个远程服务器上面，那么我们就需要自定义类加载器去加载这些类。 继承继承java.lang.ClassLoader 重写父类的findClass方法 在findClass()方法中调用defineClass()。 这个方法在编写自定义classloader的时候非常重要，它能将class二进制内容转换成Class对象，如果不符合要求的会抛出各种异常 注意： 一个ClassLoader创建时如果没有指定parent，那么它的parent默认就是AppClassLoader。 为什么不去重定义loadClass方法呢？其实也可以，但是loadClass方法内部已经实现了搜索类的策略。除非你是非常熟悉否则还是不建议这样去做。这里建议重载findClass方法，因为在loadClass中最后会去调用findClass方法去加载类。而且这个方法内部默认是空的。 分析loadClass方法源码/*** A class loader is an object that is responsible for loading classes. The class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to locate or generate data that constitutes a definition for the class. A typical strategy is to transform the name into a file name and then read a &quot;class file&quot; of that name from a file system.**/ 大致意思如下：class loader是一个负责加载classes的对象，ClassLoader类是一个抽象类，需要给出类的二进制名称，class loader尝试定位或者产生一个class的数据，一个典型的策略是把二进制名字转换成文件名然后到文件系统中找到该文件。 protected synchronized Class loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; // 首先检查该name指定的class是否有被加载 Class c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; //如果parent不为null，则调用parent的loadClass进行加载 c = parent.loadClass(name, false); &#125;else&#123; //parent为null，则调用BootstrapClassLoader进行加载 c = findBootstrapClass0(name); &#125; &#125;catch(ClassNotFoundException e) &#123; //如果仍然无法加载成功，则调用自身的findClass进行加载 c = findClass(name); // &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; 自定义类加载器package com.kingge.com;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.InputStream;import java.net.URL;public class PersonalClassLoader extends ClassLoader&#123; private String rootUrl; public PersonalClassLoader(String rootUrl) &#123; this.rootUrl = rootUrl; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; Class clazz = null;//this.findLoadedClass(name); // 父类已加载 //if (clazz == null) &#123; //检查该类是否已被加载过 byte[] classData = getClassData(name); //根据类的二进制名称,获得该class文件的字节码数组 if (classData == null) &#123; throw new ClassNotFoundException(); &#125; clazz = defineClass(name, classData, 0, classData.length); //将class的字节码数组转换成Class类的实例 //ClassLoader内置方法 /* * Converts an array of bytes into an instance of class * Before the &lt;tt&gt;Class&lt;/tt&gt; can be used it must be resolved.*/ //&#125; return clazz; &#125; private byte[] getClassData(String name) &#123; InputStream is = null; try &#123; String path = classNameToPath(name); URL url = new URL(path); byte[] buff = new byte[1024*4]; int len = -1; is = url.openStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); while((len = is.read(buff)) != -1) &#123; baos.write(buff,0,len); &#125; return baos.toByteArray(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (is != null) &#123; try &#123; is.close(); &#125; catch(IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return null; &#125; private String classNameToPath(String name) &#123; return rootUrl + &quot;/&quot; + name.replace(&quot;.&quot;, &quot;/&quot;) + &quot;.class&quot;; &#125;&#125; 测试类： package com.kingge.com;public class ClassLoaderTest&#123; public static void main(String[] args) &#123; try &#123; /*ClassLoader loader = ClassLoaderTest.class.getClassLoader(); //获得ClassLoaderTest这个类的类加载器 while(loader != null) &#123; System.out.println(loader); loader = loader.getParent(); //获得父加载器的引用 &#125; System.out.println(loader);*/ String rootUrl = &quot;http://localhost:8080/console/res&quot;; PersonalClassLoader networkClassLoader = new PersonalClassLoader(rootUrl); String classname = &quot;HelloWorld&quot;; Class clazz = networkClassLoader.loadClass(classname); System.out.println(clazz.getClassLoader()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出： com.kingge.com.PersonalClassLoader@65b54208 其中HelloWorld.class文件的位置在于： 其实很多服务器都自定义了类加载器 用于加载web应用指定目录下的类库（jar或class），如：Weblogic、Jboss、tomcat等，下面我以Tomcat为例，展示该web容器都定义了哪些个类加载器： 下面以tomcat为例子 1、新建一个web工程httpweb 2、新建一个ClassLoaderServletTest，用于打印web容器中的ClassLoader层次结构 一下代码来自网上：import java.io.IOException; import java.io.PrintWriter; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; public class ClassLoaderServletTest extends HttpServlet &#123; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.setContentType(&quot;text/html&quot;); PrintWriter out = response.getWriter(); ClassLoader loader = this.getClass().getClassLoader(); while(loader != null) &#123; out.write(loader.getClass().getName()+&quot;&lt;br/&gt;&quot;); loader = loader.getParent(); &#125; out.write(String.valueOf(loader)); out.flush(); out.close(); &#125; public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125; &#125; 3、配置Servlet，并启动服务 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app version=&quot;2.4&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;&gt; &lt;servlet&gt; &lt;servlet-name&gt;ClassLoaderServletTest&lt;/servlet-name&gt; &lt;servlet-class&gt;ClassLoaderServletTest&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ClassLoaderServletTest&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/ClassLoaderServletTest&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;/web-app&gt; 运行截图： 总结 这种自定义的方式目的就是为了，能够控制类的加载流程，那么这种远程加载类的方式类似于我们常用的Hessian 来访问多个系统获取类 好的网站http://blog.csdn.net/briblue/article/details/54973413","categories":[{"name":"java深入理解","slug":"java深入理解","permalink":"http://kingge.top/categories/java深入理解/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"ClassLoader","slug":"ClassLoader","permalink":"http://kingge.top/tags/ClassLoader/"},{"name":"类加载器","slug":"类加载器","permalink":"http://kingge.top/tags/类加载器/"},{"name":"自定义类加载器","slug":"自定义类加载器","permalink":"http://kingge.top/tags/自定义类加载器/"},{"name":"类加载器源码分析","slug":"类加载器源码分析","permalink":"http://kingge.top/tags/类加载器源码分析/"}]},{"title":"为什么毕业后选择留在小城市","slug":"为什么毕业后选择留在小城市","date":"2017-08-21T06:13:44.000Z","updated":"2017-08-23T01:12:42.498Z","comments":true,"path":"2017/08/21/为什么毕业后选择留在小城市/","link":"","permalink":"http://kingge.top/2017/08/21/为什么毕业后选择留在小城市/","excerpt":"","text":"前言经常看到大学要毕业的学生，会有一种纠结感。越是临近毕业，这种感觉就越是强烈这种感觉实际上就是一种选择恐惧症，又或者说是小城综合征。他们对于毕业后究竟是选择去一线城市就业还是去二三线城市就业，产生了很大的选择恐惧。 为什么会产生这种心理大致的原因有那么几个： 身边的认识的人，都是选择回到自己的家乡进行就业，自己收到了影响。 大城市的生活节奏会比小城市更加的紧凑，你会很忙。 父母希望自己回去，离家近的地方工作。 恋人不跟随自己，她或他选择了回到了故乡就业，自己左右为难。 有些人已经在大城市实习过，对于大城市已经厌倦。 作者的选择 本人就是属于最后一种，大三的时候去的深圳实习，在一家IT互联网公司上班，加班很多，虽然加班这种现象在深圳是一种常态。但是每天加班到晚上一点多，也是很累，所以毕业后也就选择回到了自己的家乡就业。 回到了","categories":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/categories/心情/"}],"tags":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/tags/心情/"},{"name":"总结","slug":"总结","permalink":"http://kingge.top/tags/总结/"},{"name":"有感","slug":"有感","permalink":"http://kingge.top/tags/有感/"}]},{"title":"max-allowed-packet的问题","slug":"max-allowed-packet的问题","date":"2017-08-17T02:37:15.000Z","updated":"2017-08-17T09:44:39.015Z","comments":true,"path":"2017/08/17/max-allowed-packet的问题/","link":"","permalink":"http://kingge.top/2017/08/17/max-allowed-packet的问题/","excerpt":"","text":"前言 近期，因启动项目有个批量插入的sql结果太大，超过了mysql自带的缓存，报了这个错误 修改： 定位到mysql的安装目录下面，然后修改my.ini 的max_allowed_packet = 8M默认是1M","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"异常","slug":"异常","permalink":"http://kingge.top/tags/异常/"},{"name":"sql异常","slug":"sql异常","permalink":"http://kingge.top/tags/sql异常/"}]},{"title":"YUM仓库配置","slug":"YUM仓库配置","date":"2017-08-06T14:12:30.000Z","updated":"2019-06-02T06:17:59.046Z","comments":true,"path":"2017/08/06/YUM仓库配置/","link":"","permalink":"http://kingge.top/2017/08/06/YUM仓库配置/","excerpt":"","text":"1.1 概述YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。-需要联网 1.2 为什么要制作本地YUM源YUM源虽然可以简化我们在Linux上安装软件的过程，但是生产环境通常无法上网，不能连接外网的YUM源，说以就无法使用yum命令安装软件了。为了在内网中也可以使用yum安装相关的软件，就要配置yum源。 YUM源其实就是一个保存了多个RPM包的服务器，可以通过http的方式来检索、下载并安装相关的RPM包。 1.3 yum的常用命令1）基本语法： yum install -y rpm软件包 （功能描述：安装httpd并确认安装） yum list （功能描述：列出所有可用的package和package组） yum clean all （功能描述：清除所有缓冲数据） yum deplist rpm软件包 （功能描述：列出一个包所有依赖的包） yum remove rpm软件包 （功能描述：删除httpd） 2）案例实操 ​ yum install -y tree //安装文档树结构展示插件 1.4 关联网络yum源1）前期文件准备 （1）前提条件linux系统必须可以联网 （2）在Linux环境中访问该网络地址：http://mirrors.163.com/.help/centos.html，在使用说明中点击CentOS6-&gt;再点击保存 （3）查看文件保存的位置 在打开的终端中输入如下命令，就可以找到文件的保存位置。 [kingge@hadoop101 下载]$ pwd /home/kingge/下载 2）替换本地yum文件 ​ （1）把下载的文件移动到/etc/yum.repos.d/目录 [root@hadoop101 下载]# mv CentOS6-Base-163.repo /etc/yum.repos.d/ ​ （2）进入到/etc/yum.repos.d/目录 [root@hadoop101 yum.repos.d]# pwd /etc/yum.repos.d ​ （3）用CentOS6-Base-163.repo替换CentOS-Base.rep [root@hadoop101 yum.repos.d]# mv CentOS6-Base-163.repo CentOS-Base.rep 3）安装命令 ​ （1）[root@hadoop101 yum.repos.d]#yum clean all ​ （2）[root@hadoop101 yum.repos.d]#yum makecache ​ （3）[root@hadoop101 yum.repos.d]# yum install -y createrepo （4）[root@hadoop101 yum.repos.d]#yum install -y httpd 1.5 制作本地yum源1.5.1 制作只有本机能访问的本地YUM源（1）准备一台Linux服务器，版本CentOS-6.8-x86_64-bin-DVD1.iso （2）配置好这台服务器的IP地址 （3）将CentOS-6.8-x86_64-bin-DVD1.iso镜像挂载到/mnt/cdrom目录 [root@hadoop101 /]# mkdir /mnt/cdrom [root@hadoop101 /]# mount -t iso9660 /dev/cdrom /mnt/cdrom （4）安装相应的软件 [root@hadoop101 yum.repos.d]#yum install -y httpd ​ （5）启动httpd服务 [root@hadoop101 yum.repos.d]#service httpd start （6）使用浏览器访问http://192.168.1.101:80（如果访问不通，检查防火墙是否开启了80端口或关闭防火墙），测试网络是否畅通 （7）将YUM源配置到httpd（Apache Server）中 [root@hadoop101 html]# mkdir Packages [root@hadoop101 html]# chown kingge:kingge Packages/ [root@hadoop101 html]# cp -r /mnt/cdrom/Packages/* /var/www/html/Packages/ （8）执行创建仓库命令：createrepo 路径 [root@hadoop101 Packages]# createrepo ./ （9）修改本机上的YUM源配置文件，将源指向自己 备份原有的YUM源的配置文件 [root@hadoop101 /]# cd /etc/yum.repos.d/ [root@hadoop101 yum.repos.d]# cp CentOS-Base.repo CentOS-Base.repo.bak ​ 编辑CentOS-Base.repo文件 [root@hadoop101 yum.repos.d]# vi CentOS-Base.repo [base] name=CentOS-Local baseurl=file:///var/www/html/Packages gpgcheck=0 enabled=1 #增加改行，使能 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 添加上面内容保存退出 （10）清除YUM缓存 [root@hadoop101 yum.repos.d]# yum clean all （11）列出可用的YUM仓库 [root@hadoop101 yum.repos.d]# yum repolist （12）安装相应的软件 [root@hadoop101 yum.repos.d]# yum install -y tree [root@hadoop101 Packages]# yum install -y firefox-45.0.1-1.el6.centos.x86_64.rpm 1.5.2 制作其他主机通过网络能访问的本地YUM源前期准备：检查yum源服务器的httpd服务是否启动： ​ Ps aux | grep httpd –查看该进程是否存在 或者 netstat –anp | grep 80 直接查看是否监听80端口 ​ 启动 httpd服务： service httpd start/restart 启动后，可能使用yum源的服务器访问不到yum仓库（例如下面的101服务器访问不到102服务器的yum），那么有可能是101服务器防火墙屏蔽了80端口，应该设置一下防火墙规则 （1）让其他需要安装RPM包的服务器指向这个YUM源，准备一台新的服务器，备份或删除原有的YUM源配置文件 备份原有的YUM源的配置文件 [root@hadoop102 /]#cd /etc/yum.repos.d/ [root@hadoop102 yum.repos.d]# cp CentOS-Base.repo CentOS-Base.repo.bak ​ 编辑CentOS-Base.repo文件 [root@hadoop102 yum.repos.d]# vi CentOS-Base.repo [base] name=CentOS-hadoop101 baseurl=http://192.168.1.101/Packages gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 添加上面内容保存退出 （2）在这台新的服务器上执行YUM的命令 [root@hadoop102 yum.repos.d]# yum clean all [root@hadoop102 yum.repos.d]# yum repolist （3）安装软件 [root@hadoop102 yum.repos.d]# yum install -y httpd","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"Shell编程","slug":"Shell编程","date":"2017-08-05T16:00:00.000Z","updated":"2019-06-02T06:10:23.295Z","comments":true,"path":"2017/08/06/Shell编程/","link":"","permalink":"http://kingge.top/2017/08/06/Shell编程/","excerpt":"","text":"一、引言因为公司遜要开发一款saas系统，需要使用自动部署脚本，部署web项目，所以就去了解了一下相关的知识，这里并不做深入的研究。 1.1 概述Shell是一个命令行解释器，它为用户提供了一个向Linux内核发送请求以便运行程序的界面系统级程序，用户可以用Shell来启动、挂起、停止甚至是编写一些程序。 Shell还是一个功能相当强大的编程语言，易编写、易调试、灵活性强。Shell是解释执行的脚本语言，在Shell中可以调用Linux系统命令。 1.2 shell脚本的执行方式1）echo输出内容到控制台 ​ （1）基本语法：​ echo [选项] [输出内容] 选项： -e： 支持反斜线控制的字符转换 控制字符 作 用 \\ 输出\\本身 \\a 输出警告音 \\b 退格键，也就是向左删除键 \\c 取消输出行末的换行符。和“-n”选项一致 \\e ESCAPE键 \\f 换页符 \\n 换行符 \\r 回车键 \\t 制表符，也就是Tab键 \\v 垂直制表符 \\0nnn 按照八进制ASCII码表输出字符。其中0为数字零，nnn是三位八进制数 \\xhh 按照十六进制ASCII码表输出字符。其中hh是两位十六进制数 ​（2）案例 ​ [kingge@hadoop101 sbin]$ echo &quot;helloworld&quot; helloworld 2）脚本格式 （1）脚本以 #!/bin/bash 开头（2）脚本必须有可执行权限 3）第一个Shell脚本 （1）需求：创建一个Shell脚本，输出helloworld （2）实操： [kingge@hadoop101 datas]$ touch helloworld.sh [kingge@hadoop101 datas]$ vi helloworld.sh ​ 在helloworld.sh中输入如下内容 #!**/**bin**/**bash echo &quot;helloworld&quot; 4）脚本的常用执行方式 第一种：输入脚本的绝对路径或相对路径 （1）首先要赋予helloworld.sh 脚本的+x权限 [kingge@hadoop101 datas]$ chmod 777 helloworld.sh （2）执行脚本 ​ /root/helloWorld.sh ​ ./helloWorld.sh 第二种：bash或sh+脚本（不用赋予脚本+x权限） ​ sh /root/helloWorld.sh ​ sh helloWorld.sh 1.3 shell中的变量1）Linux Shell中的变量分为，系统变量和用户自定义变量。 2）系统变量：$HOME、$PWD、$SHELL、$USER等等 3）显示当前shell中所有变量：set 1.3.1 定义变量1）基本语法： ​ （1）定义变量：变量=值 （2）撤销变量：unset 变量 （3）声明静态变量：readonly变量，注意：不能unset 2）变量定义规则 ​ （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头。 ​ （2）等号两侧不能有空格 ​ （3）变量名称一般习惯为大写 3）案例 ​ （1）定义变量A A=8 ​ （2）撤销变量A unset A ​ （3）声明静态的变量B=2，不能unset readonly B=2 ​ （4）可把变量提升为全局环境变量，可供其他shell程序使用 export 变量名 1.3.2 将命令的返回值赋给变量​ （1）A=ls -la 反引号，运行里面的命令，并把结果返回给变量A ​ （2）A=$(ls -la) 等价于反引号 1.3.3 设置环境变量1）基本语法： ​ （1）export 变量名=变量值 （功能描述：设置环境变量的值） （2）source 配置文件 （功能描述：让修改后的配置信息立即生效） （3）echo $变量名 （功能描述：查询环境变量的值） 2）案例： ​ （1）在/etc/profile文件中定义JAVA_HOME环境变量 ​ export JAVA_HOME=/opt/module/jdk1.7.0_79 ​ export PATH=$PATH:$JAVA_HOME/bin （2）查看环境变量JAVA_HOME的值 ​ [kingge@hadoop101 datas]$ echo $JAVA_HOME /opt/module/jdk1.7.0_79 1.3.4 位置参数变量1）基本语法 ​ $n （功能描述：n为数字，$0代表命令本身，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如$&#123;10&#125;）$* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）$@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）$# （功能描述：这个变量代表命令行中所有参数的个数） 2）案例 （1）输出输入的的参数1，参数2，所有参数，参数个数 #!/bin/bashecho &quot;$0 $1 $2&quot;echo &quot;$*&quot;echo &quot;$@&quot;echo &quot;$#&quot; （2）$*与$@的区别 #!/bin/bash for i in &quot;$*&quot; #$*中的所有参数看成是一个整体，所以这个for循环只会循环一次 do echo &quot;The parameters is: $i&quot; done x=1 for y in &quot;$@&quot; #$@中的每个参数都看成是独立的，所以“$@”中有几个参数，就会循环几次 do echo &quot;The parameter$x is: $y&quot; x=$(( $x +1 )) done a）$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数 b）当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“​$1 $2 …​$n”的形式输出所有参数；“$@”会将各个参数分开，以“​$1” “$2”…”​$n”的形式输出所有参数 1.3.5 预定义变量1）基本语法： （1）“$((运算式))”或“$[运算式]”（2）expr m + n 注意expr运算符间要有空格（3）expr m - n（4）expr \\*, /, % 乘，除，取余 2）案例 计算（2+3）X4的值 （1）采用$[运算式]方式[root@hadoop101 datas]# S=$[(2+3)*4] [root@hadoop101 datas]# echo $S （2）expr分布计算 S=`expr 2 + 3` expr $S \\* 4 （3）expr一步完成计算 expr `expr 2 + 3` \\* 4 1.4 运算符1）基本语法： （1）“$((运算式))”或“$[运算式]” （2）expr m + n 注意expr运算符间要有空格 （3）expr m - n （4）expr *, /, % 乘，除，取余 2）案例：计算（2+3）X4的值 ​ （1）采用$[运算式]方式 ​ [root@hadoop101 datas]# S=$[(2+3)*4] ​ [root@hadoop101 datas]# echo $S ​ （2）expr分布计算 ​ S=expr 2 + 3 ​ expr $S * 4 ​ （3）expr一步完成计算 ​ expr expr 2 + 3 * 4 1.5 条件判断1.5.1 判断语句1）基本语法： [ condition ]（注意condition前后要有空格） #非空返回true，可使用$?验证（0为true，&gt;1为false） 2）案例： [kingge] 返回true [] 返回false [condition] &amp;&amp; echo OK || echo notok 条件满足，执行后面的语句 1.5.2 常用判断条件1）两个整数之间比较 = 字符串比较 -lt 小于 -le 小于等于 -eq 等于 -gt 大于 -ge 大于等于 -ne 不等于 2）按照文件权限进行判断 -r 有读的权限 -w 有写的权限 -x 有执行的权限 3）按照文件类型进行判断 -f 文件存在并且是一个常规的文件 -e 文件存在 -d 文件存在并是一个目录 4）案例 ​ （1）23是否大于等于22 ​ [ 23 -ge 22 ] ​ （2）student.txt是否具有写权限 ​ [ -w student.txt ] ​ （3）/root/install.log目录中的文件是否存在 ​ [ -e /root/install.log ] 1.6 流程控制1.6.1 if判断1）基本语法： if [ 条件判断式 ];then 程序 fi 或者 if [ 条件判断式 ] then ​ 程序 fi ​ 注意事项：（1）[ 条件判断式 ]，中括号和条件判断式之间必须有空格 2）案例 #!/bin/bashif [ $1 -eq &quot;123&quot; ]then echo &quot;123&quot;elif [ $1 -eq &quot;456&quot; ]then echo &quot;456&quot;fi 1.6.2 case语句1）基本语法： case $变量名 in “值1”） ​ 如果变量的值等于值1，则执行程序1 ​ ;; “值2”） ​ 如果变量的值等于值2，则执行程序2 ​ ;; …省略其他分支… *） ​ 如果变量的值都不是以上的值，则执行此程序 ​ ;; esac 2）案例 !/bin/bashcase $1 in&quot;1&quot;) echo &quot;1&quot;;;&quot;2&quot;) echo &quot;2&quot;;;*) echo &quot;other&quot;;;esac 1.6.3 for循环1）基本语法1： for 变量 in 值1 值2 值3… do ​ 程序 done 2）案例： ​ （1）打印输入参数 #!/bin/bash#打印数字for i in &quot;$*&quot; do echo &quot;The num is $i &quot; donefor j in &quot;$@&quot; do echo &quot;The num is $j&quot; done 3）基本语法2： ​ for (( 初始值;循环控制条件;变量变化 )) do ​ 程序 done 4）案例 （1）从1加到100 #!/bin/bashs=0for((i=0;i&lt;=100;i++))do s=$[$s+$i]doneecho &quot;$s&quot; 1.6.4 while循环1）基本语法： while [ 条件判断式 ] do ​ 程序 done 2）案例 ​ （1）从1加到100 #!/bin/bashs=0i=1while [ $i -le 100 ]do s=$[$s+$i] i=$[$i+1]doneecho $s 1.7 read读取控制台输入1）基本语法： ​ read(选项)(参数) ​ 选项： -p：指定读取值时的提示符； -t：指定读取值时等待的时间（秒）。 参数 ​ 变量：指定读取值的变量名 2）案例 ​ 读取控制台输入的名称 #!/bin/bashread -t 7 -p &quot;please 7 miao input your name &quot; NAMEecho $NAME 1.8 函数1.8.1 系统函数1）basename基本语法 basename [pathname] [suffix] basename [string] [suffix] （功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。 选项： suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。 2）案例 [kingge@hadoop101 opt]$ basename /opt/test.txt test.txt [kingge@hadoop101 opt]$ basename /opt/test.txt .txt test 3）dirname基本语法 ​ dirname 文件绝对路径 （功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）） 4）案例 ​ [kingge@hadoop101 opt]$ dirname /opt/test.txt /opt 1.8.2 自定义函数1）基本语法： [ function ] funname[()]&#123; Action; [return int;]&#125; funname 注意： ​ （1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。 ​ （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255) 2）案例 ​ （1）计算输入参数的和 #!/bin/bashfunction sum()&#123; s=0 s=$[ $1 + $2 ] echo &quot;$s&quot;&#125;read -p &quot;Please input the number1: &quot; n1;read -p &quot;Please input the number2: &quot; n2;sum $n1 $n2;","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"实现多数据量的导入数据库","slug":"实现多数据量的导入数据库","date":"2017-08-01T06:37:15.000Z","updated":"2017-08-17T08:21:15.695Z","comments":true,"path":"2017/08/01/实现多数据量的导入数据库/","link":"","permalink":"http://kingge.top/2017/08/01/实现多数据量的导入数据库/","excerpt":"","text":"引言 在做一个项目的时候，涉及到需要从一个表格中获取百万条数据然后插入到数据库中，最后采用JDBC的executeBantch方法实现这个功能。 采取的策略 尽量关闭字段索引（因为再插入数据的时候还是需要维护索引的，在创建索引和维护索引 会耗费时间,随着数据量的增加而增加，可以在插入数据后再去为字段创建索引） 虽然索引可以提高查询速度但是，插入数据的时候会导致索性的更新。索性越多，插入会越慢。可以看文档描述:Although it can be tempting to create an indexes for every possible column used in a query, unnecessary indexes waste space and waste time for MySQL to determine which indexes to use. Indexes also add to the cost of inserts, updates, and deletes because each index must be updated. You must find the right balance to achieve fast queries using the optimal set of indexes. 分批次提交数据 在分布式条件下，还可以考虑在不同的数据库结点提交，有底层的消息系统完成数据扩展 过滤预处理数据 预处理数据的场景：为了避免插入的数据（假设ListA）跟数据库中某些数据重复，那么我们会把要插入的数据去数据库中查询是否已经存在，获得返回的已经存在数据（ListB）。然后在插入数据的时候判断当前数据是否在ListB中，那么当前数据不能够插入数据库。过滤出来，最后得到一个可以插入数据库的ListC 代码关键代码/*数据分析结束*/ /*往数据库写数据开始*/ Connection conn=null; PreparedStatement idsUserAdd=null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\") ; conn = DriverManager.getConnection(ConfigTool.getProperty(\"jdbc.url\").toString() , ConfigTool.getProperty(\"jdbc.username\").toString() , ConfigTool.getProperty(\"jdbc.password\").toString()); conn.setAutoCommit(false); //构造预处理statement idsUserAdd = conn.prepareStatement(\"INSERT INTO dc_matedata (\"+ \" ID,`NAME`, DATATYPE,`CODE`,TYPE_ID,`LENGTH`, \"+ \" DATANAME, VALUEAREA,`RESTRICT`, REMARK,MD_DATE)\"+ \" values(?,?,?,?,?,?,?,?,?,?,now())\"); //最大列表的数目当做循环次数 int xhcs=addMetadataList.size();//addMetadataList需要插入的数据 for(int i=0;i&lt;xhcs;i++)&#123; idsUserAdd.setString(1,addMetadataList.get(i).get(\"id\").toString()); idsUserAdd.setString(2,addMetadataList.get(i).get(\"name\").toString()); idsUserAdd.setString(3,addMetadataList.get(i).get(\"dataType\").toString()); idsUserAdd.setString(4,addMetadataList.get(i).get(\"code\").toString()); idsUserAdd.setString(5,addMetadataList.get(i).get(\"typeId\").toString()); idsUserAdd.setString(6,addMetadataList.get(i).get(\"dataLength\").toString()); idsUserAdd.setString(7,addMetadataList.get(i).get(\"dataName\").toString()); idsUserAdd.setString(8,addMetadataList.get(i).get(\"valueArea\").toString()); idsUserAdd.setString(9,addMetadataList.get(i).get(\"dataRestrict\").toString()); idsUserAdd.setString(10,addMetadataList.get(i).get(\"dataRemark\").toString()); idsUserAdd.addBatch(); //每10000次提交一次 if(i%10000==0||i==xhcs-1)&#123;//可以设置不同的大小；如50，100，500，1000等等 i==xhcs-1（最后一次） idsUserAdd.executeBatch(); conn.commit(); idsUserAdd.clearBatch(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw e; &#125;finally &#123; try &#123; if(idsUserAdd!=null) idsUserAdd.close(); if(conn!=null) conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); throw e; &#125; &#125; /*往数据库写数据结束*/ 完整代码/** * 校验需要导入的元数据信息，封装错误信息并批量插入数据库 */ @Override public List&lt;Map&lt;String, Object&gt;&gt; saveDCMetadataBatch(List&lt;Map&lt;String, Object&gt;&gt; list, boolean valid, boolean addError) throws Exception&#123; List&lt;Map&lt;String,Object&gt;&gt; errorList=new ArrayList&lt;Map&lt;String,Object&gt;&gt;();//获得不能够添加成功的数据 Map&lt;String,Object&gt; map=new HashMap&lt;String,Object&gt;();//查询条件 Map&lt;String,String&gt; codeMap=new HashMap&lt;String,String&gt;();//每个分类对应的元数据的编号最大值 Map&lt;String,Object&gt; metaName=new HashMap&lt;String,Object&gt;();//查询数据库中是否存在相同的数据（这里校验的是：元数据的中文简称） Map&lt;String,Object&gt; metaDataName=new HashMap&lt;String,Object&gt;();//查询数据库中是否存在相同的数据（这里校验的是：元数据的数据项名称） map.put(\"metaName\",list);//需要查询的元数据中文名称 map.put(\"metaDataTypeId\",list);//导入的元数据的编号 List&lt;Map&lt;String, Object&gt;&gt; metaExistList = dCMatedataDao.getDCMetadata(map);//根据元数据名称查询当前分类下是否存在同样元数据 map.put(\"metaName\",null);//置空 map.put(\"metaDataName\",list); List&lt;Map&lt;String, Object&gt;&gt; metaExistListTwo = dCMatedataDao.getDCMetadata(map);//根据元数据数据项名称查询存在的元数据 //保存重复的信息 for(int i=0;i&lt;metaExistList.size();i++) metaName.put(metaExistList.get(i).get(\"name\").toString()+metaExistList.get(i).get(\"code\").toString() ,metaExistList.get(i).get(\"id\"));//添加父类的编号为后缀-唯一性保证 for(int i=0;i&lt;metaExistListTwo.size();i++) metaDataName.put(metaExistListTwo.get(i).get(\"dataname\").toString()+metaExistListTwo.get(i).get(\"code\").toString(), metaExistListTwo.get(i).get(\"id\")); /*整理出来的数据-开始*/ List&lt;Map&lt;String,Object&gt;&gt; addMetadataList=new ArrayList&lt;Map&lt;String,Object&gt;&gt;(); /*整理出来的数据-结束*/ for (int i = 0; i &lt; list.size(); i++) &#123; Map&lt;String, Object&gt; MetadataObj = list.get(i); try &#123; String metadatId = StringUtil.getUUID();//元数据id /*校验开始*/ if (valid)&#123; if(validUser(MetadataObj,\"name\",addError)!=null)&#123;//验证输入的数据是否符合格式和必填。 errorList.add(MetadataObj); continue; &#125; &#125; /*前端校验结束*/ /*校验是否存在同名的元数据*/ String dataCodeCheck = MetadataObj.get(\"dataCode\").toString().trim(); //元数据父分类编号 String name = MetadataObj.get(\"name\").toString().trim();//元数据中文简称 if (metaName.containsKey(name+dataCodeCheck)) &#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"中文简称已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; /*校验是否存在相同数据项的元数据*/ String dataName = MetadataObj.get(\"dataName\").toString().trim();//数据项名 if (metaDataName.containsKey(dataName+dataCodeCheck)) &#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"数据项名已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; String dataCode = MetadataObj.get(\"dataCode\").toString().trim(); //元数据父分类编号 List&lt;Map&lt;String, Object&gt;&gt; footCount = dCMatedataDao.getFootCount(dataCode); if( footCount.size() &gt; 0)&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"分类编码不是最后一级分类\"); &#125; errorList.add(MetadataObj); continue; &#125; Map&lt;String, Object&gt; typeByCode = dCMatedataDao.getMetadataTypeByCode(dataCode); if( typeByCode == null || typeByCode.size() &lt; 1)&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"分类编码不存在，请先添加分类\"); &#125; errorList.add(MetadataObj); continue; &#125; //校验是在添加的List中是否存在相同的数据项名或者中文简称 //校验导入文件中是否存在一样的中文简称或者数据项名 boolean nameExist = false; boolean dataNameExist = false; for (int j = 0; j &lt; addMetadataList.size(); j++)&#123; Map&lt;String, Object&gt; map2 = addMetadataList.get(j); String typeId = map2.get(\"typeId\").toString(); String nameE = map2.get(\"name\").toString(); String dataNameE = map2.get(\"dataName\").toString(); if( typeId.equals(typeByCode.get(\"id\").toString()) &amp;&amp; nameE.equals(name))&#123; nameExist=true; break; &#125; if( typeId.equals(typeByCode.get(\"id\").toString()) &amp;&amp; dataNameE.equals(dataName))&#123; dataNameExist=true; break; &#125; &#125; if( nameExist )&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"中文简称已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; if( dataNameExist )&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"数据项名已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; //进入这里说明校验结束，开始填充添加的数据 String type_id = typeByCode.get(\"id\").toString();//元数据所属分类id String dataType = MetadataObj.get(\"dataType\").toString().trim(); //元数据类型 String dataLength = MetadataObj.get(\"dataLength\").toString().trim(); //元数据长度 String code = \"\"; //// if( codeMap.get(dataCode) == null||StringUtil.isEmpty(codeMap.get(dataCode)) )&#123;//表示当前分类不存在已经添加的元数据--因为编码map中不存在对应分类的最大编码 Map maxCodeByPid = this.selectMetadataMaxCode(type_id); if( maxCodeByPid == null )&#123;//表示当前分类下不存在任何子分类 code = StringUtil.getCode(\"0\", dataCode);//则从01开始编号 codeMap.put(dataCode, \"01\");//保存当前分类下元数据编号最大值 &#125;else&#123; String object = (String) maxCodeByPid.get(\"codeNum\");//当前分类节点下的元数据的编号最大值。 int pSituation = object.indexOf(dataCode); int pLength = pSituation+dataCode.length() ; String substring = object.substring(pLength); //截取出最大编号值得最大值 code = StringUtil.getCode(substring, dataCode); int temp = Integer.parseInt(substring);//保存当前分类下元数据编号最大值 temp+=1; codeMap.put(dataCode, temp+\"\"); &#125; &#125;else&#123; String maxCode = codeMap.get(dataCode); code = StringUtil.getCode(maxCode, dataCode); //保存当前分类下元数据编号最大值 int temp = Integer.parseInt(maxCode); temp+=1; codeMap.put(dataCode, temp+\"\"); &#125; /// Map&lt;String, Object&gt; metadatList = new LinkedHashMap&lt;String, Object&gt;(); metadatList.put(\"id\", metadatId); metadatList.put(\"name\",name); metadatList.put(\"dataType\",dataType); metadatList.put(\"code\",code); metadatList.put(\"typeId\",type_id); metadatList.put(\"dataLength\",dataLength); metadatList.put(\"dataName\",dataName); metadatList.put(\"valueArea\", MetadataObj.get(\"valueArea\")==null?\"\":MetadataObj.get(\"valueArea\") ); metadatList.put(\"dataRestrict\",MetadataObj.get(\"dataRestrict\")==null?\"\":MetadataObj.get(\"dataRestrict\")); metadatList.put(\"dataRemark\",MetadataObj.get(\"dataRemark\")==null?\"\":MetadataObj.get(\"dataRemark\")); metadatList.put(\"mdDate\",new Date()); addMetadataList.add(metadatList); &#125; catch (Exception e)&#123; if(addError) &#123; MetadataObj.put(\"errInfo\", e.getMessage()); &#125; errorList.add(MetadataObj); &#125; &#125; /*数据分析结束*/ /*往数据库写数据开始*/ Connection conn=null; PreparedStatement idsUserAdd=null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\") ; conn = DriverManager.getConnection(ConfigTool.getProperty(\"jdbc.url\").toString() , ConfigTool.getProperty(\"jdbc.username\").toString() , ConfigTool.getProperty(\"jdbc.password\").toString()); conn.setAutoCommit(false); //构造预处理statement idsUserAdd = conn.prepareStatement(\"INSERT INTO dc_matedata (\"+ \" ID,`NAME`, DATATYPE,`CODE`,TYPE_ID,`LENGTH`, \"+ \" DATANAME, VALUEAREA,`RESTRICT`, REMARK,MD_DATE)\"+ \" values(?,?,?,?,?,?,?,?,?,?,now())\"); //最大列表的数目当做循环次数 int xhcs=addMetadataList.size(); for(int i=0;i&lt;xhcs;i++)&#123; idsUserAdd.setString(1,addMetadataList.get(i).get(\"id\").toString()); idsUserAdd.setString(2,addMetadataList.get(i).get(\"name\").toString()); idsUserAdd.setString(3,addMetadataList.get(i).get(\"dataType\").toString()); idsUserAdd.setString(4,addMetadataList.get(i).get(\"code\").toString()); idsUserAdd.setString(5,addMetadataList.get(i).get(\"typeId\").toString()); idsUserAdd.setString(6,addMetadataList.get(i).get(\"dataLength\").toString()); idsUserAdd.setString(7,addMetadataList.get(i).get(\"dataName\").toString()); idsUserAdd.setString(8,addMetadataList.get(i).get(\"valueArea\").toString()); idsUserAdd.setString(9,addMetadataList.get(i).get(\"dataRestrict\").toString()); idsUserAdd.setString(10,addMetadataList.get(i).get(\"dataRemark\").toString()); idsUserAdd.addBatch(); //每10000次提交一次 if(i%10000==0||i==xhcs-1)&#123;//可以设置不同的大小；如50，100，500，1000等等 idsUserAdd.executeBatch(); conn.commit(); idsUserAdd.clearBatch(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw e; &#125;finally &#123; try &#123; if(idsUserAdd!=null) idsUserAdd.close(); if(conn!=null) conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); throw e; &#125; &#125; /*往数据库写数据结束*/ return errorList; &#125; 总结 有些网友发现使用StringBuffer 来拼接入参，不通过prepareStatement的预处理，虽然前者速度很快，但是使用prepareStatement可以防止SQL注入 有的好的建议大家都可以提出来","categories":[{"name":"JDBC","slug":"JDBC","permalink":"http://kingge.top/categories/JDBC/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"JDBC","slug":"JDBC","permalink":"http://kingge.top/tags/JDBC/"},{"name":"批量导入","slug":"批量导入","permalink":"http://kingge.top/tags/批量导入/"},{"name":"SSM","slug":"SSM","permalink":"http://kingge.top/tags/SSM/"},{"name":"项目经验","slug":"项目经验","permalink":"http://kingge.top/tags/项目经验/"}]},{"title":"Java工程师书单（初级、中级、高级）","slug":"Java工程师书单（初级、中级、高级）","date":"2017-06-21T06:13:44.000Z","updated":"2017-08-24T07:33:21.781Z","comments":true,"path":"2017/06/21/Java工程师书单（初级、中级、高级）/","link":"","permalink":"http://kingge.top/2017/06/21/Java工程师书单（初级、中级、高级）/","excerpt":"","text":"当你的能力承受不住你的欲望，你就应该静下心来读书 初级书籍《编写高质量代码——改善Java程序的151个建议》 这是一本值得入门java的人放在床头的书。此书内容广泛、要点翔实。大多数优秀程序设计书籍都需要看老外写的，但是这本讲述提高java编程水平的书还是不错的，适合具有基本java编程能力的人。对于程序猿而言，工作久了，就感觉编程习惯对一个人很重要。习惯好，不仅工作效率告，而且bug少。这本书对提高个人的好的编程习惯很有帮助。 《Java程序员修炼之道》 此书涵盖了Java7的新特性和Java开发的关键技术，对当前大量开源技术并存，多核处理器、并发以及海量数据给Java开发带来的挑战作出了精辟的分析，提供了实践前沿的深刻洞见，涉及依赖注入、现代并发、类与字节码、性能调优等底层概念的剖析。**书中的道理很浅显，可是对于菜鸟却是至理名言。基本为你勾勒了一个成熟软件程序员专家所需要的所有特性。。 《Java8实战》 没看过。嘻嘻嘻 《有效的单元测试》 此书由敏捷技术实践专家撰写，系统且深入地阐释单元测试用于软件设计的工具、方法、原则和佳实践；深入剖析各种测试常见问题，包含大量实践案例，可操作性强，能为用户高效编写测试提供系统实践指南。**介绍了单元测试的各个方面，TDD、test double、测试的坏味道、可测试的设计等等，每个主题需要深入的话，还需要配合其它书籍和实践，非常适合入门单元测试。书中例子非常全面，看完对使用 Junit 进行单元测试会有一个大的长进，而且用java语言编写，内容很新 《Java核心技术：卷1》 不推荐卷2，因为这个作为初级书单来讲，太难了。 《代码整洁之道》 没看过 《数据结构与算法分析-Java语言描述》 本书是java数据结构与算法方面的三宝之一，除了这三本其他的已经没有意义了。这三宝分别是:**黑宝书《数据结构与算法分析java语言描述》mark allen weiss蓝宝书《java数据结构和算法》robert lafore红宝书《算法》robert sedgewick黑宝书胜在公式推理和证明以及算法的简洁和精炼，此外习题较多。蓝宝书胜在对算法的深入浅出的讲解，演示和举例，让艰涩的理论变得很容易理解。红宝书胜在系出名门斯坦福，演示通俗易懂，内容丰富。有了这三宝，算法不用愁，学完以后再看《算法导论》就容易多了。本书从讲解什么是数据结构开始，延伸至高级数据结构和算法分析，强调数据结构和问题求解技术。本书的目的是从抽象思维和问题求解的观点提供对数据结构的实用介绍，试图包含有关数据结构、算法分析及其Java实现的所有重要的细节 中级书单《重构：改善既有代码的设计》 重构，绝对是写程序过程中最重要的事之一。在写程序之前我们不可能事先了解所有的需求，设计肯定会有考虑不周的地方，而且随着项目需求的修改，也有可能原来的设计已经被改得面目全非了。更何况，我们很少有机会从头到尾完成一个项目，基本上都是接手别人的代码，我们要做的是重构，从小范围的重构开始。**重构是设计,设计是art,重构也是art. 一个函数三行只是语不惊人死不休的说法,是对成百上千行代码的矫枉过正。 更一个般的看法是一个函数应该写在一页纸内。 《Effective Java》 必读 《Java并发编程实战》 没看过： 本书深入浅出地介绍了Java线程和并发，是一本完美的Java并发参考手册。书中从并发性和线程安全性的基本概念出发，介绍了如何使用类库提供的基本并发构建块，用于避免并发危险、构造线程安全的类及验证线程安全的规则，如何将小的线程安全类组合成更大的线程安全类，如何利用线程来提高并发应用程序的吞吐量。**java进阶必看，多线程的最佳书籍。 实战Java高并发程序设计》 没看过 《算法》 没看过 《Head First 设计模式》 这是我看过最幽默最搞笑最亲切同时又让我收获巨大的技术书籍！ 森森的膜拜Freeman(s)！Amen！ 深入浅出，娓娓道来，有的地方能笑死你！写得很有趣，图文并茂，比起四人帮的那本，好懂了不知道多少倍。计算机世界的head first系列基本都是经典。不过只看书学明白设计模式是不可能的，这些只是前人的总结，我们唯有实践实践再实践了。**读这本书不仅仅是学习知识，而是在学习一种思考的方法，学习一种认知的技巧，学习一种成长的阶梯。 总之，用你闲暇的时间来读这本书，并不亚于你专注的工作或学习。笔者强烈推荐此书，要成长为一名高级程序员，设计模式已经是必备技能了。 《Java编程思想》 没看过 高级书单 《深入理解Java虚拟机》 没看过 《Java性能权威指南》 没看过 《深入分析Java Web技术内幕》 没看过 《大型网站系统与Java中间件实践》 没看过 《大型网站技术架构：核心原理与案例分析》 没看过 《企业应用架构模式》 没看过 Spring3.x企业应用开发实战 这本书适合初学者看或者当做一本参考书。对于提高者而言，略看就行 Spring揭秘 没看过 Java程序性能优化:让你的Java程序更快、更稳定 没看过 总结talk is less show me your code，希望大家有好的书籍也可以推荐","categories":[{"name":"读书系统","slug":"读书系统","permalink":"http://kingge.top/categories/读书系统/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"书籍推荐","slug":"书籍推荐","permalink":"http://kingge.top/tags/书籍推荐/"}]},{"title":"linux-PRM软件包管理工具","slug":"linux-PRM软件包管理工具","date":"2017-06-13T12:12:30.000Z","updated":"2019-06-02T05:47:38.887Z","comments":true,"path":"2017/06/13/linux-PRM软件包管理工具/","link":"","permalink":"http://kingge.top/2017/06/13/linux-PRM软件包管理工具/","excerpt":"","text":"1.1 概述RPM（RedHat Package Manager），Rethat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。 RPM包的名称格式 Apache-1.3.23-11.i386.rpm - “apache” 软件名称 - “1.3.23-11”软件的版本号，主版本和此版本 - “i386”是软件所运行的硬件平台 - “rpm”文件扩展名，代表RPM包 1.2 常用命令1.2.1 查询（rpm -qa）1）基本语法： rpm -qa （功能描述：查询所安装的所有rpm软件包） 过滤 rpm -qa | grep rpm软件包 2）案例 [root@hadoop101 Packages]# rpm -qa |grep firefox firefox-45.0.1-1.el6.centos.x86_64 1.2.2 卸载（rpm -e）1）基本语法： （1）rpm -e RPM软件包 或者（2） rpm -e –nodeps 软件包 –nodeps 如果该RPM包的安装依赖其它包，即使其它包没装，也强迫安装。 2）案例 [root@hadoop101 Packages]# rpm -e firefox 1.2.3 安装（rpm -ivh）1）基本语法： ​ rpm -ivh RPM包全名 ​ -i=install，安装 ​ -v=verbose，显示详细信息 ​ -h=hash，进度条 ​ –nodeps，不检测依赖进度 2）案例 [root@hadoop101 Packages]# pwd /media/CentOS_6.8_Final/Packages [root@hadoop101 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm warning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY Preparing… ########################################### [100%] 1:firefox ########################################### [100%]","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"linux基本操作命令","slug":"linux基本操作命令","date":"2017-06-13T07:34:03.000Z","updated":"2019-06-02T05:52:23.561Z","comments":true,"path":"2017/06/13/linux基本操作命令/","link":"","permalink":"http://kingge.top/2017/06/13/linux基本操作命令/","excerpt":"","text":"一、常用基本命令此文章紧接linux基础文章，如果没有安装linux，那么情先参见（linux基础 ） 1.1 帮助命令1.1.1 man 获得帮助信息1）基本语法： ​ man [命令或配置文件] （功能描述：获得帮助信息） ​ （1）显示说明 NAME 命令的名称和单行描述 SYNOPSIS 怎样使用命令 DESCRIPTION 命令功能的深入讨论 EXAMPLES 怎样使用命令的例子 SEE ALSO 相关主题（通常是手册页） （2）数字说明 1.用户在shell环境中可以操作的命令或是可执行的文件 2.系统内核(kernel)可以调用的函数 3.常用的函数or函数库 4.设备配置文件 5.配置文件的格式 6.游戏相关 7.linux网络协议和文件系统 8.系统管理员可以用的命令 9.跟内核有关系的文件 2）案例 [root@hadoop106 home]# man ls 1.1.2 help 获得shell内置命令的帮助信息1）基本语法： ​ help 命令 （功能描述：获得shell内置命令的帮助信息） 2）案例： ​ [root@hadoop101 bin]# help cd 1.1.3 常用快捷键1）ctrl + c：停止进程 2）ctrl+l：清屏 3）ctrl + q：退出 4）善于用tab键 5）上下键：查找执行过的命令 6）ctrl +alt：linux和Windows之间切换 1.2 文件目录类1.2.1 pwd 显示当前工作目录的绝对路径1）基本语法： ​ pwd （功能描述：显示当前工作目录的绝对路径） ​ 2）案例 [root@hadoop101 home]# pwd /home 1.2.2 ls 列出目录的内容1）基本语法： ls [选项] [目录或是文件] 选项： -a ：全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用) -l ：长数据串列出，包含文件的属性与权限等等数据；(常用) 每行列出的信息依次是： 文件类型与权限 链接数 文件属主 文件属组 文件大小用byte来表示 建立或最近修改的时间 名字 2）案例 [kingge@hadoop101 ~]$ ls -al 总用量 44 drwx——. 5 kingge kingge 4096 5月 27 15:15 . drwxr-xr-x. 3 root root 4096 5月 27 14:03 .. drwxrwxrwx. 2 root root 4096 5月 27 14:14 hello -rwxrw-r–. 1 kingge kingge 34 5月 27 14:20 test.txt 1.2.3 mkdir 创建一个新的目录1）基本语法： ​ mkdir [-p] 要创建的目录 ​ 选项： -p：创建多层目录 2）案例 [root@hadoop101 opt]# mkdir test [root@hadoop101 opt]# mkdir -p user/kingge 1.2.4 rmdir 删除一个空的目录1）基本语法： ​ rmdir 要删除的空目录 2）案例 [root@hadoop101 opt]# mkdir test [root@hadoop101 opt]# rmdir test 1.2.5 touch 创建空文件1）基本语法： ​ touch 文件名称 2）案例 [root@hadoop101 opt]# touch test.java 1.2.6 cd 切换目录1）基本语法： ​ （1）cd 绝对路径 ​ （2）cd 相对路径 ​ （3）cd ~或者cd （功能描述：回到自己的家目录） ​ （4）cd - （功能描述：回到上一次所在目录） ​ （5）cd .. （功能描述：回到当前目录的上一级目录） ​ （6）cd -P （功能描述：跳转到实际物理路径，而非快捷方式路径） 2）案例 （1）使用 mkdir 命令创建kingge目录 [root@hadoop101 ~]# mkdir kingge （2）使用绝对路径切换到kingge目录 [root@hadoop101 ~]# cd /root/kingge/ （3）使用相对路径切换到kingge目录 [root@hadoop101 ~]# cd ./kingge/ （4）表示回到自己的家目录，亦即是 /root 这个目录 [root@hadoop101 kingge]# cd ~ （5）cd- 回到上一次所在目录 [root@hadoop101 kingge]# cd - （6）表示回到当前目录的上一级目录，亦即是 /root 的上一级目录的意思； [root@hadoop101 ~]# cd .. 1.2.7 cp 复制文件或目录1）基本语法： （1）cp source dest （功能描述：复制source文件到dest） （2）cp -r sourceFolder targetFolder （功能描述：递归复制整个文件夹） 2）案例 （1）复制文件 [root@hadoop101 opt]# cp test.java test （2）递归复制整个文件夹 [root@hadoop101 opt]# cp -r test test1 1.2.8 rm 移除文件或目录1）基本语法 ​ （1）rmdir deleteEmptyFolder （功能描述：删除空目录） （2）rm -rf deleteFile （功能描述：递归删除目录中所有内容） 2）案例 ​ 1）删除空目录 [root@hadoop101 opt]# rmdir test 2）递归删除目录中所有内容 [root@hadoop101 opt]# rm -rf test1 1.2.9 mv 移动文件与目录或重命名1）基本语法： ​ （1）mv oldNameFile newNameFile （功能描述：重命名） ​ （2）mv /temp/movefile /targetFolder （功能描述：递归移动文件） 2）案例： ​ 1）重命名 [root@hadoop101 opt]# mv test.java test1.java 2）移动文件 [root@hadoop101 opt]# mv test1.java test1 1.2.10 cat 查看文件内容查看文件内容，从第一行开始显示。 1）基本语法 ​ cat [选项] 要查看的文件 选项： -A ：相当于 -vET 的整合选项，可列出一些特殊字符而不是空白而已； -b ：列出行号，仅针对非空白行做行号显示，空白行不标行号！ -E ：将结尾的断行字节 $ 显示出来； -n ：列出行号，连同空白行也会有行号，与 -b 的选项不同； -T ：将 [tab] 按键以 ^I 显示出来； -v ：列出一些看不出来的特殊字符 2）案例 [kingge@hadoop101 ~]$ cat -A test.txt hellda $ dasadf ^I$ da^I^I^I$ das$ 1.2.11 tac查看文件内容查看文件内容，从最后一行开始显示，可以看出 tac 是 cat 的倒著写。 1）基本语法： ​ tac [选项参数] 要查看的文件 2）案例 [root@hadoop101 test1]# cat test1.java hello kingge kingge1 [root@hadoop101 test1]# tac test1.java kingge1 kingge hello 1.2.12 more 查看文件内容查看文件内容，一页一页的显示文件内容。 1）基本语法： ​ more 要查看的文件 2）功能使用说明 空白键 (space)：代表向下翻一页； Enter:代表向下翻『一行』； q:代表立刻离开 more ，不再显示该文件内容。 Ctrl+F 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 :f 输出文件名和当前行的行号 3）案例 [root@hadoop101 test1]# more test1.java 1.2.13 less 查看文件内容less 的作用与 more 十分相似，都可以用来浏览文字档案的内容，不同的是 less 允许使用[pageup] [pagedown]往回滚动。 1）基本语法： ​ less 要查看的文件 2）功能使用说明 空白键 ：向下翻动一页； [pagedown]：向下翻动一页； [pageup] ：向上翻动一页； /字串 ：向下搜寻『字串』的功能；n：向下查找；N：向上查找； ?字串 ：向上搜寻『字串』的功能；n：向上查找；N：向下查找； q ：离开 less 这个程序； 3）案例 [root@hadoop101 test1]# less test1.java 1.2.14 head查看文件内容查看文件内容，只看头几行。 1）基本语法 head -n 10 文件 （功能描述：查看文件头10行内容，10可以是任意行数） 2）案例 [root@hadoop101 test1]# head -n 2 test1.java hello kingge 1.2.15 tail 查看文件内容查看文件内容，只看尾巴几行。 1）基本语法 （1）tail -n 10 文件 （功能描述：查看文件头10行内容，10可以是任意行数） （2）tail -f 文件 （功能描述：实时追踪该文档的所有更新） 2）案例 （1）查看文件头1行内容 [root@hadoop101 test1]# tail -n 1 test1.java kingge （2）实时追踪该档的所有更新 [root@hadoop101 test1]# tail -f test1.java hello kingge kingge 1.2.16 重定向命令1）基本语法： （1）ls -l &gt;文件 （功能描述：列表的内容写入文件a.txt中（覆盖写）） （2）ls -al &gt;&gt;文件 （功能描述：列表的内容追加到文件aa.txt的末尾） 2）案例 ​ （1）[root@hadoop101 opt]# ls -l&gt;t.txt （2）[root@hadoop101 opt]# ls -l&gt;&gt;t.txt （3）[root@hadoop101 test1]# echo hello&gt;&gt;test1.java 1.2.17 echo1）基本语法： （1）echo 要显示的内容 &gt;&gt; 存储内容的的文件 （功能描述：将要显示的内容，存储到文件中） ​ （2）echo 变量 （功能描述：显示变量的值） 2）案例 [root@hadoop101 test1]# echo $JAVA_HOME /opt/module/jdk1.7.0_79 1.2.18 ln软链接1）基本语法： ln -s [原文件] [目标文件] （功能描述：给原文件创建一个软链接，软链接存放在目标文件目录） 删除软链接： rm -rf kingge，而不是rm -rf kingge/ 2）案例： [root@hadoop101 module]# ln -s /opt/module/test.txt /opt/t.txt [root@hadoop101 opt]# ll lrwxrwxrwx. 1 root root 20 6月 17 12:56 t.txt -&gt; /opt/module/test.txt 创建一个软链接 [kingge@hadoop101 opt]$ ln -s /opt/module/hadoop-2.7.2/ /opt/software/hadoop cd不加参数进入是软链接的地址 [kingge@hadoop101 software]$ cd hadoop [kingge@hadoop101 hadoop]$ pwd /opt/software/hadoop cd加参数进入是实际的物理地址 [kingge@hadoop101 software]$ cd -P hadoop [kingge@hadoop101 hadoop-2.7.2]$ pwd /opt/module/hadoop-2.7.2 1.2.19 history查看所敲命令历史1）基本语法： ​ history 2）案例 [root@hadoop101 test1]# history 1.3 时间日期类1）基本语法 date [OPTION]… [+FORMAT] 1.3.1 date显示当前时间1）基本语法： ​ （1）date （功能描述：显示当前时间） ​ （2）date +%Y （功能描述：显示当前年份） （3）date +%m （功能描述：显示当前月份） （4）date +%d （功能描述：显示当前是哪一天） （5）date +%Y%m%d date +%Y/%m/%d … （功能描述：显示当前年月日各种格式 ） ​ （6）date “+%Y-%m-%d %H:%M:%S” （功能描述：显示年月日时分秒） 2）案例 [root@hadoop101 /]# date 2017年 06月 19日 星期一 20:53:30 CST [root@hadoop101 /]# date +%Y%m%d 20170619 [root@hadoop101 /]# date “+%Y-%m-%d %H:%M:%S” 2017-06-19 20:54:58 1.3.2 date显示非当前时间1）基本语法： （1）date -d ‘1 days ago’ （功能描述：显示前一天日期） （2）date -d yesterday +%Y%m%d （同上） （3）date -d next-day +%Y%m%d （功能描述：显示明天日期） （4）date -d ‘next monday’ （功能描述：显示下周一时间） 2）案例： [root@hadoop101 /]# date -d ‘1 days ago’ 2017年 06月 18日 星期日 21:07:22 CST [root@hadoop101 /]# date -d next-day +%Y%m%d 20170620 [root@hadoop101 /]# date -d ‘next monday’ 2017年 06月 26日 星期一 00:00:00 CST 1.3.3 date设置系统时间1）基本语法： ​ date -s 字符串时间 2）案例 ​ [root@hadoop106 /]# date -s “2017-06-19 20:52:18” 1.3.4 cal查看日历1）基本语法： cal [选项] （功能描述：不加选项，显示本月日历） 选项： -3 ，显示系统前一个月，当前月，下一个月的日历 具体某一年，显示这一年的日历。 2）案例： [root@hadoop101 /]# cal [root@hadoop101 /]# cal -3 ​ [root@hadoop101 /]# cal 2016 1.4 用户管理命令1.4.1 useradd 添加新用户1）基本语法： ​ useradd 用户名 （功能描述：添加新用户） 2）案例： ​ [root@hadoop101 opt]# user kingge 1.4.2 passwd 设置用户密码1）基本语法： ​ passwd 用户名 （功能描述：设置用户密码） 2）案例 ​ [root@hadoop101 opt]# passwd kingge 1.4.3 id 判断用户是否存在1）基本语法： ​ id 用户名 2）案例： ​ [root@hadoop101 opt]#id kingge 1.4.4 su 切换用户1）基本语法： su 用户名称 （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量） su - 用户名称 （功能描述：切换到用户并获得该用户的环境变量及执行权限） 2）案例 [root@hadoop101 opt]#su kingge [root@hadoop101 opt]#su - kingge 1.4.5 userdel 删除用户1）基本语法： ​ （1）userdel 用户名 （功能描述：删除用户但保存用户主目录） （2）userdel -r 用户名 （功能描述：用户和用户主目录，都删除） 2）案例： （1）删除用户但保存用户主目录 ​ [root@hadoop101 opt]#userdel kingge （2）删除用户和用户主目录，都删除 ​ [root@hadoop101 opt]#userdel -r kingge 1.4.6 who 查看登录用户信息1）基本语法 ​ （1）whoami （功能描述：显示自身用户名称） （2）who am i （功能描述：显示登录用户的用户名） （3）who （功能描述：看当前有哪些用户登录到了本台机器上） 2）案例 [root@hadoop101 opt]# whoami [root@hadoop101 opt]# who am i ​ [root@hadoop101 opt]# who 1.4.7 设置kingge普通用户具有root权限1）修改配置文件 修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) ALL或者配置成采用sudo命令时，不需要输入密码 Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) NOPASSWD:ALL修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。 2）案例 [kingge@hadoop101 opt]$ sudo mkdir module [root@hadoop101 opt]# chown kingge:kingge module/ 1.4.8 cat /etc/passwd 查看创建了哪些用户cat /etc/passwd 1.4.9 usermod修改用户1）基本语法： usermod -g 用户组 用户名 2）案例： 将用户kingge加入dev用户组 [root@hadoop101 opt]#usermod -g dev kingge 1.5 用户组管理命令每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同， 如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 1.5.1 groupadd 新增组1）基本语法 groupadd 组名 2）案例： ​ 添加一个kingge组 [root@hadoop101 opt]#groupadd kingge 1.5.2 groupdel删除组1）基本语法： groupdel 组名 2）案例 [root@hadoop101 opt]# groupdel kingge 1.5.3 groupmod修改组1）基本语法： groupmod -n 新组名 老组名 2）案例 ​ 修改kingge组名称为kingge1 [root@hadoop101 kingge]# groupmod -n kingge1 kingge 1.5.4 cat /etc/group 查看创建了哪些组cat /etc/group 1.5.5 综合案例[root@hadoop101 kingge]# groupadd dev [root@hadoop101 kingge]# groupmod -n device dev [root@hadoop101 kingge]# usermod -g device kingge [root@hadoop101 kingge]# su kingge [kingge@hadoop101 ~]$ mkdir kingge [kingge@hadoop101 ~]$ ls -l drwxr-xr-x. 2 kingge device 4096 5月 27 16:31 kingge [root@hadoop101 kingge]# usermod -g kingge kingge 1.6 文件权限类1.6.1 文件属性Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls –l命令来显示一个文件的属性以及文件所属的用户和组。 1）从左到右的10个字符表示： 如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示: （1）0首位表示类型 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等 - 代表文件 d 代表目录 c 字符流，装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置) s socket p 管道 l 链接文档(link file)； b 设备文件，装置文件里面的可供储存的接口设备(可随机存取装置) （2）第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User （3）第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group （4）第7-9位确定其他用户拥有该文件的权限 —Other 文件类型 属主权限 属组权限 其他用户权限 0 1 2 3 4 5 6 7 8 9 d R w x R - x R - x 目录文件 读 写 执行 读 写 执行 读 写 执行 2）rxw作用文件和目录的不同解释 （1）作用到文件： [ r ]代表可读(read): 可以读取，查看 [ w ]代表可写(write): 可以修改，但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件. [ x ]代表可执行(execute):可以被系统执行 （2）作用到目录： [ r ]代表可读(read): 可以读取，ls查看目录内容 [ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录 [ x ]代表可执行(execute):可以进入该目录 3）案例 [kingge@hadoop101 ~]$ ls -l 总用量 8 drwxrwxr-x. 2 kingge kingge 4096 5月 27 14:14 hello -rw-rw-r–. 1 kingge kingge 34 5月 27 14:20 test.txt （1）如果查看到是文件：链接数指的是硬链接个数。创建硬链接方法 ln [原文件] [目标文件] [root@hadoop101 xiyou]# ln sunhouzi/shz.txt ./shz.txt （2）如果查看的是文件夹：链接数指的是子文件夹个数。 [root@hadoop101 xiyou]# ls -al kingge/ 总用量 8 drwxr-xr-x. 2 root root 4096 9月 3 19:02 . drwxr-xr-x. 5 root root 4096 9月 3 21:21 .. 1.6.2 chmod改变权限1）基本语法： ​ chmod [{ugoa}{+-=}{rwx}] [文件或目录] [mode=421 ] [文件或目录] 2）功能描述 改变文件或者目录权限 文件: r-查看；w-修改；x-执行文件 目录: r-列出目录内容；w-在目录中创建和删除；x-进入目录 删除一个文件的前提条件:该文件所在的目录有写权限，你才能删除该文件。 3）案例 [root@hadoop101 test1]# chmod u+x test1.java [root@hadoop101 test1]# chmod g+x test1.java [root@hadoop101 test1]# chmod o+x test1.java [root@hadoop101 test1]# chmod 777 test1.java [root@hadoop101 test1]# chmod -R 777 testdir 1.6.3 chown改变所有者1）基本语法： chown [最终用户] [文件或目录] （功能描述：改变文件或者目录的所有者） 2）案例 [root@hadoop101 test1]# chown kingge test1.java [root@hadoop101 test1]# ls -al -rwxr-xr-x. 1 kingge kingge 551 5月 23 13:02 test1.java 修改前： [root@hadoop101 xiyou]# ll drwxrwxrwx. 2 root root 4096 9月 3 21:20 sunhouzi 修改后 [root@hadoop101 xiyou]# chown -R kingge:kingge sunhouzi/ [root@hadoop101 xiyou]# ll drwxrwxrwx. 2 kingge kingge 4096 9月 3 21:20 sunhouzi 1.6.4 chgrp改变所属组1）基本语法： ​ chgrp [最终用户组] [文件或目录] （功能描述：改变文件或者目录的所属组） 2）案例 [root@hadoop101 test1]# chgrp kingge test1.java [root@hadoop101 test1]# ls -al -rwxr-xr-x. 1 root kingge 551 5月 23 13:02 test1.java 1.6.5 su 切换用户1）基本语法： su -username （功能描述：切换用户） 2）案例 [root@hadoop101 kingge]# su kingge [kingge@hadoop101 ~]$ [kingge@hadoop101 ~]$ su root 密码： [root@hadoop101 kingge]# 1.7 磁盘分区类1.7.1 fdisk查看分区1）基本语法： ​ fdisk -l （功能描述：查看磁盘分区详情） ​ 注意：在root用户下才能使用 2）功能说明： ​ （1）Linux分区 这个硬盘是20G的，有255个磁面；63个扇区；2610个磁柱；每个 cylinder（磁柱）的容量是 8225280 bytes=8225.280 K（约为）=8.225280M（约为）； Device Boot Start End Blocks Id System 分区序列 引导 从X磁柱开始 到Y磁柱结束 容量 分区类型ID 分区类型 （2）Win7分区 3）案例 [root@hadoop101 /]# fdisk -l Disk /dev/sda: 21.5 GB, 21474836480 bytes 255 heads, 63 sectors/track, 2610 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x0005e654 Device Boot Start End Blocks Id System /dev/sda1 * 1 26 204800 83 Linux Partition 1 does not end on cylinder boundary. /dev/sda2 26 1332 10485760 83 Linux /dev/sda3 1332 1593 2097152 82 Linux swap / Solaris 1.7.2 df查看硬盘1）基本语法： ​ df 参数 （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况） 参数： -a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统； -k ：以 KBytes 的容量显示各文件系统； -m ：以 MBytes 的容量显示各文件系统； -h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示； -H ：以 M=1000K 取代 M=1024K 的进位方式； -T ：显示文件系统类型，连同该 partition 的 filesystem 名称 (例如 ext3) 也列出； -i ：不用硬盘容量，而以 inode 的数量来显示 2）案例 [root@hadoop101 ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda2 15G 3.5G 11G 26% / tmpfs 939M 224K 939M 1% /dev/shm /dev/sda1 190M 39M 142M 22% /boot 1.7.3 mount/umount挂载/卸载对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构 Linux中每个分区都是用来组成整个文件系统的一部分，她在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。 0**）挂载前准备（必须要有光盘或者已经连接镜像文件）** 1**）挂载光盘语法：** mount [-t vfstype] [-o options] device dir （1）-t vfstype 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。 常用类型有： 光盘或光盘镜像：iso9660 DOS fat16文件系统：msdos Windows 9x fat32文件系统：vfat Windows NT ntfs文件系统：ntfs Mount Windows文件网络共享：smbfs UNIX(LINUX) 文件网络共享：nfs （2）-o options 主要用来描述设备或档案的挂接方式。常用的参数有： loop：用来把一个文件当成硬盘分区挂接上系统 ro：采用只读方式挂接设备 rw：采用读写方式挂接设备 iocharset：指定访问文件系统所用字符集 （3）device 要挂接(mount)的设备 （4）dir设备在系统上的挂接点(mount point) 2**）案例** （1）光盘镜像文件的挂载 [root@hadoop101 ~]# mkdir /mnt/cdrom/ 建立挂载点 [root@hadoop101 ~]# mount -t iso9660 /dev/cdrom /mnt/cdrom/ 设备/dev/cdrom挂载到 挂载点 ： /mnt/cdrom中 [root@hadoop101 ~]# ll /mnt/cdrom/ 3**）卸载光盘语法：** [root@hadoop101 ~]# umount 设备文件名或挂载点 4**）案例** [root@hadoop101 ~]# umount /mnt/cdrom 5**）开机自动挂载语法：** [root@hadoop101 ~]# vi /etc/fstab 添加红框中内容，保存退出。 1.8 搜索查找类1.8.1 find 查找文件或者目录1）基本语法： ​ find [搜索范围] [匹配条件] 2）案例 （1）按文件名：根据名称查找/目录下的filename.txt文件。 [root@hadoop101 ~]# find /opt/ -name *.txt （2）按拥有者：查找/opt目录下，用户名称为-user的文件 [root@hadoop101 ~]# find /opt/ -user kingge ​ （3）按文件大小：在/home目录下查找大于200m的文件（+n 大于 -n小于 n等于） [root@hadoop101 ~]find /home -size +204800 1.8.2 grep 过滤查找及“|”管道符0）管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理 1）基本语法 grep+参数+查找内容+源文件 参数： -c：只输出匹配行的计数。 -I：不区分大小写(只适用于单字符)。 -h：查询多文件时不显示文件名。 -l：查询多文件时只输出包含匹配字符的文件名。 -n：显示匹配行及行号。 -s：不显示不存在或无匹配文本的错误信息。 -v：显示不包含匹配文本的所有行。 2）案例 [root@hadoop101 opt]# ls | grep -n test 4:test1 5:test2 1.8.3 which 文件搜索命令1）基本语法： ​ which 命令 （功能描述：搜索命令所在目录及别名信息） 2）案例 ​ [root@hadoop101 opt]# which ls ​ /bin/ls 1.9 进程线程类进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。 1.9.1 ps查看系统中所有进程1）基本语法： ​ ps -aux （功能描述：查看系统中所有进程） 2）功能说明 ​ USER：该进程是由哪个用户产生的 ​ PID：进程的ID号 %CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源； %MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源； VSZ：该进程占用虚拟内存的大小，单位KB； RSS：该进程占用实际物理内存的大小，单位KB； TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。 STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台 START：该进程的启动时间 TIME：该进程占用CPU的运算时间，注意不是系统时间 COMMAND：产生此进程的命令名 3）案例 ​ [root@hadoop101 datas]# ps -aux 1.9.2 top查看系统健康状态1）基本命令 ​ top [选项] ​ （1）选项： ​ -d 秒数：指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令： -i：使top不显示任何闲置或者僵死进程。 -p：通过指定监控进程ID来仅仅监控某个进程的状态。 ​ （2）操作选项： P： 以CPU使用率排序，默认就是此项 M： 以内存的使用率排序 N： 以PID排序 q： 退出top ​ （3）查询结果字段解释 第一行信息为任务队列信息 内容 说明 12:26:46 系统当前时间 up 1 day, 13:32 系统的运行时间，本机已经运行1天 13小时32分钟 2 users 当前登录了两个用户 load average: 0.00, 0.00, 0.00 系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。 第二行为进程信息 Tasks: 95 total 系统中的进程总数 1 running 正在运行的进程数 94 sleeping 睡眠的进程 0 stopped 正在停止的进程 0 zombie 僵尸进程。如果不是0，需要手工检 查僵尸进程 第三行为CPU信息 Cpu(s): 0.1%us 用户模式占用的CPU百分比 0.1%sy 系统模式占用的CPU百分比 0.0%ni 改变过优先级的用户进程占用的CPU百分比 99.7%id 空闲CPU的CPU百分比 0.1%wa 等待输入/输出的进程的占用CPU百分比 0.0%hi 硬中断请求服务占用的CPU百分比 0.1%si 软中断请求服务占用的CPU百分比 0.0%st st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。 第四行为物理内存信息 Mem: 625344k total 物理内存的总量，单位KB 571504k used 已经使用的物理内存数量 53840k free 空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了 65800k buffers 作为缓冲的内存数量 第五行为交换分区（swap）信息 Swap: 524280k total 交换分区（虚拟内存）的总大小 0k used 已经使用的交互分区的大小 524280k free 空闲交换分区的大小 409280k cached 作为缓存的交互分区的大小 2）案例 ​ [root@hadoop101 kingge]# top -d 1 [root@hadoop101 kingge]# top -i [root@hadoop101 kingge]# top -p 2575 执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。 1.9.3 pstree查看进程树1）基本语法： ​ pstree [选项] ​ 选项 -p： 显示进程的PID -u： 显示进程的所属用户 2）案例： ​ [root@hadoop101 datas]# pstree -u [root@hadoop101 datas]# pstree -p 1.9.4 kill终止进程1）基本语法： ​ kill -9 pid进程号 ​ 选项 -9 表示强迫进程立即停止 2）案例： ​ 启动mysql程序 ​ 切换到root用户执行 ​ [root@hadoop101 桌面]# kill -9 5102 1.9.5 netstat显示网络统计信息1）基本语法： ​ netstat -anp （功能描述：此命令用来显示整个系统目前的网络情况。例如目前的连接、数据包传递数据、或是路由表内容） ​ 选项： ​ -an 按一定顺序排列输出 ​ -p 表示显示哪个进程在调用 ​ -nltp 查看tcp协议进程端口号 2）案例 查看端口50070的使用情况 [root@hadoop101 hadoop-2.7.2]# netstat -anp | grep 50070 tcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 6816/java ​ 端口号 进程号 1.9.6 前后台进程切换1）基本语法： fg %1 （功能描述：把后台进程转换成前台进程） ctrl+z bg %1 （功能描述：把前台进程发到后台） 1.10 压缩和解压类1.10.1 gzip/gunzip压缩1）基本语法： gzip+文件 （功能描述：压缩文件，只能将文件压缩为*.gz文件） gunzip+文件.gz （功能描述：解压缩文件命令） 2）特点： （1）只能压缩文件不能压缩目录 （2）不保留原来的文件 3）案例 （1）gzip压缩 [root@hadoop101 opt]# ls test.java [root@hadoop101 opt]# gzip test.java [root@hadoop101 opt]# ls test.java.gz （2）gunzip解压缩文件 [root@hadoop101 opt]# gunzip test.java.gz [root@hadoop101 opt]# ls test.java 1.10.2 zip/unzip压缩1）基本语法： zip + 参数 + XXX.zip + 将要压缩的内容 （功能描述：压缩文件和目录的命令，window/linux通用且可以压缩目录且保留源文件） 参数： -r 压缩目录 2）案例： （1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip [root@hadoop101 opt]# zip test.zip test1.java test.java adding: test1.java (stored 0%) adding: test.java (stored 0%) [root@hadoop101 opt]# ls test1.java test.java test.zip （2）解压 mypackage.zip [root@hadoop101 opt]# unzip test.zip Archive: test.zip extracting: test1.java extracting: test.java ​ [root@hadoop101 opt]# ls test1.java test.java test.zip 1.10.3 tar打包1）基本语法： tar + 参数 + XXX.tar.gz + 将要打包进去的内容 （功能描述：打包目录，压缩后的文件格式.tar.gz） 参数： -c 产生.tar打包文件 -v 显示详细信息 -f 指定压缩后的文件名 -z 打包同时压缩 -x 解包.tar文件 2）案例 （1）压缩：tar -zcvf XXX.tar.gz n1.txt n2.txt ​ 压缩多个文件 [root@hadoop101 opt]# tar -zcvf test.tar.gz test1.java test.java test1.java test.java [root@hadoop101 opt]# ls test1.java test.java test.tar.gz 压缩目录 [root@hadoop101 opt]# tar -zcvf test.java.tar.gz test1 test1/ test1/hello test1/test1.java test1/test/ test1/test/test.java [root@hadoop106 opt]# ls test1 test.java.tar.gz （2）解压：tar -zxvf XXX.tar.gz ​ 解压到当前目录 [root@hadoop101 opt]# tar -zxvf test.tar.gz 解压到/opt目录 [root@hadoop101 opt]# tar -zxvf test.tar.gz -C /opt .11 后台服务管理类.11.1 service后台服务管理1）service network status 查看指定服务的状态 2）service network stop 停止指定服务 3）service network start 启动指定服务 4）service network restart 重启指定服务 5）service –status-all 查看系统中所有的后台服务 .11.2 chkconfig设置后台服务的自启配置1）chkconfig 查看所有服务器自启配置 2）chkconfig iptables off 关掉指定服务的自动启动 3）chkconfig iptables on 开启指定服务的自动启动 1.12 crond系统定时任务1.12.1 crond服务管理[root@hadoop101 ~]# service crond restart （重新启动服务） 1.12.2 crontab定时任务设置1）基本语法 crontab [选项] 选项： -e： 编辑crontab定时任务 -l： 查询crontab任务 -r： 删除当前用户所有的crontab任务 2）参数说明 ​ [root@hadoop101 ~]# crontab -e （1）进入crontab编辑界面。会打开vim编辑你的工作。 * 执行的任务 项目 含义 范围 第一个“*” 一小时当中的第几分钟 0-59 第二个“*” 一天当中的第几小时 0-23 第三个“*” 一个月当中的第几天 1-31 第四个“*” 一年当中的第几月 1-12 第五个“*” 一周当中的星期几 0-7（0和7都代表星期日） （2）特殊符号 特殊符号 含义 * 代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。 ， 代表不连续的时间。比如“0 8,12,16 * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 - 代表连续的时间范围。比如“0 5 1-6命令”，代表在周一到周六的凌晨5点0分执行命令 */n 代表每隔多久执行一次。比如“/10 * 命令”，代表每隔10分钟就执行一遍命令 （3）特定时间执行命令 时间 含义 45 22 * 命令 在22点45分执行命令 0 17 1 命令 每周1 的17点0分执行命令 0 5 1,15 命令 每月1号和15号的凌晨5点0分执行命令 40 4 1-5 命令 每周一到周五的凌晨4点40分执行命令 /10 4 命令 每天的凌晨4点，每隔10分钟执行一次命令 0 0 1,15 * 1 命令 每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。 3）案例： /5 * /bin/echo ”11” &gt;&gt; /tmp/test","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"linux基础","slug":"linux基础","date":"2017-06-12T01:17:03.000Z","updated":"2019-06-02T04:09:46.634Z","comments":true,"path":"2017/06/12/linux基础/","link":"","permalink":"http://kingge.top/2017/06/12/linux基础/","excerpt":"","text":"一 、Linux入门概述linux 系统也是接触了许久，不过一直没有机会总结一下，所以决定出个linux相关安装和配置以及常用指令的专栏。 1.1 概述​ Linux内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。 ​ Linux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。Linux能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 ​ 目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debain、Fedora、SuSE、OpenSUSE ​ 下面的操作，我使用的是Centos 1.2 下载地址centos下载地址： 网易镜像：http://mirrors.163.com/centos/6/isos/ 1.3 Linux特点 Linux里面一切皆是文件 Linux里面没有后缀名这一说 1.4 Linux和Windows区别目前国内Linux更多的是应用在服务器上，而桌面操作系统更多使用的是window。主要区别如下。 二 、VM安装相关（运行环境）2.1 安装VMWare虚拟机 详情这里就不说了，自行百度。 2.2 安装CentOS ​ 需要注意的是：下面的步骤，按需省略。 1 检查BIOS虚拟化支持 2 新建虚拟机 3 新建虚拟机向导 4 创建虚拟空白光盘 5 安装Linux系统对应的CentOS版 6 虚拟机命名和定位磁盘位置 7 处理器配置，看自己是否是双核、多核 8 设置内存为2GB 9 网络设置NAT 10 选择IO控制器类型 11 选择磁盘类型 12 新建虚拟磁盘 13 设置磁盘容量 14 你在哪里存储这个磁盘文件 15 新建虚拟机向导配置完成 16 VM设置 17 加载ISO 18 加电并安装配置CentOS 19 加电后初始化欢迎进入页面 回车选择第一个开始安装配置，此外，在Ctrl+Alt可以实现Windows主机和VM之间窗口的切换 20 是否对CD媒体进行测试，直接跳过**Skip** 21 CentOS欢迎页面，直接点击Next 22 选择简体中文进行安装 23 选择语言键盘 23 选择存储设备 24 给计算机起名 25 设置网络环境 安装成功后再设置。 26 选择时区 27 设置root密码 （一定记住） 28 硬盘分区-1 29 根分区新建 l Boot l swap分区设置 l 分区完成 30 程序引导，直接下一步 31 现在定制系统软件 32 Web环境 33 可扩展文件系统支持 34 基本系统 35 应用程序 36 开发、弹性存储、数据库、服务器 可以都不勾，有需要，以后使用中有需要再手动安装 37 桌面 除了KDE，其他都选就可以了。 38 语言支持 39 系统管理、虚拟化、负载平衡器、高可用性可以都不选 40 完成配置，开始安装CentOS 41 等待安装完成，等待等待等待等待……20分钟左右 42 安装完成，重新引导 43 欢迎引导页面 44 许可证 45 创建用户，可以先不创建，用root账户登录就行 46 时间和日期 47 Kdump,去掉 48 重启后用root登录 2.3 安装VMTools工具1）什么是VMtools VM tools顾名思义就是Vmware的一组工具。主要用于虚拟主机显示优化与调整，另外还可以方便虚拟主机与本机的交互，如允许共享文件夹，甚至可以直接从本机向虚拟主机拖放文件、鼠标无缝切换、显示分辨率调整等，十分实用。 安装过程自行百度 三 、Linux目录结构3.1 概览 3.2 树状目录结构 /bin：是Binary的缩写，这个目录存放着系统必备执行命令/boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件，自己的安装别放这里/dev：Device(设备)的缩写，该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。/etc：所有的系统管理所需要的配置文件和子目录。/home：存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。/lib：系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。/lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。/media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。/misc: 该目录可以用来存放杂项文件或目录，即那些用途或含义不明确的文件或目录可以存放在该目录下。/mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。/net 存放着和网络相关的一些文件./opt：这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。/proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。/root：该目录为系统管理员，也称作超级权限者的用户主目录。/sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。/selinux：这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙/srv：service缩写，该目录存放一些服务启动之后需要提取的数据。/sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。/tmp：这个目录是用来存放一些临时文件的。/usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。/var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 四 VI/VIM编辑器4.1 概述所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。但是目前我们使用比较多的是 vim 编辑器。 Vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 简单的来说vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。vim 则可以说是程序开发者的一项很好用的工具。连vim 的官方网站 (http://www.vim.org) 自己也说 vim 是一个程序开发工具而不是文字处理软件。 4.2 一般模式以 vi 打开一个档案就直接进入一般模式了(这是默认的模式)。在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、粘贴』来处理你的文件数据。 常用语法 1）yy （功能描述：复制光标当前一行） y数字y （功能描述：复制一段(从第几行到第几行)） 2）p （功能描述：箭头移动到目的行粘贴） 3）u （功能描述：撤销上一步） 4）dd （功能描述：删除光标当前行） d数字d （功能描述：删除光标(含)后多少行） 5）x （功能描述：删除一个字母，相当于del） X （功能描述：删除一个字母，相当于Backspace） 6）yw （功能描述：复制一个词） 7）dw （功能描述：删除一个词） 8）shift+^ （功能描述：移动到行头） 9）shift+$ （功能描述：移动到行尾） 10）1+shift+g （功能描述：移动到页头，数字） 11）shift+g （功能描述：移动到页尾） 12）数字N+shift+g （功能描述：移动到目标行） 4.3 编辑模式在一般模式中可以进行删除、复制、贴上等等的动作，但是却无法编辑文件内容的！ 要等到你按下『i, I, o, O, a, A, r, R』等任何一个字母之后才会进入编辑模式。 注意了！通常在 Linux 中，按下这些按键时，在画面的左下方会出现『INSERT 或 REPLACE 』的字样，此时才可以进行编辑。而如果要回到一般模式时， 则必须要按下『Esc』这个按键即可退出编辑模式。 常用语法 1）进入编辑模式 （1）i 当前光标前 （2）a 当前光标后 （3）o 当前光标行的下一行 2）退出编辑模式 按『Esc』键 4.4 指令模式在一般模式当中，输入『 : / ?』3个中的任何一个按钮，就可以将光标移动到最底下那一行。 在这个模式当中， 可以提供你『搜寻资料』的动作，而读取、存盘、大量取代字符、离开 vi 、显示行号等动作是在此模式中达成的！ 常用语法 1）基本语法 （1）: 选项 ​ 选项： w 保存 q 退出 ！ 感叹号强制执行 （2）/ 查找，/被查找词，n是查找下一个，shift+n是往上查找 （3）? 查找，?被查找词，n是查找上一个，shift+n是往下查找 2）案例 :wq! 强制保存退出 五 系统管理操作5.1 查看网络IP和网关1）查看虚拟网络编辑器 2）修改ip地址 3）查看网关 5.2 配置网络ip地址 0）查看当前ip基本语法： &gt; &gt; &gt; [root@hadoop101 /]# ifconfig&gt; &gt; 1）在终端命令窗口中输入（如果不是克隆的虚拟机可以跳过这一步）*******&gt; &gt; [root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules&gt; &gt; 进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址&gt; 2）修改IP地址 [root@hadoop101 /]#vim /etc/sysconfig/network-scripts/ifcfg-eth0需要修改的内容有5项：IPADDR=192.168.1.101GATEWAY=192.168.1.2ONBOOT=yesBOOTPROTO=staticDNS1=192.168.1.2 （1）修改前 ​ （2）修改后 ：wq 保存退出 3）执行service network restart 3）执行service network restart 4）如果报错，reboot，重启虚拟机 5.3 配置主机名0）查看主机名基本语法： [root@hadoop101 /]#hostname 1）修改linux的主机映射文件（hosts文件） （1）进入Linux系统查看本机的主机名。通过hostname命令查看[root@hadoop101 ~]# hostnamehadoop100（2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件[root@hadoop101 /]# vi /etc/sysconfig/network文件中内容NETWORKING=yesNETWORKING_IPV6=noHOSTNAME= hadoop101注意：主机名称不要有“_”下划线（3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。（4）保存退出。（5）打开/etc/hosts[root@hadoop101 /]# vim /etc/hosts添加如下内容192.168.1.101 hadoop101（6）并重启设备，重启后，查看主机名，已经修改成功 2）修改window7的主机映射文件（hosts文件）–方面在电脑使用域名进行访问hadoop相关的组件-例如hdfs，mapreduce等等。 ​ （1）进入C:\\Windows\\System32\\drivers\\etc路径 （2）打开hosts文件并添加如下内容192.168.1.101 hadoop101192.168.1.102 hadoop102192.168.1.103 hadoop103192.168.1.104 hadoop104192.168.1.105 hadoop105192.168.1.106 hadoop106192.168.1.107 hadoop107192.168.1.108 hadoop108 5.4 防火墙1）基本语法： service iptables status （功能描述：查看防火墙状态）chkconfig iptables -list （功能描述：查看防火墙开机启动状态）service iptables stop （功能描述：临时关闭防火墙）chkconfig iptables off （功能描述：关闭防火墙开机启动）chkconfig iptables on （功能描述：开启防火墙开机启动） 2）扩展 Linux系统有7个运行级别(runlevel)运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆运行级别2：多用户状态(没有NFS)运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式运行级别4：系统未使用，保留运行级别5：X11控制台，登陆后进入图形GUI模式运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 5.5 关机重启在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机 。 正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt 1）基本语法： ​ （1）sync （功能描述：将数据由内存同步到硬盘中）​ （2）shutdown [选项] 时间 ​ 选项：​ -h：关机​ -r：重启（3）halt （功能描述：关闭系统，等同于shutdown -h now 和 poweroff）（4）reboot （功能描述：就是重启，等同于 shutdown -r now） 2）案例 （1）将数据由内存同步到硬盘中[root@hadoop101 /]#sync （2）计算机将在10分钟后关机，并且会显示在登录用户的当前屏幕中[root@hadoop101 /]#shutdown -h 10 ‘This server will shutdown after 10 mins’（3）立马关机[root@hadoop101 /]# shutdown -h now （4）系统立马重启[root@hadoop101 /]# shutdown -r now（5）重启（等同于 shutdown -r now）[root@hadoop101 /]# reboot （6）关机（等同于shutdown -h now 和 poweroff）[root@hadoop101 /]#halt 注意：不管是重启系统还是关闭系统，首先要运行sync命令，把内存中的数据写到磁盘中。 5.6 找回root密码重新安装系统吗？当然不用！进入单用户模式更改一下root密码即可。 1）重启Linux，见到下图，在3秒钟之内按下回车 2）三秒之内要按一下回车，出现如下界面 3）按下e键就可以进入下图 4）移动到下一行，再次按e键 5）移动到下一行，进行修改 修改完成后回车键，然后按b键进行重新启动进入系统 6）移动到下一行，进行修改 最终修改完密码，reboot一下即可。 6.1 安装远程连接linux服务器工具Linux一般作为服务器使用，而服务器一般放在机房，你不可能在机房操作你的Linux服务器。这时我们就需要远程登录到Linux服务器来管理维护系统。 Linux系统中是通过SSH服务实现的远程登录功能，默认ssh服务端口号为 22。Window系统上 Linux 远程登录客户端有SecureCRT, Putty, SSH Secure Shell,XShell等 我这里安装的是 xshell。安装流程自行百度，比较简单。 # 好了到此，linux 相关的环境安装就已经结束了，linux相关的指令操作，可以参考下一章节。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"Mysql索引详解","slug":"Mysql索引详解","date":"2016-08-01T02:37:15.000Z","updated":"2017-08-17T09:35:34.792Z","comments":true,"path":"2016/08/01/Mysql索引详解/","link":"","permalink":"http://kingge.top/2016/08/01/Mysql索引详解/","excerpt":"","text":"前言 索引对查询的速度有着至关重要的影响，理解索引也是进行数据库性能调优的起点。考虑如下情况，假设数据库中一个表有10^6条记录，DBMS的页面大小为4K，并存储100条记录。如果没有索引，查询将对整个表进行扫描，最坏的情况下，如果所有数据页都不在内存，需要读取10^4个页面，如果这10^4个页面在磁盘上随机分布，需要进行10^4次I/O，假设磁盘每次I/O时间为10ms(忽略数据传输时间)，则总共需要100s(但实际上要好很多很多)。如果对之建立B-Tree索引，则只需要进行log100(10^6)=3次页面读取，最坏情况下耗时30ms。这就是索引带来的效果，很多时候，当你的应用程序进行SQL查询速度很慢时，应该想想是否可以建索引。进入正题： 有些硬啃的干货还是得了解的，下面先了解索引的基本知识 索引分类 单列索引 主键索引 唯一索引 普通索引 组合索引用到的表CREATE TABLE `award` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '用户id', `aty_id` varchar(100) NOT NULL DEFAULT '' COMMENT '活动场景id', `nickname` varchar(12) NOT NULL DEFAULT '' COMMENT '用户昵称', `is_awarded` tinyint(1) NOT NULL DEFAULT 0 COMMENT '用户是否领奖', `award_time` int(11) NOT NULL DEFAULT 0 COMMENT '领奖时间', `account` varchar(12) NOT NULL DEFAULT '' COMMENT '帐号', `password` char(32) NOT NULL DEFAULT '' COMMENT '密码', `message` varchar(255) NOT NULL DEFAULT '' COMMENT '获奖信息', `created_time` int(11) NOT NULL DEFAULT 0 COMMENT '创建时间', `updated_time` int(11) NOT NULL DEFAULT 0 COMMENT '更新时间', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='获奖信息表'; 单列索引普通索引 这个是最基本的索引 创建语法：其sql格式是： 第一种方式 : CREATE INDEX IndexName ON `TableName`(`字段名`(length)) 第二种方式 : ALTER TABLE TableName ADD INDEX IndexName(`字段名`(length)) 创建例子：第一种方式 : CREATE INDEX account_Index ON `award`(`account`);第二种方式: ALTER TABLE award ADD INDEX account_Index(`account`) 唯一索引 与普通索引类似,但是不同的是唯一索引要求所有的类的值是唯一的,这一点和主键索引一样.但是他允许有空值 创建语法：其sql格式是： 第一种方式 : CREATE UNIQUE INDEX IndexName ON `TableName`(`字段名`(length)); 第二种方式 : ALTER TABLE TableName ADD UNIQUE (column_list) 创建例子：CREATE UNIQUE INDEX account_UNIQUE_Index ON `award`(`account`); 主键索引 他与唯一索引的不同在于不允许有空值(在B+TREE中的InnoDB引擎中,主键索引起到了至关重要的地位) 创建语法：其sql格式是： 第一种方式 : CREATE UNIQUE INDEX IndexName ON `TableName`(`字段名`(length)); 第二种方式 : ALTER TABLE TableName ADD UNIQUE (column_list) 创建例子：CREATE UNIQUE INDEX account_UNIQUE_Index ON `award`(`account`); 单列索引的总结mysql&gt;SELECT ｀uid｀ FROM people WHERE lname｀='Liu' AND ｀fname｀='Zhiqun' AND ｀age｀=26因为我们不想扫描整表，故考虑用索引。单列索引：ALTER TABLE people ADD INDEX lname (lname);将lname列建索引，这样就把范围限制在lname='Liu'的结果集1上，之后扫描结果集1，产生满足fname='Zhiqun'的结果集2，再扫描结果集2，找到 age=26的结果集3，即最终结果。由 于建立了lname列的索引，与执行表的完全扫描相比，效率提高了很多，但我们要求扫描的记录数量仍旧远远超过了实际所需 要的。虽然我们可以删除lname列上的索引，再创建fname或者age 列的索引，但是，不论在哪个列上创建索引搜索效率仍旧相似。&gt; 所以就需要组合索引 组合索引 一个表中含有多个单列索引不代表是组合索引,通俗一点讲 组合索引是:包含多个字段但是只有索引名称 创建语法：其sql格式是： CREATE INDEX IndexName On `TableName`(`字段名`(length),`字段名`(length),...); 创建例子：CREATE INDEX nickname_account_createdTime_Index ON `award`(`nickname`, `account`, `created_time`); 如果你建立了 组合索引(nickname_account_createdTime_Index) 那么他实际包含的是3个索引 (nickname) (nickname,account)(nickname,account,created_time) 组合索引的最左前缀 上面的例子中给nickname,account,created_time 这三个字段建立索引他会去创建三个索引，但是在执行查询的时候只会用其中一个索引去查询，mysql会选择一个最严格(获得结果集记录数最少)的索引，所以where子句中使用最频繁的一列放在最左边。所谓最左前缀原则就是先要看第一列，在第一列满足的条件下再看左边第二列 全文索引 文本字段上(text)如果建立的是普通索引,那么只有对文本的字段内容前面的字符进行索引,其字符大小根据索引建立索引时申明的大小来规定.如果文本中出现多个一样的字符,而且需要查找的话,那么其条件只能是 where column lick &apos;%xxxx%&apos; 这样做会让索引失效.这个时候全文索引就祈祷了作用了ALTER TABLE tablename ADD FULLTEXT(column1, column2)有了全文索引，就可以用SELECT查询命令去检索那些包含着一个或多个给定单词的数据记录了。ELECT * FROM tablenameWHERE MATCH(column1, column2) AGAINST(‘xxx′, ‘sss′, ‘ddd′)这条命令将把column1和column2字段里有xxx、sss和ddd的数据记录全部查询出来。 总结使用索引的优点 可以通过建立唯一索引或者主键索引,保证数据库表中每一行数据的唯一性. 建立索引可以大大提高检索的数据,以及减少表的检索行数 在表连接的连接条件 可以加速表与表直接的相连 在分组和排序字句进行数据检索,可以减少查询时间中 分组 和 排序时所消耗的时间(数据库的记录会重新排序) 建立索引,在查询中使用索引 可以提高性能 使用索引的缺点 在创建索引和维护索引 会耗费时间,随着数据量的增加而增加 索引文件会占用物理空间,除了数据表需要占用物理空间之外,每一个索引还会占用一定的物理空间 当对表的数据进行 INSERT,UPDATE,DELETE 的时候,索引也要动态的维护,这样就会降低数据的维护速度,(建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快)。 使用索引需要注意的地方 在经常需要搜索的列上,可以加快索引的速度 主键列上可以确保列的唯一性 在表与表的而连接条件上加上索引,可以加快连接查询的速度 在经常需要排序(order by),分组(group by)和的distinct 列上加索引 可以加快排序查询的时间, (单独order by 用不了索引，索引考虑加where 或加limit) 在一些where 之后的 &lt; &lt;= &gt; &gt;= BETWEEN IN 以及某个情况下的like 建立字段的索引(B-TREE) like语句的 如果你对nickname字段建立了一个索引.当查询的时候的语句是 nickname lick ‘%ABC%’ 那么这个索引讲不会起到作用.而nickname lick ‘ABC%’ 那么将可以用到索引 索引不会包含NULL列,如果列中包含NULL值都将不会被包含在索引中,复合索引中如果有一列含有NULL值那么这个组合索引都将失效,一般需要给默认值0或者 ‘ ‘字符串 使用短索引,如果你的一个字段是Char(32)或者int(32),在创建索引的时候指定前缀长度 比如前10个字符 (前提是多数值是唯一的..)那么短索引可以提高查询速度,并且可以减少磁盘的空间,也可以减少I/0操作. 不要在列上进行运算,这样会使得mysql索引失效,也会进行全表扫描 选择越小的数据类型越好,因为通常越小的数据类型通常在磁盘,内存,cpu,缓存中 占用的空间很少,处理起来更快 什么情况下不建立索引 查询中很少使用到的列 不应该创建索引,如果建立了索引然而还会降低mysql的性能和增大了空间需求. 很少数据的列也不应该建立索引,比如 一个性别字段 0或者1,在查询中,结果集的数据占了表中数据行的比例比较大,mysql需要扫描的行数很多,增加索引,并不能提高效率 定义为text和image和bit数据类型的列不应该增加索引 当表的修改(UPDATE,INSERT,DELETE)操作远远大于检索(SELECT)操作时不应该创建索引,这两个操作是互斥的关系 好的文章转：SQL优化转：MySQL索引原理及慢查询优化","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"索引","slug":"索引","permalink":"http://kingge.top/tags/索引/"}]},{"title":"C++文件流操作的读与写","slug":"C-文件流操作的读与写","date":"2014-11-08T13:04:00.000Z","updated":"2017-08-17T06:33:56.710Z","comments":true,"path":"2014/11/08/C-文件流操作的读与写/","link":"","permalink":"http://kingge.top/2014/11/08/C-文件流操作的读与写/","excerpt":"","text":"对文件的写入put和&lt;&lt; 写入方式 put的操作：是对文件进行写入的操作，写入一个字符（可以使字母也可以是asci码值） file.put(' A');file.put('\\n');file &lt;&lt; \"xiezejing1994\"; 输出： &nbsp;&nbsp;&nbsp;&nbsp;A// 注意到A这里有几个空格 但是不影响左对齐xiezejing1994// 也就是说A的前面不会有空格 ##操作和&lt;&lt; 读写方式区别 put操作和 file &lt;&lt;‘A’这个基本上是一样的，但是有个区别就是他不可以这样file &lt;&lt;’ A’;（A的前面有空格）因为他是格式化输入 所以中间不能有”空格“但是这样file &lt;&lt;”‘ A”;（也就是以字符串的格式输入则会有空格） 文件的读操作1.getline（） getline（ cin ，string类型 ） getline( cin, z ); file1 &lt;&lt; z; （file1 为文件流对象） 例子： char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ); cout &lt;&lt; c; &#125; 假设文件1.txt内有' A xiezejing1994 这样文本它的输出：' Axiezejing1994 也就是说他没有读到换行的功能 不会输出' A xiezejing1994（原因就是getlibe其实里面有三个参数，第三个参数默认为'\\n'） 2.getline（ fstream，string ）while ( getline( file,z ) )&#123; cout &lt;&lt; z;&#125; 3.get（） char c[100]; while ( !file.eof() ) &#123; //file.getline( c,100 ,'\\0'); file.get( c,100 ,'\\0'); cout &lt;&lt; c; &#125;输出同getline一样----必须要写三个参数 否则只会输出一行（第三个参数为'\\n'也是只会输出一行）。非常严格的输出。 4.get操作 char c; file.get(c); while ( !file.eof() ) &#123; cout &lt;&lt; c; file.get(c); &#125;-----和getline的区别在于 他是读取单个字符的，所以会读取到结束符号故会输出' Axiezejing1994 对文件是否读到末尾的判断1.feof（） 该函数只有“已经读取了”结束标志时 feof（）才会返回非零值 也就是说当文件读取到文件结束标志位时他的返回值不是非零还是零 故还要在进行一次读. 例子 假设在1.txt中只有abc三个字符在进行 while（！feof(fp)） &#123; ch = getc(fp); putchar(ch); &#125;//实际上输出的是四个字符改为ch = getc（fp）；while （ ！feof（fp））&#123; putchar（ch）； ch = getc（fp）；&#125;// 这样就可以正常运行3. 可以不调用函数eof 直接就是 while （ file ） // file 就是文件流的对象&#123; 。。。。操作&#125;4.char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ,'\\0'); cout &lt;&lt; c; &#125;这个 和char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ,'\\n'); cout &lt;&lt; c; &#125;假设文本为上面的。输出分别为' A xiezejing1994' Axiezejing1994 读写1.read( 数组名，接收的个数 )2.write( 数组名，gcount函数 )#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main()&#123; ifstream file( \"D:\\\\jjj.txt\"); ofstream file1( \"D:\\\\j.txt\" , ios::app); string z; if ( !file ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125; char c[100]; while ( !file.eof() ) &#123; file.read( c,100 ); file1.write( c, file.gcount() ); &#125; file.close(); file.close(); return 0;&#125; **判断打开是否正确** 1. if( !file )2.if ( !file.good() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;3. if ( !file.is_open() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;4. if ( file.fail() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;","categories":[{"name":"c++","slug":"c","permalink":"http://kingge.top/categories/c/"}],"tags":[{"name":"文件","slug":"文件","permalink":"http://kingge.top/tags/文件/"},{"name":"C++","slug":"C","permalink":"http://kingge.top/tags/C/"},{"name":"文件读写","slug":"文件读写","permalink":"http://kingge.top/tags/文件读写/"}]}]}