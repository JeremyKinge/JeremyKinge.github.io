{"meta":{"title":"King哥","subtitle":"To know everything, no words don't talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）","description":"To know everything, no words don't talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）","author":"Jeremy Kinge","url":"http://kingge.top"},"pages":[{"title":"","date":"2017-08-14T09:28:56.000Z","updated":"2017-08-17T10:01:05.524Z","comments":true,"path":"about/index.html","permalink":"http://kingge.top/about/index.html","excerpt":"","text":"这个人真的很吊，什么都没留下，但是你不得不承认这个人他真的很吊啊。"},{"title":"分类","date":"2017-08-14T08:51:40.000Z","updated":"2017-08-14T08:52:32.618Z","comments":true,"path":"categories/index.html","permalink":"http://kingge.top/categories/index.html","excerpt":"","text":""},{"title":"","date":"2017-08-14T09:29:06.000Z","updated":"2020-05-04T12:12:00.671Z","comments":true,"path":"picture/index.html","permalink":"http://kingge.top/picture/index.html","excerpt":"","text":"该板块尚且还没有开发的心思，no pic"},{"title":"标签","date":"2014-12-22T04:39:04.000Z","updated":"2017-08-14T09:29:28.873Z","comments":true,"path":"tags/index.html","permalink":"http://kingge.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"hbase总结","date":"2020-05-10T04:04:22.318Z","updated":"2020-05-10T04:03:50.347Z","comments":true,"path":"2020/05/10/hbase总结/","link":"","permalink":"http://kingge.top/2020/05/10/hbase总结/","excerpt":"","text":"layout: wtitle: hbase总结date: 2018-07-02 22:22:10 categories: hadoop #分类tags: [hbase,数据结构化] 一、HBaes介绍1.1、HBase的起源HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储（非结构化数据也可存储-数据挖掘）。 官方网站：http://hbase.apache.org – 2006年Google发表BigTable白皮书。 – 2006年开始开发HBase。 – 2008年北京成功开奥运会，程序员默默地将HBase弄成了Hadoop的子项目。 – 2010年HBase成为Apache顶级项目。 – 现在很多公司二次开发出了很多发行版本，你也开始使用了。 1.2、HBase的角色1.2.1、HMaster功能： 1) 监控RegionServer 2) 处理RegionServer故障转移 3) 处理元数据的变更 4) 处理region的分配或移除 5) 在空闲时间进行数据的负载均衡 6) 通过Zookeeper发布自己的位置给客户端 1.2.2、RegionServer功能： 1) 负责存储HBase的实际数据 2) 处理分配给它的Region 3) 刷新缓存到HDFS 4) 维护HLog（保存数据本身和对数据的操作） 5) 执行压缩 6) 负责处理Region分片 组件： 1) Write-Ahead logs HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 2) HFile 这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。 3) Store HFile存储在Store中，一个Store对应HBase表中的一个列族。 4) MemStore 顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。 5) Region Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。 1.3、HBase的架构HBase一种是作为存储的分布式文件系统，另一种是作为数据处理模型的MR框架。因为日常开发人员比较熟练的是结构化的数据进行处理，但是在HDFS直接存储的文件往往不具有结构化，所以催生出了HBase在HDFS上的操作。如果需要查询数据，只需要通过键值便可以成功访问。 架构图如下图所示： HBase内置有Zookeeper，但一般我们会有其他的Zookeeper集群来监管master和regionserver，Zookeeper通过选举，保证任何时候，集群中只有一个活跃的HMaster，HMaster与HRegionServer 启动时会向ZooKeeper注册，存储所有HRegion的寻址入口，实时监控HRegionserver的上线和下线信息。并实时通知给HMaster，存储HBase的schema和table元数据，默认情况下，HBase 管理ZooKeeper 实例，Zookeeper的引入使得HMaster不再是单点故障。一般情况下会启动两个HMaster，非Active的HMaster会定期的和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个HMaster反而增加了Active HMaster的负担。 一个RegionServer可以包含多个HRegion，每个RegionServer维护一个HLog，和多个HFiles以及其对应的MemStore。RegionServer运行于DataNode上，数量可以与DatNode数量一致，请参考如下架构图： 二、HBase部署与使用2.1、部署2.1.1、Zookeeper正常部署首先保证Zookeeper集群的正常部署，并启动之： $ ~/modules/zookeeper-3.4.5/bin/zkServer.sh start 2.1.2、Hadoop正常部署Hadoop集群的正常部署并启动： $ ~/modules/hadoop-2.7.2/sbin/start-dfs.sh $ ~/modules/hadoop-2.7.2/sbin/start-yarn.sh 2.1.3、HBase的解压解压HBase到指定目录： $ tar -zxf ~/softwares/installations/hbase-1.3.1-bin.tar.gz -C ~/modules/ 2.1.4、HBase的配置文件需要修改HBase对应的配置文件。 hbase-env.sh**修改内容：** export JAVA_HOME=/home/admin/modules/jdk1.8.0_121 export HBASE_MANAGES_ZK=false //是否使用hbase内嵌的zookeeper hbase-site.xml**修改内容：** &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop101:9000/hbase&lt;/value&gt; &lt;/property&gt;//这个地址要跟hadoop 的 core-site.xml 的namenode地址一致&lt;!—是否开启分布式--&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 和reginserver通信的端口 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop101:2181, hadoop102:2181, hadoop103:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt; /opt/module/zookeeper-3.4.5/zkData&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers： linux01 linux02 linux03 2.1.5、HBase需要依赖的Jar包由于HBase需要依赖Hadoop，所以替换HBase的lib目录下的jar包，以解决兼容问题： 1) 删除原有的jar： $ rm -rf /home/admin/modules/hbase-1.3.1/lib/hadoop-* $ rm -rf /home/admin/modules/hbase-1.3.1/lib/zookeeper-3.4.6.jar 2) 拷贝新jar，涉及的jar有： hadoop-annotations-2.7.2.jarhadoop-auth-2.7.2.jarhadoop-client-2.7.2.jarhadoop-common-2.7.2.jarhadoop-hdfs-2.7.2.jarhadoop-mapreduce-client-app-2.7.2.jarhadoop-mapreduce-client-common-2.7.2.jarhadoop-mapreduce-client-core-2.7.2.jarhadoop-mapreduce-client-hs-2.7.2.jarhadoop-mapreduce-client-hs-plugins-2.7.2.jarhadoop-mapreduce-client-jobclient-2.7.2.jarhadoop-mapreduce-client-jobclient-2.7.2-tests.jarhadoop-mapreduce-client-shuffle-2.7.2.jarhadoop-yarn-api-2.7.2.jarhadoop-yarn-applications-distributedshell-2.7.2.jarhadoop-yarn-applications-unmanaged-am-launcher-2.7.2.jarhadoop-yarn-client-2.7.2.jarhadoop-yarn-common-2.7.2.jarhadoop-yarn-server-applicationhistoryservice-2.7.2.jarhadoop-yarn-server-common-2.7.2.jarhadoop-yarn-server-nodemanager-2.7.2.jarhadoop-yarn-server-resourcemanager-2.7.2.jarhadoop-yarn-server-tests-2.7.2.jarhadoop-yarn-server-web-proxy-2.7.2.jarzookeeper-3.4.5.jar 尖叫提示：这些jar包的对应版本应替换成你目前使用的hadoop版本，具体情况具体分析。 查找jar包举例： $ find /home/admin/modules/hadoop-2.7.2/ -name hadoop-annotations* 然后将找到的jar包复制到HBase的lib目录下即可。 2.1.6、HBase软连接Hadoop配置 $ ln -s ~/modules/hadoop-2.7.2/etc/hadoop/core-site.xml ~/modules/hbase-1.3.1/conf/core-site.xml $ ln -s ~/modules/hadoop-2.7.2/etc/hadoop/hdfs-site.xml ~/modules/hbase-1.3.1/conf/hdfs-site.xml 2.1.7、HBase远程scp到其他集群 $ scp -r /home/admin/modules/hbase-1.3.1/ linux02:/home/admin/modules/ $ scp -r /home/admin/modules/hbase-1.3.1/ linux03:/home/admin/modules/ 2.1.8、HBase服务的启动启动方式1**：** $ bin/hbase-daemon.sh start master $ bin/hbase-daemon.sh start regionserver$ bin/hbase-daemon.sh stop regionserver 尖叫提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。 修复提示： a、同步时间服务 请参看帮助文档：《大数据帮助文档》 b、属性：hbase.master.maxclockskew设置更大的值 &lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;Time difference of regionserver from master&lt;/description&gt; &lt;/property&gt; 启动方式2*： $ bin/start-hbase.sh 对应的停止服务： $ bin/stop-hbase.sh 尖叫提示：如果使用的是JDK8以上版本，则应在hbase-evn.sh中移除“HBASE_MASTER_OPTS”和“HBASE_REGIONSERVER_OPTS”配置。 2.1.9、查看Hbse页面启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如： http://hadoop101:16010 2.2、简单使用2.2.1、基本操作1) 进入HBase**客户端命令行** $ bin/hbase shell 2) 查看帮助命令 hbase(main)&gt; help 3) 查看当前数据库中有哪些表 hbase(main)&gt; list 2.2.2、表的操作1) 创建表（同时创建列镞info**）** hbase(main)&gt; create ‘student’,’info’ 2) 插入数据到表（列镞中的列可以动态生成-*info**这个列镞必须是已经存在的）-student类似一个map集合 hbase(main) &gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:name&apos;,&apos;Thomas&apos;hbase(main) &gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:sex&apos;,&apos;male&apos;hbase(main) &gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:age&apos;,&apos;18&apos;hbase(main) &gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:name&apos;,&apos;Janna&apos;hbase(main) &gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:sex&apos;,&apos;female&apos;hbase(main) &gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:age&apos;,&apos;20&apos; Put相同的键值，会覆盖列的值，跟map的性质一样。 例如：put ‘student’,’1001’,’info:name’,’kingge’ –那么这一列的值就会被覆盖为kingge. 这个时候产生一个问题，原先的Thomas***就消失了嘛？其实不是，之前的值还是存在的。通过时间戳来区分***。（version 版本） 3) 扫描查看表数据 hbase(main) &gt; scan &apos;student&apos;hbase(main) &gt; scan &apos;student&apos;,&#123;STARTROW =&gt; &apos;1001&apos;, STOPROW =&gt; &apos;1001&apos;&#125;hbase(main) &gt; scan &apos;student&apos;,&#123;STARTROW =&gt; &apos;1001&apos;&#125; 时间戳是在插入数据时，默认添加的，目的是为了保存冗余的数据。 注意在hbase中比较大小时按位比较。那么比较大小时STARTROW，根据***ascall**码按位比较***。例如存在行的rowkey有abc、aaa、abcd、e、ac。那么STARTROW =&gt; ‘ac’.首先比较第一个字母a，这五个rowkey都满足，再比较第二个字母c，最终只有e、ac满足。故输出这两行的值 4) 查看表结构 hbase(main):012:0&gt; describe ‘student’ 5) 更新指定字段的数据（原数据依旧存在，跟java**的map**有所不同） hbase(main) &gt; put ‘student’,’1001’,’info:name’,’Nick’ hbase(main) &gt; put ‘student’,’1001’,’info:age’,’100’ 6) 查看“指定行”或“指定列族:**列”的数据** hbase(main) &gt; get ‘student’,’1001’ hbase(main) &gt; get ‘student’,’1001’,’info:name’ 7) 删除数据 删除某rowkey**的全部数据：** hbase(main) &gt; deleteall ‘student’,’1001’ 删除某rowkey**的某一列数据：** hbase(main) &gt; delete ‘student’,’1002’,’info:sex’ 8) 清空表数据 hbase(main) &gt; truncate ‘student’ 尖叫提示：清空表的操作顺序为先disable，然后再truncating。 9) 删除表 首先需要先让该表为disable**状态：** hbase(main) &gt; disable ‘student’ 然后才能drop**这个表：** hbase(main) &gt; drop ‘student’ 尖叫提示：如果直接drop表，会报错：Drop the named table. Table must first be disabled ERROR: Table student is enabled. Disable it first. 10) 统计表数据行数（按照rowkey**统计）** hbase(main) &gt; count ‘student’ 11) 变更表信息 将info列族中的数据存放3个版本：（默认只保留一个版本） hbase(main) &gt; alter ‘student’,{NAME=&gt;’info’,VERSIONS=&gt;3} 2.2.3 hbase的表结构 每个表必须有一个唯一的rowkey，已经时间戳（ts）,info是一个列镞，也就是说info里面可能包含多个列（图中就包含name和sex两个列- nick和male是这两个列的值）。 那么我们需要给name列赋值，就需要通过一系列属性定位- 2.3、读写流程2.3.0 Region的寻址 2.3.1、HBase读数据流程1) HRegionServer保存着.META.的这样一张表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取-ROOT-表所在位置，进而找到.META.表所在的位置信息，即找到这个.META.表在哪个HRegionServer上保存着。 2) 接着Client通过刚才获取到的HRegionServer的IP来访问.META.表所在的HRegionServer，从而读取到.META.，进而获取到.META.表中存放的元数据。 3) Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。 4) 最后HRegionServer把查询到的数据响应给Client。-详细的请看视频，涉及到memorystore（存储写入的数据，内存存储）和blockcache（存储读取的数据。内存存储）的hfile（物理逻辑上的输出-在hdfs中）的读取。 2.3.2、HBase写数据流程1) Client也是先访问zookeeper，找到-ROOT-表，进而找到.META.表，并获取.META.表信息。 2) 确定当前将要写入的数据所对应的RegionServer服务器和Region。 3) Client向该RegionServer服务器发起写入数据请求，然后RegionServer收到请求并响应。 4) Client先把数据写入到HLog，以防止数据丢失。 5) 然后将数据写入到Memstore。 6) 如果Hlog和Memstore均写入成功，则这条数据写入成功。在此过程中，如果Memstore达到阈值，会把Memstore中的数据flush到StoreFile中。(溢出后，会重新创建一块memostore存储接下来需要写入的数据。当前溢出的memostore会把数据打包放入到一个队列中，等到flush，也即是说一个memstore溢出后写入，对应着一个storefile，一对多的关系) 7) 当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile（避免datanode出现大量小文件）。当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。 尖叫提示：因为内存空间是有限的，所以说溢写过程必定伴随着大量的小文件产生。 从上图可以看出氛围3步骤： 第1步：Client获取数据写入的Region所在的RegionServer 第2步：请求写Hlog 第3步：请求写MemStore 只有当写Hlog和写MemStore都成功了才算请求写入完成。MemStore后续会逐渐刷到HDFS中。 备注：Hlog存储在HDFS，当RegionServer出现异常，需要使用Hlog来恢复数据。 重申强调上述涉及到的3个机制： \\ Flush**机制：** 当MemStore达到阈值，将Memstore中的数据Flush进Storefile 涉及属性： hbase.hregion.memstore.flush.size**：134217728** 即：128M就是Memstore的默认阈值 hbase.regionserver.global.memstore.upperLimit**：0.4** 即： 这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。 RegionServer的flush是通过将请求添加一个队列，模拟生产消费模式来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请 求时，可能会导致内存陡增，最坏的情况是触发OOM。 hbase.regionserver.global.memstore.lowerLimit**：0.38** 即： 当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个 MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于 hbase.regionserver.global.memstore.lowerLimit。 \\ Compact**机制：** 把小的Memstore文件合并成大的Storefile文件。 \\ Split**机制** 当Region达到阈值，会把过大的Region一分为二。 2.4、JavaAPI2.4.1、安装Maven并配置环境变量 $ tar -zxf ~/softwares/installations/apache-maven-3.5.0-bin.tar.gz -C ~/modules/ 在环境变量中添加： MAVEN_HOME=/home/admin/modules/apache-maven-3.5.0 export PATH=$PATH:$MAVEN_HOME/bin 2.4.2、新建Maven Project 需要三个配置文件信息：core-site.xml**、hdfs-site.xml**、hbase-site.xml 新建项目后在pom.xml中添加依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;----下面这个可以不导入&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt; 2.4.3、编写HBaseAPI注意，这部分的学习内容，我们先学习使用老版本的API，接着再写出新版本的API调用方式。因为在企业中，有些时候我们需要一些过时的API来提供更好的兼容性。 1) 首先需要获取Configuration**对象：** public static Configuration conf;static&#123; //使用HBaseConfiguration的单例方法实例化 conf = HBaseConfiguration.create();conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.216.20&quot;);conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);&#125; 2) 判断表是否存在： public static boolean isTableExist(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; //在HBase中管理、访问表需要先创建HBaseAdmin对象//Connection connection = ConnectionFactory.createConnection(conf);//HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();-新api HBaseAdmin admin = new HBaseAdmin(conf);-老api return admin.tableExists(tableName);&#125; 3) 创建表 public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; HBaseAdmin admin = new HBaseAdmin(conf); //判断表是否存在 if(isTableExist(tableName))&#123; System.out.println(&quot;表&quot; + tableName + &quot;已存在&quot;); //System.exit(0); &#125;else&#123; //创建表属性对象,表名需要转字节 HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName)); //创建多个列族 for(String cf : columnFamily)&#123; descriptor.addFamily(new HColumnDescriptor(cf)); &#125; //根据对表的配置，创建表 admin.createTable(descriptor); System.out.println(&quot;表&quot; + tableName + &quot;创建成功！&quot;); &#125;&#125; 4) 删除表 public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; HBaseAdmin admin = new HBaseAdmin(conf); if(isTableExist(tableName))&#123; admin.disableTable(tableName); admin.deleteTable(tableName); System.out.println(&quot;表&quot; + tableName + &quot;删除成功！&quot;); &#125;else&#123; System.out.println(&quot;表&quot; + tableName + &quot;不存在！&quot;); &#125;&#125; 5) 向表中插入数据 public static void addRowData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException&#123; //创建HTable对象 HTable hTable = new HTable(conf, tableName); //向表中插入数据 Put put = new Put(Bytes.toBytes(rowKey)); //向Put对象中组装数据 put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); hTable.put(put); hTable.close(); System.out.println(&quot;插入数据成功&quot;);&#125; 6) 删除多行数据 public static void deleteMultiRow(String tableName, String... rows) throws IOException&#123; HTable hTable = new HTable(conf, tableName); List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); for(String row : rows)&#123; Delete delete = new Delete(Bytes.toBytes(row)); deleteList.add(delete); &#125; hTable.delete(deleteList); hTable.close();&#125; 7) 得到所有数据 public static void getAllRows(String tableName) throws IOException&#123; HTable hTable = new HTable(conf, tableName); //得到用于扫描region的对象 Scan scan = new Scan(); //使用HTable得到resultcanner实现类的对象 ResultScanner resultScanner = hTable.getScanner(scan); for(Result result : resultScanner)&#123; Cell[] cells = result.rawCells(); for(Cell cell : cells)&#123; //得到rowkey System.out.println(&quot;行键:&quot; + Bytes.toString(CellUtil.cloneRow(cell))); //得到列族 System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125;&#125; 8) 得到某一行所有数据 public static void getRow(String tableName, String rowKey) throws IOException&#123; HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); //get.setMaxVersions();显示所有版本 //get.setTimeStamp();显示指定时间戳的版本 Result result = table.get(get); for(Cell cell : result.rawCells())&#123; System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow())); System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); System.out.println(&quot;时间戳:&quot; + cell.getTimestamp()); &#125;&#125; 9) 获取某一行指定“列族:**列”的数据** public static void getRowQualifier(String tableName, String rowKey, String family, String qualifier) throws IOException&#123; HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier)); Result result = table.get(get); for(Cell cell : result.rawCells())&#123; System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow())); System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell))); &#125;&#125; 2.5、MapReduce通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。 2.5.1、官方HBase-MapReduce1) 查看HBase**的MapReduce任务的所需的依赖-也就是说当前hbase想要执行mapreduce所需要的依赖** $ bin/hbase mapredcp 2) 执行环境变量的导入（*注意此次环境变量只会在当前会话框有效，临时的*） $ export HBASE_HOME= /opt/module/hbase-1.3.1/ $ export HADOOP_CLASSPATH=${HBASE_HOME}/bin/hbase mapredcp 2.2) 执行环境变量的导入（永久-**在hadoop启动时自动加载jar包到环境中）** \\1. 打开hadoop的 hadoop-env.sh 写入命令脚本（如果配置了hbase_home那么可以不书写hbase的全路径，用${HBASE_HOME} 代替即可） 必须放在上面for循环下面，避免被覆盖，同时也要保留for循环已经为hadoop_classpath赋的值。（为了避免被覆盖推荐放到***hadoop-env.sh**的最末尾***） export HADOOP_CLASSPATH=/opt/module/hbase-1.3.1/bin/hbase mapredcp \\3. hadoop-env.sh分发到hadoop集群 3) 运行官方的MapReduce**任务** – 案例一：统计Student**表中有多少行数据** $ ~/modules/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar rowcounter student – 案例二：使用MapReduce**将本地数据导入到HBase** (1) 在本地创建一个tsv**格式的文件：fruit.tsv** 1001 Apple Red 1002 Pear Yellow 1003 Pineapple Yellow 尖叫提示：上面的这个数据不要从word中直接复制，有格式错误 (2) 创建HBase**表** hbase(main):001:0&gt; create ‘fruit’,’info’ (3) 在HDFS**中创建input_fruit文件夹并上传fruit.tsv文件** $ ~/modules/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/ $ ~/modules/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/ (4) 执行MapReduce**到HBase的fruit表中（*fruit**表必须已经存在，否则报错***） $ ~/modules/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar importtsv \\ -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \\ hdfs://linux01:8020/input_fruit (5) 使用scan**命令查看导入后的结果** hbase(main):001:0&gt; scan ‘fruit’ –***需要注意，从mapreduce**导入数据到hbase**，一般都是结构化的数据，因为非结构化数据导入，你无法确定某一列的数据究竟是对应的是hbase**表那个列镞的列。*** 2.5.2、自定义HBase-MapReduce1（hbase导入到hbase）目标：将fruit表中的一部分数据，通过MR迁入到fruit_mr表中。 分步实现： 1) 构建ReadFruitMapper**类，用于读取fruit**表中的数据 package com.z.hbase_mr;import java.io.IOException;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.util.Bytes;// ImmutableBytesWritable 相当于rowkey、 Put rowkey对应一行的数据public class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; //将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。 Put put = new Put(key.get()); //遍历添加column行 for(Cell cell: value.rawCells())&#123; //添加/克隆列族:info if(&quot;info&quot;.equals(Bytes.toString(CellUtil.cloneFamily(cell))))&#123; //添加/克隆列：name if(&quot;name&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; //将该列cell加入到put对象中 put.add(cell); //添加/克隆列:color &#125;else if(&quot;color&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; //向该列cell加入到put对象中 put.add(cell); &#125; &#125; &#125; //将从fruit读取到的每行数据写入到context中作为map的输出 context.write(key, put); &#125;&#125; 2) 构建WriteFruitMRReducer**类，用于将读取到的fruit表中的数据写入到fruit_mr表中** package com.z.hbase_mr;import java.io.IOException;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; //读出来的每一行数据写入到fruit_mr表中 for(Put put: values)&#123; context.write(NullWritable.get(), put); &#125; &#125;&#125; 3) 构建Fruit2FruitMRRunner extends Configured implements Tool**用于组装运行Job**任务 private Configuration conf = null; public void setConf(Configuration conf) &#123;//需要注意的是这里获取的conf是hadoop//生成的配置文件，我们这里运行的是hbase，那么需要覆盖为hbase的环境变量。 this.conf = new HBaseConfiguration(conf); &#125;// HBaseConfiguration实际上是hadoop的configuration的重载 public Configuration getConf() &#123; return this.conf; &#125;//组装Job public int run(String[] args) throws Exception &#123; //得到Configuration Configuration conf = this.getConf(); //创建Job任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Fruit2FruitMRRunner.class); //配置Job Scan scan = new Scan(); scan.setCacheBlocks(false); scan.setCaching(500); //设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本 TableMapReduceUtil.initTableMapperJob( &quot;fruit&quot;, //数据源的表名 scan, //scan扫描控制器 ReadFruitMapper.class,//设置Mapper类 ImmutableBytesWritable.class,//设置Mapper输出key类型 Put.class,//设置Mapper输出value值类型 job//设置给哪个JOB ); //设置Reducer TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRReducer.class, job); //设置Reduce数量，最少1个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess)&#123; throw new IOException(&quot;Job running with error&quot;); &#125; return isSuccess ? 0 : 1; &#125; 4) 主函数中调用运行该Job**任务** public static void main( String[] args ) throws Exception&#123;Configuration conf = HBaseConfiguration.create();int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args);System.exit(status);&#125; 5) 打包运行任务 $ ~/modules/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/hbase-0.0.1-SNAPSHOT.jar com.z.hbase.mr1.Fruit2FruitMRRunner 尖叫提示：运行任务前，如果待数据导入的表不存在，则需要提前创建之。 尖叫提示：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin） 2.5.3、自定义HBase-MapReduce2目标：实现将HDFS中的数据写入到HBase表中。 分步实现： 1) 构建ReadFruitFromHDFSMapper**于读取HDFS**中的文件数据 package com.z.hbase.mr2;import java.io.IOException;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //从HDFS中读取的数据 String lineValue = value.toString(); //读取出来的每行数据使用\\t进行分割，存于String数组 String[] values = lineValue.split(&quot;\\t&quot;); //根据数据中值的含义取值 String rowKey = values[0]; String name = values[1]; String color = values[2]; //初始化rowKey ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey)); //初始化put对象 Put put = new Put(Bytes.toBytes(rowKey)); //参数分别:列族、列、值 put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(name)); put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;color&quot;), Bytes.toBytes(color)); context.write(rowKeyWritable, put); &#125;&#125; 2) 构建WriteFruitMRFromTxtReducer**类** package com.z.hbase.mr2;import java.io.IOException;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; //读出来的每一行数据写入到fruit_hdfs表中 for(Put put: values)&#123; context.write(NullWritable.get(), put); &#125; &#125;&#125; 3) 创建Txt2FruitRunner**组装Job** public int run(String[] args) throws Exception &#123;//得到ConfigurationConfiguration conf = this.getConf();//创建Job任务Job job = Job.getInstance(conf, this.getClass().getSimpleName());job.setJarByClass(Txt2FruitRunner.class);Path inPath = new Path(&quot;hdfs://linux01:8020/input_fruit/fruit.tsv&quot;);FileInputFormat.addInputPath(job, inPath);//设置Mapperjob.setMapperClass(ReadFruitFromHDFSMapper.class);job.setMapOutputKeyClass(ImmutableBytesWritable.class);job.setMapOutputValueClass(Put.class);//设置ReducerTableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRFromTxtReducer.class, job);//设置Reduce数量，最少1个job.setNumReduceTasks(1);boolean isSuccess = job.waitForCompletion(true);if(!isSuccess)&#123;throw new IOException(&quot;Job running with error&quot;);&#125;return isSuccess ? 0 : 1;&#125; 4) 调用执行Job public static void main(String[] args) throws Exception { Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Txt2FruitRunner(), args); System.exit(status); } 5) 打包运行 $ ~/modules/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/hbase-0.0.1-SNAPSHOT.jar com.z.hbase.mr2.Txt2FruitRunner 尖叫提示：运行任务前，如果待数据导入的表不存在，则需要提前创建之。 尖叫提示：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin） 2.6、与Hive的集成2.6.1、HBase与Hive的对比1) Hive (1) 数据仓库 Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。 (2) 用于数据分析、清洗 Hive适用于离线的数据分析和清洗，延迟较高。 (3) 基于HDFS**、MapReduce** Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。 2) HBase (1) 数据库 是一种面向列存储的非关系型数据库。 (2) 用于存储结构化和非结构话的数据 适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。 (3) 基于HDFS 数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。 (4) 延迟较低，接入在线业务使用 面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。 2.6.2、HBase与Hive集成使用尖叫提示：HBase与Hive的集成在最新的两个版本中无法兼容。所以，我们只能含着泪勇敢的重新编译：hive-hbase-handler-1.2.2.jar！！好气！！ 环境准备 因为我们后续可能会在操作Hive的同时对HBase也会产生影响，所以Hive需要持有操作HBase的Jar，那么接下来拷贝Hive所依赖的Jar包（或者使用软连接的形式）。记得还有把zookeeper的jar包考入到hive的lib目录下。 $ export HBASE_HOME=/home/admin/modules/hbase-1.3.1$ export HIVE_HOME=/home/admin/modules/apache-hive-1.2.2-bin$ ln -s $HBASE_HOME/lib/hbase-common-1.3.1.jar $HIVE_HOME/lib/hbase-common-1.3.1.jar$ ln -s $HBASE_HOME/lib/hbase-server-1.3.1.jar $HIVE_HOME/lib/hbase-server-1.3.1.jar$ ln -s $HBASE_HOME/lib/hbase-client-1.3.1.jar $HIVE_HOME/lib/hbase-client-1.3.1.jar$ ln -s $HBASE_HOME/lib/hbase-protocol-1.3.1.jar $HIVE_HOME/lib/hbase-protocol-1.3.1.jar$ ln -s $HBASE_HOME/lib/hbase-it-1.3.1.jar $HIVE_HOME/lib/hbase-it-1.3.1.jar$ ln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jar$ ln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar$ ln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.3.1.jar 同时在hive-site.xml**中修改zookeeper**的属性，如下： &lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;linux01,linux02,linux03&lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt; 1) 案例一 目标：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。 分步实现： (1) 在Hive**中创建表同时关联HBase*（\\会自动的在\\hbase**中创建相应的表*）（数据存储在hbase中**） CREATE TABLE hive_hbase_emp_table(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;); 尖叫提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表 (2) 在Hive**中创建临时中间表，用于load**文件中的数据 尖叫提示：不能将数据直接load进Hive所关联HBase的那张表中 CREATE TABLE emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) row format delimited fields terminated by ‘\\t’; (3) 向Hive**中间表中load**数据 hive&gt; load data local inpath ‘/home/admin/softwares/data/emp.txt’ into table emp; (4) 通过insert**命令将中间表中的数据导入到Hive关联HBase的那张表中** hive&gt; insert into table hive_hbase_emp_table select * from emp; (5) 查看Hive**以及关联的HBase**表中是否已经成功的同步插入了数据 Hive**：** hive&gt; select * from hive_hbase_emp_table; HBase**：** hbase&gt; scan ‘hbase_emp_table’ 2) 案例二（常用场景） 目标：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。 注：该案例2紧跟案例1的脚步，所以完成此案例前，请先完成案例1。 分步实现： (1) 在Hive**中创建外部表** CREATE EXTERNAL TABLE relevance_hbase_emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;); (2) 关联后就可以使用Hive**函数进行一些分析操作了（数据存储在hbase中）** hive (default)&gt; select * from relevance_hbase_emp; 2.6.3 需要注意删除表的时候，要先删除hive的表然后再删除habse的。如果反之，删除完hbase的表，那么再去删除hive的表时就会报错（这个时候，退出hive重新登录即可，但是不推荐） 2.7、与Sqoop的集成Sqoop supports additional import targets beyond HDFS and Hive. Sqoop can also import records into a table in HBase. 之前我们已经学习过如何使用Sqoop在Hadoop集群和关系型数据库中进行数据的导入导出工作，接下来我们学习一下利用Sqoop在HBase和RDBMS中进行数据的转储。 相关参数： 参数 描述 –column-family Sets the target column family for the import 设置导入的目标列族。 –hbase-create-table If specified, create missing HBase tables 是否自动创建不存在的HBase表（这就意味着，不需要手动提前在HBase中先建立表） –hbase-row-key Specifies which input column to use as the row key.In case, if input table contains composite key, then must be in the form of a comma-separated list of composite key attributes. mysql中哪一列的值作为HBase的rowkey，如果rowkey是个组合键，则以逗号分隔。（注：避免rowkey的重复） –hbase-table Specifies an HBase table to use as the target instead of HDFS. 指定数据将要导入到HBase中的哪张表中。 –hbase-bulkload Enables bulk loading. 是否允许bulk形式的导入。 1) 案例 目标：将RDBMS中的数据抽取到HBase中 分步实现： (1) 配置sqoop-env.sh**，添加如下内容：** export HBASE_HOME=/home/admin/modules/hbase-1.3.1 (2) 在Mysql**中新建一个数据库db_library**，一张表book CREATE DATABASE db_library; CREATE TABLE db_library.book( id int(4) PRIMARY KEY NOT NULL AUTO_INCREMENT, name VARCHAR(255) NOT NULL, price VARCHAR(255) NOT NULL); (3) 向表中插入一些数据 INSERT INTO db_library.book (name, price) VALUES(‘Lie Sporting’, ‘30’); INSERT INTO db_library.book (name, price) VALUES(‘Pride &amp; Prejudice’, ‘70’); INSERT INTO db_library.book (name, price) VALUES(‘Fall of Giants’, ‘50’); (4) 执行Sqoop**导入数据的操作** $ bin/sqoop import \\ –connect jdbc:mysql://linux01:3306/db_library \\ –username root \\ –password 123456 \\ –table book \\ –columns “id,name,price” \\ –column-family “info” \\ –hbase-create-table \\ –hbase-row-key “id” \\ –hbase-table “hbase_book” \\ –num-mappers 1 \\ –split-by id //一个id一条数据的分割方式，导入hbase 尖叫提示：sqoop1.4.6只支持HBase1.0.1之前的版本的自动创建HBase表的功能 解决方案：手动创建HBase表 hbase&gt; create ‘hbase_book’,’info’ (5) 在HBase**中scan**这张表得到如下内容 hbase&gt; scan ‘hbase_book’ 思考：尝试使用复合键作为导入数据时的rowkey。 2.8、常用的Shell操作1) satus 例如：显示服务器状态 hbase&gt; status ‘linux01’ 2) whoami 显示HBase当前用户，例如： hbase&gt; whoami 3) list 显示当前所有的表 hbase&gt; list 4) count 统计指定表的记录数，例如： hbase&gt; count ‘hbase_book’ 5) describe 展示表结构信息 hbase&gt; describe ‘hbase_book’ 6) exist 检查表是否存在，适用于表量特别多的情况 hbase&gt; exist ‘hbase_book’ 7) is_enabled/is_disabled 检查表是否启用或禁用 hbase&gt; is_enabled ‘hbase_book’ hbase&gt; is_disabled ‘hbase_book’ 8) alter 该命令可以改变表和列族的模式，例如： 为当前表增加列族： hbase&gt; alter ‘hbase_book’, NAME =&gt; ‘CF2’, VERSIONS =&gt; 2 为当前表删除列族： hbase&gt; alter ‘hbase_book’, ‘delete’ =&gt; ‘CF2’ 9) disable 禁用一张表 hbase&gt; disable ‘hbase_book’ 10) drop 删除一张表，记得在删除表之前必须先禁用 hbase&gt; drop ‘hbase_book’ 11) delete 删除一行中一个单元格的值，例如： hbase&gt; delete ‘hbase_book’, ‘rowKey’, ‘CF:C’ 12) truncate 清空表数据，即禁用表-删除表-创建表 hbase&gt; truncate ‘hbase_book’ 13) create 创建表，例如： hbase&gt; create ‘table’, ‘cf’ 创建多个列族： hbase&gt; create ‘t1’, {NAME =&gt; ‘f1’}, {NAME =&gt; ‘f2’}, {NAME =&gt; ‘f3’} 2.9、数据的备份与恢复2.9.1、备份停止HBase服务后，使用distcp命令运行MapReduce任务进行备份，将数据备份到另一个地方，可以是同一个集群，也可以是专用的备份集群。 即，把数据转移到当前集群的其他目录下（也可以不在同一个集群中）: $ bin/hadoop distcp \\ hdfs://linux01:8020/hbase \\ hdfs://linux01:8020/HbaseBackup/backup20171009 尖叫提示：执行该操作，一定要开启Yarn服务 2.9.2、恢复非常简单，与备份方法一样，将数据整个移动回来即可。 $ bin/hadoop distcp \\ hdfs://linux01:8020/HbaseBackup/backup20170930 \\ hdfs://linux01:8020/hbase 2.10、节点的管理2.10.1、服役（commissioning）当启动regionserver时，regionserver会向HMaster注册并开始接收本地数据，开始的时候，新加入的节点不会有任何数据，平衡器开启的情况下，将会有新的region移动到开启的RegionServer上。如果启动和停止进程是使用ssh和HBase脚本，那么会将新添加的节点的主机名加入到conf/regionservers文件中。 2.10.2、退役（decommissioning）顾名思义，就是从当前HBase集群中删除某个RegionServer，这个过程分为如下几个过程： 1) 停止负载平衡器（HMaster**上操作）** hbase&gt; balance_switch false 2) 在退役节点上停止RegionServer hbase&gt; hbase-daemon.sh stop regionserver 3) RegionServer**一旦停止，会关闭维护的所有region** 4) Zookeeper**上的该RegionServer**节点消失 5) Master**节点检测到该RegionServer**下线，开启平衡器 6) 下线的RegionServer**的region**服务得到重新分配 该关闭方法比较传统，需要花费一定的时间，而且会造成部分region短暂的不可用。 另一种方案： 1) RegionServer**先卸载所管理的region** $ bin/graceful_stop.sh 2) 自动平衡数据 3) 和之前的2~6**步是一样的** 2.11、版本的确界1) 版本的下界 默认的版本下界是0，即禁用。row版本使用的最小数目是与生存时间（TTL Time To Live）相结合的，并且我们根据实际需求可以有0或更多的版本，使用0，即只有1个版本的值写入cell。 2) 版本的上界 之前默认的版本上界是3，也就是一个row保留3个副本（基于时间戳的插入）。该值不要设计的过大，一般的业务不会超过100。如果cell中存储的数据版本号超过了3个，再次插入数据时，最新的值会将最老的值覆盖。（现版本已默认为1） 三、HBase的优化3.1、高可用在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。（也就是说，加入***HMaster**挂掉了，hbase**集群还是能工作的，只不过此时所有的读写都操作同一个reginserver**，那么会把它呈报，后面也会不工作。因为失去了HMaster**，失去了负债均衡的能力***） 1) 关闭HBase**集群（如果没有开启则跳过此步）** $ bin/stop-hbase.sh 2) 在conf**目录下创建backup-masters**文件 $ touch conf/backup-masters 3) 在backup-masters**文件中配置高可用HMaster**节点 $ echo linux02 &gt; conf/backup-masters 4) 将整个conf**目录scp**到其他节点 $ scp -r conf/ linux02:/opt/modules/cdh/hbase-0.98.6-cdh5.3.6/ $ scp -r conf/ linux03:/opt/modules/cdh/hbase-0.98.6-cdh5.3.6/ 5) 重新启动HBase**后打开页面测试查看** 0.98版本之前：http://linux01:60010 0.98版本之后：http://linux01:16010 3.2、Hadoop的通用性优化1) NameNode**元数据备份使用SSD** 2) 定时备份NameNode**上的元数据** 每小时或者每天备份，如果数据极其重要，可以5~10分钟备份一次。备份可以通过定时任务复制元数据目录即可。 3) 为NameNode**指定多个元数据目录** 使用dfs.name.dir或者dfs.namenode.name.dir指定。这样可以提供元数据的冗余和健壮性，以免发生故障。 4) NameNode**的dir**自恢复 设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。 5) HDFS**保证RPC**调用会有较多的线程数 hdfs-site.xml 属性：dfs.namenode.handler.count 解释：该属性是NameNode服务默认线程数，的默认值是10，根据机器的可用内存可以调整为50~100 属性：dfs.datanode.handler.count 解释：该属性默认值为10，是DataNode的处理线程数，如果HDFS客户端程序读写请求比较多，可以调高到15~20，设置的值越大，内存消耗越多，不要调整的过高，一般业务中，5~10即可。 6) HDFS**副本数的调整** hdfs-site.xml 属性：dfs.replication 解释：如果数据量巨大，且不是非常之重要，可以调整为2~3，如果数据非常之重要，可以调整为3~5。 7) HDFS**文件块大小的调整** hdfs-site.xml 属性：dfs.blocksize 解释：块大小定义，该属性应该根据存储的大量的单个文件大小来设置，如果大量的单个文件都小于100M，建议设置成64M块大小，对于大于100M或者达到GB的这种情况，建议设置成256M，一般设置范围波动在64M~256M之间。 8) MapReduce Job**任务服务线程数调整** mapred-site.xml 属性：mapreduce.jobtracker.handler.count 解释：该属性是Job任务线程数，默认值是10，根据机器的可用内存可以调整为50~100 9) Http**服务器工作线程数** mapred-site.xml 属性：mapreduce.tasktracker.http.threads 解释：定义HTTP服务器工作线程数，默认值为40，对于大集群可以调整到80~100 10) 文件排序合并优化 mapred-site.xml 属性：mapreduce.task.io.sort.factor 解释：文件排序时同时合并的数据流的数量，这也定义了同时打开文件的个数，默认值为10，如果调高该参数，可以明显减少磁盘IO，即减少文件读取的次数。 11) 设置任务并发 mapred-site.xml 属性：mapreduce.map.speculative 解释：该属性可以设置任务是否可以并发执行，如果任务多而小，该属性设置为true可以明显加快任务执行效率，但是对于延迟非常高的任务，建议改为false，这就类似于迅雷下载。 12) MR**输出数据的压缩** mapred-site.xml 属性：mapreduce.map.output.compress、mapreduce.output.fileoutputformat.compress 解释：对于大集群而言，建议设置Map-Reduce的输出为压缩的数据，而对于小集群，则不需要。 13) 优化Mapper**和Reducer**的个数 mapred-site.xml 属性： mapreduce.tasktracker.map.tasks.maximum mapreduce.tasktracker.reduce.tasks.maximum 解释：以上两个属性分别为一个单独的Job任务可以同时运行的Map和Reduce的数量。 设置上面两个参数时，需要考虑CPU核数、磁盘和内存容量。假设一个8核的CPU，业务内容非常消耗CPU，那么可以设置map数量为4，如果该业务不是特别消耗CPU类型的，那么可以设置map数量为40，reduce数量为20。这些参数的值修改完成之后，一定要观察是否有较长等待的任务，如果有的话，可以减少数量以加快任务执行，如果设置一个很大的值，会引起大量的上下文切换，以及内存与磁盘之间的数据交换，这里没有标准的配置数值，需要根据业务和硬件配置以及经验来做出选择。 在同一时刻，不要同时运行太多的MapReduce，这样会消耗过多的内存，任务会执行的非常缓慢，我们需要根据CPU核数，内存容量设置一个MR任务并发的最大值，使固定数据量的任务完全加载到内存中，避免频繁的内存和磁盘数据交换，从而降低磁盘IO，提高性能。 大概估算公式： map = 2 + ⅔cpu_core reduce = 2 + ⅓cpu_core 3.3、Linux优化1) 开启文件系统的预读缓存可以提高读取速度（kb**）** $ sudo blockdev –setra 32768 /dev/sda 尖叫提示：ra是readahead的缩写 2) 关闭进程睡眠池 即不允许后台进程进入睡眠状态，如果进程空闲，则直接kill掉释放资源 $ sudo sysctl -w vm.swappiness=0 3) 调整ulimit**上限，默认值为比较小的数字** $ ulimit -n 查看允许最大进程数 $ ulimit -u 查看允许打开最大文件数 优化修改： $ sudo vi /etc/security/limits.conf 修改打开文件数限制 末尾添加： soft nofile 1024000 hard nofile 1024000 Hive - nofile 1024000 hive - nproc 1024000 $ sudo vi /etc/security/limits.d/90-nproc.conf 修改用户打开进程数限制 修改为： # soft nproc 4096 #root soft nproc unlimited soft nproc 40960 root soft nproc unlimited 4) 开启集群的时间同步NTP 集群中某台机器同步网络时间服务器的时间，集群中其他机器则同步这台机器的时间。 5) 更新系统补丁 更新补丁前，请先测试新版本补丁对集群节点的兼容性。 3.4、Zookeeper优化1) 优化Zookeeper**会话超时时间** hbase-site.xml 参数：zookeeper.session.timeout 解释：In hbase-site.xml, set zookeeper.session.timeout to 30 seconds or less to bound failure detection (20-30 seconds is a good start).该值会直接关系到master发现服务器宕机的最大周期，默认值为30秒（不同的HBase版本，该默认值不一样），如果该值过小，会在HBase在写入大量数据发生而GC时，导致RegionServer短暂的不可用，从而没有向ZK发送心跳包，最终导致认为从节点shutdown。一般20台左右的集群需要配置5台zookeeper。 3.5、HBase优化3.5.1、预分区(避免region被无线切分)（他的本质实际上就是预估rowkey的范围）每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据索要投放的分区提前大致的规划好，以提高HBase性能。 1) 手动设定预分区（很明显生成五个分区 负无穷到1000,1000-2000,2000-3000**，3000-4000,4000-**正无穷） hbase&gt; create animal,’info’,’partition1’,SPLITS =&gt; [‘1000’,’2000’,’3000’,’4000’] 2) 生成16**进制序列预分区(直接指定15个分区)** create ‘staff2’,’info’,’partition2’,{NUMREGIONS =&gt; 15, SPLITALGO =&gt; ‘HexStringSplit’} 3) 按照文件中设置的规则预分区 创建splits.txt文件内容如下： aaaa bbbb cccc dddd 然后执行： create ‘staff3’,’partition3’,SPLITS_FILE =&gt; ‘splits.txt’ 4) 使用JavaAPI**创建预分区** //自定义算法，产生一系列Hash散列值存储在二维数组中byte[][] splitKeys = 某个散列值函数//创建HBaseAdmin实例HBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create());//创建HTableDescriptor实例HTableDescriptor tableDesc = new HTableDescriptor(tableName);//通过HTableDescriptor实例和散列值二维数组创建带有预分区的HBase表hAdmin.createTable(tableDesc, splitKeys); 注意：分区的rowkey必须是有序的递增。否则没有意义 例如：create animal,’info’,’partition1’,SPLITS =&gt; [‘4000’,’2000’] 上面的分区脚本很明显会分成三个分区，分别是：-无穷大到4000,4000到2000,2000到正无穷大。这样的分区很明显是存在重叠的。 3.5.2、RowKey设计一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案。 1) 生成随机数、hash**、散列值** 比如：原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。 2) 字符串反转 20170524000001转成10000042507102 20170524000002转成20000042507102 这样也可以在一定程度上散列逐步put进来的数据。 3) 字符串拼接 20170524000001_a12e 20170524000001_93i7 3.5.3、内存优化HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。 3.5.4、基础优化1) 允许在HDFS**的文件中追加内容** 不是不允许追加内容么？没错，请看背景故事： http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/ hdfs-site.xml**、hbase-site.xml** 属性：dfs.support.append 解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。 2) 优化DataNode**允许的最大文件打开数** hdfs-site.xml 属性：dfs.datanode.max.transfer.threads 解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096 3) 优化延迟高的数据操作的等待时间 hdfs-site.xml 属性：dfs.image.transfer.timeout 解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。 4) 优化数据的写入效率 mapred-site.xml 属性： mapreduce.map.output.compress mapreduce.map.output.compress.codec 解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。 5) 优化DataNode**存储** 属性：dfs.datanode.failed.volumes.tolerated 解释： 默认为0，意思是当DataNode中有一个磁盘出现故障，则会认为该DataNode shutdown了。如果修改为1，则一个磁盘出现故障时，数据会被复制到其他正常的DataNode上，当前的DataNode继续工作。 6) 设置RPC**监听数量** hbase-site.xml 属性：hbase.regionserver.handler.count 解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 7) 优化HStore**文件大小** hbase-site.xml 属性：hbase.hregion.max.filesize 解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。 8) 优化hbase**客户端缓存** hbase-site.xml 属性：hbase.client.write.buffer 解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。 9) 指定scan.next**扫描HBase**所获取的行数 hbase-site.xml 属性：hbase.client.scanner.caching 解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。 10) flush**、compact、split机制** 当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。 涉及属性： 即：128M就是Memstore的默认阈值 hbase.hregion.memstore.flush.size：134217728 即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。 hbase.regionserver.global.memstore.upperLimit：0.4 hbase.regionserver.global.memstore.lowerLimit：0.38 即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit 四、HBase项目4.1、涉及概念梳理：命名空间4.1.1、命名空间的结构 1) Table：表，所有的表都是命名空间的成员，即表必属于某个命名空间，如果没有指定，则在default默认的命名空间中。 2) RegionServer group**：**一个命名空间包含了默认的RegionServer Group。 3) Permission**：**权限，命名空间能够让我们来定义访问控制列表ACL（Access Control List）。例如，创建表，读取表，删除，更新等等操作。 4) Quota**：**限额，可以强制一个命名空间可包含的region的数量。（属性：hbase.quota.enabled） 4.1.2、命名空间的使用1) 创建命名空间 hbase(main):002:0&gt; create_namespace ‘ns_school’ 2) 创建表时指定命名空间 hbase(main):004:0&gt; create ‘ns_school:tbl_student’,’info’ 3) 观察HDFS**中的目录结构的变化** 4.2、微博系统4.1.1、需求分析1) 微博内容的浏览，数据库表设计 2) 用户社交体现：关注用户，取关用户 3) 拉取关注的人的微博内容 4.1.2、代码实现代码设计总览： 1) 创建命名空间以及表名的定义 2) 创建微博内容表 3) 创建用户关系表 4) 创建用户微博内容接收邮件表 5) 发布微博内容 6) 添加关注用户 7) 移除（取关）用户 8) 获取关注的人的微博内容 9) 测试 1) 创建命名空间以及表名的定义 //获取配置confprivate Configuration conf = HBaseConfiguration.create();//微博内容表的表名private static final byte[] TABLE_CONTENT = Bytes.toBytes(&quot;ns_weibo:content&quot;);//用户关系表的表名private static final byte[] TABLE_RELATION = Bytes.toBytes(&quot;ns_weibo:relation&quot;);//微博收件箱表的表名private static final byte[] TABLE_INBOX = Bytes.toBytes(&quot;ns_weibo:inbox&quot;);/** * 初始化命名空间 * @param args */public void initNamespace()&#123; HBaseAdmin admin = null; try &#123; Connection connection = ConnectionFactory.createConnection(conf); admin = (HBaseAdmin) connection.getAdmin(); //命名空间类似于关系型数据库中的schema，可以想象成文件夹 NamespaceDescriptor weibo = NamespaceDescriptor .create(&quot;ns_weibo&quot;) .addConfiguration(&quot;creator&quot;, &quot;Jinji&quot;) .addConfiguration(&quot;create_time&quot;, System.currentTimeMillis() + &quot;&quot;) .build(); admin.createNamespace(weibo); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 2) 创建微博内容表 表结构： 方法名 creatTableeContent Table Name ns_weibo:content RowKey 用户ID_时间戳 ColumnFamily info ColumnLabel 标题,内容,图片 Version 1个版本 代码： /** * 创建微博内容表 * Table Name:ns_weibo:content * RowKey:用户ID_时间戳 * ColumnFamily:info * ColumnLabel:标题,内容,图片URL * Version:1个版本 */public void createTableContent()&#123; HBaseAdmin admin = null; Connection connection = null; try &#123; connection = ConnectionFactory.createConnection(conf); admin = (HBaseAdmin) connection.getAdmin(); //创建表表述 HTableDescriptor contentTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_CONTENT)); //创建列族描述 HColumnDescriptor infoColumnDescriptor = new HColumnDescriptor(Bytes.toBytes(&quot;info&quot;)); //设置块缓存 infoColumnDescriptor.setBlockCacheEnabled(true); //设置块缓存大小 infoColumnDescriptor.setBlocksize(2097152); //设置压缩方式// infoColumnDescriptor.setCompressionType(Algorithm.SNAPPY); //设置版本确界 infoColumnDescriptor.setMaxVersions(1); infoColumnDescriptor.setMinVersions(1); contentTableDescriptor.addFamily(infoColumnDescriptor); admin.createTable(contentTableDescriptor); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally&#123; if(null != admin)&#123; try &#123; admin.close(); connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 3) 创建用户关系表 表结构： 方法名 createTableRelations Table Name ns_weibo:relation RowKey 用户ID ColumnFamily attends、fans ColumnLabel 关注用户ID，粉丝用户ID ColumnValue 用户ID Version 1个版本 代码： /** * 用户关系表 * Table Name:ns_weibo:relation * RowKey:用户ID * ColumnFamily:attends,fans * ColumnLabel:关注用户ID，粉丝用户ID * ColumnValue:用户ID * Version：1个版本 */public void createTableRelation()&#123; HBaseAdmin admin = null; try &#123; Connection connection = ConnectionFactory.createConnection(conf); admin = (HBaseAdmin) connection.getAdmin(); HTableDescriptor relationTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_RELATION)); //关注的人的列族 HColumnDescriptor attendColumnDescriptor = new HColumnDescriptor(Bytes.toBytes(&quot;attends&quot;)); //设置块缓存 attendColumnDescriptor.setBlockCacheEnabled(true); //设置块缓存大小 attendColumnDescriptor.setBlocksize(2097152); //设置压缩方式// attendColumnDescriptor.setCompressionType(Algorithm.SNAPPY); //设置版本确界 attendColumnDescriptor.setMaxVersions(1); attendColumnDescriptor.setMinVersions(1); //粉丝列族 HColumnDescriptor fansColumnDescriptor = new HColumnDescriptor(Bytes.toBytes(&quot;fans&quot;)); fansColumnDescriptor.setBlockCacheEnabled(true); fansColumnDescriptor.setBlocksize(2097152); fansColumnDescriptor.setMaxVersions(1); fansColumnDescriptor.setMinVersions(1); relationTableDescriptor.addFamily(attendColumnDescriptor); relationTableDescriptor.addFamily(fansColumnDescriptor); admin.createTable(relationTableDescriptor); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 4) 创建微博收件箱表 表结构： 方法名 createTableInbox Table Name ns_weibo:inbox RowKey 用户ID ColumnFamily info ColumnLabel 用户ID ColumnValue 取微博内容的RowKey Version 1000 代码： /** * 创建微博收件箱表 * Table Name: ns_weibo:inbox * RowKey:用户ID * ColumnFamily:info * ColumnLabel:用户ID_发布微博的人的用户ID * ColumnValue:关注的人的微博的RowKey * Version:1000 */public void createTableInbox()&#123; HBaseAdmin admin = null; try &#123; Connection connection = ConnectionFactory.createConnection(conf); admin = (HBaseAdmin) connection.getAdmin(); HTableDescriptor inboxTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_INBOX)); HColumnDescriptor infoColumnDescriptor = new HColumnDescriptor(Bytes.toBytes(&quot;info&quot;)); infoColumnDescriptor.setBlockCacheEnabled(true); infoColumnDescriptor.setBlocksize(2097152); infoColumnDescriptor.setMaxVersions(1000); infoColumnDescriptor.setMinVersions(1000); inboxTableDescriptor.addFamily(infoColumnDescriptor);; admin.createTable(inboxTableDescriptor); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 5) 发布微博内容 a、微博内容表中添加1条数据 b、微博收件箱表对所有粉丝用户添加数据 代码：Message.java package com.z.hbase.weibo;public class Message &#123; private String uid; private String timestamp; private String content; public String getUid() &#123; return uid; &#125; public void setUid(String uid) &#123; this.uid = uid; &#125; public String getTimestamp() &#123; return timestamp; &#125; public void setTimestamp(String timestamp) &#123; this.timestamp = timestamp; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125; @Override public String toString() &#123; return &quot;Message [uid=&quot; + uid + &quot;, timestamp=&quot; + timestamp + &quot;, content=&quot; + content + &quot;]&quot;; &#125;&#125; 代码：public void publishContent(String uid, String content) /** * 发布微博 * a、微博内容表中数据+1 * b、向微博收件箱表中加入微博的Rowkey */public void publishContent(String uid, String content)&#123; Connection connection = null; try &#123; connection = ConnectionFactory.createConnection(conf); //a、微博内容表中添加1条数据，首先获取微博内容表描述 Table contentTable = connection.getTable(TableName.valueOf(TABLE_CONTENT)); //组装Rowkey long timestamp = System.currentTimeMillis(); String rowKey = uid + &quot;_&quot; + timestamp; //添加微博内容 Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;content&quot;), timestamp, Bytes.toBytes(content)); contentTable.put(put); //b、向微博收件箱表中加入发布的Rowkey //b.1、查询用户关系表，得到当前用户有哪些粉丝 Table relationTable = connection.getTable(TableName.valueOf(TABLE_RELATION)); //b.2、取出目标数据 Get get = new Get(Bytes.toBytes(uid)); get.addFamily(Bytes.toBytes(&quot;fans&quot;)); Result result = relationTable.get(get); List&lt;byte[]&gt; fans = new ArrayList&lt;byte[]&gt;(); //遍历取出当前发布微博的用户的所有粉丝数据 for(Cell cell : result.rawCells())&#123; fans.add(CellUtil.cloneQualifier(cell)); &#125; //如果该用户没有粉丝，则直接return if(fans.size() &lt;= 0) return; //开始操作收件箱表 Table inboxTable = connection.getTable(TableName.valueOf(TABLE_INBOX)); //每一个粉丝，都要向收件箱中添加该微博的内容，所以每一个粉丝都是一个Put对象 List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;(); for(byte[] fan : fans)&#123; Put fansPut = new Put(fan); fansPut.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(uid), timestamp, Bytes.toBytes(rowKey)); puts.add(fansPut); &#125; inboxTable.put(puts); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != connection)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 6) 添加关注用户 a、在微博用户关系表中，对当前主动操作的用户添加新关注的好友 b、在微博用户关系表中，对被关注的用户添加新的粉丝 c、微博收件箱表中添加所关注的用户发布的微博 代码实现：public void addAttends(String uid, String… attends) /** * 关注用户逻辑 * a、在微博用户关系表中，对当前主动操作的用户添加新的关注的好友 * b、在微博用户关系表中，对被关注的用户添加粉丝（当前操作的用户） * c、当前操作用户的微博收件箱添加所关注的用户发布的微博rowkey */public void addAttends(String uid, String... attends)&#123; //参数过滤 if(attends == null || attends.length &lt;= 0 || uid == null || uid.length() &lt;= 0)&#123; return; &#125; Connection connection = null; try &#123; connection = ConnectionFactory.createConnection(conf); //用户关系表操作对象（连接到用户关系表） Table relationTable = connection.getTable(TableName.valueOf(TABLE_RELATION)); List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;(); //a、在微博用户关系表中，添加新关注的好友 Put attendPut = new Put(Bytes.toBytes(uid)); for(String attend : attends)&#123; //为当前用户添加关注的人 attendPut.addColumn(Bytes.toBytes(&quot;attends&quot;), Bytes.toBytes(attend), Bytes.toBytes(attend)); //b、为被关注的人，添加粉丝 Put fansPut = new Put(Bytes.toBytes(attend)); fansPut.addColumn(Bytes.toBytes(&quot;fans&quot;), Bytes.toBytes(uid), Bytes.toBytes(uid)); //将所有关注的人一个一个的添加到puts（List）集合中 puts.add(fansPut); &#125; puts.add(attendPut); relationTable.put(puts); //c.1、微博收件箱添加关注的用户发布的微博内容（content）的rowkey Table contentTable = connection.getTable(TableName.valueOf(TABLE_CONTENT)); Scan scan = new Scan(); //用于存放取出来的关注的人所发布的微博的rowkey List&lt;byte[]&gt; rowkeys = new ArrayList&lt;byte[]&gt;(); for(String attend : attends)&#123; //过滤扫描rowkey，即：前置位匹配被关注的人的uid_ RowFilter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(attend + &quot;_&quot;)); //为扫描对象指定过滤规则 scan.setFilter(filter); //通过扫描对象得到scanner ResultScanner result = contentTable.getScanner(scan); //迭代器遍历扫描出来的结果集 Iterator&lt;Result&gt; iterator = result.iterator(); while(iterator.hasNext())&#123; //取出每一个符合扫描结果的那一行数据 Result r = iterator.next(); for(Cell cell : r.rawCells())&#123; //将得到的rowkey放置于集合容器中 rowkeys.add(CellUtil.cloneRow(cell)); &#125; &#125; &#125; //c.2、将取出的微博rowkey放置于当前操作的用户的收件箱中 if(rowkeys.size() &lt;= 0) return; //得到微博收件箱表的操作对象 Table inboxTable = connection.getTable(TableName.valueOf(TABLE_INBOX)); //用于存放多个关注的用户的发布的多条微博rowkey信息 List&lt;Put&gt; inboxPutList = new ArrayList&lt;Put&gt;(); for(byte[] rk : rowkeys)&#123; Put put = new Put(Bytes.toBytes(uid)); //uid_timestamp String rowKey = Bytes.toString(rk); //截取uid String attendUID = rowKey.substring(0, rowKey.indexOf(&quot;_&quot;)); long timestamp = Long.parseLong(rowKey.substring(rowKey.indexOf(&quot;_&quot;) + 1)); //将微博rowkey添加到指定单元格中 put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(attendUID), timestamp, rk); inboxPutList.add(put); &#125; inboxTable.put(inboxPutList); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != connection)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 7) 移除（取关）用户 a、在微博用户关系表中，对当前主动操作的用户移除取关的好友(attends) b、在微博用户关系表中，对被取关的用户移除粉丝 c、微博收件箱中删除取关的用户发布的微博 代码：public void removeAttends(String uid, String… attends) /** * 取消关注（remove) * a、在微博用户关系表中，对当前主动操作的用户删除对应取关的好友 * b、在微博用户关系表中，对被取消关注的人删除粉丝（当前操作人） * c、从收件箱中，删除取关的人的微博的rowkey * */public void removeAttends(String uid, String... attends)&#123; //过滤数据 if(uid == null || uid.length() &lt;= 0 || attends == null || attends.length &lt;= 0) return; try &#123; Connection connection = ConnectionFactory.createConnection(conf); //a、在微博用户关系表中，删除已关注的好友 Table relationTable = connection.getTable(TableName.valueOf(TABLE_RELATION)); //待删除的用户关系表中的所有数据 List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); //当前取关操作者的uid对应的Delete对象 Delete attendDelete = new Delete(Bytes.toBytes(uid)); //遍历取关，同时每次取关都要将被取关的人的粉丝-1 for(String attend : attends)&#123; attendDelete.addColumn(Bytes.toBytes(&quot;attends&quot;), Bytes.toBytes(attend)); //b、在微博用户关系表中，对被取消关注的人删除粉丝（当前操作人） Delete fansDelete = new Delete(Bytes.toBytes(attend)); fansDelete.addColumn(Bytes.toBytes(&quot;fans&quot;), Bytes.toBytes(uid)); deleteList.add(fansDelete); &#125; deleteList.add(attendDelete); relationTable.delete(deleteList); //c、删除取关的人的微博rowkey 从 收件箱表中 Table inboxTable = connection.getTable(TableName.valueOf(TABLE_INBOX)); Delete inboxDelete = new Delete(Bytes.toBytes(uid)); for(String attend : attends)&#123; inboxDelete.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(attend)); &#125; inboxTable.delete(inboxDelete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 8) 获取关注的人的微博内容 a、从微博收件箱中获取所关注的用户的微博RowKey b、根据获取的RowKey，得到微博内容 代码实现：public List getAttendsContent(String uid) /** * 获取微博实际内容 * a、从微博收件箱中获取所有关注的人的发布的微博的rowkey * b、根据得到的rowkey去微博内容表中得到数据 * c、将得到的数据封装到Message对象中 */public List&lt;Message&gt; getAttendsContent(String uid)&#123; Connection connection = null; try &#123; connection = ConnectionFactory.createConnection(conf); Table inboxTable = connection.getTable(TableName.valueOf(TABLE_INBOX)); //a、从收件箱中取得微博rowKey Get get = new Get(Bytes.toBytes(uid)); //设置最大版本号 get.setMaxVersions(5); List&lt;byte[]&gt; rowkeys = new ArrayList&lt;byte[]&gt;(); Result result = inboxTable.get(get); for(Cell cell : result.rawCells())&#123; rowkeys.add(CellUtil.cloneValue(cell)); &#125; //b、根据取出的所有rowkey去微博内容表中检索数据 Table contentTable = connection.getTable(TableName.valueOf(TABLE_CONTENT)); List&lt;Get&gt; gets = new ArrayList&lt;Get&gt;(); //根据rowkey取出对应微博的具体内容 for(byte[] rk : rowkeys)&#123; Get g = new Get(rk); gets.add(g); &#125; //得到所有的微博内容的result对象 Result[] results = contentTable.get(gets); //将每一条微博内容都封装为消息对象 List&lt;Message&gt; messages = new ArrayList&lt;Message&gt;(); for(Result res : results)&#123; for(Cell cell : res.rawCells())&#123; Message message = new Message(); String rowKey = Bytes.toString(CellUtil.cloneRow(cell)); String userid = rowKey.substring(0, rowKey.indexOf(&quot;_&quot;)); String timestamp = rowKey.substring(rowKey.indexOf(&quot;_&quot;) + 1); String content = Bytes.toString(CellUtil.cloneValue(cell)); message.setContent(content); message.setTimestamp(timestamp); message.setUid(userid); messages.add(message); &#125; &#125; return messages; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null;&#125; 9) 测试 – 测试发布微博内容 public void testPublishContent(WeiBo wb) – 测试添加关注 public void testAddAttend(WeiBo wb) – 测试取消关注 public void testRemoveAttend(WeiBo wb) – 测试展示内容 public void testShowMessage(WeiBo wb) 代码： /** * 发布微博内容 * 添加关注 * 取消关注 * 展示内容 */public void testPublishContent(WeiBo wb)&#123; wb.publishContent(&quot;0001&quot;, &quot;今天买了一包空气，送了点薯片，非常开心！！&quot;); wb.publishContent(&quot;0001&quot;, &quot;今天天气不错。&quot;);&#125;public void testAddAttend(WeiBo wb)&#123; wb.publishContent(&quot;0008&quot;, &quot;准备下课！&quot;); wb.publishContent(&quot;0009&quot;, &quot;准备关机！&quot;); wb.addAttends(&quot;0001&quot;, &quot;0008&quot;, &quot;0009&quot;);&#125;public void testRemoveAttend(WeiBo wb)&#123; wb.removeAttends(&quot;0001&quot;, &quot;0008&quot;);&#125;public void testShowMessage(WeiBo wb)&#123; List&lt;Message&gt; messages = wb.getAttendsContent(&quot;0001&quot;); for(Message message : messages)&#123; System.out.println(message); &#125;&#125;public static void main(String[] args) &#123; WeiBo weibo = new WeiBo(); weibo.initTable(); weibo.testPublishContent(weibo); weibo.testAddAttend(weibo); weibo.testShowMessage(weibo); weibo.testRemoveAttend(weibo); weibo.testShowMessage(weibo);&#125; 五、总结不一定所有的企业都会使用HBase，大数据的框架可以是相互配合相互依赖的，同时，根据不同的业务，部分框架之间的使用也可以是相互独立的。例如有些企业在处理整个业务时，只是用HDFS+Spark部分的内容。所以在学习HBase框架时，一定要有宏观思维，了解其框架特性，不一定非要在所有的业务中使用所有的框架，要具体情况具体分析，酌情选择。 5.1、HBase在商业项目中的能力每天： 1) 消息量：发送和接收的消息数超过60亿-一天每秒七万次请求 2) 将近1000亿条数据的读写 3) 高峰期每秒150万左右操作 4) 整体读取数据占有约55%，写入占有45% 5) 超过2PB的数据，涉及冗余共6PB数据 6) 数据每月大概增长300千兆字节。 5.2、HBase2.0新特性2017年8月22日凌晨2点左右，HBase发布了2.0.0 alpha-2，相比于上一个版本，修复了500个补丁，我们来了解一下2.0版本的HBase新特性。 最新文档： http://hbase.apache.org/book.html#ttl 官方发布主页： http://mail-archives.apache.org/mod_mbox/www-announce/201708.mbox/%3CCADcMMgFzmX0xYYso-UAYbU7V8z-Obk1J4pxzbGkRzbP5Hps+iA@mail.gmail.com 举例： 1) region**进行了多份冗余** 主region负责读写，从region维护在其他HregionServer中，负责读以及同步主region中的信息，如果同步不及时，是有可能出现client在从region中读到了脏数据（主region还没来得及把memstore中的变动的内容flush）。 2) 更多变动 https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12340859&amp;styleName=&amp;projectId=12310753&amp;Create=Create&amp;atl_token=A5KQ-2QAV-T4JA-FDED%7Ce6f233490acdf4785b697d4b457f7adb0a72b69f%7Clout 六．好的总结文档https://www.cnblogs.com/qcloud1001/p/7615526.html","categories":[],"tags":[]},{"title":"","slug":"oozie总结","date":"2020-05-10T02:20:46.167Z","updated":"2020-05-10T02:20:36.832Z","comments":true,"path":"2020/05/10/oozie总结/","link":"","permalink":"http://kingge.top/2020/05/10/oozie总结/","excerpt":"","text":"layout: wtitle: oozie总结date: 2018-07-18 15:32:22categories: hadoop #分类tags: [oozie,数据采集] 一、Oozie简介Oozie英文翻译为：驯象人。一个基于工作流引擎的开源框架，由Cloudera公司贡献给Apache，提供对Hadoop Mapreduce、Pig Jobs的任务调度与协调。Oozie需要部署到Java Servlet容器中运行。主要用于定时调度任务，多任务可以按照执行的逻辑顺序调度。 二、Oozie的功能模块介绍2.1、模块1) Workflow 顺序执行流程节点，支持fork（分支多个节点），join（合并多个节点为一个） 2) Coordinator 定时触发workflow 3) Bundle Job 绑定多个Coordinator 2.2、常用节点1) 控制流节点（Control Flow Nodes**）** 控制流节点一般都是定义在工作流开始或者结束的位置，比如start,end,kill等。以及提供工作流的执行路径机制，如decision，fork，join等。 2) 动作节点（Action Nodes**）** 负责执行具体动作的节点，比如：拷贝文件，执行某个Shell脚本等等。 三、Oozie的部署3.1、部署3.1.1、解压Oozie $ tar -zxf ~/softwares/installations/cdh/oozie-4.0.0-cdh5.3.6.tar.gz -C ./ 3.1.2、修改Hadoop配置(原来学习的基础上添加如下内容) 我们知道在使用命令行操作hdfs的时候，linux默认使用的是当前登录用户。但是当我们使用oozie去操作hdfs的时候，那么它使用的用户是oozie自己创建的。但是我们知道hdfs相关的目录只有kingge这个用户（此用户是自己创建）才有权限操作，那么oozie想要操作hdfs就需要一个代理，代理kingge这个用户去操作hdfs。下面的两个配置就是这样的作用 core-site.xml &lt;!-- Oozie Server的Hostname --&gt;&lt;property&gt; //允许被用户所代理 &lt;name&gt;hadoop.proxyuser.admin.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 允许被Oozie代理的用户组 --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.admin.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 尖叫提示：*hadoop.proxyuser.admin.hosts**类似属性中的admin**用户替换成你的hadoop**用户。*** mapred-site.xml &lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;linux01:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;linux01:19888&lt;/value&gt;&lt;/property&gt; yarn-site.xml &lt;!-- 任务历史服务 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://linux01:19888/jobhistory/logs/&lt;/value&gt; &lt;/property&gt; 完成后：记得scp同步到其他机器节点 3.1.3、重启Hadoop集群 $ sh ~/start-cluster.sh //这个是自定义的脚本 如果没有。那么需要手动执行 start-dfs.sh 和start-yarn.sh 尖叫提示：需要开启JobHistoryServer, 最好执行一个MR任务进行测试。 sbin/mr-jobhistory-daemon.sh start historyserver 3.1.4、在oozie根目录下解压hadooplibs $ tar -zxf oozie-hadooplibs-4.0.0-cdh5.3.6.tar.gz -C ../ –注意这里是 两个点 .. 完成后Oozie目录下会出现hadooplibs目录。 3.1.5、在Oozie根目录下创建libext目录 – 名字必须是 libext $ mkdir libext/ 3.1.6、拷贝一些依赖的Jar包1) 将hadooplibs**里面的jar包，拷贝到libext目录下：** $ cp -ra hadooplibs/hadooplib-2.5.0-cdh5.3.6.oozie-4.0.0-cdh5.3.6/* libext/ 2) 拷贝Mysql**驱动包到libext**目录下： $ cp -a ~/softwares/installations/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar libext/ 3.1.7、将ext-2.2.zip拷贝到libext/目录下ext是一个js框架，用于展示oozie前端页面： $ cp -a ~/softwares/installations/cdh/ext-2.2.zip libext/ 3.1.8、修改Oozie配置文件oozie-site.xml 属性：oozie.service.JPAService.jdbc.driver属性值：com.mysql.jdbc.Driver解释：JDBC的驱动属性：oozie.service.JPAService.jdbc.url属性值：jdbc:mysql://linux01:3306/oozie解释：oozie所需的数据库地址属性：oozie.service.JPAService.jdbc.username属性值：root解释：数据库用户名属性：oozie.service.JPAService.jdbc.password属性值：123456解释：数据库密码属性：oozie.service.HadoopAccessorService.hadoop.configurations属性值：*=/home/admin/modules/cdh/hadoop-2.5.0-cdh5.3.6/etc/hadoop解释：让Oozie引用Hadoop的配置文件 3.1.9、在Mysql中创建Oozie的数据库进入Mysql并创建oozie数据库： $ mysql -uroot -p123456 mysql&gt; create database oozie; 3.1.10、初始化Oozie1) 上传Oozie**目录下的yarn.tar.gz文件到HDFS：** 尖叫提示：yarn.tar.gz文件会自行解压 $ bin/oozie-setup.sh sharelib create -fs hdfs://linux01:8020 -locallib oozie-sharelib-4.0.0-cdh5.3.6-yarn.tar.gz 执行成功之后，去50070检查对应目录有没有文件生成。 存储oozie和各个框架的依赖的jar包 2) 创建oozie.sql**文件** $ bin/oozie-setup.sh db create -run -sqlfile oozie.sql 3) 打包项目，生成war**包** $ bin/oozie-setup.sh prepare-war 3.1.11、启动Oozie服务（附关闭Oozie服务） $ bin/oozied.sh start 启动成功后生成 Bootstrap 进程 如需正常关闭Oozie服务，请使用： $ bin/oozied.sh stop 3.1.12、访问Oozie的Web页面 http://linux01:11000/oozie 四、Oozie的使用4.1、案例一：Oozie调度shell脚本目标：使用Oozie调度Shell脚本 分步实现： 1) 解压官方案例模板 $ tar -zxf oozie-examples.tar.gz 2) 创建工作目录 $ mkdir oozie-apps/ 3) 拷贝任务模板到oozie-apps/**目录** $ cp -r examples/apps/shell/ oozie-apps/ 4) 随意编写一个脚本p1.sh $ vi oozie-apps/shell/p1.sh内容如下：#!/bin/bash/sbin/ifconfig &gt; /opt/module/p1.log 5) 修改job.properties**和workflow.xml**文件 job.properties #HDFS地址nameNode=hdfs://linux01:8020#ResourceManager地址jobTracker=linux02:8032#队列名称queueName=defaultexamplesRoot=oozie-appsoozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/shellEXEC=p1.sh workflow.xml &lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt;&lt;start to=&quot;shell-node&quot;/&gt;&lt;action name=&quot;shell-node&quot;&gt; &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt; &lt;!-- &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt; --&gt; &lt;file&gt;/user/admin/oozie-apps/shell/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;capture-output/&gt; &lt;/shell&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt;&lt;/action&gt;&lt;decision name=&quot;check-output&quot;&gt; &lt;switch&gt; &lt;case to=&quot;end&quot;&gt; $&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;] eq &apos;Hello Oozie&apos;&#125; &lt;/case&gt; &lt;default to=&quot;fail-output&quot;/&gt; &lt;/switch&gt;&lt;/decision&gt;&lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;&lt;/kill&gt;&lt;kill name=&quot;fail-output&quot;&gt; &lt;message&gt;Incorrect output, expected [Hello Oozie] but was [$&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;]&#125;]&lt;/message&gt;&lt;/kill&gt;&lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; 6) 上传任务配置 $ ~/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -put oozie-apps/ /user/admin 7) 执行任务 $ bin/oozie job -oozie http://linux01:11000/oozie -config oozie-apps/shell/job.properties -run 执行成功后： 去oozie**浏览器端查看即可** 8) 杀死某个任务 $ bin/oozie job -oozie http://linux01:11000/oozie -kill 0000004-170425105153692-oozie-z-W 4.2、案例二：Oozie逻辑调度执行多个Job目标：使用Oozie执行多个Job调度 分步执行： 1) 解压官方案例模板 $ tar -zxf oozie-examples.tar.gz 2) 编写脚本 $ vi oozie-apps/shell/p2.sh 内容如下： #!/bin/bash /bin/date &gt; /tmp/p2.log 3) 修改job.properties**和workflow.xml**文件 job.properties nameNode=hdfs://linux01:8020jobTracker=linux02:8032queueName=defaultexamplesRoot=oozie-appsoozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/shellEXEC1=p1.shEXEC2=p2.sh workflow.xml &lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt; &lt;start to=&quot;p1-shell-node&quot;/&gt; &lt;action name=&quot;p1-shell-node&quot;&gt; &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC1&#125;&lt;/exec&gt; &lt;file&gt;/user/admin/oozie-apps/shell/$&#123;EXEC1&#125;#$&#123;EXEC1&#125;&lt;/file&gt; &lt;!-- &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt;--&gt; &lt;capture-output/&gt; &lt;/shell&gt; &lt;ok to=&quot;p2-shell-node&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;action name=&quot;p2-shell-node&quot;&gt; &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC2&#125;&lt;/exec&gt; &lt;file&gt;/user/admin/oozie-apps/shell/$&#123;EXEC2&#125;#$&#123;EXEC2&#125;&lt;/file&gt; &lt;!-- &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt;--&gt; &lt;capture-output/&gt; &lt;/shell&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;decision name=&quot;check-output&quot;&gt; &lt;switch&gt; &lt;case to=&quot;end&quot;&gt; $&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;] eq &apos;Hello Oozie&apos;&#125; &lt;/case&gt; &lt;default to=&quot;fail-output&quot;/&gt; &lt;/switch&gt; &lt;/decision&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;kill name=&quot;fail-output&quot;&gt; &lt;message&gt;Incorrect output, expected [Hello Oozie] but was [$&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;]&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; 4) 上传任务配置 $ ~/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -rmr /user/admin/oozie-apps/ $ ~/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -put oozie-apps/ /user/admin 5) 执行任务 $ bin/oozie job -oozie http://linux01:11000/oozie -config oozie-apps/shell/job.properties -run 4.3、案例三：Oozie调度MapReduce任务目标：使用Oozie调度MapReduce任务 分步执行： 1) 找到一个可以运行的mapreduce**任务的jar**包（可以用官方的，也可以是自己写的） 2) 拷贝官方模板到oozie-apps $ cp -r examples/apps/map-reduce/ oozie-apps/ 3) 测试一下wordcount**在yarn**中的运行 $ ~/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/yarn jar ~/modules/cdh/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar wordcount /input/ /output/ 4) 配置map-reduce**任务的job.properties**以及workflow.xml job.properties nameNode=hdfs://linux01:8020jobTracker=linux02:8032queueName=defaultexamplesRoot=oozie-apps#hdfs://linux01:8020/user/admin/oozie-apps/map-reduce/workflow.xmloozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/map-reduce/workflow.xmloutputDir=map-reduce workflow.xml &lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.2&quot; name=&quot;map-reduce-wf&quot;&gt; &lt;start to=&quot;mr-node&quot;/&gt; &lt;action name=&quot;mr-node&quot;&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; //在执行某个命令之前，准备工作，删除某个目录 &lt;delete path=&quot;$&#123;nameNode&#125;/output/&quot;/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置调度MR任务时，使用新的API --&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.new-api&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.new-api&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Job Key输出类型 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.output.key.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.Text&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Job Value输出类型 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.output.value.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.IntWritable&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定输入路径 --&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/input/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定输出路径 --&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/output/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Map类 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.map.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.examples.WordCount$TokenizerMapper&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Reduce类 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.reduce.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.examples.WordCount$IntSumReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; 5) 拷贝待执行的jar**包到map-reduce的lib目录下** $ cp -a ~/modules/cdh/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar oozie-apps/map-reduce/lib 6) 上传配置好的app**文件夹到HDFS** $ ~/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -put oozie-apps/map-reduce/ /user/admin/oozie-apps 7) 执行任务 $ bin/oozie job -oozie http://linux01:11000/oozie -config oozie-apps/map-reduce/job.properties -run 4.4、案例四：Oozie定时任务/循环任务目标：Coordinator周期性调度任务 分步实现： 1) 配置Linux**时区以及时间服务器** 检查系统当前时区： # date -R 注意这里，如果显示的时区不是+0800，你可以删除localtime文件夹后，再关联一个正确时区的链接过去，命令如下： rm -rf /etc/localtime # ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime同步时间： ntpdate pool.ntp.org修改NTP配置文件： # vi /etc/ntp.conf去掉下面这行前面的# ,并把网段修改成自己的网段：restrict 192.168.122.0 mask 255.255.255.0 nomodify notrap注释掉以下几行：#server 0.centos.pool.ntp.org#server 1.centos.pool.ntp.org#server 2.centos.pool.ntp.org把下面两行前面的#号去掉,如果没有这两行内容,需要手动添加server 127.127.1.0 # local clockfudge 127.127.1.0 stratum 10 重启NTP服务： # systemctl start ntpd.service，注意，如果是centOS7以下的版本，使用命令：service ntpd start# systemctl enable ntpd.service，注意，如果是centOS7以下的版本，使用命令：chkconfig ntpd on 集群其他节点去同步这台时间服务器时间： 首先需要关闭这两台计算机的ntp服务# systemctl stop ntpd.service，centOS7以下，则：service ntpd stop# systemctl disable ntpd.service，centOS7以下，则：chkconfig ntpd off# systemctl status ntpd，查看ntp服务状态# pgrep ntpd，查看ntp服务进程id同步第一台服务器linux01的时间：# ntpdate linux01 使用root用户制定计划任务,周期性同步时间： crontab -e /10 * /usr/sbin/ntpdate linux01重启定时任务： systemctl restart crond.service， centOS7以下使用：service crond restart，其他台机器的配置同理。 2) 配置oozie-site.xml**文件** 属性：oozie.processing.timezone 属性值：GMT+0800 解释：修改时区为东八区区时 尖叫提示：该属性去oozie-default.xml中找到即可 3) 修改js**框架中的关于时间设置的代码** $ vi ~/modules/cdh/oozie-4.0.0-cdh5.3.6/oozie-server/webapps/oozie/oozie-console.js 修改如下： function getTimeZone() { Ext.state.Manager.setProvider(new Ext.state.CookieProvider()); return Ext.state.Manager.get(“TimezoneId”,”GMT+0800”); } 4) 重启oozie**服务，并重启浏览器（一定要注意清除缓存）** $ bin/oozied.sh stop $ bin/oozied.sh start 5) 拷贝官方模板配置定时任务 $ cp -r examples/apps/cron/ oozie-apps/ 6) 修改模板job.properties**和coordinator.xml**以及workflow.xml job.properties nameNode=hdfs://linux01:8020jobTracker=linux02:8032queueName=defaultexamplesRoot=oozie-appsoozie.coord.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron#start：必须设置为未来时间，否则任务失败start=2017-07-29T17:00+0800end=2017-07-30T17:00+0800workflowAppUri=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cronEXEC1=p1.shEXEC2=p2.sh coordinator.xml &lt;coordinator-app name=&quot;cron-coord&quot; frequency=&quot;$&#123;coord:minutes(5)&#125;&quot; start=&quot;$&#123;start&#125;&quot; end=&quot;$&#123;end&#125;&quot; timezone=&quot;GMT+0800&quot; xmlns=&quot;uri:oozie:coordinator:0.2&quot;&gt;&lt;action&gt; &lt;workflow&gt; &lt;app-path&gt;$&#123;workflowAppUri&#125;&lt;/app-path&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;jobTracker&lt;/name&gt; &lt;value&gt;$&#123;jobTracker&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;nameNode&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;queueName&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/workflow&gt;&lt;/action&gt;&lt;/coordinator-app&gt; workflow.xml &lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.5&quot; name=&quot;one-op-wf&quot;&gt;&lt;start to=&quot;p3-shell-node&quot;/&gt; &lt;action name=&quot;p3-shell-node&quot;&gt; &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC3&#125;&lt;/exec&gt; &lt;file&gt;/user/admin/oozie-apps/cron/$&#123;EXEC3&#125;#$&#123;EXEC3&#125;&lt;/file&gt; &lt;!-- &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt;--&gt; &lt;capture-output/&gt; &lt;/shell&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt;&lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;&lt;/kill&gt;&lt;kill name=&quot;fail-output&quot;&gt; &lt;message&gt;Incorrect output, expected [Hello Oozie] but was [$&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;]&#125;]&lt;/message&gt;&lt;/kill&gt;&lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; 7) 上传配置 $ ~/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -put oozie-apps/cron/ /user/admin/oozie-apps 8) 启动任务 $ bin/oozie job -oozie http://linux01:11000/oozie -config oozie-apps/cron/job.properties -run 尖叫提示：oozie允许的最小执行任务的频率是5分钟 五、可能遇到的问题总结1) Mysql**权限配置** 授权所有主机可以使用root用户操作所有数据库和数据表 mysql&gt; grant all on . to root@’%’ identified by ‘123456’; mysql&gt; flush privileges; mysql&gt; exit; 2) workflow.xml**配置的时候不要忽略file**属性 3) jps**查看进程时，注意有没有bootstrap** 4) 关闭oozie 如果bin/oozied.sh stop无法关闭，则可以使用kill -9 [pid]，之后oozie根目录下的oozie-server/temp/xxx.pid文件一定要删除。 5) Oozie**重新打包时，一定要注意先关闭进程，删除对应文件夹下面的pid文件。（可以参考第4条目）** 6) 配置文件一定要生效 起始标签和结束标签无对应则不生效，配置文件的属性写错了，那么则执行默认的属性。 7) libext**下边的jar存放于某个文件夹中，导致share/lib创建不成功。** 8) 调度任务时，找不到指定的脚本，可能是oozie-site.xml**里面的Hadoop**配置文件没有关联上。 9) 修改Hadoop**配置文件，需要重启集群。一定要记得scp**到其他节点。 10) JobHistoryServer**必须开启，集群要重启的。** 11) Mysql**配置如果没有生效的话，默认使用derby**数据库。 12) 在本地修改完成的job**配置，必须重新上传到HDFS**。 13) 将HDFS**中上传的oozie**配置文件下载下来查看是否有错误。 14) Linux**用户名和Hadoop**的用户名不一致。 15**）sharelib**找不到，包括重新初始化oozie 如果部署oozie**出错，修复执行，初始化oozie**： 1**、停止oozie（要通过jps检查bootstrap**进程是否已经不存在） 2**、删除oozie-server/temp/*** 3**、删除HDFS上的sharelib文件夹** 4**、删除oozie.sql文件，删除Mysql中删除oozie**库，重新创建 5**、重新按照顺序执行文档中“初始化oozie”**这个步骤 16**）*因为*oozie**的任务是放在yarn**上执行的，那么实际上就是调度nodemanager**来执行oozie**任务，那么有可能机会存在一个问题。你oozie**脚本操作的是集群中hadoop101**这台服务器的某个文件，但是因为hadoop101**的nodemanager**没有空闲时间处理当前oozie**任务，那就就会交给其他服务器的nodemanager**执行，那么在执行脚本的时候，发现找不到需要的文件，可能就会报错。因为文件是存储在hadoop101**中**","categories":[],"tags":[]},{"title":"Hessian 多系统访问","slug":"Hessian 多系统访问","date":"2020-05-06T13:26:31.834Z","updated":"2017-08-31T09:44:24.038Z","comments":true,"path":"2020/05/06/Hessian 多系统访问/","link":"","permalink":"http://kingge.top/2020/05/06/Hessian 多系统访问/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"下个开源项目的计划","slug":"下个开源项目的计划","date":"2020-04-28T12:59:59.000Z","updated":"2020-05-04T08:28:49.785Z","comments":true,"path":"2020/04/28/下个开源项目的计划/","link":"","permalink":"http://kingge.top/2020/04/28/下个开源项目的计划/","excerpt":"","text":"1.已经做的事​ 首先我们不会重复造轮子，因为如果市面上已经存在很好很优秀的框架或者解决方案，那么我建议使用那些成熟的解决方案。 ​ 那么比如说，我之前实现的可靠消息最终一致性方案（rtm），就是利用了其他mq没有事务消息这个特征(Rocketmq独有)，但是又想在不改用mq的前提下，那么又想保证数据最终一致性，那么就可以使用rtm系统来作为中间商 协调上下游系统的事务处理，保证了数据的最终一致性。 ​ 话句话说，RTM实际上是基于本地消息表这种解决方案来实现的。 而且实际上rtm的理念其实就是跟rocketmq的事务消息类似，但是rtm更加全面。 ​ rocketmq事务消息的弊端​ 我们知道， 事务消息 仅仅只是保证本地事务和MQ消息发送形成整体的 原子性，而投递到MQ服务器后，并无法保证消费者一定能消费成功！也就是下游系统可能会发送失败。 ​ 如果 消费端消费失败 后的处理方式，建议是记录异常信息然后 人工处理，并不建议回滚上游服务的数据(因为两者是 解耦 的，而且 回滚复杂度 太高) ​ 那么如果使用事务消息怎么解决下游系统消费失败的问题呢？ 常见两个解决方案： 我们可以利用 MQ 的两个特性 重试 和 死信队列 来协助消费端处理： 消费失败后mq进行一定次数的 重试 重试后也失败的话该消息丢进 死信队列 里 另外起一个线程监听消费 死信队列 里的消息，记录日志并且预警！ 因为有 重试 所以消费者需要实现 幂等性 RTM系统优点 - 广告一波​ 在兼有rocketmq事务消息的同时，能够保证下游系统一定能消费消息（提供消费失败一定次数和时间间隔重试以及记录超过重试次数的消息），从而保证了数据的最终一致性，同时提供管理界面，管理已经超过重发次数上限的消息，重新发送。 ​ 所以说，当你的项目架构在最初的技术选型时，并没有使用rocketmq，那么又想保证数据最终一致性，那么就可以引入rtm系统，非常方便快捷。 2.分布式事务选型分布式解决方案，一般有如下几种： XA分布式协议 2pc - 一般不适用 3pc TCC Try、Confirm、Cancel，实际上用到了补偿的概念 这种方案说实话几乎很少用人使用，我问过得人，用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿，会造成补偿代码巨大，非常之恶心。 ​ 比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用TCC，严格严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，在资金上出现问题。 阿里开源的seata，但是阿里的seata并不是纯正的tcc框架，因为 纯正的tcc框架，很麻烦，需要你手动把各种接口实现出来3个接口，try，confirm，cancel。bytetcc框架，就是一个纯的tcc框架，可以了解一下。 可靠消息最终一致性 推荐使用 本地消息表 最大努力通知方案 这个方案的大致意思就是： 1）系统A本地事务执行完之后，发送个消息到MQ 2）这里会有个专门消费MQ的最大努力通知服务，这个服务会消费MQ然后写入数据库中记录下来，或者是放入个内存队列（DelayQueue）也可以，接着调用系统B的接口 3）要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃。 实际上rtm系统，已经实现了可靠消息最终一致性和本地消息表这两种方案。 3.自我实现最大努力通知方案最大努力通知与可靠消息一致性有什么不同 1、解决方案思想不同 ​ 可靠消息一致性，发起通知方需要保证将消息发出去，并且将消息发到接收通知方，消息的可靠性关键由发起通知 方来保证。 ​ 最大努力通知，发起通知方尽最大的努力将业务处理结果通知给接收通知方，但是可能消息接收不到，此时需要接收通知方主动调用发起通知方的接口查询业务处理结果，通知的可靠性关键在接收通知方。 2、两者的业务应用场景不同 ​ 可靠消息一致性关注的是交易过程的事务一致，以异步的方式完成交易。 ​ 最大努力通知关注的是交易后的通知事务，即将交易结果可靠的通知出去。 3、技术解决方向不同 ​ 可靠消息一致性要解决消息从发出到接收的一致性，即消息发出并且被接收到。 ​ 最大努力通知无法保证消息从发出到接收的一致性，只提供消息接收的可靠性机制。可靠机制是，最大努力的将消 息通知给接收方，当消息无法被接收方接收时，由接收方主动查询消息（业务处理结果）。 也就是我们要实现的最大通知服务关注的是跟下游服务之间的可靠性通信，而可靠消息一致性中可靠消息服务（rtm）关注的是跟上游的数据一致性，同时保证消息一定能发送到下游。","categories":[{"name":"开源项目","slug":"开源项目","permalink":"http://kingge.top/categories/开源项目/"}],"tags":[{"name":"开源项目","slug":"开源项目","permalink":"http://kingge.top/tags/开源项目/"},{"name":"最大努力通知方案","slug":"最大努力通知方案","permalink":"http://kingge.top/tags/最大努力通知方案/"}]},{"title":"可靠消息解决数据最终一致性-开源项目rtm","slug":"rtm数据最终一致性","date":"2020-04-18T14:57:58.000Z","updated":"2020-05-04T08:15:02.308Z","comments":true,"path":"2020/04/18/rtm数据最终一致性/","link":"","permalink":"http://kingge.top/2020/04/18/rtm数据最终一致性/","excerpt":"","text":"基础概念为什么使用消息队列？MQ出现的原因/优点在回答这个问题之前，我们想一下，在没有消息队列之前我们的多个业务相互调用时他的逻辑实现是怎么样的？ 画个图： ​ 这是一个很普通的业务调用，也是我们写的比较多的。整个业务逻辑是这样的，客户下订单，调用订单服务生成订单，那么在生成订单的时候，会去调用库存服务减少库存，再去调用用户服务查询用户信息。 ​ 这样咋一看没有什么问题，但是如果后面订单服务业务改进，需要在下订单的同时，需要查询优惠券服务，查询下订单时所使用的优惠券信息。那么这个时候，你必然要修改订单服务，然后增加调用的代码逻辑。 ​ 如下图： 而且后面如果订单服务还需要去调用其他服务，那么你就还需要疯狂的修改订单服务。 而且，例如后面业务改进，订单服务不需要再调用优惠券服务了？那么你又得在订单服务中移除该服务。 非但如此，订单服务要时时刻刻考虑调用的库存服务、用户服务等等系统如果挂了咋办？我要不要重发？我要不要把消息存起来？？？？ 总而言之，订单服务所需要承载的责任太重了，而且需要负责的东西太多，这就是耦合。 那么MQ的第一个作用就体现出来了：解耦 上面的服务使用MQ后： 那么经过改进后，整个订单服务就舒服多了，也清净了。订单服务处理完用户下单请求后，只需要发送一条消息到MQ中，代表我下单了，然后整个订单服务就结束了，不需要同步等待库存服务等等其他服务的响应。 ​ 库存服务，只需要订阅MQ的主题，然后完成自己的业务逻辑即可。 那么MQ还有其他什么优点呢？ 其实在上面我已经说出来了，你有没有发现，在上面的例图中，我们引入了MQ，使得订单服务不再需要同步等待调用其他服务的返回结果。 所以MQ的第二个优点就是：异步 ​ 画个图来说明一下，A系统接收一个请求，需要在自己本地写库，还需要在BCD三个系统写库，自己本地写库要3ms，BCD三个系统分别写库要300ms、450ms、200ms。最终请求总延时是3 + 300 + 450 + 200 = 953ms，接近1s，用户感觉搞个什么东西，慢死了慢死了。 未使用MQ前 使用MQ后 MQ的第三个优点：削峰 ​ 每天0点到11点，A系统风平浪静，每秒并发请求数量就100个（qps=100）。结果每次一到11点~1点，每秒并发请求数量突然会暴增到1万条。但是系统最大的处理能力就只能是每秒钟处理1000个请求啊。。。尴尬了，系统会死。。。 **所以MQ充当的角色类似于一个水库。汛期来临时水库首当其冲，起到缓冲作用，避免大量水流冲击下游。这个时候水库只需要开放部分出口，然后水流慢慢的留向下游，最后把水库里的水慢慢消耗最终接近平缓即可。** 未用MQ前？ 使用mq后 总而言之： 队列的常见使用场景吧，其实场景有很多，但是比较核心的优点有3个：解耦、异步、削峰 MQ缺点缺点呢？显而易见的 系统可用性降低：系统引入的外部依赖越多，越容易挂掉，本来你就是A系统调用BCD三个系统的接口就好了，人ABCD四个系统好好的，没啥问题，你偏加个MQ进来，万一MQ挂了咋整？MQ挂了，整套系统崩溃了，你不就完了么。（需要保证MQ高可用 - 增加系统复杂程度） 系统复杂性提高：硬生生加个MQ进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已，搞什么捏。（消息可靠性和消费幂等性） 一致性问题：A系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是BCD三个系统那里，BD两个系统写库成功了，结果C系统写库失败了，咋整？你这数据就不一致了。（分布式事务/数据最终一致性） ​ 所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，最好之后，你会发现，妈耶，系统复杂度提升了一个数量级，也许是复杂了10倍。但是关键时刻，用，还是得用的。。。但是得想好。 https://www.showdoc.cc/rmq?page_id=1796661553395018 怎么解决数据的一致性分布式事务 - 强一致性数据最终一致性 - 弱一致性CAP理论​ 2000年7月，加州大学伯克利分校的Eric Brewer教授在ACM PODC会议上提出CAP猜想。Brewer认为在设计一个大规模的分布式系统时会遇到三个特性：一致性（consistency）、可用性（Availability）、分区容错（partition-tolerance），而一个分布式系统最多只能满足其中的2项。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。 C：数据一致性（强一致性），集群中同一数据的多个副本是否实时相同。（一致性也分为，强一致性和弱一致性） A：可用性，指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。 P:分区容错性，也就是如果出现网络震荡的时候，服务集群不能够全部挂掉，保证高可用。将同一服务分布在多个系统中，从而保证某一个系统宕机，仍然有其他系统提供相同的服务。 ​ 在分布式系统中，P肯定要保证的。 ​ 为什么要保证p呢？ 当业务量猛增，单个服务器已经无法满足我们的业务需求的时候，就需要使用分布式系统，使用多个节点提供相同的功能（需要部署服务集群），从而整体上提升系统的性能，这就是使用分布式系统的第一个原因。那么分区容错性就必须满足。 ​ 那么我们只能在C和A中二选一，为什么A和C不能够同时选择呢？举个简单例子。我们在部署了五个订单服务，组成了集群。有一天需求修改了，那么需要更新这五个订单服务。这个时候，我们为了保证客户还是能够访问系统，那么就升级部分服务器，也就意味着，必然存在客户调用了新旧订单服务。那么这个时候数据肯定是不一致的。反之亦然，为了保证数据一致性，我们关闭五个服务器，然后更新后再重启，那么在这期间服务对外是不可用的，无法保证系统可用性。 ​ 例如我们在使用springcloud的Eureka服务注册中心时，他实现的机制就是 AP，能够保证服务的使用。 ​ DUbbo的服务注册中心采用的是Zookeeper，那么因为zookeeper集群（选举机制），他实现的机制是CP（但是他的C数据一致性是，数据弱一致性，也就是存在某个时间salve的数据是不一致的–因为zk 的是更新操作是采取投票过半机制决定本次更新是否成功。） ​ 所以具体的业务场景采用不同的策略，不过大多数互联网项目采用的是AP机制，以能够保证服务的使用为主。 因为在大谈用户体验的今天，如果业务系统时常出现“系统异常”、响应时间过长等情况，这使得用户对系统的好感度大打折扣。所以可用性还是要保证的 BASE理论​ CAP理论告诉我们一个悲惨但不得不接受的事实——我们只能在C、A、P中选择两个条件。而对于业务系统而言，我们往往选择牺牲一致性来换取系统的可用性和分区容错性。不过这里要指出的是，所谓的“牺牲一致性”并不是完全放弃数据一致性，而是牺牲强一致性换取弱一致性 ​ BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。 ​ BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。 BA：基本可用（Basically Available） ​ 指分布式系统在出现不可预知故障的时候，允许损失部分可用性。 S：软状态（ Soft State） ​ 指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性。 E：最终一致（ Eventual Consistency） ​ 强调的是所有的数据更新操作，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 base理论的核心就是，经过一段时间后，能够保证数据的强一致性，在这之前，数据的一致性不是那么的强调。 ​ 举个简单的例子，例如在双十一购物中心，某件商品的点赞数量我们需不需要它实时的显示数量？很明显是不需要的，我们显示的数量可以使几个小时之前的。我们只需要在双十一过后，再去统计出最终该商品的点赞数量，然后显示即可。 ​ 其实大部分最终一致性是通过消息队列的方式实现的。例如我们对商品的点赞，发送一条消息到消息队列，然后消息消费者消费消息，实现点赞数量增加。这样的流程肯定会存在一定的延时，但是最终结果肯定是正确的，所以实现了最终一致性。 rocketmq事务消息设么是事务消息？先看案例场景 例如下图的场景：生成订单记录 -&gt; MQ -&gt; 增加积分 我们是应该先 创建订单记录，还是先 发送MQ消息 呢？ 先发送MQ消息：这个明显是不行的，因为如果消息发送成功，而订单创建失败的话是没办法把消息收回来的。因为发送消息后，下游消费者，已经消费提交。 先创建订单记录：如果订单创建成功后MQ消息发送失败 抛出异常，因为两个操作都在本地事务中所以订单数据是可以 回滚 的。 上面的 方式二 看似没问题，但是 网络是不可靠的！如果 MQ 的响应因为网络原因没有收到，所以在面对不确定的结果只好进行回滚；但是 MQ 端又确实是收到了这条消息的，只是回给客户端的 响应丢失 了！ 所以 事务消息 就是用来保证 本地事务 与 MQ消息发送 的原子性！ 原理图一 主要的逻辑分为两个流程： 事务消息发送及提交： 发送 half消息 MQ服务端 响应消息写入结果 根据发送结果执行 本地事务（如果写入失败，此时half消息对业务 不可见，本地逻辑不执行） 根据本地事务状态执行 Commit 或者 Rollback（Commit操作生成消息索引，消息对消费者 可见） 回查流程： 对于长时间没有 Commit/Rollback 的事务消息（pending 状态的消息），mq服务端发起一次 回查 Producer 收到回查消息，检查回查消息对应的 本地事务状态 根据本地事务状态，重新 Commit 或者 Rollback 逻辑时序图 rocketmq事务消息的弊端根据图1，你会发现，存在一个问题，那就是假设库存服务成功消费到了消息，但是删减库存失败。那么因为删除失败也是属于业务的一部分，那么他就会返回ack消息给，mq。那么整个流程结束。 从上面的原理可以发现 事务消息 仅仅只是保证本地事务和MQ消息发送形成整体的 原子性，而投递到MQ服务器后，并无法保证消费者一定能消费成功！ 如果 消费端消费失败 后的处理方式，建议是记录异常信息然后 人工处理，并不建议回滚上游服务的数据(因为两者是 解耦 的，而且 复杂度 太高) 我们可以利用 MQ 的两个特性 重试 和 死信队列 来协助消费端处理： 消费失败后进行一定次数的 重试 重试后也失败的话该消息丢进 死信队列 里 另外起一个线程监听消费 死信队列 里的消息，记录日志并且预警！ 因为有 重试 所以消费者需要实现 幂等性 总而言之，rocketmq实现事务消息的两个核心概念：两阶段提交、事务状态定时回查 我要做什么？项目介绍​ 因为我们知道rocketmq是原生支持事务消息的，但是如果项目中，最初选型的时候，并没有选用rmq，而是选用了其他的MQ，例如rabbitmq，activemq，kafka等等，但是又想保证最终一致性事务呢？ ​ 那么我们仿照上面rmq的事务消息的原理，来自己实现一个提供事务消息的项目工程。 ​ 那么这个项目我决定命名为RTM（滑稽脸）。 RTM( Reliable transaction message )是基于可靠消息的最终一致性的分布式事务解决方案。 框架定位 RTM本身不生产消息队列，只是消息的搬运工。RTM框架提供消息预发送、消息发送、消息确认、消息恢复、消息管理等功能，结合成熟的消息中间件，解决分布式事务，达到数据最终一致性。 RTM解决的问题 引入消息中间件的场景 ​ 存在问题，1处和2处，可能因为网络原因，导致数据一致性的问题产生，也就是无法保证A系统和B系统数据的一致性，无法保证。 ​ 举个例子，A系统发送完消息到MQ后，在执行自己业务过程中出现异常，本地事务回滚。但是此时消息已经发到MQ，下游服务B系统已经消费消息，B系统执行完自己的业务。那么此时A系统失败，B系统成功。这样就造成了整个业务流不是原子的，存在数据不一致性。 ​ 上面的场景还存在着很多数据不一致性的场景。这里就不一一列举，下面在讲解到RTM的细节时会一一说明。 引入RTM后的场景 ​ 可以看到我们在RTM中引进了，rocketmq的事务消息的概念，RTM在这里仅仅只是作为一个协调者，协调上下游服务的业务操作。确保了是上下游服务能够保证数据的一致性，达到数据最终一致性，符合BASE理论。 ​ RTM提供了，发送半消息、半消息确认、消息发送到MQ、消息消费确认、消息重复投递等等功能。 RTM详细流程介绍​ 通过上面的阐述，我们大概知道了RTM在分布式系统中的地位，协调者。但是至于他的请求流程，怎么保证数据一致性，下面我们拉一一说明。 ​ 我们将从三个方面来说明 正常使用流程 消息发送到RTM，RTM成功投递流程 RTM确认消费者成功消费流程 正常流程 梳理一下流程： （1）首先A系统在执行业务之前，先投递半消息到RTM，RTM持久化成功后（这里使用mysql），发送成功消息给A系统，紧接着A系统执行本地业务。（一般这段逻辑需要开启本地事务，这样可以保证了消息发送成功后再执行本地业务，消息发送失败那么也就没有必要执行A系统本地业务了） （2）接着A系统在执行完本地业务后，异步发送确认消息给RTM，然后RTM标记半消息为确认可投递，接着RTM会把消息发送给MQ。 （3）系统B收到MQ的消息，然后执行自己的业务逻辑，之后再调用RTM接口，确认消息已经消费，接着RTM会把该消息从数据库中删除。到这里整个流程结束 解释几个问题。 在步骤二中，为什么需要异步通知RTM消息可投递。 我们反过来想，如果用同步会发生什么？首先来看一下A系统调用RTM系统的伪代码 @Transactional(rollbackFor = RuntimeException.class)//开启事务 public void addOrder(Order order) &#123; // 1.调用RMQ，创建预发送消息 String msgID = rtmService.addHalfMessage(order); // 2.执行业务 。。。。。 // 3.同步调用RTM，确认发送消息 rtmService.confirmHalfMessage(msgID); &#125; 假设在第三处，调用RTM时，因为网络延迟或者其他原因，导致confirmHalfMessage()抛出异常，那么addOrder()方法回滚，A系统执行失败。但是实际上RTM还是调用成功了，也就是意味着RTM会往MQ发送消息，然后B系统收到消息后成功执行。那么此时A、B系统的数据是不一致性。 所以这里需要异步调用RTM，确认消息。目的就是解耦，保证了A系统的正常执行。 但是如果使用异步后？出现A系统成功执行，但是调用RTM确认消息发送失败时，怎么处理呢？这个问题RTM会提供回查机制，确认半消息是删除还是确认投递。 总结​ 可以看到整个RTM正常流程下是能够保证数据的一致性的，满足base理论。 消息发送到RTM，RTM成功投递流程​ 上面我们只是把整个RTM使用的正常流程梳理了一遍，但是在使用过程中肯定会出现很多问题，出现问题的同时可能还会造成数据的不一致性问题，那么怎么保证数据的一致性呢？出现问题怎么解决呢？ ​ ​ 接下来我们来看一下，A系统在跟RTM通信这个阶段出现问题怎么解决？怎么保证数据一致性？ ​ 接下来我们就来分析一下如果上面这六步如果出现了问题，那么RTM是怎么解决的，只要保证了这六步的正确性，那么我们也就保证了消息发送阶段的一致性。 1处发送异常，A系统发送半消息失败。那么因为A系统还没执行本业务（没有执行到4处），而且RTM系统也没有持久化半消息。那么此时不会产生数据不一致，那么RTM系统不需要做处理。 @Transactional(rollbackFor = RuntimeException.class)//开启事务 public void addOrder(Order order) &#123;//A系统调用RTM，伪代码 // 1.调用RMQ，创建预发送消息 String msgID = rtmService.addHalfMessage(order);//上面1处，实际上就是对应这里的代码段，假设这里发生异常，因为addOrder()方法添加了事务，所以addORder()执行失败。数据一致性没有问题。 // 2.执行业务 。。。。。 // 3.同步调用RTM，确认发送消息 rtmService.confirmHalfMessage(msgID); &#125; 2发生异常，RTM持久化半消息失败。同理，因为A系统还没执行本业务（没有执行到4处），而且RTM系统也没有持久化半消息。那么此时不会产生数据不一致，那么RTM系统不需要做处理。 3.处发生异常，RTM持久化消息成功。但是A系统还没执行本业务（没有执行到4处），这个时候数据不一致。那么RTM提供回查机制，RTM的会有定时器定时检查RTM系统中没有确认的消息，向A系统发起请求，请求检查A系统业务状态，如果执行业务失败，那么RTM删除持久化的半消息，否则A系统执行业务成功，那么RTM确认投递消息到MQ。 什么情况下执行2处代码成功，但是上游却报异常呢？例如A系统设定调用 rtmService.addHalfMessage(order)的超时时间是5s，（也就是说在5s内addHalfMessage方法要给我响应，否则我就抛异常），但是addHalfMessage()方法执行需要7s，那么很明显A系统执行addHalfMessage()方法就会超时，然后A系统请求超时抛异常，A系统业务执行失败。但是A系统报异常并不影响RTM继续执行addHalfMessage()的逻辑，此时过了7s，addHalfMessage()执行成功。RTM成功持久化半消息。 所以我们A系统需要提供回查的接口给RTM系统调用，让给RTM系统确认半消息是删除还是投递。 4处发生异常，也就是A系统执行本地业务失败。此时RTM系统已经持久化半消息，那么数据是不一致的。 怎么解决？跟解决上面的问题三一样，也是需要RTM的回查机制，进行回查A系统确定当前持久化的半消息是删除还是投递。 5处发生异常，A系统业务执行成功。但是RTM系统的半消息没有确认投递，数据不一致。 解决办法，还是需要RTM系统的回查功能，进行回查A系统确定当前持久化的半消息是删除还是投递。 6处发生异常，A系统业务执行成功，RTM确认投递消息失败，数据不一致。 解决办法，还是需要RTM系统的回查功能，进行回查A系统确定当前持久化的半消息是删除还是投递。 总结​ 我们发现，除了一二处的异常，未产生数据不一致，不需要RTM系统进行干预外，3456处的异常需要RTM系统的回查机制进行确认RTM系统已经持久化的半消息是删除 还是投递。 ​ 这个时候，我们就应该得出，RTM系统需要具备一个定时回查A系统的模块 分析到这里，我们发现。 上游系统A需要提供一个查询本地业务执行结果的接口。 RTM系统提供，创建半消息接口，确认半消息接口，定时回查A系统业务功能 RTM确认消费者成功消费流程我们上面解决了上游系统跟RTM系统交互的可靠性，那么RTM系统跟下游系统的数据一致性，在呢么解决呢？请看流程图。 同理我们来分析，下面这几处的如果出现了异常RTM系统是如何处理的？ 7处发生异常，RTM系统的确认是否消费功能，会定时检查RTM系统中已经确认可以投递的消息（也即是经过操作6之后的消息），如果存在，那么就会重新投递到MQ中。（也就是说，B系统必须保证接口服务的幂等性，因为可能存在重复消费） 8处发生异常，同理RTM系统也会通过确认是否消费功能，定时重发RTM系统中未被B系统确认消费的消息。 9处发生异常，同理RTM系统也会通过确认是否消费功能，定时重发RTM系统中未被B系统确认消费的消息。 10处发生异常，同理RTM系统也会通过确认是否消费功能，定时重发RTM系统中未被B系统确认消费的消息。 总而言之，如果下游B系统如果没有向RTM系统确认消费消息，那么RTM系统就会通过定时器反复向MQ重发消息。（B系统必须保证接口服务的幂等性，因为可能存在重复消费） 超时重试次数和重试时间但是有个问题，那就是，无限次重发么？ 隔几秒发一次呢？ 关于这个问题，我们可以仿照Rocketmq的重试逻辑，重试多次次后，那么就标记消息为死亡。（类似于rocketmq的死信队列） ​ 同时重试的时间间隔，也是采取递增的方式。例如重复通知时间间隔（单位：分钟） 举例： [0, 1, 4, 10, 30, 60] 第一次立即通知，如果业务方没有返回成功，则1分钟后再次通知。如果业务方还是没有返回成功，则4分钟后再次通知（此时距离第一次通知已经过了5分钟）。以此类推。那么这里一共可以重试几次呢？7次。 七次后还是没有得到下游B系统的确认消费通知，那么就标记当前消息为死亡。 那么消息死亡后怎么办？消息重试次数超过限制次数后，消息就会被移动到死亡表中，然后你可以在后台进行人工重试。 总结 经过上面的分析，我们知道，RTM系统需要提供一个接口，给下游服务B系统，确认消费成功。 到这里，我们RTM系统一共需要哪些接口服务，我们再总结一下： 上游系统A需要提供一个查询本地业务执行结果的接口。 RTM系统提供，创建半消息接口，确认半消息接口，定时回查A系统业务功能 RTM系统提供，确认消费成功接口，定时重发消息功能，定时 RTM项目总结​ 经过上面的总结，我想你应该对于RTM系统的定位，已经流程都已经有了一个比较充分的了解。那么我们接下来就总结一下实现RTM项目所需要的功能模块。 ​ 项目模块 首先上游系统需要提供什么？ 需要提供一个可以查询本地事务结果的接口，方便RTM回查上游系统业务结果，来决定持久化在RTM的半消息是删除还是确认投递。 RTM系统需要提供什么？ （1）message-lifecycle-management模块（消息生命周期管理模块） 创建半消息的接口 提供上游系统在进行本地业务之前进行调用添加半消息。 确认半消息投递接口 提供给上游系统在执行成功本地业务后，调用该接口，实现确认半消息为可投递，并投递消息到MQ中。 确认消息消费接口 提供给下游系统，在成功消费MQ中的消息后，调用确认消息已经消费。 （2）period-check-message模块（定时检查消息模块） 定时回查上游系统业务处理结果接口功能 RTM系统定时调用，确认是否需要投递消息到MQ 定时检查RTM持久化的消息是否已经被下游系统消费功能 RTM系统定时调用，如果没有被消费，那么重复投递到MQ中 定时检查RTM持久化的消息是否超过投递次数（也即是重复发送到mq次数达到上限，需要标记消息为死亡） RTM系统定时调用，同时把消息移动到死亡表中 定时检查RTM持久化的消息是否已经被下游系统消费（如果是那么把已经消费的消息从系统中移除，放到消息历史表中） RTM系统定时调用 ​ 也就是说，这个模块需要提供四个定时器。 下游系统需要提供什么？ 不需要提供什么，但是需要保证消息消费的幂等性。 数据库表（第一版）第一版，因为时间关系，暂时没有提供前台界面，进行管理消息，后面有时间了，会加上去。 （1）数据库rtm 创建数据库rtm CREATE DATABASE /*!32312 IF NOT EXISTS*/`rtm` /*!40100 DEFAULT CHARACTER SET utf8mb4 */;USE `rtm`; （2）数据库表 消息表 drop table if exists t_k_message;create table t_k_message( msg_id varchar(70) not null comment 'id', msg_name varchar(70) comment '消息所属业务', topic varchar(70) comment '消息所属主题(看对接mq使用)', quene_name varchar(70) comment '消息所属队列(看对接mq使用)', msg_content varchar(300) comment '消息内容', msg_status varchar(7) comment '消息状态，0-待确认，1-已确认发送中，2-已消费', msg_d_status varchar(7) comment '消息是否死亡，0-正常，1-已死亡', retry_counts varchar(20) comment '重试次数', check_url varchar(300) comment '消息回查地址', check_timeout varchar(10) comment '消息回查超时时间', check_duration varchar(10) comment '消息回查周期时间，消息未确认时在这时间内需要回查', create_msg_uid varchar(70) comment '消息创建人id', create_msg_time datetime comment '消息创建时间', update_msg_uid varchar(70) comment '消息更新人id', update_msg_time datetime comment '消息更新时间', confirm_msg_time datetime comment '消息确认投递时间', resend_msg_uid varchar(70) comment '消息重发人id', resend_msg_time datetime comment '消息重发时间', primary key (msg_id))ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;alter table t_k_message comment '消息表'; 确认消费消息表（历史表）也就是保存的是msg_status==2的消息 drop table if exists t_k_message_consumed;create table t_k_message_consumed( msg_id varchar(70) not null comment 'id', msg_name varchar(70) comment '消息所属业务', topic varchar(70) comment '消息所属主题(看对接mq使用)', quene_name varchar(70) comment '消息所属队列(看对接mq使用)', msg_content varchar(300) comment '消息内容', msg_status varchar(7) comment '消息状态，0-待确认，1-已确认发送中，2-已消费', msg_d_status varchar(7) comment '消息是否死亡，0-正常，1-已死亡', retry_counts varchar(20) comment '重试次数', check_url varchar(300) comment '消息回查地址', check_timeout varchar(10) comment '消息回查超时时间', check_duration varchar(10) comment '消息回查周期时间，消息未确认时在这时间内需要回查', create_msg_uid varchar(70) comment '消息创建人id', create_msg_time datetime comment '消息创建时间', update_msg_uid varchar(70) comment '消息更新人id', update_msg_time datetime comment '消息更新时间', confirm_msg_time datetime comment '消息确认投递时间', resend_msg_uid varchar(70) comment '消息重发人id', resend_msg_time datetime comment '消息重发时间', primary key (msg_id))ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;alter table t_k_message comment '已消费消息历史表'; 超时死亡消息表 - 保存的是msg_d_status==1的消息 drop table if exists t_k_dead_message;create table t_k_dead_message( msg_id varchar(70) not null comment 'id', msg_name varchar(70) comment '消息所属业务', topic varchar(70) comment '消息所属主题(看对接mq使用)', quene_name varchar(70) comment '消息所属队列(看对接mq使用)', msg_content varchar(300) comment '消息内容', msg_status varchar(7) comment '消息状态，0-待确认，1-已确认发送中，2-已消费', msg_d_status varchar(7) comment '消息是否死亡，0-正常，1-已死亡', retry_counts varchar(20) comment '重试次数', check_url varchar(300) comment '消息回查地址', check_timeout varchar(10) comment '消息回查超时时间', check_duration varchar(10) comment '消息回查周期时间，消息未确认时在这时间内需要回查', create_msg_uid varchar(70) comment '消息创建人id', create_msg_time datetime comment '消息创建时间', update_msg_uid varchar(70) comment '消息更新人id', update_msg_time datetime comment '消息更新时间', confirm_msg_time datetime comment '消息确认投递时间', resend_msg_uid varchar(70) comment '消息重发人id', resend_msg_time datetime comment '消息重发时间', primary key (msg_id))ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;alter table t_k_message comment '消息表'; 项目代码模块结构 RTM项目使用环境 依赖环境 环境 版本 说明 JDK 1.8 MySQL 5.7.25 Zookeeper 3.4.14 kafka 5.15.6 因为原先上下游系统使用的是kafka，所以这里接入的就是kafka（如果是其他的mq则接入你自己的mq即可） -可选 Maven 3.3.9 RTM代码实现父工程 rtm接口层 rtm-api保存rtm，对外暴露的接口 依赖 rtm-pojo 实现三个接口： package com.kingge.rtm.api;import com.kingge.rtm.pojo.TKMessage;/** * @program: rtm * @description: rtm暴露接口，提供上下游服务调用 * @author: JeremyKing * @create: 2020-04-27 11:26 **/public interface IRtmService &#123; /** * @Description: 创建半消息 * @Param: 消息实体 * @return: 消息id * @Author: JeremyKing * @Date: 2020/4/27 0027 */ public String addHalfMessage(TKMessage message); /** * @Description: 根据消息id，确认并发送半消息到mq * @Param: 消息id * @return: void * @Author: JeremyKing * @Date: 2020/4/27 0027 */ public void submitAndSendHalfMessage(String msg_id); /** * @Description: 根据消息id，确认消费消息 * @Param: 消息id * @return: void * @Author: JeremyKing * @Date: 2020/4/27 0027 */ public void confirmConsumeMessage(String msg_id);&#125; 实体类 rtm-pojo保存rtm项目的实体类 根据mybatis-reverse，逆向生成数据库表对应的实体类和mapper、xml文件。 工具层 rtm-common消息生命周期管理服务 rtm-mlmmessage-lifecycle-management模块（消息生命周期管理模块） - 的简写，mlm 定时检查消息模块 rtm-pcmperiod-check-message模块（定时检查消息模块） - 简写，pcm github地址https://github.com/JeremyKinge/rtm.git 怎么使用rtm目前rtm只提供了，dubbo版本，那么也就是在使用时，你需要配置dubbo环境。 上游系统怎么使用？只需要在业务类中，通过dubbo的方式注入rtm服务即可。 import com.alibaba.dubbo.config.annotation.Reference;import com.kingge.rtm.api.IRtmService;@ReferenceIRtmService iRtmService; 例如你的业务逻辑如下：订单支付后，需要给用户增加积分 @Transactional(rollbackFor = RuntimeException.class)//开启事务 public void payOrder(Order order) &#123; // 1.调用RMQ，创建预发送消息 TKMessage message = new TKMessage(); String msgID = iRtmService.addHalfMessage(TKMessage message); // 2.执行业务 。。。。。 // 3.异步调用RTM，确认发送消息 iRtmService.submitAndSendHalfMessage(message.getMsgid()); &#125; 下游系统怎么用？根据上面的例子，下游会去mq订阅积分消息，那么需要在处理订阅的消息方法中。下游服务在处理完消息后，调用： iRtmService.confirmConsumeMessage(message.getMsgid()); 完成确认消息已经消费即可。 需要注意的是：下游消费服务时，需要注意保证消息幂等性，因为可能因为网络等等原因，可能rtm会重发消息。 RTM项目亮点0.首先是给没有提供事务消息机制的mq提供了事务消息的方式 1.超时重试机制，类似rocketmq，根据时间递增间隔重试 2.死亡消息，可以在后台进行重发，手动干预 3.消息重试，采取线程池的方式，根据线程池的coresize大小（避免线程递增到maxsize），去数据库中取相应数量需要重复发送搭到mq的消息。 4.重发消息时，从重发次数大到小进行重发（例如，RTM支持的最大重试次数是7。那么就先查重发次数是6的消息，处理完后。再接着查重发次数是5的消息），保证了优先处理快要到重试次数上限的消息，优先处理。 总结​ 也就是说rtm系统 在兼有rocketmq事务消息的同时，能够保证下游系统一定能消费消息（提供消费失败一定次数和时间间隔重试以及记录超过重试次数的消息），从而保证了数据的最终一致性，同时提供管理界面，管理已经超过重发次数上限的消息，重新发送。 ​ 所以说，当你的项目架构在最初的技术选型时，并没有使用rocketmq，那么又想保证数据最终一致性，那么就可以引入rtm系统，非常方便快捷。 RTM项目存在的缺点1.目前没有支持多种mq的版本（现在只是实现了对接kafka版本） 2.分布式集群的搭建测试 3.项目模块的拆分还不够清晰，例如rtm-pcm模块，可以拆分成两个模块，一个是rtm-pcm-api模块（提供接口，他的实现类在rtm-pcm中），一个是rtm-pcm模块（真正的业务放到这里） 4.rtm-pcm或者rtm-web 模块过于依赖rtm-mlm模块，导致前两者，都需要在application.yaml中配置跟rtm-mlm相同的数据库数据源信息（因为前两者使用了rtm-mlm的mapper调用数据库。）。后面考虑由rtm-mlm提供数据库操作接口，让前两者通过dubbo的方式调用mapper调用数据库。 而不用直接使用rtm-mlm的mapper去调用。 在前两者移除 rtm-mlm依赖，依赖rtm-mlm-api 也就是说在增加一个rtm-mlm-api模块。里面提供操作数据库的接口，rtm-mlm实现接口。","categories":[{"name":"开源项目","slug":"开源项目","permalink":"http://kingge.top/categories/开源项目/"}],"tags":[{"name":"开源项目","slug":"开源项目","permalink":"http://kingge.top/tags/开源项目/"},{"name":"事务消息","slug":"事务消息","permalink":"http://kingge.top/tags/事务消息/"},{"name":"kafka","slug":"kafka","permalink":"http://kingge.top/tags/kafka/"}]},{"title":"java核心技术杂记","slug":"java基础核心技术","date":"2020-03-09T04:57:58.000Z","updated":"2020-05-09T08:56:11.451Z","comments":true,"path":"2020/03/09/java基础核心技术/","link":"","permalink":"http://kingge.top/2020/03/09/java基础核心技术/","excerpt":"","text":"String​ 通过日常编码，发现，我们在编程的过程中，使用频率最高的变量或者对象，往往是字符串（可以通过分析jvm内存，可得，string类型的数据一般占用的内存排行在最前列），那么怎么优化string类型的字符串成为了一个重点。 ​ 而且优化的目的就是，复用已经存在的字符串，让他的存取类比于java基本类型的存取。 string并不是基本类型。 通过看源码，发现String是一个final不可变的，换言说，如果你给一个String变量重新赋值，那么最终是会重新建立一个string类型数据复制给他，而不会覆盖之前的值。final修饰的成员变量，因此任何看似对String内容进行操作的方法，实际上都是返回了一个新的String对象，这就造就了一个String对象的被创建后，就一直会保持不变（所以要警惕，string的多次修改，因为会创建很多string值，那么常量池可能会占尽） public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; 同时我们知道，string的值，是存放在jvm方法区中，具体位置是，方法区的常量池。这样做的好处就是，能够共享已经存在的string数据，避免重复创建。 举个简单例子。 string a = &quot;123&quot;;string b = &quot;123&quot;;a == b? 答案是true，为什么，因为创建a 变量时，就会在常量池中创建 123字符串，然后把该字符串在常量池的地址，复制给a变量。当发现b的值也是123的时候，那么他会现在常量池中寻找是否存在123字符串，如果存在，那么就直接把123的地址复制给b变量。 string a = &quot;123&quot;;string b = &quot;12&quot;;b+=&quot;3&quot;a == b?答案，肯定也是true，因为b+=&quot;3&quot;会发生字符串堆叠，编译器会优化，生成123。但是我们知道a已经在常量池中创建了123，那么就会把地址直接返回给b。那么上面这段代码，创建了多少个对象？四个，a，“123”，b,&quot;12&quot; string b = &quot;12&quot;;String a = &quot;1212&quot;String c = b+&quot;12&quot;a == c?答案是 false，因为c指向的是堆内存的地址。为什么呢？b+&quot;12&quot;因为是变量加上常量，那么只有在运行时才能够确定c的值。通过反编译这段代码（javap -verbose），发现实际上最终实现是：new Stringbuilder(&quot;12&quot;).append(&quot;12&quot;).toString(); 所以String c = b+&quot;12&quot;,一共创建了两个对象，一个是stringbuilder，一个是string，两个对象都在堆中。 反编译代码： 再来看一个例子 public class Test &#123; public static void main(String[] args) &#123; String s = &quot;1&quot;+&quot;2&quot;+&quot;3&quot;;//这一行一共生成对少个对象？ 答案是1个，涉及到字符串堆叠的问题。 //他会直接在常量池生成&quot;123&quot;一个对象，s只是一个局部变量 String s1 = new String(&quot;hello&quot;);//这里的意思是，在堆内存创建一个String对象，然后他指向常量池的“hello”，这里一共创建了两个对象，一个是常量池的hello，一个是在堆中的对象 String s2 = &quot;hello&quot;;//s2指向常量池的“hello”，这里的hello是s1创建的。 String s3 = &quot;h&quot;+&quot;e&quot;+&quot;l&quot;+&quot;l&quot;+&quot;o&quot;; System.out.println(s);//输出123 System.out.println(s1 == s2);//输出false，我们知道，s1指向的是堆内存分配的String的地址。s2指向的是常量池的“hello” System.out.println(s1.equals(s2));//true，equal比较的是内容，string 重写了object的equals方法，object比较的是地址。 System.out.println(s2 == s3);//返回时true // 因为jvm会自动优化，在执行s3的时候，产生字符串堆叠，生成 //hello,然后发现常量池中已经存在一个hello（s1创建的），所以s3和s2都指向同一个常量池地址 &#125;&#125; String类型的常量池比较特殊。它的主要使用方法有两种： 1.直接使用双引号声明出来的String对象会直接存储在常量池中。 2.如果不是用双引号声明的String对象，可以使用String提供的intern方法。intern 方法会从字符串常量池中查询当前字符串是否存在，若不存在就会将当前字符串放入常量池中 查看string重写的equals方法 public boolean equals(Object anObject) &#123; if (this == anObject) &#123;//1.如果地址相同，那么说明内容肯定一样 return true; &#125; if (anObject instanceof String) &#123;//2.紧接着获取两个要比较对象的char数组，然后逐个比较 String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false;&#125; 什么是字面值？字面值就是可以直接看到是多少的数据，在程序运行中其值不能发生改变。 以下这些就是字面值，字面值，都是存放在常量池中 /** 字符串，用双引号括起来的内容，例如：monkey* 整数，所有整数，例如：1024* 小数，所有小数，例如：3.14* 字符，用单引号括起来的内容,里面只能放单个数字,单个字母或单个符号* 布尔，只有true和false* 空，null*/public class ConstantDemo01 &#123; public static void main(String[] args) &#123; System.out.println(\"monkey\"); //字符串 System.out.println(\"monkey1024\"); //字符串 System.out.println(1024); //整数 System.out.println(3.14); //小数 System.out.println('a'); //''中必须放的是单个字符 //error //System.out.println('ab'); //这个ab不是单个字符，而是字符串 System.out.println(' '); //带表空格字符 System.out.println(true); //boolean类只有两个值,true和false System.out.println(false); &#125;&#125; Stringbuffer和Stringbuilder这两者其实是一模一样的，区别在于，stringbuffer是线程安全的，而stringbuilder是线程不安全的。二者都继承了 AbstractStringBuilder，里面包含了基本操作。区别仅在于Stringbuffer的方法都加了 synchronized。 我们在使用stringbuilder或者stringbuffer的时候，需要考虑到是否存在锁优化的问题（详情参见java并发总结文章） 总结1.string是一个不可变的变量，常量池中一定不存在两个相同的字符串。不可变的意思是，假设创建了这个值就不会再改变，针对于这个字符串的改变，都会重新创建一个新的字符串对象。（例如调用string的，substring，concat等方法，都是返回一个新的string对象） 2.两个字面值字符串相加，在编译期间就可以确定他们的值，他们的加值就存放在字符串常量池中。但是一个字面量加上一个字符串变量，只有在运行期间才会确定值，他们最终是通过stringbuilder的append方法实现相加，最终通过调用toString方法返回一个新的string对象。 关于string.iten的源码：http://cmsblogs.com/?p=5248 好的博客地址：https://www.cnblogs.com/dengchengchao/p/9713859.html 重要 https://blog.csdn.net/JohnDeng520/article/details/94914717 深入理解Java中的String（大坑）https://blog.csdn.net/qq_34490018/article/details/82110578 https://blog.csdn.net/ifwinds/article/details/80849184 重要，iten方法的存在就是为了避免，创建过多的对象。 https://www.cnblogs.com/airnew/p/11628017.html 重要 Exception和Error有什么区别？异常的出现，是为了解决在编码过程中，某个逻辑可能出现的意料之外的情况，我们通过捕获这种情况，然后做相应的修正逻辑或者后继处理，使得我们的程序更加健壮。 我们知道出现的意料之外的情况，可以能分为两种，出现问题后我们获取问题然后做拯救措施，程序紧着可以运行，另一种就是出现问题，就算我们再拯救也没用，那么就直接让程序挂掉，然后我们事后做分析。那就是，可以恢复和不可恢复，进而言之分为，exception和error。 Exception 和 Error 都是继承了 Throwable 类，在 Java 中只有 Throwable 类型的实例才可以被抛出（throw）或者捕获（catch），它是异常处理机制的基本组成类型。 Exception 和 Error 体现了 Java 平台设计者对不同异常情况的分类。Exception 是程序正常运行中，可以预料的意外情况，可能并且应该被捕获，进行相应处理。 Error 是指在正常情况下，不大可能出现的情况，绝大部分的 Error 都会导致程序（比如 JVM 自身）处于非正常的、不可恢复状态。既然是非正常情况，所以不便于也不需要捕获，常见的比如 OutOfMemoryError 之类，都是 Error 的子类。 Exception 又分为可检查（checked）异常和不检查（unchecked）异常，可检查异常在源代码里必须显式地进行捕获处理，这是编译期检查的一部分。前面我介绍的不可查的 Error，是 Throwable 不是 Exception。 不检查异常就是所谓的运行时异常，类似 NullPointerException、ArrayIndexOutOfBoundsException 之类，通常是可以编码避免的逻辑错误，具体根据需要来判断是否需要捕获，并不会在编译期强制要求。 你了解哪些 Error、Exception 或者 RuntimeException？ NullPointerException，在写if判断逻辑的时候，没有考虑充分，导致某种情况的入参，没有做到对象的初始化，然后用对象去调用某个方法时，出现问题。 ClassCastException 类转化异常。报表导入时，做映射的时候。 继承于RuntimeException的异常都是可检查异常，继承Exception是非检查异常 try-with-resources 自动关闭资源Try-with-resources是java7中一个新的异常处理机制，它能够很容易地关闭在try-catch语句块中使用的资源。 在以前的代码中，我们一般是通过finally做最后的资源回收工作， private static void printFile() throws IOException &#123;​ InputStream input = null;​ try &#123;​ input = &lt;strong&gt;new FileInputStream(&quot;file.txt&quot;);​ int data = &lt;strong&gt;input.read()​ while(data != -1)&#123;​ System.out.print((char) data);​ data = &lt;strong&gt;input.read()​ &#125;​ &#125; finally &#123;​ if(input != null)&#123;​ input.close();​ &#125;​ &#125;&#125; 在java7中，对于上面的例子可以用try-with-resource 结构这样写： private static void printFileJava7() throws IOException &#123; try(FileInputStream input = new FileInputStream(&quot;file.txt&quot;)) &#123; int data = input.read(); while(data != -1)&#123; System.out.print((char) data); data = input.read(); &#125; &#125;&#125; 这就是try-with-resource 结构的用法。FileInputStream 类型变量就在try关键字后面的括号中声明。而且一个FileInputStream 类型被实例化并被赋给了这个变量。 当try语句块运行结束时，FileInputStream 会被自动关闭。这是因为FileInputStream 实现了java中的java.lang.AutoCloseable接口。所有实现了这个接口的类都可以在try-with-resources结构中使用。 当try-with-resources结构中抛出一个异常，同时FileInputStreami被关闭时（调用了其close方法）也抛出一个异常，try-with-resources结构中抛出的异常会向外传播，而FileInputStreami被关闭时抛出的异常被抑制了。 知识扩展先开看第一个吧，下面的代码反映了异常处理中哪些不当之处？ try &#123; // 业务代码 // … Thread.sleep(1000L);&#125; catch (Exception e) &#123; // Ignore it&#125; 这段代码虽然很短，但是已经违反了异常处理的两个基本原则。 第一，尽量不要捕获类似 Exception 这样的通用异常，而是应该捕获特定异常，在这里是 Thread.sleep() 抛出的 InterruptedException。 这是因为在日常的开发和合作中，我们读代码的机会往往超过写代码，软件工程是门协作的艺术，所以我们有义务让自己的代码能够直观地体现出尽量多的信息，而泛泛的 Exception 之类，恰恰隐藏了我们的目的。另外，我们也要保证程序不会捕获到我们不希望捕获的异常。比如，你可能更希望 RuntimeException 被扩散出来，而不是被捕获。 进一步讲，除非深思熟虑了，否则不要捕获 Throwable 或者 Error，这样很难保证我们能够正确程序处理 OutOfMemoryError。 第二，不要生吞（swallow）异常。这是异常处理中要特别注意的事情，因为很可能会导致非常难以诊断的诡异情况。 生吞异常，往往是基于假设这段代码可能不会发生，或者感觉忽略异常是无所谓的，但是千万不要在产品代码做这种假设！ 如果我们不把异常抛出来，或者也没有输出到日志（Logger）之类，程序可能在后续代码以不可控的方式结束。没人能够轻易判断究竟是哪里抛出了异常，以及是什么原因产生了异常。 体会一下Throw early, catch late 原则 我们接下来看下面的代码段，体会一下Throw early, catch late 原则。 public void readPreferences(String fileName)&#123; //...perform operations... InputStream in = new FileInputStream(fileName); //...read the preferences file...&#125; 如果 fileName 是 null，那么程序就会抛出 NullPointerException，但是由于没有第一时间暴露出问题，堆栈信息可能非常令人费解，往往需要相对复杂的定位。这个 NPE 只是作为例子，实际产品代码中，可能是各种情况，比如获取配置失败之类的。在发现问题的时候，第一时间抛出，能够更加清晰地反映问题。 我们可以修改一下，让问题“throw early”，对应的异常信息就非常直观了。 public void readPreferences(String filename) &#123; Objects. requireNonNull(filename); //...perform other operations... InputStream in = new FileInputStream(filename); //...read the preferences file...&#125; 至于“catch late”，其实是我们经常苦恼的问题，捕获异常后，需要怎么处理呢？最差的处理方式，就是我前面提到的“生吞异常”，本质上其实是掩盖问题。如果实在不知道如何处理，可以选择保留原有异常的 cause 信息，直接再抛出或者构建新的异常抛出去。在更高层面，因为有了清晰的（业务）逻辑，往往会更清楚合适的处理方式是什么。 有的时候，我们会根据需要自定义异常，这个时候除了保证提供足够的信息，还有两点需要考虑： 是否需要定义成 Checked Exception，因为这种类型设计的初衷更是为了从异常情况恢复，作为异常设计者，我们往往有充足信息进行分类。 在保证诊断信息足够的同时，也要考虑避免包含敏感信息，因为那样可能导致潜在的安全问题。如果我们看 Java 的标准类库，你可能注意到类似 java.net.ConnectException，出错信息是类似“ Connection refused (Connection refused)”，而不包含具体的机器名、IP、端口等，一个重要考量就是信息安全。类似的情况在日志中也有，比如，用户数据一般是不可以输出到日志里面的。 业界有一种争论（甚至可以算是某种程度的共识），Java 语言的 Checked Exception 也许是个设计错误，反对者列举了几点： Checked Exception 的假设是我们捕获了异常，然后恢复程序。但是，其实我们大多数情况下，根本就不可能恢复。Checked Exception 的使用，已经大大偏离了最初的设计目的。 Checked Exception 不兼容 functional 编程，如果你写过 Lambda/Stream 代码，相信深有体会。 我们从性能角度来审视一下 Java 的异常处理机制，这里有两个可能会相对昂贵的地方： try-catch 代码段会产生额外的性能开销，或者换个角度说，它往往会影响 JVM 对代码进行优化，所以建议仅捕获有必要的代码段，尽量不要一个大的 try 包住整段的代码；与此同时，利用异常控制代码流程，也不是一个好主意，远比我们通常意义上的条件语句（if/else、switch）要低效。 Java 每实例化一个 Exception，都会对当时的栈进行快照，这是一个相对比较重的操作。如果发生的非常频繁，这个开销可就不能被忽略了。 所以，对于部分追求极致性能的底层类库，有种方式是尝试创建不进行栈快照的 Exception。这本身也存在争议，因为这样做的假设在于，我创建异常时知道未来是否需要堆栈。问题是，实际上可能吗？小范围或许可能，但是在大规模项目中，这么做可能不是个理智的选择。如果需要堆栈，但又没有收集这些信息，在复杂情况下，尤其是类似微服务这种分布式系统，这会大大增加诊断的难度。 当我们的服务出现反应变慢、吞吐量下降的时候，检查发生最频繁的 Exception 也是一种思路。关于诊断后台变慢的问题，我会在后面的 Java 性能基础模块中系统探讨。 强引用、软引用、弱引用、幻象引用有什么区别？不同的引用类型，主要体现的是对象不同的可达性（reachable）状态和对垃圾收集的影响。也就是说，他是跟 所谓强引用（“Strong” Reference），就是我们最常见的普通对象引用，只要还有强引用指向一个对象，就能表明对象还“活着”，垃圾收集器不会碰这种对象（我们平常典型编码Object obj = new Object()中的obj就是强引用。通过关键字new 创建的对象所关联的引用就是强引用）。对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为 null，就是可以被垃圾收集的了，当然具体回收时机还是要看垃圾收集策略。 软引用（通过这个类实现SoftReference），是一种相对强引用弱化一些的引用，可以让对象豁免一些垃圾收集，只有当 JVM 认为内存不足时，才会去试图回收软引用指向的对象。JVM 会确保在抛出 OutOfMemoryError 之前，清理软引用指向的对象。软引用通常用来实现内存敏感的缓存，如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。虽说难以下咽，但是弃之可惜，所以他是强引用的一种折中方案，当jvm堆内存充足不会回收，但是当堆内存不足的时候，就会被回收。换句话说，就是尽量留下来 弱引用（WeakReference）并不能使对象豁免垃圾收集，仅仅是提供一种访问在弱引用状态下对象的途径。这就可以用来构建一种没有特定约束的关系，比如，维护一种非强制性的映射关系，如果试图获取时对象还在，就使用它，否则重现实例化。它同样是很多缓存实现的选择。 对于幻象引用，有时候也翻译成虚引用，你不能通过它访问对象。幻象引用仅仅是提供了一种确保对象被 finalize 以后，做某些事情的机制，比如，通常用来做所谓的 Post-Mortem 清理机制。 动态代理和反射是基于什么原理？​ ​ 什么叫代理，就是我代替你去做某件事情，例如代购，通过代理功能，我们可以在调用被代理对象（委托类）的某个方法的前后做一些逻辑补充操作，例如我们要做饭，那么我们在做饭之前先洗米，插上电源—&gt;&gt; 做饭 —&gt;拔下电源。 这个就是我们常说的切面，就是在原来的基础上（不改变委托类），切入我们想要的逻辑。 代理模式是对象的结构模式。代理模式给某一个对象提供一个代理对象，并由代理对象控制对原对象的引用。 ​ 代理模式是一种常用的设计模式。代理模式为其对象提供了一种代理以控制对这个对象的访问。代理模式可以将主要业务与次要业务进行松耦合的组装。根据代理类的创建时机和创建方式的不同，可以将其分为静态代理和动态代理两种形式：在程序运行前就已经存在的编译好的代理类是为静态代理。在程序运行期间根据需要动态创建代理类及其实例来完成具体的功能是为动态代理。（动态代理的实现方式有两种，JDK动态代理和cglib动态代理） 静态代理继承就是通过继承委托类，生成代理类，然后重写委托类的方法，重新实现逻辑，完成代理的逻辑。 代理类是委托类的子类（有点cglib动态代理的味道） 聚合代理类和委托类，都实现同一个接口，代理类依赖委托类（代理类注入委托类） 接下来看一下静态代理的实现代码： // 委托接口public interface IHelloService &#123; /** * 定义接口方法 * @param userName * @return */ String sayHello(String userName);&#125;// 委托类实现public class HelloService implements IHelloService &#123; @Override public String sayHello(String userName) &#123; System.out.println(&quot;helloService&quot; + userName); return &quot;HelloService&quot; + userName; &#125;&#125;// 代理类public class StaticProxyHello implements IHelloService &#123; private IHelloService helloService = new HelloService(); @Override public String sayHello(String userName) &#123; /** 代理对象可以在此处包装一下*/ System.out.println(&quot;代理对象包装礼盒...&quot;); return helloService.sayHello(userName); &#125;&#125;// 测试静态代理类public class MainStatic &#123; public static void main(String[] args) &#123; StaticProxyHello staticProxyHello = new StaticProxyHello(); staticProxyHello.sayHello(&quot;isole&quot;); &#125;&#125; ​ 可以看到，代理类和委托类，都实现共同的接口IHelloService。静态代理，实际上就是在代理类中，注入委托类，然后代理类可以实现一些额外的方法，然后真正的调用时。通过代理类去调用委托类。我们可以看待，代码的结构是固定的，在编译前就可以确定代理类有哪些方法。 ​ 使用静态代理的缺点是：只适用委托方法少的情况下, 试想一下如果委托类有几百上千个方法, 岂不是很难受, 要在代理类中写一堆的代理方法。换句话说，我们有三个开发，每个人在调用委托类的方法之前，都会有自己的迁入逻辑补充，那么也就意味着，需要在StaticProxyHello代理类中，实现各自的方法，那么代理类中的代理方法就会疯狂增多，那么就会导致代理类很难管理。也就是说，我们想要按需在代理类中添加我们想要切入的逻辑，那么这个就是要动态的添加代理方法了。 ①代理类和被代理类实现了相同的接口，导致代码的重复，如果接口增加一个方法，那么除了被代理类需要实现这个方法外，代理类也要实现这个方法，增加了代码维护的难度。 ②代理对象只服务于一种类型的对象，如果要服务多类型的对象。势必要为每一种对象都进行代理，静态代理在程序规模稍大时就无法胜任了。比如上面的例子，只是对用户的业务功能（IUserService）进行代理，如果是商品（IItemService）的业务功能那就无法代理，需要去编写商品服务的代理类。 实现静态代理有四个步骤： 定义业务接口 定义委托类并实现业务接口 定义代理类并实现业务接口，同时依赖委托类（委托类是代理类的一个成员变量） 这个需求，就需要动态代理。 动态代理代理类在程序运行时创建的代理方式被成为 动态代理。在了解动态代理之前, 我们先简回顾一下 JVM 的类加载机制中的加载阶段要做的三件事情 ( 附 Java 中的类加载器 ) 通过一个类的全名或其它途径来获取这个类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的 Class 对象, 作为方法区中对这个类访问的入口。 ​ 而我们要说的动态代理，主要就发生在第一个阶段, 这个阶段类的二进制字节流的来源可以有很多, 比如 zip 包、网络、运行时计算生成、其它文件生成 (JSP)、数据库获取。其中运行时计算生成就是我们所说的动态代理技术，在 Proxy 类中, 就是运用了 ProxyGenerator.generateProxyClass 来为特定接口生成形式为 *$Proxy 的代理类的二进制字节流。所谓的动态代理就是想办法根据接口或者目标对象计算出代理类的字节码然后加载进 JVM 中。实际计算的情况会很复杂，我们借助一些诸如 JDK 动态代理实现、CGLIB 第三方库来完成。 JDK 动态代理在 Java 的动态代理中, 主要涉及 2 个类,java.lang.reflect.Proxy和java.lang.reflect.InvocationHandler 我们需要一个实现 InvocationHandler 接口的中间类, 这个接口只有一个方法 invoke 方法。 public interface InvocationHandler &#123; /** * 调用处理 * @param proxy 代理类对象 * @param methon 标识具体调用的是代理类的哪个方法 * @param args 代理类方法的参数 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;&#125; 实际上最关键的就是这个中间类，通过中间类，我们可以拦截委托类所有方法的调用，然后做一些额外的工作。 ​ 我们对处理类（中间类生成的代理对象）中的所有方法的调用都会变成对 invoke 方法的调用，这样我们可以在 invoke 方法中添加统一的处理逻辑（也可以根据 method 参数判断是哪个方法）。中间类 (实现了 InvocationHandler 的类) 有一个委托类对象引用, 在 Invoke 方法中调用了委托类对象的相应方法，通过这种聚合的方式持有委托类对象引用，把外部对 invoke 的调用最终都转为对委托类对象的调用。 ​ 实际上，中间类与委托类构成了静态代理关系（他们的关系就是在中间类总注入委托类，然后调用，编译时即可确定关系），在这个关系中，中间类是代理类，委托类是委托类。然后代理类与中间类也构成一个静态代理关系，在这个关系中，中间类是委托类，代理类是代理类。也就是说，动态代理关系由两组静态代理关系组成，这就是动态代理的原理。 jdk的reflect实现动态代理 // 委托类接口public interface IHelloService &#123; /** * 方法1 * @param userName * @return */ String sayHello(String userName); /** * 方法2 * @param userName * @return */ String sayByeBye(String userName);&#125;// 委托类public class HelloService implements IHelloService &#123; @Override public String sayHello(String userName) &#123; System.out.println(userName + \" hello\"); return userName + \" hello\"; &#125; @Override public String sayByeBye(String userName) &#123; System.out.println(userName + \" ByeBye\"); return userName + \" ByeBye\"; &#125;&#125;// 中间类public class JavaProxyInvocationHandler implements InvocationHandler &#123; /** * 中间类持有委托类对象的引用,这里会构成一种静态代理关系 */ private Object obj ; /** * 有参构造器,传入委托类的对象 * @param obj 委托类的对象 */ public JavaProxyInvocationHandler(Object obj)&#123; this.obj = obj; &#125; /** * 动态生成代理类对象,Proxy.newProxyInstance * @return 返回代理类的实例 */ public Object newProxyInstance() &#123; return Proxy.newProxyInstance( //指定代理对象的类加载器，然后委托类跟代理类由同一个类加载器加载 obj.getClass().getClassLoader(), //代理对象需要实现的接口，可以同时指定多个接口 obj.getClass().getInterfaces(), //方法调用的实际处理者，代理对象的方法调用都会转发到这里 this); &#125; /** * * @param proxy 代理对象 * @param method 代理方法 * @param args 方法的参数 * @return * @throws Throwable */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"invoke before\"); Object result = method.invoke(obj, args); System.out.println(\"invoke after\"); return result; &#125;&#125;// 测试动态代理类public class MainJavaProxy &#123; public static void main(String[] args) &#123; JavaProxyInvocationHandler proxyInvocationHandler = new JavaProxyInvocationHandler(new HelloService()); IHelloService helloService = (IHelloService) proxyInvocationHandler.newProxyInstance(); helloService.sayByeBye(\"paopao\"); helloService.sayHello(\"yupao\"); &#125;&#125; ​ 在上面的测试动态代理类中, 我们调用 Proxy 类的 newProxyInstance 方法来获取一个代理类实例。这个代理类实现了我们指定的接口并且会把方法调用分发到指定的调用处理器*（也就是invocationhandler的invoke方法）。 ​ 首先通过 newProxyInstance 方法获取代理类的实例, 之后就可以通过这个代理类的实例调用代理类的方法，对代理类的方法调用都会调用中间类 (实现了 invocationHandle 的类) 的 invoke 方法，在 invoke 方法中我们调用委托类的对应方法，然后加上自己的处理逻辑。 ​ java 动态代理最大的特点就是动态生成的代理类和委托类实现同一个接口。java 动态代理其实内部是通过反射机制实现的，也就是已知的一个对象，在运行的时候动态调用它的方法，并且调用的时候还可以加一些自己的逻辑在里面。 Proxy.newProxyInstance 源码阅读。 上面说过, Proxy.newProxyInstance 通过反射机制用来动态生成代理类对象, 为接口创建一个代理类，这个代理类实现这个接口。具体源码如下： public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException &#123; // 检查空指针 Objects.requireNonNull(h); // 用原型实例指定创建对象的种类,并且通过拷贝这些原型创建新的对象 final Class&lt;?&gt;[] intfs = interfaces.clone(); // 获取系统的安全接口,不为空的话需要验证是否允许访问这种关系的代理访问 final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * 查找或生成代理类 Class,通过类加载器和接口，如果已经存在代理类，则直接返回 */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * 通过构造器来创建实例 */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; //获取所有的构造器 final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; // 构造器不是public的话需要设置可以访问 if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; // 返回创建的代理类Class的实例对象 return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125; &#125; 接着分析一下 getProxyClass0（）方法 我们发现他会先从缓存中查找是否存在相应的代理类的class对象，有则直接返回，没有则新增。 /** * Generate a proxy class. Must call the checkProxyAccess method * to perform permission checks before calling this. */private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) &#123; if (interfaces.length &gt; 65535) &#123; throw new IllegalArgumentException(&quot;interface limit exceeded&quot;); &#125; // If the proxy class defined by the given loader implementing // the given interfaces exists, this will simply return the cached copy; // otherwise, it will create the proxy class via the ProxyClassFactory //意思是：如果代理类被指定的类加载器loader定义了，并实现了给定的接口interfaces， //那么就返回缓存的代理类对象，否则使用ProxyClassFactory创建代理类。 return proxyClassCache.get(loader, interfaces);&#125; 超详细源码分析：https://www.jianshu.com/p/269afd0a52e6 为什么实现同一个接口是实现jdk动态代理的基础为什么jdk动态代理就不能通过继承某个类的方式实现呢？ 我们可以通过查看jdk动态动态代理方式生成的代理类，我们发现，代理类他最终的结构是： public final class ProxyClass extend Proxy implements IHelloService可以看到生成的代理类已经继承了 Proxy类，那么java只支持单继承，那么所以就不能通过继承的方式生成代理类。 总结 ​ 我们发现，实际上，委托类和代理类本质上都是实现了同一个接口，实现同一个接口是实现动态代理的基础。jdk的动态代理，实际上是两组静态代理实现。代理类和中间类是静态代理关系，中间类和委托类是静态代理关系。 ​ 而且动态代理跟静态代理的区别，在于，我们不用实现一个静态的代理类（例如静态代理的StaticProxyHello），我们通过一个中间类（invocationhandler是实现类）生成代理类。然后我们可以定制自己在调用委托类方法之前，切入自己的逻辑。 也就是说，代理类是运行的时候才生成的。故叫动态代理 ​ 同时我们注意源码的Class&lt;?&gt; cl = getProxyClass0(loader, intfs); 你会发现，它是会先查找缓存中是否存在代理类Class对象，如果存在则不新增。这样的好处就是，不会在jvm的metaspace区占满内存。 itss项目和固定资产项目，使用到了，动态反射，实现了资产的导入。 CGLIB动态代理​ JDK 动态代理依赖接口实现，而当我们只有类没有接口的时候就需要使用另一种动态代理技术 CGLIB 动态代理。首先 CGLIB 动态代理是第三方框架实现的，在 maven 工程中我们需要引入 cglib 的包, 如下: &lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt;&lt;/dependency&gt; ​ CGLIB 代理是针对类来实现代理的，原理是对指定的委托类生成一个子类并重写其中业务方法来实现代理。代理类对象是由 Enhancer 类创建的。CGLIB 创建动态代理类的模式是: 查找目标类上的所有非 final 的 public 类型的方法 (final 的不能被重写) 将这些方法的定义转成字节码 将组成的字节码转换成相应的代理的 Class 对象然后通过反射获得代理类的实例对象 实现 MethodInterceptor 接口, 用来处理对代理类上所有方法的请求 // 委托类,是一个简单类public class CglibHelloClass &#123; /** * 方法1 * @param userName * @return */ public String sayHello(String userName)&#123; System.out.println(\"目标对象的方法执行了\"); return userName + \" sayHello\"; &#125; public String sayByeBye(String userName)&#123; System.out.println(\"目标对象的方法执行了\"); return userName + \" sayByeBye\"; &#125;&#125;/** * CglibInterceptor 用于对方法调用拦截以及回调 * */public class CglibInterceptor implements MethodInterceptor &#123; /** * CGLIB 增强类对象，代理类对象是由 Enhancer 类创建的， * Enhancer 是 CGLIB 的字节码增强器，可以很方便的对类进行拓展 */ private Enhancer enhancer = new Enhancer(); /** * * @param obj 被代理的对象 * @param method 代理的方法 * @param args 方法的参数 * @param proxy CGLIB方法代理对象 * @return cglib生成用来代替Method对象的一个对象，使用MethodProxy比调用JDK自身的Method直接执行方法效率会有提升 * @throws Throwable */ @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println(\"方法调用之前\"); Object o = proxy.invokeSuper(obj, args); System.out.println(\"方法调用之后\"); return o; &#125; /** * 使用动态代理创建一个代理对象 * @param c * @return */ public Object newProxyInstance(Class&lt;?&gt; c) &#123; /** * 设置产生的代理对象的父类,增强类型 */ enhancer.setSuperclass(c); /** * 定义代理逻辑对象为当前对象，要求当前对象实现 MethodInterceptor 接口 */ enhancer.setCallback(this); /** * 使用默认无参数的构造函数创建目标对象,这是一个前提,被代理的类要提供无参构造方法 */ return enhancer.create(); &#125;&#125;//测试类public class MainCglibProxy &#123; public static void main(String[] args) &#123; CglibInterceptor cglibProxy = new CglibInterceptor(); CglibHelloClass cglibHelloClass = (CglibHelloClass) cglibProxy.newProxyInstance(CglibHelloClass.class); cglibHelloClass.sayHello(\"isole\"); cglibHelloClass.sayByeBye(\"sss\"); &#125;&#125; ​ 对于需要被代理的类，它只是动态生成一个子类以覆盖非 final 的方法，同时绑定钩子回调自定义的拦截器。值得说的是，它比 JDK 动态代理还要快。值得注意的是，我们传入目标类作为代理的父类。 ​ 不同于 JDK 动态代理，我们不能使用目标对象来创建代理（我们是通过接口来实现代理类）。目标对象只能被 CGLIB 创建。 ​ 在例子中，默认的无参构造方法被使用来创建目标对象。 ​ 可以看到使用CGLIB实现动态代理，少实现了一个类，那就是IHelloService。因为CGLIB实现的代理类的父类就是委托类。 ​ 使用cglib要注意，开启使用缓存标志，否则，就会导致，因为动态生成过多的Class对象，从而挤爆永久代（metaspace） 总结​ 静态代理比较容易理解, 需要被代理的类和代理类实现自同一个接口, 然后在代理类中调用真正实现类, 并且静态代理的关系在编译期间就已经确定了。 ​ 而动态代理的关系是在运行期间确定的。静态代理实现简单，适合于代理类较少且确定的情况，而动态代理则给我们提供了更大的灵活性。 ​ JDK 动态代理所用到的代理类在程序调用到代理类对象时才由 JVM 真正创建，JVM 根据传进来的 业务实现类对象 以及 方法名 ，动态地创建了一个代理类的 class 文件并被字节码引擎执行，然后通过该代理类对象进行方法调用。我们需要做的，只需指定代理类的预处理、调用后操作即可。 ​ 静态代理和动态代理都是基于接口实现的, 而对于那些没有提供接口只是提供了实现类的而言, 就只能选择 CGLIB 动态代理了 JDK 动态代理和 CGLIB 动态代理的区别 JDK 动态代理基于 Java 反射机制实现, 必须要实现了接口的业务类才能用这种方法生成代理对象。 CGLIB 动态代理基于 ASM 框架通过生成业务类的子类来实现。 JDK 动态代理的优势是最小化依赖关系，但是需要定义一个公有的接口（IHelloService），减少依赖意味着简化开发和维护并且有 JDK 自身支持。还可以平滑进行 JDK 版本升级，代码实现简单。 基于 CGLIB 框架的优势是无须实现接口，达到代理类无侵入，我们只需操作我们关系的类，不必为其它相关类增加工作量，性能比较高。 描述代理的几种实现方式? 分别说出优缺点? 代理可以分为 “静态代理” 和 “动态代理”，动态代理又分为 “JDK 动态代理” 和 “CGLIB 动态代理” 实现。 静态代理：代理对象和实际对象都继承了同一个接口，在代理对象中指向的是实际对象的实例，这样对外暴露的是代理对象而真正调用的是 Real Object. 优点：可以很好的保护实际对象的业务逻辑对外暴露，从而提高安全性。 缺点：不同的接口要有不同的代理类实现，会很冗余 JDK 动态代理： 为了解决静态代理中，生成大量的代理类造成的冗余； JDK 动态代理只需要实现 InvocationHandler 接口，重写 invoke 方法便可以完成代理的实现， jdk 的代理是利用反射生成代理类 Proxyxx.class 代理类字节码，并生成对象 jdk 动态代理之所以只能代理接口是因为代理类本身已经 extends 了 Proxy，而 java 是不允许多重继承的，但是允许实现多个接口 优点：解决了静态代理中冗余的代理实现类问题。 缺点：JDK 动态代理是基于接口设计实现的，如果没有接口，会抛异常。 CGLIB 代理： 由于 JDK 动态代理限制了只能基于接口设计，而对于没有接口的情况，JDK 方式解决不了； CGLib 采用了非常底层的字节码技术，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑，来完成动态代理的实现。 实现方式实现 MethodInterceptor 接口，重写 intercept 方法，通过 Enhancer 类的回调方法来实现。 但是 CGLib 在创建代理对象时所花费的时间却比 JDK 多得多，所以对于单例的对象，因为无需频繁创建对象，用 CGLib 合适，反之，如果需要创建大量对象，使用 JDK 方式要更为合适一些。 同时，由于 CGLib 由于是采用动态创建子类的方法，对于 final 方法，无法进行代理。 优点：没有接口也能实现动态代理，而且采用字节码增强技术，性能也不错。 缺点：技术实现相对难理解些。 总而言之，JDK的动态代理有一个限制，就是使用动态代理的对象必须实现一个或多个接口。如果想代理没有实现接口的类，就可以使用CGLIB实现。 但是使用CGLIB要非常注意，因为 为什么面试会问？从考察知识点的角度，这道题涉及的知识点比较庞杂，所以面试官能够扩展或者深挖的内容非常多，比如： 考察你对反射机制的了解和掌握程度。 动态代理解决了什么问题，在你业务系统中的应用场景是什么？ JDK 动态代理在设计和实现上与 cglib 等方式有什么不同，进而如何取舍？ java反射我们知道JDK生成代理类的方式，最终实现是是通过，反射的机制实现的，那么什么是反射呢？ 反射目的就是为了程序在运行过程中动态创建某个类。 首先要知道Class类 ​ 对象照镜子后可以得到的信息：某个类的数据成员名、方法和构造器、某个类到底实现了哪些接口。对于每个类而言，JRE 都为其保留一个不变的 Class 类型的对象。一个 Class 对象包含了特定某个类的有关信息。 Class 对象只能由系统建立对象（不能自己new），在jvm加载字节码文件到元空间（永久代），就会自动创建Class对象。 一个类在 JVM 中只会有一个Class实例 （类加载机制，双亲委任机制保证了字节码文件的唯一性） 每个类的实例都会记得自己是由哪个 Class 实例所生成 。 ​ 所以我们都是通过Class类获取某个对象的，已经定义方法，成员变量，构造函数等等，然后在运行过程中动态执行某个方法。 获取一个对象的Class对象的方式有三种。 1.通过对象的getClass方法获取 Student stu1 = new Student();/ Class class = stu1.getClass（）； 2. 通过类的class属性获取，该方法最为安全可靠，程序性能更高 Class class = Student.class（） 3，通过Class对象的forName静态方法获取 但是可能会抛出ClassNotFoundException异常 Class class = Class.forName（&quot;www.kingge.top.Student&quot;） ​ ​ 三种方式常用第三种，第一种对象都有了还要反射干什么(使用反射的目的是为了创建对象)。第二种需要导入类的包，依赖太强，不导包就抛编译错误。一般都第三种，一个字符串可以传入也可写在配置文件中等多种方法。 总结 反射的出现，目的就是让程序在运行过程中，动态生成我们所需要的类。 反射的实现，就是通过Class类，Class类是唯一的，因为通过双亲委任机制可以得知。 Java hashCode() 和 equals()的若干问题解答 Comparable和Comparatorhttp://cmsblogs.com/?p=15717 这两个接口，都可以用来实现对象的排序。也就是说他们两个的功能实际上就是一样的。所以不要混着使用。也就是说，一个类最后不要同时实现这两个接口。 那么，他们的使用规则一般是：一个类，一般是通过实现Comparable接口实现排序（作为内部排序），但是当我们在不想修改类的代码结构的同时又想改变内部排序，这个时候可以使用Comparator接口实现重写排序逻辑（作为外部排序）。这样在排序的时候，默认的内部排序，就会被外部排序逻辑覆盖。 JMM内存模型​ JMM(Java内存模型Java Memory Model,简称JMM)本身是一种抽象的概念 ，并不真实存在，它描述的是一组规则或规范通过规范定制了程序中各个变量(包括实例字段,静态字段和构成数组对象的元素)的访问方式。 JMM关于同步规定: 1.线程解锁前,必须把共享变量的值刷新回主内存2.线程加锁前,必须读取主内存的最新值到自己的工作内存3.加锁解锁是同一把锁 ​ 由于JVM运行程序的实体是线程,而每个线程创建时JVM都会为其创建一个工作内存(有些地方成为栈空间，实际上就是java虚拟机栈)，工作内存是每个线程的私有数据区域（java虚拟机栈），而Java内存模型中规定所有变量都存储在主内存，主内存是共享内存区域，所有线程都可访问，但线程对变量的操作(读取赋值等)必须在工作内存中进行，所以线程要将操作的变量从主内存拷贝到自己的工作空间，然后对变量进行操作，操作完成再将变量写回主内存，不能直接操作主内存中的变量,各个线程中的工作内存储存着主内存中的变量副本拷贝，因此不同的线程无法访问对方的工作内存,此案成间的通讯(传值) 必须通过主内存来完成,其简要访问过程如下图: 这个就是并发问题出现的根源之一，数据可见性。 也就是说：JMM的存在就是为了保证，原子性，可见性，有序性。也即是，保证多线程访问资源的安全性 Volatile 首先声明，他不是一种锁。 它会确保我们对于某个变量的读取和写入，都一定会同步到主内存里，而不是从 Cache 里面读取（也就是我们通俗的说禁用缓存）。 volatile是java虚拟机提供的的轻量级的同步机制，它能够保证可见性和禁止指令重排序。但是不能够保证原子性（那就意味着，会产生线程不安全问题） 什么叫可见性 一个线程对于共享资源的修改，对于另一个线程是可见的。也就是说，我修改后的值，你是可以看到的。也就意味着，假设另一个线程改完了，那么会通知到另一个线程。 什么叫原子性 也就是一个线程操作某个逻辑的时候，它能够保证在执行的过程中，不会发生线程切换，要么都完成，要么都失败，而且我们能够所说的原子性是针对于CPU指令而言的（高级语言里一条语句往往需要多条 CPU 指令完成）。例如我们在执行，自增操作的时候，count+=1；你以为这是一个原子操作，就是执行一条指令而已，其实不是，因为他至少会分成三个cpu指令去执行。 指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器； 指令 2：之后，在寄存器中执行 +1 操作； 指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）。 ​ 操作系统做任务切换，可以发生在任何一条CPU 指令执行完，是的，是 CPU 指令，而不是高级语言里的一条语句。对于上面的三条指令来说，我们假设 count=0，如果线程 A 在指令 1 执行完后做线程切换，线程 A 和线程 B 按照下图的序列执行，那么我们会发现两个线程都执行了 count+=1 的操作，但是得到的结果不是我们期望的 2，而是 1。 指令重排序 参见《java并发章节》 那么volatile是怎么保证可见性（数据一致性）呢？首先查看下面例子： public class VolatileTets &#123; public static void main(String[] args) &#123; ShareData shareData = new ShareData();//主内存中，初始化了ShareData对象，并初始化了number值为0 new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println( Thread.currentThread().getName() +&quot; 开始执行&quot; ); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; shareData.addNumber(); System.out.println( Thread.currentThread().getName() +&quot; 更新完毕 number的值是：&quot;+ shareData.number ); &#125; &#125;, &quot;a&quot;).start(); while(shareData.number == 0) &#123; &#125; System.out.println( Thread.currentThread().getName() + &quot; main线程执行结束 &quot; ); &#125;&#125;class ShareData&#123; public int number = 0; public void addNumber() &#123; this.number = 60; &#125; &#125; 我们知道上诉代码，一共创建了两个线程，一个是main线程，一个是a线程。 那么为什么在a线程内部停了三秒呢？目的就是，让a线程跟main线程，获取的共享资源的number都是一样的（也就是0），拷贝到自己的java虚拟机栈的number都是0。这样就能够保证，main线程，执行成功while循环，然后main线程不会结束。这样才能够验证volatile的可见性。 上诉代码，的执行结果，我们其实也能够看的出来，输出如下： a线程 开始执行a线程 更新完毕 number的值是：60 main线程，永久挂在那里，a线程执行完毕。 也就意味着，a线程虽然修改了number 的值，同时把主内存的number修改为60，但是对于main线程而言是不可见的，没有通知main线程，main线程认为还是0。否则，main线程应该结束while循环。 那么我们说volatile可以解决可见性，那么是真是假呢？ 我们把 public int number = 0; 修改为public volatile int number = 0;，再次运行代码。 程序输出： a线程 开始执行a线程 更新完毕 number的值是：60main main线程执行结束 我们惊喜的发现，main线程竟然退出了！！！，那也就意味着，a线程修改完number后，写入到主内存，然后成功通知了挂起来的main线程。 所以，到这里，我们就成功的验证了volatile的可见性机制。 验证volatile不支持原子性验证例子： public class VolatileTets &#123; public static void main(String[] args) &#123; ShareData shareData = new ShareData(); for (int i = 0; i &lt; 20000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; shareData.add(); &#125; &#125;, String.valueOf(i)).start(); &#125; Thread.yield(); System.out.println( shareData.number ); &#125;&#125;class ShareData&#123; public volatile int number = 0; public void add() &#123; this.number++; &#125; &#125; 你会发现，输出的值，都是随机的，并不是我们预期的20000，所以volatile并不能保证原子性。 解决方式解决方案：使用synchronized内部锁或者Lock显示锁，但是加锁又太重，杀鸡用牛刀，我们可以考虑使用AtomicInteger来实现number++的问题。 volatile怎么禁止指令重排序？ 什么叫指令重排序 计算机在执行程序时,为了提高性能,编译器和处理器常常会做指令重排,一把分为以下3种： 单线程环境里面能确保程序最终执行结果和代码顺序执行的结果一致。 多线程环境中线程交替执行，由于编译器优化重排的存在，两个线程使用的变量能否保持一致性是无法确定的,结果无法预测。 但是处理器在进行重新排序是必须要考虑指令之间的数据依赖性。 指令重排例子1 public void mySort()&#123; int x=11;//语句1 int y=12;//语句2 x=x+5;//语句3 y=x*x;//语句4&#125;例如我们从编程角度看上面代码，以为执行顺序是1234。但是，可能不是，可能会发生指令重排。例如，重拍后是按照 2134、1324这样的执行顺序。问题:请问语句4 可以重排后变成第一条码?存在数据的依赖性，没办法排到第一个，因为要执行语句4，需要依赖 y或者x，放到第一条，这个时候，y或者x还没有初始化。那么很明显语句四肯定执行失败，所以语句四不可能被重排到第一个 指令重排序例子2 public class CP &#123; int a = 0; boolean flag = false; public void initValue() &#123; a = 1;//1 flag = true;//2 &#125; public void updateValue() &#123; if(flag) &#123;//3 a = a + 5;//4 &#125; &#125;&#125; 这里有两个线程，分别调用initValue、updateValue这两个方法。 按照代码的顺序读，我们知道，当线程1调用完成initValue后，接着调用updateValue，那么代码3判断是成功的，然后a的最终值是6。 我们知道initValue方法的flag和a变量是没有依赖关系的，所以可能发生了指令重排，代码1和代码2的位置调换 public void initValue() { flag = true;//1 a = 1;//2 } 那么假设线程1执行完成代码1后，进行了线程切换，线程2获得了执行机会，线程2去执行updateValue，那么if判断通过，这个时候，a的值是0，那么执行a+5，那么a最终结果是5 很明显跟上面的结果是6不相等。所以就会产生线程不安全问题。 解决方案，就是给flag，添加volatile修饰符，这样就能够通过给flag变量添加内存屏障的方式，禁止，指令重排。 你在哪些地方用到过volatile1.首先我们知道 Atomic包下的类，大量使用到了volatile，例如AtomicInteger，AtomicReference等等。 2.单例模式DCL代码 public class SingletonDemo &#123; private static volatile SingletonDemo instance=null; private SingletonDemo()&#123; System.out.println(Thread.currentThread().getName()+&quot;\\t 构造方法&quot;); &#125; /** * 双重检测机制 * @return */ public static SingletonDemo getInstance()&#123; if(instance==null)&#123; synchronized (SingletonDemo.class)&#123; if(instance==null)&#123; instance=new SingletonDemo(); &#125; &#125; &#125; return instance; &#125; public static void main(String[] args) &#123; for (int i = 1; i &lt;=10; i++) &#123; new Thread(() -&gt;&#123; SingletonDemo.getInstance(); &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 以上是最终的实现代码，可以解决多线程下单例请求问题。 但是你可能有个问题，为什么需要双重判断机制？问题一 假设getInstance方法修改为： public static SingletonDemo getInstance(){ synchronized (SingletonDemo.class){ if(instance==null){ instance=new SingletonDemo(); } } return instance; } ​ 你觉得，有什么问题？那就是可能会引发多余的请求加锁操作，假设instance实例已经初始化了，但是每一次线程访问getInstance方法的时候，都会请求锁，这样就会耗费时间。所以我们需要在最外一层再包裹一个if判断。如果已经初始化，那么就直接返回。 那为什么最里层也要判断一下呢？问题二 假设剔除最里层的if判断，最终代码如下： public static SingletonDemo getInstance()&#123; if(instance==null)&#123;//代码1 //代码2 synchronized (SingletonDemo.class)&#123;//代码3 instance=new SingletonDemo();//代码4 &#125; &#125; return instance; &#125; 那么这样会有什么问题呢？ 假设有 三个线程同时执行到了，代码二这个位置，那么开始执行代码3，只有一个线程能获取锁，然后实例化instance，然后返回instance，当前线程执行结束。这个时候，其他两个线程获得执行机会，也会执行到代码3，获取锁，然后又再次实例化instance。这个问题就出来，instance就不再是单例了。 总结DCL(双端检锁) 机制不一定线程安全,原因是有指令重排的存在,加入volatile可以禁止指令重排。 假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 instance == null ，于是同时对 SingletonDemo.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）；线程 A 会创建一个 SingletonDemo实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 instance == null 时会发现，已经创建过 SingletonDemo实例了，所以线程 B 不会再创建一个 SingletonDemo实例。 这看上去一切都很完美，无懈可击，但实际上这个 getInstance() 方法并不完美。问题出在哪里呢？出在 new 操作上，我们以为的 new 操作应该是： 分配一块内存 M； 在内存 M 上初始化 SingletonDemo 对象； 然后 M 的地址赋值给 instance 变量。 因为第2、第3步骤没有什么关系（没有相互依赖），那么是可以调换顺序的 但是实际上优化后的执行路径却是这样的： 分配一块内存 M； 将 M 的地址赋值给 instance 变量； 最后在内存 M 上初始化 SingletonDemo对象。 ​ 优化后会导致什么问题呢？我们假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。 线程A进入第二个判空条件，进行初始化时，发生了时间片切换，即使没有释放锁，线程B刚要进入第一个判空条件时，发现条件不成立，直接返回instance引用，不用去获取锁。如果对instance进行volatile语义声明，就可以禁止指令重排序，避免该情况发生。 对于CPU缓存和内存的疑问，CPU缓存不存在于内存中的，它是一块比内存更小、读写速度更快的芯片，至于什么时候把数据从缓存写到内存，没有固定的时间，同样地，对于有volatile语义声明的变量，线程A执行完后会强制将值刷新到内存中，线程B进行相关操作时会强制重新把内存中的内容写入到自己的缓存，这就涉及到了volatile的写入屏障问题，当然也就是所谓happen-before问题。 好的总结文档：https://dzone.com/articles/java-volatile-keyword-0 MESI cpu缓存一致性协议！！！重要！！！！保证了可见性MESI协议是一种基于无效的缓存一致性协议，他是基于硬件级别的优化 什么是基于无效呢？ 首先讲一下CPU和缓存的关系。计算机在数据处理或信号控制的时候，常与内存进行数据访问，但是内存和CPU的速度差别很大，所以会造成CPU资源浪费问题，为了解决两者的速度不匹配，所以在两者之间加了L1、L2、L3等缓存。在多核计算机中有多个CPU，每个CPU都有自己的缓存，所以就会造成缓存的数据不一致问题。在早期解决缓存不一致是对总线使用LOCK（I/O总线）#锁，使得CPU访问某个变量的时候，其他CPU无法访问。但是这种效率很低。 MESI的主要思想：当CPU写数据时，如果该变量是共享数据，给其他CPU发送信号，使得其他的CPU中的该变量的缓存行无效。 执行写操作的时候有两种策略，一种是write-back caches，另一种是write-through caches。MESI支持write-back。 Write-through: write is done synchronously both to the cache and to the backing store. Write-back (also called write-behind): initially, writing is done only to the cache. The write to the backing store is postponed until the modified content is about to be replaced by another cache block. Write through就是直接写回主存 ​ 最简单的一种写入策略，叫作写直达（Write-Through）。在这个策略里，每一次数据都要写入到主内存里面。在写直达的策略里面，写入前，我们会先去判断数据是否已经在 Cache 里面了。如果数据已经在 Cache 里面了，我们先把数据写入更新到 Cache 里面，再写入到主内存里面；如果数据不在 Cache 里，我们就只更新主内存。 ​ 写直达的这个策略很直观，但是问题也很明显，那就是这个策略很慢。无论数据是不是在 Cache 里面，我们都需要把数据写到主内存里面。这个方式就有点儿像我们上面用 volatile 关键字，始终都要把数据同步到主内存里面。 Write back就是先标记不写回，等到使用的时候再写回主存。 这个时候，我们就想了，既然我们去读数据也是默认从 Cache 里面加载，能否不用把所有的写入都同步到主内存里呢？只写入 CPU Cache 里面是不是可以？ 当然是可以的。在 CPU Cache 的写入策略里，还有一种策略就叫作写回（Write-Back）。这个策略里，我们不再是每次都把数据写入到主内存，而是只写到 CPU Cache 里。只有当 CPU Cache 里面的数据要被“替换”的时候，我们才把数据写入到主内存里面去。 写回策略的过程是这样的：如果发现我们要写入的数据，就在 CPU Cache 里面，那么我们就只是更新 CPU Cache 里面的数据。同时，我们会标记 CPU Cache 里的这个 Block 是脏（Dirty）的。所谓脏的，就是指这个时候，我们的 CPU Cache 里面的这个 Block 的数据，和主内存是不一致的。 如果我们发现，我们要写入的数据所对应的 Cache Block 里，放的是别的内存地址的数据，那么我们就要看一看，那个 Cache Block 里面的数据有没有被标记成脏的。如果是脏的话，我们要先把这个 Cache Block 里面的数据，写入到主内存里面。然后，再把当前要写入的数据，写入到 Cache 里，同时把 Cache Block 标记成脏的。如果 Block 里面的数据没有被标记成脏的，那么我们直接把数据写入到 Cache 里面，然后再把 Cache Block 标记成脏的就好了。 在用了写回这个策略之后，我们在加载内存数据到 Cache 里面的时候，也要多出一步同步脏 Cache 的动作。如果加载内存里面的数据到 Cache 的时候，发现 Cache Block 里面有脏标记，我们也要先把 Cache Block 里的数据写回到主内存，才能加载数据覆盖掉 Cache。 可以看到，在写回这个策略里，如果我们大量的操作，都能够命中缓存。那么大部分时间里，我们都不需要读写主内存，自然性能会比写直达的效果好很多。 ​ 然而，无论是写回还是写直达，其实都还没有解决我们在上面 volatile 程序示例中遇到的问题，也就是多个线程，或者是多个 CPU 核的缓存一致性的问题（就是如果两个线程都想写回内存，那么怎么解决这个问题）。这也就是我们在写入修改缓存后，需要解决的第二个问题。 要解决这个问题，我们需要引入一个新的方法，叫作 MESI 协议。这是一个维护缓存一致性协议。这个协议不仅可以用在 CPU Cache 之间，也可以广泛用于各种需要使用缓存，同时缓存之间需要同步的场景下。 什么 叫缓存一致性问题 以下是多核cpu的cache缓存结构，一般而言多核 CPU 里的每一个 CPU 核，都有独立的属于自己的 L1 Cache 和 L2 Cache。多个 CPU 之间，只是共用 L3 Cache 和主内存。 那什么是缓存一致性呢？我们拿一个有两个核心的 CPU，来看一下。你可以看这里这张图，我们结合图来说。 ​ 比方说，iPhone 降价了，我们要把 iPhone 最新的价格更新到内存里。为了性能问题，它采用了上一讲我们说的写回策略，先把数据写入到 L2 Cache 里面，然后把 Cache Block 标记成脏的。这个时候，数据其实并没有被同步到 L3 Cache 或者主内存里。1 号核心希望在这个 Cache Block 要被交换出去的时候，数据才写入到主内存里。 ​ 如果我们的 CPU 只有 1 号核心这一个 CPU 核，那这其实是没有问题的。不过，我们旁边还有一个 2 号核心呢！这个时候，2 号核心尝试从内存里面去读取 iPhone 的价格，结果读到的是一个错误的价格。这是因为，iPhone 的价格刚刚被 1 号核心更新过。但是这个更新的信息，只出现在 1 号核心的 L2 Cache 里，而没有出现在 2 号核心的 L2 Cache 或者主内存里面。这个问题，就是所谓的缓存一致性问题，1 号核心和 2 号核心的缓存，在这个时候是不一致的。 为了解决这个缓存不一致的问题，我们就需要有一种机制，来同步两个不同核心里面的缓存数据。那这样的机制需要满足什么条件呢？我觉得能够做到下面两点就是合理的。 第一点叫写传播（Write Propagation）。写传播是说，在一个 CPU 核心里，我们的 Cache 数据更新，必须能够传播到其他的对应节点的 Cache Line 里。 第二点叫事务的串行化（Transaction Serialization），事务串行化是说，我们在一个 CPU 核心里面的读取和写入，在其他的节点看起来，顺序是一样的。 第一点写传播很容易理解。既然我们数据写完了，自然要同步到其他 CPU 核的 Cache 里。但是第二点事务的串行化，可能没那么好理解，我这里仔细解释一下。 ​ 我们还拿刚才修改 iPhone 的价格来解释。这一次，我们找一个有 4 个核心的 CPU。1 号核心呢，先把 iPhone 的价格改成了 5000 块。差不多在同一个时间，2 号核心把 iPhone 的价格改成了 6000 块。这里两个修改，都会传播到 3 号核心和 4 号核心。 我们发现，这样还是会导致，数据的不一致性。 ​ 事实上，我们需要的是，从 1 号到 4 号核心，都能看到相同顺序的数据变化。比如说，都是先变成了 5000 块，再变成了 6000 块。这样，我们才能称之为实现了事务的串行化。 ​ 而在 CPU Cache 里做到事务串行化，需要做到两点，第一点是一个 CPU 核心对于数据的操作，需要同步通信给到其他 CPU 核心。第二点是，如果两个 CPU 核心里有同一个数据的 Cache，那么对于这个 Cache 数据的更新，需要有一个“锁”的概念。只有拿到了对应 Cache Block 的“锁”之后，才能进行对应的数据更新。接下来，我们就看看实现了这两个机制的 MESI 协议。 总线嗅探机制和 MESI 协议​ 要解决缓存一致性问题，首先要解决的是多个 CPU 核心之间的数据传播问题。最常见的一种解决方案呢，叫作总线嗅探（Bus Snooping）。这个名字听起来，你多半会很陌生，但是其实特很好理解。 ​ 这个策略，本质上就是把所有的读写请求都通过总线（Bus）广播给所有的 CPU 核心，然后让各个核心去“嗅探”这些请求，再根据本地的情况进行响应。 ​ 总线本身就是一个特别适合广播进行数据传输的机制，所以总线嗅探这个办法也是我们日常使用的 Intel CPU 进行缓存一致性处理的解决方案。 ​ 基于总线嗅探机制，其实还可以分成很多种不同的缓存一致性协议。不过其中最常用的，就是今天我们要讲的 MESI 协议。和很多现代的 CPU 技术一样，MESI 协议也是在 Pentium 时代，被引入到 Intel CPU 中的。 ​ MESI 协议，是一种叫作写失效（Write Invalidate）的协议。在写失效协议里，只有一个 CPU 核心负责写入数据，其他的核心，只是同步读取到这个写入。在这个 CPU 核心写入 Cache 之后，它会去广播一个“失效”请求告诉所有其他的 CPU 核心。其他的 CPU 核心，只是去判断自己是否也有一个“失效”版本的 Cache Block，然后把这个也标记成失效的就好了。 ​ 相对于写失效协议，还有一种叫作写广播（Write Broadcast）的协议。在那个协议里，一个写入请求广播到所有的 CPU 核心，同时更新各个核心里的 Cache。 ​ 写广播在实现上自然很简单，但是写广播需要占用更多的总线带宽。写失效只需要告诉其他的 CPU 核心，哪一个内存地址的缓存失效了，但是写广播还需要把对应的数据传输给其他 CPU 核心。 MESI 协议的由来呢，来自于我们对 Cache Line 的四个不同的标记，分别是： M：代表已修改（Modified） E：代表独占（Exclusive） S：代表共享（Shared） I：代表已失效（Invalidated） 我们先来看看“已修改”和“已失效”，这两个状态比较容易理解。所谓的“已修改”，就是我们上一讲所说的“脏”的 Cache Block。Cache Block 里面的内容我们已经更新过了，但是还没有写回到主内存里面。而所谓的“已失效“，自然是这个 Cache Block 里面的数据已经失效了，我们不可以相信这个 Cache Block 里面的数据。 然后，我们再来看“独占”和“共享”这两个状态。这就是 MESI 协议的精华所在了。无论是独占状态还是共享状态，缓存里面的数据都是“干净”的。这个“干净”，自然对应的是前面所说的“脏”的，也就是说，这个时候，Cache Block 里面的数据和主内存里面的数据是一致的。 那么“独占”和“共享”这两个状态的差别在哪里呢？这个差别就在于，在独占状态下，对应的 Cache Line 只加载到了当前 CPU 核所拥有的 Cache 里。其他的 CPU 核，并没有加载对应的数据到自己的 Cache 里。这个时候，如果要向独占的 Cache Block 写入数据，我们可以自由地写入数据，而不需要告知其他 CPU 核。 在独占状态下的数据，如果收到了一个来自于总线的读取对应缓存的请求，它就会变成共享状态。这个共享状态是因为，这个时候，另外一个 CPU 核心，也把对应的 Cache Block，从内存里面加载到了自己的 Cache 里来。 而在共享状态下，因为同样的数据在多个 CPU 核心的 Cache 里都有。所以，当我们想要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他 CPU 核心里面的 Cache，都变成无效的状态，然后再更新当前 Cache 里面的数据。这个广播操作，一般叫作 RFO（Request For Ownership），也就是获取当前对应 Cache Block 数据的所有权。 ​ 有没有觉得这个操作有点儿像我们在多线程里面用到的读写锁。在共享状态下，大家都可以并行去读对应的数据。但是如果要写，我们就需要通过一个锁，获取当前写入位置的所有权。 ​ 整个 MESI 的状态，可以用一个有限状态机来表示它的状态流转。需要注意的是，对于不同状态触发的事件操作，可能来自于当前 CPU 核心，也可能来自总线里其他 CPU 核心广播出来的信号。我把对应的状态机流转图放在了下面，你可以对照着Wikipedia 里面 MESI 的内容，仔细研读一下。 好的文档：https://www.cnblogs.com/yanlong300/p/8986041.html ​ https://www.jianshu.com/p/0e036fa7af2a ​ https://www.cnblogs.com/ynyhl/p/12119690.html 不错 面试中如果问到你对volatile的理解？那么首先你应该从内存模型，原子性，有序性，可见性的理解，然后才是volatile关键字的理解和他解决的问题。 ， synchronized的对象头核心知识synchronized(obj)到底锁的是什么 ？ - 对象头synchronized (obj) &#123; ///业务代码&#125; ​ synchronized到底锁的是整个业务代码块还是obj？ ​ 我们知道synchronized锁住的是obj对象，达到互斥的效果，那么在那里记录是那个线程锁住的呢？。而且synchronized是会锁升级的，那么在哪里记录这些锁的信息？哪里记录线程对obj对象上锁成功呢？ ​ 举个例子，我们知道ReentrantLock的lock方法，是通过对state的cas操作标识是否上锁成功，state如果能从0设置成1，那么说明上锁成功，如果多次重入，那么state标识重入次数。 ​ 我们发现单从这段代码synchronized (obj)，他并没有像lock 对象那样，有个成员属性state来做一些锁的标志和判断。而且synchronized 只是一个关键字，那么只有一种解释，那就是，加锁的信息，肯定是在obj对象里面保存着！！ ​ 换句话说，我们的问题是：如果某个线程对obj对象加锁成功，那么他是怎么标记加锁成功的，到底修改了obj对象什么信息，来表示加锁成功？ 这里先说结论：加锁实际上是改变了对象的对象头！！ java对象的布局 - 计算对象大小我们要了解对象头，那么首先首当其冲就要先了解一下对象的构成。首先我们可以确定，对象的属性肯定是构成对象的一部分。 public class Student &#123; public int value;//对象属性&#125; 除了对象属性之外，还有对象头和数据对齐两个模块。 也就是说对象是由：对象属性、对象头、数据对齐三个组件构成。 其中对象头是肯定存在的，但是对象属性和数据对齐却不一定存在。一个对象可以没有成员属性，这个我们是知道的，那么数据对齐是什么意思呢？我们首先要明确，java定义对象的大小时，规定，对象的字节大小必须是8的倍数。 ​ 那么数据对齐就是为了凑够8的倍数而来的。举个例子： public class Student &#123; public boolean flag;//1byte public int value;//4个byte &#125;//首先至于boolean到底占几个字节，java并没有明确规定，他说这个取决于jvm的实现，我这里假设是占1byte。 ​ 那么上面student对象单从成员属性而言，一共是占了5个字节（这里先暂时不考虑对象头），那么很明显5不是8的倍数。所以需要补上3个字节，一共是8个字节，那么就是8的倍数。那么这里所说的补上3个字节，就是数据对齐模块的大小和作用。 ​ 换言之，假设对象成员属性的大小本身就是8的倍数，那么数据对齐也就不存在了。 所以说，对象的布局中，至于对象头是固定存在的，其他两个是不一定存在。 也就是说，一个对象的大小，等于这三个组成的大小之和。 怎么证明对象是由这三个部分组成？ 接下来我们通过一个工具类来输出对象的信息，从而证明。 &lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt; &lt;/dependency&gt; //计算对象大小 public class Obj &#123; public boolean flag; public int value; public static void main(String[] args) &#123; Obj obj = new Obj(); System.out.println( VM.current().details() ); System.out.println( ClassLayout.parseInstance(obj).toPrintable() ); &#125;&#125; 输出： Running 64-bit HotSpot VM.//此时表示是64位虚拟机上，因为32位虚拟机下面的输出会有不同 Using compressed oop with 0-bit shift.、、 Using compressed klass with 3-bit shift.//开启指针压缩 Objects are 8 bytes aligned. Field sizes by type: 4, 1, 1, 2, 2, 4, 4, 8, 8 [bytes] Array element sizes: 4, 1, 1, 2, 2, 4, 4, 8, 8 [bytes] com.kingge.obj.Obj object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 05 c0 00 20 (00000101 11000000 00000000 00100000) (536920069) 12 4 int Obj.value 0 16 1 boolean Obj.flag false 17 7 (loss due to the next object alignment)Instance size: 24 bytesSpace losses: 0 bytes internal + 7 bytes external = 7 bytes total ​ 可以看到一共是24个字节，其中对象头是12个字节，两个成员变量一共是5个字节，那么因为加起来17个字节，并不是8的倍数，那么需要数据对齐，于是加上7个字节的数据对齐。 ​ 从而证明了，我们那上面所说的结论是正确的。 假设对象修改为： public class Obj &#123; public int value; public static void main(String[] args) &#123; Obj obj = new Obj(); System.out.println( ClassLayout.parseInstance(obj).toPrintable() ); &#125;&#125; 输出： com.kingge.obj.Obj object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 05 c0 00 20 (00000101 11000000 00000000 00100000) (536920069) 12 4 int Obj.value 0Instance size: 16 bytesSpace losses: 0 bytes internal + 0 bytes external = 0 bytes total ​ 你会发现，数据对齐，没有了？那是因为，刚好属性+对象头一共是16个字节，是8的倍数。 总结一下对象是由：对象头（大小固定，64位虚拟机下一共占12字节），成员属性（大小根据数据类型决定），数据对齐（不一定存在）。 那么对象的大小就是由着三个部分组成。 对象头的构成在上面的分析中，我们得到64位虚拟机下，对象头的大小是12字节，一共是96位。 我们通过查看一下官方文档，获得对象头构成： http://openjdk.java.net/groups/hotspot/docs/HotSpotGlossary.html object headerCommon structure at the beginning of every GC-managed heap object. (Every oop points to an object header.) Includes fundamental information about the heap object&apos;s layout, type, GC state, synchronization state, and identity hash code. Consists of two words. In arrays it is immediately followed by a length field. Note that both Java objects and VM-internal objects have a common object header format. ----- 翻译来自有道，可能不准确，但是可以知道大概意思---- 每个gc管理的堆对象开头的公共结构。(每个oop都指向一个对象标头。)包括堆对象的布局、类型、GC状态、同步状态和标识哈希码的基本信息。由两个词组成。在数组中，它后面紧跟着一个长度字段。注意，Java对象和vm内部对象都有一个通用的对象头格式。 ​ 也就是说：对象头包含了这些信息，堆对象的布局、类型、GC状态、同步状态和标识哈希码的基本信息。我么终于发现了，在文章最开始提出的问题的答案，那就是在那里记录了加锁的信息？就是这里的同步状态，而且我们发现，对象头还保存了对象hashcode的值。 我们知道了对象头包含的信息，但是并没有说明对象头由哪些部分组成？ ​ 上面的翻译已经说了 Consists of two words。也就是对象头由两个部分组成：klass pointer和mark word klass pointer - 保存了类的指针，也就是当前对象是基于那个Class创建的。The second word of every object header. Points to another object (a metaobject) which describes the layout and behavior of the original object. For Java objects, the &quot;klass&quot; contains a C++ style &quot;vtable&quot;.mark wordThe first word of every object header. Usually a set of bitfields including synchronization state and identity hash code. May also be a pointer (with characteristic low bit encoding) to synchronization related information. During GC, may contain GC state bits. 那么markword他的结构是怎么样的呢？我们通过看源码的形式查看的他的结构 openjdk\\hotspot\\src\\share\\vm\\oops\\markOop.hpp // The markOop describes the header of an object.//// Note that the mark is not a real oop but just a word.// It is placed in the oop hierarchy for historical reasons.//// Bit-format of an object header (most significant first, big endian layout below)://////////////////////这个位置就说明了，在32位和64位环境下，markword的结构、、、、、、// 32 bits:// --------// hash:25 ------------&gt;| age:4 biased_lock:1 lock:2 (normal object)// JavaThread*:23 epoch:2 age:4 biased_lock:1 lock:2 (biased object)// size:32 ------------------------------------------&gt;| (CMS free block)// PromotedObject*:29 ----------&gt;| promo_bits:3 -----&gt;| (CMS promoted object)//// 64 bits:// --------// unused:25 hash:31 --&gt;| unused:1 age:4 biased_lock:1 lock:2 (normal object)// JavaThread*:54 epoch:2 unused:1 age:4 biased_lock:1 lock:2 (biased object)// PromotedObject*:61 ---------------------&gt;| promo_bits:3 -----&gt;| (CMS promoted object)// size:64 -----------------------------------------------------&gt;| (CMS free block)//// unused:25 hash:31 --&gt;| cms_free:1 age:4 biased_lock:1 lock:2 (COOPs &amp;&amp; normal object)// JavaThread*:54 epoch:2 cms_free:1 age:4 biased_lock:1 lock:2 (COOPs &amp;&amp; biased object)// narrowOop:32 unused:24 cms_free:1 unused:4 promo_bits:3 -----&gt;| (COOPs &amp;&amp; CMS promoted object)// unused:21 size:35 --&gt;| cms_free:1 unused:7 ------------------&gt;| (COOPs &amp;&amp; CMS free block)//// - hash contains the identity hash value: largest value is// 31 bits, see os::random(). Also, 64-bit vm's require// a hash value no bigger than 32 bits because they will not// properly generate a mask larger than that: see library_call.cpp// and c1_CodePatterns_sparc.cpp.//// - the biased lock pattern is used to bias a lock toward a given// thread. When this pattern is set in the low three bits, the lock// is either biased toward a given thread or \"anonymously\" biased,// indicating that it is possible for it to be biased. When the// lock is biased toward a given thread, locking and unlocking can// be performed by that thread without using atomic operations.// When a lock's bias is revoked, it reverts back to the normal// locking scheme described below.//// Note that we are overloading the meaning of the \"unlocked\" state// of the header. Because we steal a bit from the age we can// guarantee that the bias pattern will never be seen for a truly// unlocked object.//// Note also that the biased state contains the age bits normally// contained in the object header. Large increases in scavenge// times were seen when these bits were absent and an arbitrary age// assigned to all biased objects, because they tended to consume a// significant fraction of the eden semispaces and were not// promoted promptly, causing an increase in the amount of copying// performed. The runtime system aligns all JavaThread* pointers to// a very large value (currently 128 bytes (32bVM) or 256 bytes (64bVM))// to make room for the age bits &amp; the epoch bits (used in support of// biased locking), and for the CMS \"freeness\" bit in the 64bVM (+COOPs).//// [JavaThread* | epoch | age | 1 | 01] lock is biased toward given thread// [0 | epoch | age | 1 | 01] lock is anonymously biased//// - the two lock bits are used to describe three states: locked/unlocked and monitor.//// [ptr | 00] locked ptr points to real header on stack// [header | 0 | 01] unlocked regular object header// [ptr | 10] monitor inflated lock (header is wapped out)// [ptr | 11] marked used by markSweep to mark an object// not valid at any other time//// We assume that stack/thread pointers have the lowest two bits cleared. 可以得到，markword的构成是：unused:25 hash:31 –&gt;| unused:1 age:4 biased_lock:1 lock:2 特殊提醒，我们在JVM中讲到，为什么当对象年龄达到15的时候，才会进入老年代。那么15这个数字是怎么得来的，就是这里的age：4，他占四位，四位能够表达的最大数是15，这就是-XX:MaxTenuringThreshold选项最大值为15的原因 ​ 也就是25+31+1+4+1+2 == 64bit，也就是说在64位对象头中，markword占64位，那么也就意味着，对象类型指针kclass pointer占32bit（96-64） 特殊提醒！！！！，有些时候我们发现kclass pointer的大小是：64位，也就是需要8个字节，并不是上面所说的需要32位。这两种说法都是对的，因为jvm默认开启了指针压缩，会把kclass pointer压缩成4个字节。如果没有卡其指针压缩，那么就是8个字节。 那么怎么知道jvm 是否开启了指针压缩呢？System.out.println( VM.current().details() ); 通过这个命令就可以输出，jvm当前信息。 检验kclass pointer未压缩前大小是否是8个字节测试例子： public class Obj &#123; public int value; public static void main(String[] args) &#123; Obj obj = new Obj(); System.out.println( VM.current().details() ); System.out.println( ClassLayout.parseInstance(obj).toPrintable() ); &#125;&#125; 输出： # Running 64-bit HotSpot VM.# Using compressed oop with 0-bit shift.# Using compressed klass with 3-bit shift.//默认开启指针压缩# Objects are 8 bytes aligned.# Field sizes by type: 4, 1, 1, 2, 2, 4, 4, 8, 8 [bytes]# Array element sizes: 4, 1, 1, 2, 2, 4, 4, 8, 8 [bytes]com.kingge.obj.Obj object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 05 c0 00 20 (00000101 11000000 00000000 00100000) (536920069) 12 4 int Obj.value 0Instance size: 16 bytesSpace losses: 0 bytes internal + 0 bytes external = 0 bytes total 可以看到输出：# Using compressed klass with 3-bit shift.//表示默认开启指针压缩 此时，对象头大小是12字节，其中markword占8个字节，对象指针kclass pointer 占4个字节。 下面我们去掉指针压缩，再次运行。 使用jvm参数去掉指针压缩： -XX:-UseCompressedOops 。 输出： # Running 64-bit HotSpot VM.# Objects are 8 bytes aligned.# Field sizes by type: 8, 1, 1, 2, 2, 4, 4, 8, 8 [bytes]# Array element sizes: 8, 1, 1, 2, 2, 4, 4, 8, 8 [bytes]com.kingge.obj.Obj object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 20 04 b1 17 (00100000 00000100 10110001 00010111) (397476896) 12 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 16 4 int Obj.value 0 20 4 (loss due to the next object alignment)Instance size: 24 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 我们发现，此时，对象头大小是16字节，其中markword固定占8个字节，那么很明显剩下的8个字节就是对象指针kclass pointer 的大小。 得证！！！未开启压缩的情况下，对象指针在64位虚拟机下，占8个字节。 markword构成我们知道markword在64位虚拟机下，是占8个字节。 首先我们要知道，对象一共有几个状态？ 初始状态 - 刚new出来 成为偏向锁 成为轻量级锁 成为重量级锁 GC标记-表示可垃圾回收 再来看一下 markword结构： unused:25 hash:31 –&gt;| unused:1 age:4 biased_lock:1 lock:2 ​ ​ 从上面可以看到，锁标记(lock)占了2位，那么两位的二级制，只有四种可能，00，01,10,11.那么他怎么表示上面这五种状态呢？ ​ 通过biased_lock偏向锁的1个标志位，来表示，对象的五种状态，右下图可见。 ​ 偏向锁和无锁状态表示为同一个状态（lock都是01），然后根据图中偏向锁的标识再去标识是无锁还是偏向锁状态； 下面我们拉分析一下在初始状态下，对象的对象头的markword信息。 com.kingge.obj.Obj object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 05 c0 00 20 (00000101 11000000 00000000 00100000) (536920069) 12 4 int Obj.value 0Instance size: 16 bytesSpace losses: 0 bytes internal + 0 bytes external = 0 bytes total 从上面的图再根据下面输出的对象结构信息，我们可以得出，markword 一共是64位，八个字节。剩下的4个字节就是kclass pointer（也就是Obj.class的指针） markword也就是这64位： OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 05 c0 00 20 (00000101 11000000 00000000 00100000) 那么其中这8个字节，就是存储的markword的信息。 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 剩下的未标黑色的8位保存的就是：unused:1 age:4 biased_lock:1 lock:2 ​ 这八位等于00000001，那么正好跟，初始化状态的对象的状态是一样的。前六位都是0，lock等于01表示正常（因为上面的代码中obj对象是刚new出来的。unuseed等于0，那么gc年龄肯定是0，也就是age的四位都是0，biaed_lock也是0，lock等于01） 那为什么在上面输出的31位的hashcode都是0？ 原因是没有调用对象的hashcode方法生成hashcode。 测试代码修改为： public class Obj &#123; public int value; public static void main(String[] args) &#123; Obj obj = new Obj(); System.out.println( VM.current().details() ); System.out.println( Integer.toHexString(obj.hashCode()));//多加了这一行，获取对象hashcode，以16进制输出。 System.out.println( ClassLayout.parseInstance(obj).toPrintable() ); &#125;&#125; 输出： obj对象hashcode：37bba400com.kingge.obj.Obj object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 a4 bb (00000001 00000000 10100100 10111011) (-1146879999) 4 4 (object header) 37 00 00 00 (00110111 00000000 00000000 00000000) (55) 8 4 (object header) 05 c0 00 20 (00000101 11000000 00000000 00100000) (536920069) 12 4 int Obj.value 0Instance size: 16 bytesSpace losses: 0 bytes internal + 0 bytes external = 0 bytes total 你会发现，输出的对象结构信息中，hashcode的值，已经存在。且为：37bba400。 跟上面的value字段输出一致。 模拟偏向锁和轻量级锁对象头测试代码： public class Obj &#123; public int value; public static void main(String[] args) throws InterruptedException &#123; // Thread.sleep(5000); 放开这一行，那么输出的就是偏向锁，或者设置-XX:BiasedLockingStartupDelay=0 Obj obj = new Obj(); System.out.println(\"befor lock\"); System.out.println(ClassLayout.parseInstance(obj).toPrintable()); synchronized (obj)&#123; System.out.println(\"lock ing\"); System.out.println(ClassLayout.parseInstance(obj).toPrintable()); &#125; System.out.println(\"after lock\"); System.out.println(ClassLayout.parseInstance(obj).toPrintable()); &#125;&#125; 输出： 如果把上面的 // Thread.sleep(5000); 注释放开或者设置-XX:BiasedLockingStartupDelay=0，那么此刻输出的是偏向锁的信息，前八位是：0 0000 1 01 为什么不是偏向锁为什么呢？从上面代码看，只有一个main线程在获取锁啊，应该是偏向锁才对啊？ 经过翻hotspot源码发现： 路径： http://hg.openjdk.java.net/jdk/jdk/file/6659a8f57d78/src/hotspot/share/runtime/globals.hpp product(bool, UseBiasedLocking, true, \"Enable biased locking in JVM\") product(intx, BiasedLockingStartupDelay, 4000, \"Number of milliseconds to wait before enabling biased locking\") range(0, (intx)(max_jint-(max_jint%PeriodicTask::interval_gran))) constraint(BiasedLockingStartupDelayFunc,AfterErgo) BiasedLockingStartupDelay, 4000 //偏向锁延迟4000ms启动//所以我们需要让main线程jvm把偏向锁的初始化工作准备好，后再去争抢锁。 想想为什么偏向锁会延迟？ 我们来看官方解释： void BiasedLocking::init() &#123; // If biased locking is enabled, schedule a task to fire a few // seconds into the run which turns on biased locking for all // currently loaded classes as well as future ones. This is a // workaround for startup time regressions due to a large number of // safepoints being taken during VM startup for bias revocation. // Ideally we would have a lower cost for individual bias revocation // and not need a mechanism like this. if (UseBiasedLocking) &#123; if (BiasedLockingStartupDelay &gt; 0) &#123; EnableBiasedLockingTask* task = new EnableBiasedLockingTask(BiasedLockingStartupDelay); task-&gt;enroll(); &#125; else &#123; VM_EnableBiasedLocking op(false); VMThread::execute(&amp;op); &#125; &#125;&#125;注意看上面的注释和代码 英文大概翻译为： 当jvm启动记载资源的时候，初始化的对象加偏向锁会耗费资源，减少大量偏向锁撤销的成本（jvm的偏向锁的优化） 这就解释了加上睡眠5000ms，偏向锁就会出现的原因； ​ 为了方便我们测试我们可以直接通过修改jvm的参数来禁止偏向锁延迟（不用在代码睡眠了）： -XX:+UseBiasedLocking -XX:BiasedLockingStartupDelay=0 ​ 注意：这块严谨来说，在jdk 1.6之后，关于使用偏向锁和轻量级锁，jvm是有优化的，在没有禁止偏向锁延迟的情况下，使用的是轻量级锁；禁止偏向锁延迟的话，使用的是偏向锁； ​ 总而言之：因为jvm 在启动的时候需要加载资源，这些对象加上偏向锁没有任何意义啊，减少了大量偏向锁撤销的成本；所以默认就把偏向锁延迟了4000ms； 如果还不能确定是否延迟，那么我们可以通过查看jvm默认启动参数来查看： $ jinfo -flag BiasedLockingStartupDelay 16212-XX:BiasedLockingStartupDelay=4000可以发现启动的jvm。BiasedLockingStartupDelay默认是4000 比较偏向锁和轻量级锁性能测试轻量级锁： public class Obj1 &#123; int i=0; public synchronized void parse()&#123; i++; &#125; public static void main(String[] args) throws InterruptedException &#123; Obj1 obj1 = new Obj1(); long start = System.currentTimeMillis(); //调用同步方法1000000000L 来计算1000000000L的++，对比偏向锁和轻量级锁的性能 for(int i =0;i&lt;1000000000L;i++)&#123; obj1.parse(); &#125; long end = System.currentTimeMillis(); System.out.println(String.format(\"%sms\", end - start));//轻量级锁：21743ms //偏向锁：1801ms //可以发现，偏向锁比轻量级锁快了12倍左右 &#125;&#125; 测试偏向锁，只需要运行的时候指定：-XX:BiasedLockingStartupDelay=0，表示不需要延迟设置偏向锁。 ​ 为什么呢？我们知道偏向锁是支持重入的，意思就是，假设下次获取锁的线程还是之前的线程，那么不需要在申请锁，只需要增加重入次数即可。这个假设是只有一个线程 需要获取锁的情况下。 ​ 但是如果存在多个线程获取锁，那么锁会升级，升级为轻量级锁，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。 重量级锁测试代码： package com.kingge.obj;import java.util.concurrent.TimeUnit;import org.openjdk.jol.info.ClassLayout;public class TestHeavyLock &#123; public static void main(String[] args) &#123; ObjLock objLock = new ObjLock(); System.out.println(\"befor lock\"); System.out.println(ClassLayout.parseInstance(objLock).toPrintable());//无锁 Thread thread = new Thread( () -&gt; &#123; synchronized (objLock) &#123; try &#123; TimeUnit.SECONDS.sleep(5);//休息五秒的目的是，让sysn也同时去获取锁，让锁升级 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; ); thread.start(); System.out.println(\"thread locking\"); System.out.println(ClassLayout.parseInstance(objLock).toPrintable());//轻量级锁 sysn(objLock);//main线程请求锁，这个时候轻量级锁已经存在，那么锁升级 System.out.println(\"after main lock\"); System.out.println(ClassLayout.parseInstance(objLock).toPrintable());//重量级锁 System.gc(); System.out.println(\"after gc\"); System.out.println(ClassLayout.parseInstance(objLock).toPrintable());//无锁 &#125; public static void sysn(ObjLock objLock) &#123; synchronized (objLock) &#123; System.out.println(\"main lock ing\"); System.out.println(ClassLayout.parseInstance(objLock).toPrintable()); &#125; &#125; &#125;class ObjLock &#123;&#125; 输出： befor lockcom.kingge.obj.ObjLock object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 01 00 00 00 (00000001 00000000 00000000 00000000) (1) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 43 c0 00 20 (01000011 11000000 00000000 00100000) (536920131) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes totalthread lockingcom.kingge.obj.ObjLock object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 60 f2 c9 1a (01100000 11110010 11001001 00011010) (449442400) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 43 c0 00 20 (01000011 11000000 00000000 00100000) (536920131) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes totalmain lock ingcom.kingge.obj.ObjLock object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 4a 3c 4a 03 (01001010 00111100 01001010 00000011) (55196746) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 43 c0 00 20 (01000011 11000000 00000000 00100000) (536920131) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes totalafter main lockcom.kingge.obj.ObjLock object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 4a 3c 4a 03 (01001010 00111100 01001010 00000011) (55196746) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 43 c0 00 20 (01000011 11000000 00000000 00100000) (536920131) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes totalafter gccom.kingge.obj.ObjLock object internals: OFFSET SIZE TYPE DESCRIPTION VALUE 0 4 (object header) 09 00 00 00 (00001001 00000000 00000000 00000000) (9) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 43 c0 00 20 (01000011 11000000 00000000 00100000) (536920131) 12 4 (loss due to the next object alignment)Instance size: 16 bytesSpace losses: 0 bytes internal + 4 bytes external = 4 bytes total 分析锁的前八位： befor lock0 0000 0 01 //无锁，gc年龄是0 thread locking0 1100 0 00 //轻量级锁，gc年龄是12，说明ObjLock对象在survivor倒腾了12次 main lock ing0 1001 0 10 //重量级锁 after main lock0 1001 0 10 //重量级锁？？？？ after gc0 0001 0 01 //gc回收变无锁（就会发现gc回收过一次之后对象由最初刚开始的 0000 变成了 0001 年龄+1了） ​ 上面的分析应该很容易看得懂，那么我们注意，打问号的地方。按道理main线程执行完sync，也就意味着thread线程在这之前也已经释放了锁（不然sync方法也无法得到锁执行），objLock此刻是没有线程去锁住才对，那么应该是是无锁状态，那为什么还是输出的是重量级锁的信息呢？ ​ 是因为重量级锁释放会有延迟，可以在sync()方法中加入睡眠。 public static void sysn(ObjLock objLock) throws InterruptedException &#123; synchronized (objLock) &#123; System.out.println(&quot;main lock ing&quot;); System.out.println(ClassLayout.parseInstance(objLock).toPrintable()); &#125; Thread.sleep(5000);&#125; //此时再看输出，你会发现。 after main lock0 0000 0 01 //已经是无锁状态 偏向锁的epoch作用这里的 epoch 值是一个什么概念呢？ 我们先从偏向锁的撤销讲起。当请求加锁的线程和锁对象标记字段保持的线程地址不匹配时（而且 epoch 值相等，如若不等，那么当前线程可以将该锁重偏向至自己），Java 虚拟机需要撤销该偏向锁。这个撤销过程非常麻烦，它要求持有偏向锁的线程到达安全点，再将偏向锁替换成轻量级锁； 如果某一类锁对象的总撤销数超过了一个阈值（对应 jvm参数 -XX:BiasedLockingBulkRebiasThreshold，默认为 20），那么 Java 虚拟机会宣布这个类的偏向锁失效；（这里说的就是批量重偏向） ​ JVM源码： product(intx, BiasedLockingBulkRebiasThreshold, 20, \\ &quot;Threshold of number of revocations per type to try to &quot; \\ &quot;rebias all objects in the heap of that type&quot;) \\ range(0, max_intx) \\ constraint(BiasedLockingBulkRebiasThresholdFunc,AfterErgo) \\ 具体的做法便是在每个类中维护一个 epoch 值，你可以理解为第几代偏向锁。当设置偏向锁时，Java 虚拟机需要将该 epoch 值复制到锁对象的标记字段中； 在宣布某个类的偏向锁失效时，Java 虚拟机实则将该类的 epoch 值加 1，表示之前那一代的偏向锁已经失效。而新设置的偏向锁则需要复制新的 epoch 值； 为了保证当前持有偏向锁并且已加锁的线程不至于因此丢锁，Java 虚拟机需要遍历所有线程的 Java 栈，找出该类已加锁的实例，并且将它们标记字段中的 epoch 值加 1。该操作需要所有线程处于安全点状态； 如果总撤销数超过另一个阈值（对应 jvm 参数 -XX:BiasedLockingBulkRevokeThreshold，默认值为 40），那么 Java 虚拟机会认为这个类已经不再适合偏向锁。此时，Java 虚拟机会撤销该类实例的偏向锁，并且在之后的加锁过程中直接为该类实例设置轻量级锁(这里说的就是偏向批量撤销) ​ JVM源码： product(intx, BiasedLockingBulkRevokeThreshold, 40, \\ &quot;Threshold of number of revocations per type to permanently &quot; \\ &quot;revoke biases of all objects in the heap of that type&quot;) \\ range(0, max_intx) \\ constraint(BiasedLockingBulkRevokeThresholdFunc,AfterErgo) 锁升级过程​ 所谓锁的升级、降级，就是 JVM 优化 synchronized 运行的机制，当 JVM 检测到不同的竞争状况时，会自动切换到适合的锁实现，这种切换就是锁的升级、降级： 当没有竞争出现时，默认会使用偏向锁。JVM 会利用 CAS 操作（compare and swap），在对象头上的 Mark Word 部分设置线程 ID，以表示这个对象偏向于当前线程，所以并不涉及真正的互斥锁。这样做的假设是基于在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏向锁可以降低无竞争开销。 如果有另外的线程试图锁定某个已经被偏向过的对象，JVM 就需要撤销（revoke）偏向锁，并切换到轻量级锁实现。轻量级锁依赖 CAS 操作 Mark Word 来试图获取锁，如果重试成功，就使用轻量级锁；否则，进一步升级为重量级锁 膨胀过程的实现比较复杂，大概实现过程如下： 1、整个膨胀过程在自旋下完成； 2、mark-&gt;has_monitor()方法判断当前是否为重量级锁，即Mark Word的锁标识位为 10，如果当前状态为重量级锁，执行步骤（3），否则执行步骤（4）； 3、mark-&gt;monitor()方法获取指向ObjectMonitor的指针，并返回，说明膨胀过程已经完成； 4、如果当前锁处于膨胀中，说明该锁正在被其它线程执行膨胀操作，则当前线程就进行自旋等待锁膨胀完成，这里需要注意一点，虽然是自旋操作，但不会一直占用cpu资源，每隔一段时间会通过os::NakedYield方法放弃cpu资源，或通过park方法挂起；如果其他线程完成锁的膨胀操作，则退出自旋并返回； 5、如果当前是轻量级锁状态，即锁标识位为 00，膨胀过程如下： 通过omAlloc方法，获取一个可用的ObjectMonitor monitor，并重置monitor数据； 通过CAS尝试将Mark Word设置为markOopDesc:INFLATING，标识当前锁正在膨胀中，如果CAS失败，说明同一时刻其它线程已经将Mark Word设置为markOopDesc:INFLATING，当前线程进行自旋等待膨胀完成； 如果CAS成功，设置monitor的各个字段：_header、_owner和_object等，并返回； 6、如果是无锁，重置监视器值； 好的相关文档https://www.cnblogs.com/JonaLin/p/11571482.html#autoid-2-0-0 非常推荐！！！！！ https://www.cnblogs.com/yrjns/p/12152975.html 题外话：Java中boolean类型占几个字节，你知道吗？https://blog.csdn.net/amoscn/article/details/97377833 cas是什么 - 无锁并发安全实现 - 轻量级锁 概念 他就是比较并交换的缩写 - compareAndSet。他的作用就是通过比较期望值，来判断本次操作能否成功。 也就是说比较当前工作内存的值和主内存中的值，如果相等，那么执行相应的逻辑操作（临界区操作），如果不相等，那么一直比较到相同为止。 那么他究竟是什么呢？ 我们来回忆一下AtomicInteger的compareAndSet方法。 第一个参数和第二个参数是相辅相成的，只有在第一个参数比较成功后，才能够成功赋值第二个参数的值。 那么第一个参数是跟谁比较呢？答案是，跟主内存中目前的值比较。 ​ 重要提示：首先我们要明确一点，那就是，线程对数据的操作，都是先把数据从主内存（电脑内存），读取出来，然后load自己的线程栈中，再进行自己的运算逻辑，然后线程结束后，再把新值写回主内存。（那么可想而知，多线程的情况下，必然会发生线程不安全问题，因为每个线程把自己的处理结果写回主内存的时机不同，导致结果出现各种变化） 接下来演示使用cas的例子： public static void main(String[] args) &#123; AtomicInteger atomicInteger = new AtomicInteger(5);//在主内存中，设置共享变量的值为5 System.out.println(atomicInteger.compareAndSet(5, 2019)+&quot;\\t current&quot;+atomicInteger.get()); System.out.println(atomicInteger.compareAndSet(5, 2014)+&quot;\\t current&quot;+atomicInteger.get()); &#125; 输出是： true current 2019false current 2019 ​ 为什么第一次compareAndSet能成功，第二次就不行了呢？这是因为，初始的AtomicInteger的值是5，那么在主内存中就是5。第一次执行cas操作，所要表达的逻辑是，我要把atomicInteger修改为2019，但是我是要从5修改为2019。也就是说，主内存中的atomicInteger的值必须是5，我才能紧接着把atomicInteger修改为2019。 很明显在第一次cas操作之前，主内存中的atomicInteger一直是5， 所以比较成功。交换值，5修改为2019，成功写入主内存中的atomicInteger。 ​ 但是第二次cas操作，表达的是我要从5修改为2014，但是通过比较主内存中的atomicInteger，发现atomicInteger的值是2019，那么比较失败，值交换也相继失败。所以主内存中atomicInteger的值保持不变，还是2019。 也就是说，CAS的本质就是，先比较后交换。 我们接着查看一下compareAndSet的源码： /** * Atomically sets the value to the given updated value * if the current value {@code ==} the expected value. * * @param expect the expected value * @param update the new value * @return {@code true} if successful. False return indicates that * the actual value was not equal to the expected value. */ public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update); } this：就是当前的atomicInteger对象- AtomicInteger atomicInteger = new AtomicInteger(5); valueOffset：我们知道atomicInteger的值是保存在value成员变量中，而且他是通过volatile修饰。而且这个值是在创建AtomicInteger之前（在调用构造器之前）通过静态代码块，进行赋值。目的就是求得，value成员变量在内存中的地址。 static {try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); } catch (Exception ex) { throw new Error(ex); } } expect：我们的预期值，也即是5，也即是要更新为2019的条件值。那么value在主内存中现在的值是多少呢？怎么获取呢？ 通过前两个参数获取，通过this和valueOffset，就可以定位到，value属性在内存中的地址，从而获取它的值。 update：要把内存中的值，更新update的值。 抽象出来的CAS的逻辑 CAS(V, E, N) V：要更新的变量，目前内存中的变量的值。 E：预期值，条件值。要把v更新为n的条件值 N：新值如果V值等于E值，则将V值设为N值；如果V值不等于E值，说明其他线程做了更新，那么当前线程什么也不做。（放弃操作或重新读取数据） 疑问 ​ compareAndSet方法，咋一看，是没有添加任何线程同步的处理，例如没有synchronized或者Lock，那么他是线程安全的么？ 答案：是线程安全的，因为使用cas原理（一种硬件原语），cas就能够保证线程安全。 AtomicInteger的CAS底层原理为什么cas能够保证线程安全？ ​ 首先回顾一下我们做过的一个实例：实现两个线程对一个数自增，例如各自对number（共享数据）增加一百万。我们知道如果不对自增方法添加synchronized（或者使用Lock），那么就会导致，最终得数是变化不断的，是不会出现预期的两百万的得值，而是一个游离变化不断得值，因为这个就是线程安全的问题。 通过学习我们知道有多种方案，解决线程安全问题： 通过给方法添加synchronized关键字 可以使用Lock来实现多线程同步问题。 也可以使用AtomicInteger来解决多线程同步问题。 前面两种，我们都已经测试过，但是AtomicInteger为什么能够保证线程安全？同时他是通过什么机制来保证线程安全？通过上面的学习我们知道 AtomicInteger 是通过CAS来保证线程安全，但是是怎么保证的？ 那么我们可以通过AtomicInteger来推导出CAS的底层原理。 看实现代码：通过添加synchronized public class CASDemo &#123; static class Share&#123; private int number = 0;// public synchronized void add() &#123; number++; &#125; public int get() &#123; return number; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Share share = new Share(); Thread pp = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 1000000; i++) &#123; share.add1(); &#125; &#125; &#125;,&quot;线程1&quot;); Thread ppp = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 1000000; i++) &#123; share.add1(); &#125; &#125; &#125;,&quot;线程2&quot;); pp.start(); ppp.start(); pp.join(); ppp.join(); System.err.println( share.get() ); &#125;&#125; 答案，肯定是正确的，因为使用了synchronized内部锁，进行了线程安全的控制。 我们知道可以使用，AtomicInteger来实现相同的功能 public class CASDemo &#123; static class Share&#123; AtomicInteger atomicInteger = new AtomicInteger(0);//在主内存中，设置共享变量的值为0 public void add1() &#123; atomicInteger.incrementAndGet(); &#125; public int get1() &#123; return atomicInteger.get(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Share share = new Share(); Thread pp = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 1000000; i++) &#123; share.add1(); &#125; &#125; &#125;,&quot;线程1&quot;); Thread ppp = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 1000000; i++) &#123; share.add1(); &#125; &#125; &#125;,&quot;线程2&quot;); pp.start(); ppp.start(); pp.join(); ppp.join(); System.err.println( share.get1() ); &#125;&#125; 我们发现add1方法，根本就没有添加synchronized关键字修饰，但是他为什么能够保证数值就是2000000呢？ 通过查看上面的代码，我们发现是通过atomicInteger.incrementAndGet()解决了，number++在多线程访问下竟态线程安全问题。那么具体是怎么实现呢，往下看 通过查看incrementAndGet方法 /** * Atomically increments by one the current value. * * @return the updated value */ public final int incrementAndGet() { return unsafe.getAndAddInt(this, valueOffset, 1) + 1; } ​ 发现它内部实现是通过调用UnSafe类的方法。那么很明显UnSafe就是多线程同步的关键 ​ 通过上面的分析我们知道，unsafe.getAndAddInt(this, valueOffset, 1)，this参数就是atomicInteger实例，valueOffset就是atomicInteger实例的成员变量value在内存中的偏移量（也就是内存地址 ），通过前两个参数，就可以获取当前value在主内存的值。，第三个参数就是自增1。 我们接着查看unsafe.getAndAddInt源码 /** * Atomically adds the given value to the current value of a field * or array element within the given object &lt;code&gt;o&lt;/code&gt; * at the given &lt;code&gt;offset&lt;/code&gt;. * * @param o object/array to update the field/element in * @param offset field/element offset * @param delta the value to add * @return the previous value * @since 1.8 */ public final int getAndAddInt(Object o, long offset, int delta) &#123; int v; do &#123; v = getIntVolatile(o, offset); &#125; while (!compareAndSwapInt(o, offset, v, v + delta)); return v; &#125; /** Volatile version of &#123;@link #getInt(Object, long)&#125; */ public native int getIntVolatile(Object o, long offset);//调用操作系统的方法获取当前atomicInteger实例在主内存中value的值。 /** * Atomically update Java variable to &lt;tt&gt;x&lt;/tt&gt; if it is currently * holding &lt;tt&gt;expected&lt;/tt&gt;. * @return &lt;tt&gt;true&lt;/tt&gt; if successful */ public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x); 1.首先看这个方法getAndAddInt，刚开始执行一次do操作，调用getIntVolatile方法 目的是获取当前的atomicInteger共享资源，对应的value值，在主内存中现在的值（因为可能有其他线程已经修改了它的值，所以要获取最新的值）。然后赋值给v变量。 2.然后进行while判断，这个就是核心的cas的原理关键，比较和交换。 ​ compareAndSwapInt方法，一共有四个参数。第一个参数，就是atomicInteger共享资源（就是我们 AtomicInteger atomicInteger = new AtomicInteger(0)这里new出来的实例本身），保存自己首次从主内存中，捞取的value数据（因为我们知道，任何线程对数据的操作，都是先从主内存加载到自己的栈内存中，进行操作，也就是cas的expect值） 第二个参数：就是atomicInteger共享资源的value属性在内存的地址。 这样通过第一和第二参数，我们就可以获取cas的expect值。 第三个参数：就是目前value在主内存中的值。 第四个参数：就是update值，更新最新的值，v + delta。线程操作成功。 通过前面三个参数，我们就可以实现cas中的compare阶段，比较期望值跟主内存中value值，是否一致，如果一致，那么就直接更新值，实现cas的set阶段，更新成功，然后返回v变量的值（没有进行v + delta前的值） 如果比对失败，那么compareAndSwapInt返回false，那么while (!compareAndSwapInt(o, offset, v, v + delta));判断成功，然后接着进行do操作，无线循环，直到while判断成功。 我们在捋一下思路 假设线程A和线程B两个线程同时执行getAndAddInt操作(分别在不同的CPU上): 1.AtomicInteger里面的value原始值为3,即主内存中AtomicInteger的value为3,根据JMM模型,线程A和线程B各自持有一份值为3的value的副本分别到各自的工作内存. 2.线程A通过getIntVolatile(var1,var2) 拿到value值3,这时线程A被挂起. 3.线程B也通过getIntVolatile(var1,var2) 拿到value值3,此时刚好线程B没有被挂起并执行compareAndSwapInt方法比较内存中的值也是3 成功修改主内存的值为4 线程B打完收工 一切OK. 4.这是线程A恢复,执行compareAndSwapInt方法比较,发现自己手里的数值（3）和内存中的数字4不一致,说明该值已经被其他线程抢先一步修改了,那A线程修改失败,只能重新来一遍了.（while循环判断失败，重新进入do逻辑获取主内存的value值） 5.线程A重新获取value值,因为变量value是volatile修饰,所以其他线程对他的修改,线程A总是能够看到,线程A继续执行compareAndSwapInt方法进行比较替换,直到成功. 我们知道上述方法都是通过Unsafe类进行调用的，那么UnSafe是是什么？ ​ 是CAS的核心类 由于Java 方法无法直接访问底层 ,需要通过本地(native)方法来访问,UnSafe相当于一个后面,基于该类可以直接操作特额定的内存数据.UnSafe类在于sun.misc包中,其内部方法操作可以向C的指针一样直接操作内存,因为Java中CAS操作的助兴依赖于UNSafe类的方法.​ 注意UnSafe类中所有的方法都是native修饰的,也就是说UnSafe类中的方法都是直接调用操作底层资源执行响应的任务 变量ValueOffset,便是该变量在内存中的偏移地址,因为UnSafe就是根据内存偏移地址获取数据的。 那么cas到底是怎么保证了并发问题？​ CAS的全称为Compare-And-Swap ,它是一条CPU并发原语。 ​ 它的功能是判断内存某个位置的值是否为预期值,如果是则更新为新的值,这个过程是原子的. ​ CAS并发原语提现在Java语言中就是sun.misc.UnSaffe类中的各个方法.调用UnSafe类中的CAS方法,JVM会帮我实现CAS汇编指令.这是一种完全依赖于硬件 功能,通过它实现了原子操作。 ​ 再次强调,由于CAS是一种系统原语。原语属于操作系统用于范畴，是由若干条指令组成，用于完成某个功能的一个过程,并且原语的执行必须是连续的,在执行过程中不允许中断，也即是说CAS是一条原子指令,不会造成所谓的数据不一致的问题. 也就是说，cas为什么能够保证并发安全，靠的就是底层的汇编命令，指令的原子性。 为什么使用cas不使用synchronized？我们通过上面的源码知道，cas是不会加锁的，他是通过一个无线循环，来进行比对值，然后设置值的思路。这样就可以让多个线程在同一个时刻同时进入逻辑。 然而synchronized或者lock，只能在同一个时刻，只有一个线程获取锁后，才能进入逻辑。 所以并发上，cas更佳。但是synchronized至少能够保证，我做完一然后接着下一个，很稳定。cas虽然是可以大家一起做，但是不一定能成功。 但是cas只能够保证一个资源的并发安全，多个资源他无法保证。synchronized是可以保证多个资源的并发安全。 AtomicReference 实现对象的资源保护我们知道java.util.concurrent.atomic包下提供了，多种通过cas实现并发安全的各种类。 那么我们之前已经使用了AtomicInteger，通过它可以保证某个int类型数据的并发安全。 但是如果共享资源是多个属性，或者说是一个对象的话，那么怎么办？那么就可以使用 AtomicReference。 测试代码： public class AtomicReferenceDemo &#123; public static void main(String[] args) &#123; User user = new User(&quot;kingge&quot;, 12); User userCopy = new User(&quot;kingge&quot;, 12); User user1 = new User(&quot;kingger&quot;, 12); AtomicReference&lt;User&gt; reference = new AtomicReference&lt;&gt;(); reference.set(user); System.out.println(reference.compareAndSet(user, user1)+&quot;\\t&quot;+reference.get().toString()); System.out.println(reference.compareAndSet(user, user1)+&quot;\\t&quot;+reference.get().toString()); &#125;&#125;class User&#123; private String name; private int age; public User(String name, int age) &#123; super(); this.name = name; this.age = age; &#125;//省略get/set方法 @Override public String toString() &#123; return &quot;User [name=&quot; + name + &quot;, age=&quot; + age + &quot;]&quot;; &#125; &#125; 输出: true User [name=kingger, age=12]false User [name=kingger, age=12] 需要注意的是：这里需要注意下，这里的比对两个对象，比对的方式不是equals而是==,意味着比对的是内存的中地址，这个我们可以通过unsafe.compareAndSwapObject()方法查看，他是一个native方法。 CAS缺点 1.循环时间长开销很大，可能某个线程一直操作不成功，那么一直循环，对cpu造成压力大。 public final int getAndAddInt(Object o, long offset, int delta) { int v; do { v = getIntVolatile(o, offset); } while (!compareAndSwapInt(o, offset, v, v + delta)); return v; } 2.只能保证一个共享变量的原子性。 ​ 你看代码，你会发现，他是无法保证多个共享资源的并发安全。但是synchronized是可以的，可以对一段代码进行并发安全控制。 3.引出来ABA问题 什么是ABA问题 cas算法实现的一个重要前提就是需要取出内存中某个时刻的数据并在当下时刻比较替换，那么在这个时间差内，会导致数据的变化。 比如说，一个线程从a从内存位置o中取出A，这个时候另一个线程b，也从内存中取出A，并且线程b进行了一些操作将主内存的A变成了B，然后线程b又将主内存中位置o的数据从B修改为了A。这个时候，线程a进行cas操作，发现主内存中仍然是A，然后线程a操作成功。 尽管线程a的cas操作成功，但是并不代表这个过程是没有问题的，也就是说cas值关注头尾，只要对应的上就操作成功。所以说ABA问题，是存在的，但是这个也不算是问题，因为有可能你的业务就是只关注头尾是否相同，中间不论发生什么，我都不在意。 怎么解决ABA问题 很明显，解决的思路就是，通过时间戳或者记录版本号的方式，只要修改一次版本号就记录一次，自增1。 实现方式就是通过，AtomicStampedReference，类。 /** * Description: ABA问题的解决 **/public class ABADemo &#123; private static AtomicReference&lt;Integer&gt; atomicReference=new AtomicReference&lt;&gt;(100);//通过atomicReference演示ABA问题的产生 private static AtomicStampedReference&lt;Integer&gt; stampedReference=new AtomicStampedReference&lt;&gt;(100,1);//通过stampedReference演示ABA的解决方案 public static void main(String[] args) &#123; System.out.println(&quot;===以下是ABA问题的产生===&quot;); new Thread(()-&gt;&#123; atomicReference.compareAndSet(100,101); atomicReference.compareAndSet(101,100); &#125;,&quot;t1&quot;).start(); new Thread(()-&gt;&#123; //先暂停1秒 也即是保证t1线程先执行 保证完成ABA。 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(atomicReference.compareAndSet(100, 2019)+&quot;\\t&quot;+atomicReference.get()); &#125;,&quot;t2&quot;).start(); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //很明显t2是可以修改成功的，因为这里他并不关注ABA的出现，只关注头尾，因为最开始 System.out.println(&quot;===以下是ABA问题的解决===&quot;); new Thread(()-&gt;&#123; int stamp = stampedReference.getStamp(); System.out.println(Thread.currentThread().getName()+&quot;\\t 第1次版本号&quot;+stamp+&quot;\\t值是&quot;+stampedReference.getReference()); //暂停1秒钟t3线程 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; stampedReference.compareAndSet(100,101,stampedReference.getStamp(),stampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName()+&quot;\\t 第2次版本号&quot;+stampedReference.getStamp()+&quot;\\t值是&quot;+stampedReference.getReference()); stampedReference.compareAndSet(101,100,stampedReference.getStamp(),stampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName()+&quot;\\t 第3次版本号&quot;+stampedReference.getStamp()+&quot;\\t值是&quot;+stampedReference.getReference()); &#125;,&quot;t3&quot;).start(); new Thread(()-&gt;&#123; int stamp = stampedReference.getStamp(); System.out.println(Thread.currentThread().getName()+&quot;\\t 第1次版本号&quot;+stamp+&quot;\\t值是&quot;+stampedReference.getReference()); //保证线程3完成1次ABA try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean result = stampedReference.compareAndSet(100, 2019, stamp, stamp + 1); System.out.println(Thread.currentThread().getName()+&quot;\\t 修改成功否&quot;+result+&quot;\\t最新版本号&quot;+stampedReference.getStamp()); System.out.println(&quot;最新的值\\t&quot;+stampedReference.getReference()); &#125;,&quot;t4&quot;).start(); &#125; 输出： ===以下是ABA问题的产生===true 2019===以下是ABA问题的解决===t3 第1次版本号1 值是100t4 第1次版本号1 值是100t3 第2次版本号2 值是101t3 第3次版本号3 值是100t4 修改成功否false 最新版本号3最新的值 100 atomic包下的工具类，都是基于CAS实现线程安全ArrayList线程不安全解决public class ArrayListTest &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; list.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(list); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 30个线程，共同操作list，那么就会出现下面的问题：Exception in thread &quot;10&quot; Exception in thread &quot;26&quot; java.util.ConcurrentModificationException 多个线程，进行add数据的时候，可能会报这个错误 这种情况就是因为add方法，可以让多个线程同时执行，那么某个线程正在写入list数组的某个下标时，其他写成也可能在下入同一个下标，那么这个时候就会触发并发修改异常。 那么怎么解决呢？ 备选方案，使用Vector，vector提供了synchronized修饰的方法，需要加锁，并发能力下降。 我们可以使用 Collections.synchronizedList(list)，通过传递一个list，然后他会返回一个线程安全的list给你，实际上返回的线程安全list内部实现，就是通过在方法内部加上synchronized的方式实现线程安全，他跟vector是一样的。 以上两个方案，如果公司都不建议使用，那么可以使用下面的类。 我们推荐使用 CopyOnWriteArrayList 类。查看他的add方法 /** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return {@code true} (as specified by {@link Collection#add}) */ public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock(); } } 发现他实际上就是通过ReentrantLock进行加锁和解锁，很简单，所以他能够解决并发问题。 我们在深入的看一下CopyOnWriteArrayList的源码： /** The lock protecting all mutators */ final transient ReentrantLock lock = new ReentrantLock(); /** The array, accessed only via getArray/setArray. */ private transient volatile Object[] array; 可以看到他的成员属性有这两个，一个加锁的实例对象，一个保存list数据的 volatile类型的数组。 同理hashset和hashmap也是线程不安全的那么他们的解决方案，其实跟ArrayList是一样的。 hashset线程不安全解决的方案是，可以使用Collections生成线程安全的set，那么也是可以使用CopyOnWriteArraySet（你会发现，他底层依赖的就是CopyOnWriteArrayList实现）。 hashmap的解决方案是：可以使用hashtable，或者Collections集合类生成 线程安全的map，那么也是可以使用ConcurrentHashMap 特别提示：hashset的底层实现是hashmap，但是hashset的add方法参数只有一个，hashmap的入参是一个k-v键值对，怎么回事？ 原来hashset的k就是add方法的入参，但是我们只关注k，所以value的值，是一个恒定使用final修饰的new Object()对象。 HashMap源码解析请查看&lt;面试突击第三季.md&gt;里面有完整分析 公平锁/非公平锁/可重入锁/递归锁/自旋锁谈谈你的理解?请手写一个自旋锁公平锁和非公平锁公平锁 是指多个线程按照申请锁的顺序来获取锁类似排队打饭 先来后到。非公平锁 是指在多线程获取锁的顺序并不是按照申请锁的顺序,有可能后申请的线程比先申请的线程优先获取到锁,在高并发的情况下,有可能造成优先级反转或者饥饿现象。 并发包ReentrantLock的创建可以指定构造函数的boolean类型来得到公平锁或者非公平锁，默认是非公平锁。 /** * Creates an instance of &#123;@code ReentrantLock&#125;. * This is equivalent to using &#123;@code ReentrantLock(false)&#125;. */public ReentrantLock() &#123; sync = new NonfairSync();//Nonfair 默认是非公平锁 。&#125;/** * Creates an instance of &#123;@code ReentrantLock&#125; with the * given fairness policy. * * @param fair &#123;@code true&#125; if this lock should use a fair ordering policy */public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; synchronized和默认创建的lock都是非公平锁。 可重入锁(又名递归锁) 也就是说，同步方法内部，再去访问另一个同步方法，可以不用再请求锁，只需要记录重入次数即可，释放锁后减少重入次数即可 。（好处是这样就不会死锁） ReentrantLock/synchronized就是一个典型的可重入锁。 可重入锁最大的作用就是避免死锁 例子一：使用synchronized实现可重入锁。 package com.kingge.cas;class Phone&#123; public synchronized void sendSms() throws Exception&#123; System.out.println(Thread.currentThread().getName()+&quot;\\tsendSms&quot;); sendEmail(); &#125; public synchronized void sendEmail() throws Exception&#123; System.out.println(Thread.currentThread().getName()+&quot;\\tsendEmail&quot;); &#125;&#125;/** * 也就是说,线程可以进入任何一个它已经标记的锁所同步的代码块 **/public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); new Thread(()-&gt;&#123; try &#123; phone.sendSms(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;,&quot;t1&quot;).start(); new Thread(()-&gt;&#123; try &#123; phone.sendSms(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;,&quot;t2&quot;).start(); &#125;&#125; 输出： t1 sendSmst1 sendEmailt2 sendSmst2 sendEmail ​ 很明显sendSms是一个同步方法，在sendSms（外层函数）内部调用的sendEmail（内层函数）也是一个同步方法。那么假设线程获取锁后能够进入sendSms方法，那么在调用sendSms方法的时候，就不需要再申请锁了，所以锁时可以重复使用的，即是，可重入锁。 sendSms和sendEmail都是请求同一个锁资源（this），那么假设没有重入锁。程序运行，t1线程获取锁资源成功，那么执行 phone.sendSms()，接着调用sendEmail()方法，那么因为sendSms()还占据着this锁，很明显在调用sendEmail时会阻塞，sendEmail方法会等待sendSms方法释放资源，但是sendSms方法要等sendEmail方法执行完才释放资源，相互等待，产生死锁。 举个例子，你能用锁进入你家，那么进入你家的厕所，肯定是可以的，也就不要再开锁了。 需要注意的是：sendSms和sendSms本质上请求的都是同一把锁（Phone.class），所以是可以重入的。 例子二：使用Lock演示可重入锁 class Phone implements Runnable &#123; private Lock lock = new ReentrantLock(); @Override public void run() &#123; get(); &#125; private void get() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot;\\tget&quot;); set(); &#125; finally &#123; lock.unlock(); &#125; &#125; private void set() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot;\\tset&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); Thread t3 = new Thread(phone); Thread t4 = new Thread(phone); t3.start(); t4.start(); &#125;&#125; 输出： Thread-0 getThread-0 setThread-1 getThread-1 set 自旋锁 似锁非锁我们之前，学过Unsafe和cas，就已经接触过自旋锁。 也就是说：自旋锁实现的本质，就是通过while循环加上cas方法实现。 其实自旋锁，也可以说，不是锁，这样说的很绕。我们先回顾一下，我们之前学习的，内部锁（synchronized）和显示锁（lock），当多个线程访问共享资源时，只有一个线程能够获取锁，然后进入临界区，操作逻辑。那么这个时候其他锁，是在锁池等待，是阻塞的。也就是说，其他线程，根本没有进入临界区的机会。 但是自旋锁，不一样，他是允许所有，线程都进入临界区，操作共享资源数据，没有线程是阻塞的。他实际上就是乐观锁的意思，就是先尝试修改数据，如果不行再请求锁。 他是通过cas硬件原语的，机制，来实现，原子性。通过比较预期值和实际值是否一致，来决定是否做更新操作。如果一致，那么更新值，while循环结束，返回true，线程结束。 如果不一致，那么while循环，持续判断，直到判断成功。 实现一个自旋锁那么我们知道，自旋锁的本质就是，while加上cas。 package com.kingge.zixuan;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;public class SpinLockDemo &#123; //原子引用线程 AtomicReference&lt;Thread&gt; reference = new AtomicReference&lt;&gt;();//我们知道，他的底层是private volatile V value; 存储共享资源，也就是说，这段代码执行完成后，value的值是null。因为没有赋值 public void myLock() &#123; Thread currentThread = Thread.currentThread(); System.out.println( currentThread.getName() + &quot; 开始加锁 。。。 &quot; ); //自旋锁核心逻辑 //通过cas比较，获取锁 //他的核心逻辑是：只要当前reference的值跟预期值一样都是null，那么我就把reference的值设置为当前线程，也就意味着，当前线程成功取到了锁，成功进行了资源操作。 //那么如果返回true，那么就不需要进行while循环，所以这里是用!取反，只要不满足，就一直死循环，直到满足未知。 while( !reference.compareAndSet(null, currentThread) ) &#123; &#125; &#125; public void myUnLock() &#123; Thread currentThread = Thread.currentThread(); System.out.println( currentThread.getName() + &quot; 开始解锁 。。。 &quot; ); //这个意思就是，当前线程操作完了逻辑，那么解锁操作，也要比对一下。 //如果reference的值是当前线程，那么就解锁，加锁完成后把reference更新为null，这样才能够让下一个线程继续加锁。 reference.compareAndSet(currentThread, null); &#125; public static void main(String[] args) &#123; SpinLockDemo spinLockDemo = new SpinLockDemo(); new Thread(new Runnable() &#123; @Override public void run() &#123; //开始占用锁 spinLockDemo.myLock(); //处理业务逻辑，这里假设处理业务逻辑需要5s，这样也能在A线程处理业务的同时，B线程 //也进入加锁逻辑，这样才能判断自旋锁是否成功 try &#123; TimeUnit.SECONDS.sleep(5); System.out.println( Thread.currentThread().getName() +&quot; 执行完毕。。&quot; ); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //解锁 spinLockDemo.myUnLock(); &#125; &#125;, &quot;A&quot;).start(); //这里暂停1s的目的就是，让main线程暂停1s，保证A线程先启动运行。接着启动B线程 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; //开始占用锁 spinLockDemo.myLock(); //解锁 spinLockDemo.myUnLock(); &#125; &#125;, &quot;B&quot;).start(); &#125;&#125; 输出： A 开始加锁 。。。 B 开始加锁 。。。 A 执行完毕。。A 开始解锁 。。。 B 开始解锁 。。。 可以看到，虽然A、B同时加锁，但是最终只有A线程获取了锁，那么也就意味着，线程B在while循环里面自旋。直到线程A执行完，业务逻辑后，解锁。线程Bwhile循环结束，获取锁，然后执行业务逻辑，最后解锁。 独占锁(写锁)/共享锁(读锁)/互斥锁 那么既然有了ReentrantLock，为什么还需要读写锁（ReentrantReadWriteLock）呢？为了更细致化的使用锁，实现读写分离。 我们知道ReentrantLock是不管什么操作逻辑，只要进入临界区访问共享资源，那么就会加锁，也就意味着，假设，我只是想读共享资源而已，那么还要去申请锁？这个就有点不符合道理了。这样会造成什么问题呢？多线程情况下读取资源，还需要等锁，那这个并发量就下降了，而且也没有必要加锁。 也就是说 为了并发量，可以允许多个线程同时进行读取共享资源，但是，如果有一个线程想去写共享资源， 那么就不应该有其他线程对资源进行读或写。 代码例子： package com.kingge.cas;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantReadWriteLock;/** * 资源类 */class MyCaChe &#123; /** * 保证可见性 */ private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); private ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(); /** * 写 * * @param key * @param value */ public void put(String key, Object value) &#123; reentrantReadWriteLock.writeLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t正在写入的key是：&quot; + key); //模拟网络延时 try &#123; TimeUnit.MICROSECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; map.put(key, value); System.out.println(Thread.currentThread().getName() + &quot;\\t 写入完成&quot;); &#125; finally &#123; reentrantReadWriteLock.writeLock().unlock(); &#125; &#125; /** * 读 * * @param key */ public void get(String key) &#123; reentrantReadWriteLock.readLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t正在读取&quot;); //模拟网络延时 try &#123; TimeUnit.MICROSECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Object result = map.get(key); System.out.println(Thread.currentThread().getName() + &quot;\\t读取完成 &quot; + result); &#125; finally &#123; reentrantReadWriteLock.readLock().unlock(); &#125; &#125; public void clearCaChe() &#123; map.clear(); &#125;&#125;/** * Description: * 多个线程同时操作 一个资源类没有任何问题 所以为了满足并发量 * 读取共享资源应该可以同时进行 * 但是 * 如果有一个线程想去写共享资源来 就不应该有其他线程可以对资源进行读或写 * &lt;p&gt; * 小总结: * 读 读能共存 * 读 写不能共存 * 写 写不能共存 * 写操作 原子+独占 整个过程必须是一个完成的统一整体 中间不允许被分割 被打断 * **/public class ReadWriteLockDemo &#123; public static void main(String[] args) &#123; MyCaChe myCaChe = new MyCaChe(); for (int i = 1; i &lt;= 5; i++) &#123; final int temp = i; new Thread(() -&gt; &#123; myCaChe.put(temp + &quot;&quot;, temp); &#125;, String.valueOf(i)).start(); &#125; for (int i = 1; i &lt;= 5; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; myCaChe.get(finalI + &quot;&quot;); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 输出： 1 正在写入的key是：11 写入完成2 正在写入的key是：22 写入完成3 正在写入的key是：33 写入完成5 正在写入的key是：55 写入完成4 正在写入的key是：44 写入完成1 正在读取2 正在读取3 正在读取4 正在读取5 正在读取1 读取完成 12 读取完成 25 读取完成 53 读取完成 34 读取完成 4 你会发现，写操作，都是原子性，没有中断（正在写入和写入完成是一对出现）。中间不会存在其他线程的读取或者写入。 读取的时候，是可以多个线程进行读取，你会发现，读取完成和读取结束并不是一对出现，这个是允许的，因为不需要原子性。 所以满足读写锁的要求。 CountDownLatch/CyclicBarrier/SemaphoreCountDownLatchCountDownLatch是基于AQS的阻塞工具，阻塞一个或者多个线程，直到所有的线程都执行完成。 CountDownLatch解决了什么问题 当一个任务运算量比较大的时候，需要拆分为各种子任务，必须要所有子任务完成后才能汇总为总任务。使用并发模拟的时候可以使用CountDownLatch.也可以设置超时等待时间。同时CountDownLatch也提供了可以设置超时等待的await方法。 让一些线程阻塞直到另外一些完成后才被唤醒。类似于wait和notify。 CountDownLatch主要有两个方法,当一个或多个线程调用await方法时,调用线程会被阻塞.其他线程调用countDown方法计数器减1(调用countDown方法时线程不会阻塞),当计数器的值变为0,因调用await方法被阻塞的线程会被唤醒,继续执行 举个例子： ​ 假设，教室中有七个人，其中六个人是同学，一个是班长，班长要等这六个人都出教室了，然后再关门，那么这种等待其他子线程完成后主线程才操作的就很适合使用CountDownLatch。 public class CountDownLatchDemo &#123; static CountDownLatch countDownLatch = new CountDownLatch(6); public static void main(String[] args) throws InterruptedException &#123; for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + &quot;上完自习&quot;); countDownLatch.countDown(); &#125;, String.valueOf(i)).start(); &#125; countDownLatch.await();//挂起main线程，等到countDownLatch减到0时，main线程被唤醒 //也就意味着，要等前面六个线程执行完毕后才会唤醒main线程 System.out.println(Thread.currentThread().getName() + &quot;\\t班长锁门离开教室&quot;); &#125;&#125; 源码分析 例子 public class CDL extends Thread &#123; public CDL(String name) &#123; super(name); &#125; static CountDownLatch countDownLatch = new CountDownLatch(1); @Override public void run() &#123; try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println( Thread.currentThread().getName() + &quot; 执行 &quot; ); &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt;3; i++) &#123; new CDL(String.valueOf(i)).start(); &#125; System.out.println( &quot;123 &quot; ); countDownLatch.countDown(); System.out.println( &quot;345 &quot; ); &#125;&#125; 我们知道CountDownLatch是 基于共享锁的形式，建立阻塞队列。也就是 Node.SHARE，意味着，多个线程可以同时阻塞在countDownLatch.await()，等到计数器减到0时，多个线程会同时进行执行await后面的代码。 他跟 ReentrantLock不同，lock采用的是独占锁的方式，Node.EXCLUSIVE，在某个时刻，只能够允许一个线程在执行。 ​ 所以共享锁的方式，更加的验证了，CountDownLatch的应用场景，在计数器未减少到0时，线程可以同时进行自己各自的业务代码，等计数器减少到0后，那么被await的线程唤醒，然后执行。 以下使用CDL简称CountDownLatch 查看CDL构造函数 public CountDownLatch(int count) { if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;); this.sync = new Sync(count); } Sync(int count) {//Sync是CDL内部类，Sync继承了AQS setState(count); } 这里可以看到，aqs的state属性，在CDL这里的含义是，计数器的数量。而不再是之前我们所说的是否获取锁的标志位/重入次数。被赋予了新的含义 1.查看await（）方法 java.util.concurrent.CountDownLatch.await() public void await() throws InterruptedException {//顾名思义，他是请求共享锁，并且可以响应中断 sync.acquireSharedInterruptibly(1); } ​​ public final void AbstractQueuedSynchronizer.acquireSharedInterruptibly(int arg)​ throws InterruptedException {​ if (Thread.interrupted())​ throw new InterruptedException();​ if (tryAcquireShared(arg) &lt; 0)//首次进来，state一般是不等于0的，因为计数器还为减少到0，所以tryAcquireShared返回-1​ doAcquireSharedInterruptibly(arg);​ } ​​​ protected int tryAcquireShared(int acquires) {//如果计数器减少到0，那么返回1，否则返回-1​ return (getState() == 0) ? 1 : -1;​ } 接着执行 doAcquireSharedInterruptibly 方法 java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(int) /** * Acquires in shared interruptible mode. * @param arg the acquire argument */ private void doAcquireSharedInterruptibly(int arg) throws InterruptedException { final Node node = addWaiter(Node.SHARED);//这个就是将当前线程节点，放到aqs阻塞队列中。返回当前线程节点 boolean failed = true; try { for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg);//判断计数器state是否==0，如果是返回1，表示阻塞的线程可以开始唤醒执行 if (r &gt;= 0) { setHeadAndPropagate(node, r);//注意这里，这里会遍历整个aqs阻塞队列，然后逐个释放 p.next = null; // help GC failed = false; return; } } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())//我们知道线程最终都会阻塞在这里 throw new InterruptedException();//这里可以直接响应中断，直接抛出异常 } } finally { if (failed) cancelAcquire(node); } } ​ 也就是说，实际上，await（）方法的最终目的就是：将所有调用await方法的线程，都放到aqs阻塞队列中，开始在阻塞。什么时候唤醒呢？等到计数器state==0，也就是countdown到0。 ​ 我们发现他跟ReentrantLock整个加锁的流程是类似的，主要的区别在于，lock是独占锁，state表示的是锁标记。CDL在计数器到零后，会unpark所有在aqs阻塞队列的线程，而lock只会唤醒aqs阻塞队列首节点（所以说是独占锁） 2.查看CDL的countDown（） java.util.concurrent.CountDownLatch.countDown() public void countDown() { sync.releaseShared(1); } ​​ public final boolean AbstractQueuedSynchronizer.releaseShared(int arg) {​ if (tryReleaseShared(arg)) {​ doReleaseShared();​ return true;​ }​ return false;​ } 首先调用tryReleaseShared（）方法，state减-1 java.util.concurrent.CountDownLatch.Sync.tryReleaseShared(int) protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125;&#125; 如果返回true，说明state已经变为了0。那么需要唤醒阻塞的线程。 接着调用doReleaseShared方法，释放共享锁 private void doReleaseShared() &#123;//一个for循环。持续一个一个的唤醒aqs阻塞队列中节点 for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h);//释放锁，然后唤醒线程 &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) //循环唤醒aqs阻塞队列的线程，直到队列没有元素，那么跳出循环 // loop if head changed break; &#125;&#125; 3.唤醒线程后 接着调用doAcquireSharedInterruptibly的parkAndCheckInterrupt，然后，继续执行for循环，然后执行 setHeadAndPropagate(node, r)方法。 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())//我们知道线程最终都会阻塞在这里 throw new InterruptedException();//这里可以直接响应中断，直接抛出异常 private void setHeadAndPropagate(Node node, int propagate) { Node h = head; // Record old head for check below setHead(node);//将当前线程设置为头结点，这样的好处是，释放一个就将当前线程设置为头结点，然后再doReleaseShared方法中，总会触发 if (h == head)，这样才能够跳出doReleaseShared（）的for循环。唤醒所有阻塞队列线程结束。 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) { Node s = node.next; if (s == null || s.isShared()) doReleaseShared();//接着唤醒，当前节点的下一个节点，以此类推。直到唤醒完所有的阻塞队列的节点。 } } 总结 ​ CDL使用共享锁（不需要竞争）的方式阻塞所有的线程，所有线程阻塞到阻塞队列中，直到countdown到0。就会去唤醒阻塞队列中所有线程（共享锁，不需要竞争）。 使用场景例如zookeeper的server的启动挂起，唤醒关闭server就是利用CountDownLatch实现的。 CyclicBarrier这个跟CountDownLatch是相反的，他是做加法，当增加到某一个值后，那么就会唤醒阻塞的线程。 public class CyclicBarrierDemo &#123; public static void main(String[] args) &#123; CyclicBarrier barrier = new CyclicBarrier(7, () -&gt; &#123;System.out.println( &quot;增加到7了，被唤醒当前线程&quot; );&#125;); for (int i = 1; i &lt; 8; i++) &#123; final int index = i; new Thread(() -&gt; &#123; System.out.println( &quot;开始执行业务 &quot; + Thread.currentThread().getName() ); try &#123; barrier.await(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; Semaphore - 可以用作限流它的本质实际上就是基于信号量（PV操作）的机制实现。 信号量的主要用户两个目的，一个是用于多个共享资源的相互排斥使用，另一个用于并发资源数的控制。 ​ 也就是说，我们可以限制资源的数量（令牌），那么多个请求进来后，去争抢固定数量的令牌，如果令牌争抢完，后面还没有得到令牌的线程就会阻塞，直到后令牌释放，然后才会去争抢（争抢的过程中，也可以插队，也就是说Semaphore也是有公平锁和非公平锁的区分） 以下案例，模拟六辆车抢占三个车位。 public class SemaphoreDemo &#123; public static void main(String[] args) &#123; //模拟3个停车位 Semaphore semaphore = new Semaphore(3); //模拟6部汽车 for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; try &#123; //抢到资源，拿不到，那么就会阻塞在这里 semaphore.acquire(); System.out.println(Thread.currentThread().getName() + &quot;\\t抢到车位&quot;); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;\\t 停3秒离开车位&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; //释放资源 semaphore.release(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 很明显，Semaphore是可以替代，synchronized和lock的，只需要把信号量修改为1即可。因为内部锁和显示锁的本质就是抢占一个资源。 而且我们发现，Semapore实际上是CountDownLatch和CyclicBarrier的结合体。CountDownLatch是减少到某个数然后唤醒某个线程，CyclicBarrier是新增到某个数，然后唤醒某个线程。 但是Semapore是有增有减，可以提供给多个线程功能抢占资源，线程使用完资源后，马上释放，然后另一个线程可以马上的抢占资源。 源码分析​ 你会发现，Semaphore获取资源的源代码跟COuntDownlatch基本上是一样的，也是通过共享锁的形式，进行资源的争抢（只不过他有公平和非公平两种实现方式，共享公平锁和共享非公平锁的区别跟ReentrantLock一样，公平锁多了hasQueuedPredecessors方法的判断） ConcurrentHashMap源码分析ConcurrentHashMap&lt;String, String&gt; chm = new ConcurrentHashMap&lt;&gt;();chm.put(&quot;kingge&quot;, &quot;123&quot;); 以下ConcurrentHashMap简称chm。 ​ 我们阅读源码的原则是，第一时刻考虑如果是多线程访问时，这段代码会不会有问题。第二，不要通读所有代码，而是根据if条件或者其他条件，选择性的读取某段代码。 查看put操作我们知道，map是通过数组+链表/红黑树的数据结构保存数据，其中数组是保存key经过hash后得到下标。 在chm中，数组使用 transient volatile Node[] table 实现。 下面简称chm数组（如果不特别指定，那么数组，也代表chm数组） public V put(K key, V value) &#123; return putVal(key, value, false);&#125;//阅读时，要注意每个步骤的方法，他是如何保证线程安全的/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //chm的key和value不能为null，跟hashtable一样。hashmap是允许key和value为null if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode());//获取key对应 hash值，通过右移16位，再通过异或，保证高低16位的特征，这样在进行hash计算数组下标时，更加离散 int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123;//一个死循环，进行put操作 Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0)//首先判断数组是否为空，为空则进行初始化数组 tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123;//b1 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125;//end b1 else if ((fh = f.hash) == MOVED)// b12 区域 tab = helpTransfer(tab, f);//当前节点是转移状态，说明，正在数组扩容，那么当前线程就需要帮助扩容 - 我们知道chm是支持多个线程同时进行扩容操作的 else &#123;// b2 区域 V oldVal = null; synchronized (f) &#123;//直接锁住某个node数组下标，锁粒度细化，保证了其他下标能够被其他线程操作 if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1;//当前数组下标节点，链表元素的个数。方便转化为红黑树进行判断，bincount &gt;= 7,进行单链表转化为红黑树 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123;//遍历插入链表中 K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123;//key和hash值相等，那么更新value值即可 oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125;// end synchronized if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; //end b2 &#125;//end for addCount(1L, binCount); return null;&#125; initTable数组初始化工作 - sizeCtl属性首先我们要明确一点，多线程情况下，initTable()方法的调用是存在线程安全的，所以我们需要注意chm对于initTable()是如何保证线程安全的。 private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0)//如果sizeCtl小于，表示数组已经初始化，那么就没有必要再次初始化数组，当前线程让出cpu，给其他线程执行，当前线程回到就绪状态， Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123;//可以看到这里是通过了cas保证了初始化数组的原子性，sizeCtl设置为-1 try &#123; if ((tab = table) == null || tab.length == 0) &#123;//初始化数组,长度默认是16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2);//关键点 ， 假设数组长度是16，那么sc=12，相当于n*0.75， //那为什么这里要用n - (n &gt;&gt;&gt; 2) 呢？因为位运算比算术运算要快 &#125; &#125; finally &#123; sizeCtl = sc;//sizeCtl = 12，这里sizeCtl被赋予了另外一种含义，表示下次数组扩容的数量 &#125; break; &#125; &#125; return tab;&#125; 这里使用了一个chm的一个非常重要的成员变量 private transient volatile int sizeCtl ​ 通过U.compareAndSwapInt(this, SIZECTL, sc, -1) cas操作，保证了高并发下只有一个线程能够进行初始化数组，比较sizeCtl是否跟预期值一致（等于0），如果是，那么把sizeCtl设置为-1，进入初始化数组逻辑。 ​ 否则cas失败，表示数组已经初始化，那么退出whil循环（此时table已经不为null），返回数组。 sizeCtl的三个作用 通过上面的源码我们发现，是否已经初始化的数组是通过sizeCtl和cas来进行判断和操作的。而且sizeCtl一共有两个作用：sizeCtl == -1 ，表示当前已经有线程抢到了初始化chm数组的权限、sizeCtl &gt; 0，sizeCtl=sc=n*0.75，表示下一次数组扩容大小。 ​ sizectl的第三个作用，当sizeCtl是负数，但不是-1，就表示当前有几个线程在进行扩容操作，例如sizeCtl=-2，表示有两个线程在执行扩容操作（ 关于第三个作用，在下面的addCount()方法的第二段if，里面会有用到 ） tabat和casTabAt，获取key对应的数组下标同时这两个方法也是需要保证线程安全的。我们来看源代码。他们是怎么保证线程安全的 //////////初始化为数组后，for死循环，会执行到这段代码，获取key对应的数组下标if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; ////////////////为了避免高并发下，key被覆盖问题，那么需要保证tabAt的线程安全@SuppressWarnings(\"unchecked\")static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123;//通过底层保证tabAt线程安全 return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;//通过cas保证了 put操作的线程安全 static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); &#125; addCount(1L, binCount) 最终执行我们知道不管，put最终都会执行到addCount(1L, binCount)，顾名思义 ，就是计数的意思，新增put一条数据，那么size就会增1。 ​ 那么怎么保证高并发下，addCount方法线程安全呢？通过cas、加锁？虽然这两种方法都可以保证线程安全，但是会有性能影响。那么我们来看一下他是怎么进行线程安全控制的 ​ 他是怎么维护chm的size呢？ 那么我们来看一下他的源码： private final void addCount(long x, int check) &#123;//第一个传的就是1，表示增加chm元素个数，因为put一次元素个数肯定是自增1，所以 这里固定传1.那么第二个参数是是检查是否需要扩容的依据，一般是表示当前chm数组某个下标，构建的单链表的元素个数，根据该值判断是否需要扩容 CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123;//尝试对于basecount进行一次cas操作，如果失败，那么说明当前访问addCount方法的线程过多，为了避免无效的cas操作，浪费加剧cpu操作，那么往下执行，引入CounterCell数组，进行线程分流操作。 CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended);//初始化CounterCell数组 return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125;//size增加1 if (check &gt;= 0) &#123;//检查是否需要扩容 Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;//判断s当前chm元素个数，是否大于sizectl（这里等于12，因为此时sizectl得知是在initTable()方法中进行初始化的），且chm数组不为空，且chm数组长度不大于最大值。 int rs = resizeStamp(n);// if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125;// ThreadLocalRandom.getProbe() &amp; m //ThreadLocalRandom.getProbe() ，获得一个随机数，然后跟counterCells数组长度进行与运算，最终获得某个线程需要操作counterCells数组哪个下标的位置。起到分流作用，降低无效cas操作 //这样那么多个线程会被分配到不同的数组下标，然后取到数组下标已经存在的value值，再进行cas自增1。完成本次的chm元素数量增加1操作 结论： chm的size，数据个数，是通过chm的baseCount和counterCells这两个成员属性来进行控制的或者说得到的。 ​ 首先我们回顾一下，hashmap或者arrayList，都是通过一个成员属性size来进行元素个数的维护，那么为什么chm不通过这样的方式来维护元素个数呢？ ​ 实际上，chm是通过这样的策略进行元素个数的维护： ​ 如果在线程数不是很多的情况下，那么对baseCount进行cas操作，自增1，实现元素数量的增加维护。 ​ 但是假设是高并发情况下或者说对于baseCount的cas操作失败，那么就会增加一个counterCells数组来进行高并发下分流操作，避免无效的cas操作。 ​ 总而言之，在对chm元素数量自增1的时候，会尝试进行一次对baseCount进行cas的自增操作，假设失败，马上使用counterCells数组进行数组元素的维护。 下面详细分析 ​ 首先我们假设chm只通过baseCount，来进行元素个数的维护，那么在put一条数据的时候，我们知道baseCount需要自增1，那么为了线程安全，baseCount的自增需要通过加锁或者cas的方式进行，一般使用cas。 ​ 那么如果使用只使用baseCount来维护chm元素个数，那么进行cas自增1的时候就会面临一个问题，假设高并发情况下，多个线程同时执行put操作，cas只能够运行一个线程修改成功，那么其他线程就会做没有意义的cas操作，线程多的情况下，cpu压力会上升。 ​ 那么怎么改变这种情况呢？那就是引入分段的概念，就是可以让多个线程执行同时执行cas自增操作，类似于部署多个节点，支持高并发。那么就引出了counterCells数组，每个数组位置，都保存一个value值，表示chm元素个数，这样求得整个chm元素个数的时候，只需要遍历counterCells数组然后累加再加上baseCount就等于chm元素个数（详情查看chm的size()方法） ​ 那么引入后上面的高并发cas的问题，怎么解决了呢？ ​ 假设counterCells数组初始化为2，那么假设有三个线程，put操作完，三个线程需要调用addCount增加元素个数。那么就会给这三个线程，随机分配一个关于counterCells数组的下标，让他们各自去数组下标的位置，进行自增1操作。那么这样就起到了分流的作用，这样就减少了无效cas的个数。 ​ 也就是说，CounterCell数组，保存了chm的元素个数。 为了加深上面的理论，我们查看一下chm的size() 方法： public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125; final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123;//遍历CounterCell数组，累加里面的个数，再加上baseCount，就是整个chm元素个数 for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum; &#125; fullAddCount（）初始化counterCells数组// See LongAdder version for explanation private final void fullAddCount(long x, boolean wasUncontended) &#123; int h; if ((h = ThreadLocalRandom.getProbe()) == 0) &#123; ThreadLocalRandom.localInit(); // force initialization h = ThreadLocalRandom.getProbe(); wasUncontended = true; &#125; boolean collide = false; // True if last slot nonempty for (;;) &#123; CounterCell[] as; CounterCell a; int n; long v; if()&#123;//先忽略这部分代码，判断counterCells数组是否为空 ......//不为空则执行这部分代码 &#125; else if (cellsBusy == 0 &amp;&amp; counterCells == as &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) &#123;//cellsBusy==0,那么当前线程获取锁，得到执行当前counterCells数组的机会，并将cellsBusy设置为1 boolean init = false; try &#123; // Initialize table if (counterCells == as) &#123; CounterCell[] rs = new CounterCell[2];//初始化一个两个数量的CounterCell数组 rs[h &amp; 1] = new CounterCell(x);//因为CounterCell数组长度为 2，所以我们为了获取当前线程分配到所需要操作的CounterCell数组下标，那么要进行，（h &amp; 1，随机数&amp;1），因为长度为2的数组，最终有两个结果，那就是0或者1。 counterCells = rs; init = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (init) break; &#125; else if (U.compareAndSwapLong(this, BASECOUNT, v = baseCount, v + x)) break; // Fall back on using base &#125; &#125; 首先我们要知道，counterCells数组某个下标也会存在线程安全问题，因为，可能有多个线程通过随机数&amp;m的计算得到了，相同的数组下标。为了避免自增的线程安全，所需要也是需要进行加锁控制。 ​ 这里是通过chm的成员属性cellsBusy，进行数组下标锁的控制，默认是0，通过cas判断，当前数组下标是否存在操作的线程，如果不存在则把cellsBusy设置为1（U.compareAndSwapInt(this, CELLSBUSY, 0, 1)），然后执行里面的业务操作 . resizeStamp() 扩容操作扩容操作的代码如下 if (check &gt;= 0) &#123;//检查是否需要扩容 Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123;//判断s当前chm元素个数，是否大于sizectl（这里等于12，因为此时sizectl得知是在initTable()方法中进行初始化的），且chm数组不为空，且chm数组长度不大于最大值。 int rs = resizeStamp(n);//获得一个值，此时n是16，那么返回的是32795，得到一个唯一的扩容戳 if (sc &lt; 0) &#123;//这里的sc==sizeCtl==12，所以首次进来时，那么if判断不满足。但是当第二个线程进来后，发现sc已经是一个负数（因为此前已经被(U.compareAndSwapInt(this, SIZECTL, sc,(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) 设置为负数），满足条件，进入逻辑 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt);//扩容操作 &#125; else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2))//sc &gt;= 0 执行cas操作。将sc设置为一个负数。 transfer(tab, null);//扩容操作 s = sumCount(); &#125; &#125; //返回无符号整型i的最高非零位前面的0的个数，包括符号位在内；如果i为负数，这个方法将会返回0，符号位为1.比如说，10的二进制表示为 0000 0000 0000 0000 0000 0000 0000 1010，java的整型长度为32位。那么这个方法返回的就是28（32-4，因为10的最高非零是第四位的1，他前面还有28个0 - 即是：0000 0000 0000 0000 0000 0000 0000） static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1)); &#125; 此时通过resizeStamp(16)得到的值是32795，二进制是： else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2))//sc &gt;= 0 执行cas操作。 0000 ‭0000 ‭0000 ‭0000 ‭1000 0000 0001 1011‬ 然后rs &lt;&lt; RESIZE_STAMP_SHIFT，表示左移16位1000 0000 0001 1011‬ 0000 ‭0000 ‭0000 ‭0000 也就是相当于低十六位移动到高十六位，接着+21000 0000 0001 1011‬ 0000 ‭0000 ‭0000 ‭0010 高十六位代表扩容的标记，低十六位代表参加扩容的线程数（需要注意的是chm的扩容是可以多个线程并行扩容的，所以这里要记录参与扩容操作的线程数），也就是说，这里的+2，就表示当前有1个线程在扩容。最终通过cas把扩容戳转化为如上的数值，他是一个负数，执行cas成功，那么接着在执行扩容操作： transfer(tab, null); 需要注意的是chm的扩容是可以多个线程并行扩容的，所以才需要通过sc的低十六位来记录参与扩容操作的线程个数！！！！！！！！！！！！！！ transfer() 扩容操作我们知道扩容操作，要做的事情就两件： 增加chm数组的长度 转移原数组的节点到新数组 - 数据转移（这一步支持多个线程同时操作，提升效率） 因为方法比较长，所以在这里我们需要进行分段分析。为了方便分析，我们这里假设chm原数组长度是32。 private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride;// n == 32 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //NCPU是等于电脑cpu核数，我这里是4，所有NCPU=4，这里n还是32，那么stride = (4 &gt; 1) ? (32&gt;&gt;&gt; 3) / 4 : 32 就等于1. 又因为 1 &lt; MIN_TRANSFER_STRIDE,所以if成立，最终stride == 16。表示一个线程可以处理16个数组下标的位置。那么刚好此时数组长度是32，所以这里只需要2个线程进行扩容操作即可。 // 也就是表示需要2个线程同时进行扩容操作（最多支持CPU核心数的数量的线程，这里是4，但是因为数组长度是32，每个线程处理16个下标，所以这里只需要2个线程）。那么他们具体是怎么进行协调工作呢？还是分而治之的思路， //我们知道目前chm数组长度是n==32,那么我们这里一共需要2个线程同时进行扩容操作，那么每个线程刚好负责一个数组的16个下标，负责处理转移自己负责的数组下标的node节点数据到新数组。 //按道理通过上面的分析，我们可以得到这两个线程负责的数组下标区间，那么线程1负责下标为0-15的数据，线程2负责16-31的数据，转移到新数组。 //说了这么多，那么新数组的长度是多少呢？ if (nextTab == null) &#123; // initiating初始化新数组， try &#123; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1];//长度是n&lt;&lt; 1 也即是 32&lt;&lt; 1 等于64，那么也就是说，新数组长度为64位 nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab ​ 通过上面的代码，我们知道可以设置多个线程进行并发的参与扩容操作，那么这么确定每个线程自己负责哪一段的数组下标呢？ 接着往下看代码 可以看到这里是通过一个无限循环进行分割，每个线程负责的下标区间。 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //关键代码在这里，通过cas来确定每个线程负责的数组下标区间 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123;//首先nextIndex== 32，那么32 &gt; 16，所以nextBound = 32-16 == 16，然后cas操作，将TRANSFERINDEX设置为16 //首先 bound = nextBound;//所以bound = nextBound == 16 i = nextIndex - 1;// i == 32-1== 31 advance = false; //那么通过上面的操作，就得到了当前线程所需要的操作的线程区间，那就是（i，bound）=== (31,16) //那么第二个线程进来后，他负责的就是（15,0），跟我们上面的推测一致 &#125; &#125; ​ 我们知道上面的代码只是确定了线程负责数组区间，但是真正进行数据迁移到新数组的代码还在下面。 ​ 首先我们需要明确一点，迁移数据是从数组的后面往前迁移的，` if (–i &gt;= bound || finishing)` 由这段代码可以看出，i–的方式从后往前一个一个的迁移 迁移数据代码 ​ 迁移数据的代码块就是在for循环里面的，synchronize代码段。线程在他负责的数组下标区间进行，从后往前的数组下标数据的逐个迁移。最终每个数组的下标的数据，都会拆分成两个链表，高位链表和低位链表（ Node ln, hn）。低位链表，可以直接迁移到跟旧数组下标一样的新数组对应的下标的位置（例如链表本来在旧数组i==2的位置，那么迁移到新数组，也还是迁移到下标为2的位置），高位链表，需要迁移到i+n位置（例如旧数组下表是i==2，上面的代码数组长度n就是32，那么迁移到新数组的下标就是32+2 == 34） ​ 形成高低位链表迁移的好处就是，可以批量迁移节点到新数组。如果不怎么做，那么就需要把旧数组下标的链表逐个rehash，然后再逐个放到新数组，这样的效率太慢。 那么为什么低位链表可以直接平移，高位链表移到新数组对应的下标，要用原来的下标+原来数组长度呢？ 我们来证明一下： 假设原来数组长度是 n == 16，通过put（“k”,”k”），那么key要通过 hash&amp;(n-1) 算法得到他所属的数组下标，那么key==k的hash值假设等于9. 那么hash&amp;(n-1) 等于 hash 0000 1001&amp;n-1== 15 0000 1111== 0000 1001 等于9，也就是key等于k的节点应该归属于数组的9下标位置 假设n 扩容到 32，那么需要rehash原先在旧数组的值，然后通过hash&amp;(n-1) 算法得到他所属的新数组的下标 那么hash&amp;(n-1) 等于 hash 0000 1001&amp;32-1== 31 0001 1111== 0000 1001 等于9，也就是该节点在新数组还是对应下标9. 所以我们发现低位链表可以直接平移到新数组（因为key的hash的高位都是0，所以计算获取数组下标时，取决于低位） 同理证明高位链表，为什么需要n+i。假设key的hash是20，他的二进制是：0001 0100 那么hash&amp;(n-1) 等于 hash 0001 0100&amp;n-1== 15 0000 1111== 0000 0100 等于9 假设数组扩容到32 那么hash&amp;(n-1) 等于 hash 0001 0100&amp;32-1== 31 0001 1111== 0001 0100 等于16+4 == 20，也就是 n+i 证明完毕 总结​ 换句话说，addCount方法的第一个if代码块核心就是，进行chm元素个数的自增1。如果在单线程情况下，直接通过对baseCount的cas操作，进行数量的自增1. ​ 但是如果存在多个线程进行增加元素个数的操作时，不再使用对baseCount进行cas的方式进行数量增加，进而转化为使用一个counterCells数组的方式，进行分而治之的方式，通过对数组每个下标的cas操作，达到高效率高性能的元素数量自增。 执行到b2 区域 代码块如果执行到这部分代码那就表示，当前put操作的key对应的数组下标的位置，已经存在节点。（也就是冲突） else &#123; V oldVal = null; synchronized (f) &#123;// f就是当前数组下标的节点信息 node，然后锁着这个数组这个位置的节点 //可以看到加锁的粒度很细，这样能够拥有更高的性能，这样其他线程仍然能够操作node数组的其他下标节点。保证了当前节点的线程安全 if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1;//当前数组下标，链表的元素个数，这个值，我们知道 在addCount(1L, binCount)，方法会使用，通过他来判断你是否需要扩容 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125;//这个if的作用就是，判断当前插入的节点，的hash，key信息是否相等，如果相等，那么覆盖value值。然后退出当前循环，否则往下执行 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123;//因为hash和key不同，那么，在当前数组节点，构建一个链表，把新加入的节点，使用单链表连接保存 pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125;&#125; 首先来看一下Node节点的数据结构 ​ hash就是当前节点的key的hash值，key和val就是put操作是传入的key和value，原样保存，next就是冲突时，需要构建的单链表指向下一个占据相同数组下标的节点。 什么时候转化为红黑树如果链表长度大于8和node数组长度大于64的时候，如果再往当前链表添加数据，那么就会将当前链表转化为红黑树。 如果扩容后，当前数组节点的链表树小于8，他又会把红黑树转化为单链表 总结一下就是chm的核心要点 https://blog.csdn.net/yyzzhc999/article/details/96724885 阻塞队列 - BlockingQueue阻塞队列，顾名思义，首先它是一个队列,而一个阻塞队列在数据结构中所起的作用大致如图所示: 线程1往阻塞队列中添加元素二线程2从队列中移除元素 当阻塞队列是空时,从队列中获取元素的操作将会被阻塞.当阻塞队列是满时,往队列中添加元素的操作将会被阻塞. 同样 ​ 试图往已满的阻塞队列中添加新元素的线程同样也会被阻塞，直到其他线程从队列中移除一个或者多个元素或者全清空队列后使队列重新变得空闲起来并后续新增。 为什么用?有什么好处?​ 在多线程领域：所谓阻塞，在某些情况下会挂起线程(即线程阻塞)，一旦条件满足,被挂起的线程优惠被自动唤醒。 ​ 为什么需要使用BlockingQueue，好处是我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为BlockingQueue都一手给你包办好了。 ​ 在concurrent包 发布以前,在多线程环境下，我们每个程序员都必须自己去控制这些细节（通过wait和notify），尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。 BlockingQueue的结构 ArrayBlockingQueue: 由数组结构组成的有界阻塞队列. LinkedBlockingDeque: 由链表结构组成的有界(但大小默认值Integer&gt;MAX_VALUE)阻塞队列. PriorityBlockingQueue:支持优先级排序的无界阻塞队列. DelayQueue: 使用优先级队列实现的延迟无界阻塞队列. SynchronousQueue:不存储元素的阻塞队列,也即是单个元素的队列. SynchronousQueue没有容量 与其他BlcokingQueue不同,SynchronousQueue是一个不存储元素的BlcokingQueue 每个put操作必须要等待一个take操作,否则不能继续添加元素,反之亦然. 生产就马上用 /** * 阻塞队列SynchronousQueue演示 **/public class SynchronousQueueDemo &#123; public static void main(String[] args) &#123; BlockingQueue&lt;String&gt; blockingQueue = new SynchronousQueue&lt;&gt;(); new Thread(() -&gt; &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t put 1&quot;); blockingQueue.put(&quot;1&quot;);//阻塞，等待take System.out.println(Thread.currentThread().getName() + &quot;\\t put 2&quot;); blockingQueue.put(&quot;2&quot;);//阻塞，等待take System.out.println(Thread.currentThread().getName() + &quot;\\t put 3&quot;); blockingQueue.put(&quot;3&quot;);//阻塞，等待take &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;, &quot;AAA&quot;).start(); new Thread(() -&gt; &#123; try &#123; try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + blockingQueue.take());//取出阻塞队列中队列头元素 1。 然后这个时候才能够put（2） try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + blockingQueue.take());//取出阻塞队列中队列头元素 2。 然后这个时候才能够put（3） try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + blockingQueue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;, &quot;BBB&quot;).start(); &#125;&#125; 输出： AAA put 1BBB 1AAA put 2BBB 2AAA put 3BBB 3 可以看到，他是生产一个，然后生产线程阻塞，等待消费者，take消费。然后再接着生产。。。。 LinkedTransferQueue:由链表结构组成的无界阻塞队列. LinkedBlockingDeque:由了解结构组成的双向阻塞队列. BlockingQueue的核心方法 实际上，这四组api是适用于不同的应用场景的。 校验第一组api，使用的时候，会抛出异常。 public class BlockingQueueDemo &#123; public static void main(String[] args) &#123; BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3);//因为ArrayBlockingQueue是有界的，所有必须传入有界值 System.out.println(blockingQueue.add(&quot;a&quot;)); System.out.println(blockingQueue.add(&quot;b&quot;)); System.out.println(blockingQueue.add(&quot;c&quot;));// System.out.println(blockingQueue.add(&quot;d&quot;));//插入失败，提示java.lang.IllegalStateException: Queue full。因为我们设置的数组长度最大是3. /* System.out.println(blockingQueue.remove());//不指定删除的元素，那么就会默认删除队列头元素 System.out.println(blockingQueue.remove()); System.out.println(blockingQueue.remove());*/// System.out.println(blockingQueue.remove());//删除失败，因为数组最大长度只允许有三个元素。抛出异常 java.util.NoSuchElementException System.out.println( blockingQueue.element() );//获取队列头元素，不存在则抛出异常-NoSuchElementException &#125;&#125; 校验第二组api offer，插入元素，假设插入元素后，数组长度超过了设定的有界值，那么返回false（相比第一组api的add，offer不会抛出异常） poll，移除元素，同理，要移除的元素不存在则返回null 使用案例生产者消费者模式生产一个消费一个 wait和notify版本package com.kingge.test;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 共享资源类 */class ShareData &#123; private int num = 0; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void increment() throws Exception &#123; lock.lock(); try &#123; //判断 while (num != 0) &#123; //等待 不生产 condition.await(); &#125; //干活 num++; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); //通知唤醒 condition.signalAll(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void deIncrement() throws Exception &#123; lock.lock(); try &#123; //判断 while (num == 0) &#123; //等待 不消费 condition.await(); &#125; //干活 num--; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + num); //通知唤醒 condition.signalAll(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;/** * 一个初始值为0的变量 两个线程交替操作 一个加1 一个减1来5轮 **/public class ProdConsumerTraditionDemo &#123; public static void main(String[] args) &#123; ShareData shareData = new ShareData(); new Thread(() -&gt; &#123; for (int i = 1; i &lt;= 5; i++) &#123; try &#123; shareData.increment(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;AA&quot;).start(); new Thread(() -&gt; &#123; for (int i = 1; i &lt;= 5; i++) &#123; try &#123; shareData.deIncrement(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;, &quot;BB&quot;).start(); &#125;&#125; 输出： AA 1BB 0AA 1BB 0AA 1BB 0AA 1BB 0AA 1BB 0 阻塞队列版package com.kingge.test;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;class MyResource &#123; /** * 默认开启 进行生产消费的交互 */ private volatile boolean flag = true; /** * 默认值是0 */ private AtomicInteger atomicInteger = new AtomicInteger(); private BlockingQueue&lt;String&gt; blockingQueue = null; public MyResource(BlockingQueue&lt;String&gt; blockingQueue) &#123; this.blockingQueue = blockingQueue; System.out.println(blockingQueue.getClass().getName()); &#125; public void myProd() throws Exception &#123; String data = null; boolean returnValue; while (flag) &#123; data = atomicInteger.incrementAndGet() + &quot;&quot;; returnValue = blockingQueue.offer(data, 2L, TimeUnit.SECONDS); if (returnValue) &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t 插入队列数据&quot; + data + &quot;成功&quot;); &#125; else &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t 插入队列数据&quot; + data + &quot;失败&quot;); &#125; TimeUnit.SECONDS.sleep(1);//1s生产一个 &#125; System.out.println(Thread.currentThread().getName() + &quot;\\t 停止 表示 flag&quot; + flag); &#125; public void myConsumer() throws Exception &#123; String result = null; while (flag) &#123; result = blockingQueue.poll(2L, TimeUnit.SECONDS); if(null==result||&quot;&quot;.equalsIgnoreCase(result))&#123; flag=false; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;超过2m没有取到 消费退出&quot;); System.out.println(); System.out.println(); return; &#125; System.out.println(Thread.currentThread().getName() + &quot;消费队列&quot; + result + &quot;成功&quot;); &#125; &#125; public void stop() throws Exception&#123; flag=false; &#125;&#125;/** * volatile/CAS/atomicInteger/BlockQueue/线程交互/原子引用 **/public class ProdConsumerBlockQueueDemo &#123; public static void main(String[] args) throws Exception &#123; MyResource myResource = new MyResource(new ArrayBlockingQueue&lt;&gt;(10)); new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+&quot;\\t生产线程启动&quot;); try &#123; myResource.myProd(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;,&quot;Prod&quot;).start(); new Thread(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+&quot;\\t消费线程启动&quot;); try &#123; myResource.myConsumer(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;,&quot;consumer&quot;).start(); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(); System.out.println(); System.out.println(); System.out.println(&quot;时间到,停止活动&quot;); myResource.stop(); &#125;&#125; 输出： java.util.concurrent.ArrayBlockingQueueProd 生产线程启动consumer 消费线程启动Prod 插入队列数据1成功consumer消费队列1成功Prod 插入队列数据2成功consumer消费队列2成功Prod 插入队列数据3成功consumer消费队列3成功consumer消费队列4成功Prod 插入队列数据4成功consumer消费队列5成功Prod 插入队列数据5成功时间到,停止活动Prod 停止 表示 flagfalseconsumer 超过2m没有取到 消费退出 总结​ 通过查看take源码，我们就可以知道，为什么take能够实现阻塞。实际上就是利用了ReentrantLock的condition的await机制，进行阻塞，然后把当前执行take 操作的线程加入到notEmpty的condition等待队列中，然后等待其他线程加入元素后，那么就会执行notEmpty.signal（）方法唤醒在等待队列的take线程，加入到aqs阻塞队列，然后等待获取锁，然后接着执行take操作。 也就是说，阻塞队列，是基于ReentrantLock来实现的。 获取线程的第三种方式 - Callable创建线程的2种方式，一种是直接继承Thread，另外一种就是实现Runnable接口。 这2种方式都有一个缺陷就是：在执行完任务之后无法获取执行结果。 如果需要获取执行结果，就必须通过共享变量或者使用线程通信的方式来达到效果，这样使用起来就比较麻烦。 Callable和Future介绍 ​ Callable接口代表一段可以调用并返回结果的代码。Future接口表示异步任务，是还没有完成的任务给出的未来结果。所以说Callable用于产生结果，Future用于获取结果。 ​ Callable接口使用泛型去定义它的返回类型。Executors类提供了一些有用的方法在线程池中执行Callable内的任务。由于Callable任务是并行的（并行就是整体看上去是并行的，其实在某个时间点只有一个线程在执行），我们必须等待它返回的结果。 ​ java.util.concurrent.Future对象为我们解决了这个问题。在线程池提交Callable任务后返回了一个Future对象，使用它可以知道Callable任务的状态和得到Callable返回的执行结果。Future提供了get()方法让我们可以等待Callable结束并获取它的执行结果。 这里说的是通过实现Callable接口，来创建线程 @FunctionalInterfacepublic interface Callable&lt;V&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception;&#125; 相比实现Runnable的run方法，Callable的call方法，是具备返回值的，而且能够抛出异常。 那么怎么使用呢？跟Runnable一样，传入Thread构造函数中，然后创建实例？ 我们查看Thread的构造函数，发现并没有入参是Callable的构造器。 那么我们这个时候就想，有没有什么接口实现了Runnable接口. 很明显这里找到了，这个接口或者实现类。通过查看，我们可以得知FutureTask类，既提供了Callable为入参的构造器。 最终示例代码 public class CallableDemo implements Callable&lt;Integer&gt;&#123; @Override public Integer call() throws Exception &#123; System.out.println(&quot;开始执行call 方法&quot;); return 1024; &#125; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(new CallableDemo()); Thread thread = new Thread(futureTask); thread.start(); System.out.println( &quot;获取call方法返回的回执： &quot; + futureTask.get() ); &#125;&#125; 测试futureTask.get()方法的阻塞性 class CallableResource implements Callable&lt;Integer&gt;&#123; private Long awaitTime; public CallableResource( Long awaitTime ) &#123; this.awaitTime = awaitTime; &#125; @Override public Integer call() throws Exception &#123; System.out.println( Thread.currentThread().getName() + &quot; 开始执行call 方法&quot;); TimeUnit.SECONDS.sleep(this.awaitTime); return 1024; &#125; &#125;public class CallableDemo &#123; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; FutureTask&lt;Integer&gt; target = new FutureTask&lt;&gt;(new CallableResource(10l)); Thread thread = new Thread(target,&quot;AA&quot;); thread.start(); System.out.println( target.get() );//执行到这里会阻塞，等待十秒后拿到call的返回结果，才往下执行。 System.out.println( &quot;执行main线程&quot; ); &#125; &#125; 所以说，建议把futuretask.get方法放到最后，不然，一直阻塞，影响其他线程执行。 获取线程的第四种方式：线程池- 底层由ThreadPoolExecutor实现 线程池做的工作主要是控制运行的线程的数量，处理过程中将任务加入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量，超出的数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务来执行。 他的主要特点为：线程复用，控制最大并发数，管理线程。 第一：降低资源消耗，通过重复利用自己创建的线程降低线程创建和销毁造成的消耗。第二：提高响应速度，当任务到达时，任务可以不需要等到线程创建就能立即执行。第三：提高线程的可管理性，线程是稀缺资源，如果无限的创建，不仅会消耗资源（JVM的内存管理），还会降低系统的稳定性，使用线程池可以进行统一分配，调优和监控。 线程池架构实现Java中的线程池是通过Executor框架实现的。该框架中用到了Executor,Executors,ExecutorService,ThreadPoolExecutor这几个类. 其中Executors是工具类，类似于。Arrays、Collections。 编码实现 - 五种方式创建线程池Executors.newScheduledThreadPool()Executors.newWorkStealingPool(int)java8新增,使用目前机器上可以的处理器作为他的并行级别，不常用。 Executors.newFixedThreadPool(int)主要特点如下:1.创建一个定长线程池，可控制线程的最大并发数，超出的线程会在队列中等待。2.newFixedThreadPool创建的线程池corePoolSize和MaxmumPoolSize是 相等的,它使用的的LinkedBlockingQueue。 例子： public static void main(String[] args) &#123; ExecutorService fixedThreadPool = Executors.newFixedThreadPool(5);//设置一个线程个数为5的线程池 try &#123; for (int i = 0; i &lt; 10; i++) &#123;//模拟十个用户进行请求 fixedThreadPool.submit( () -&gt; &#123; System.out.println( &quot;使用线程 &quot; +Thread.currentThread().getName() + &quot; 处理业务&quot; ); &#125; ); &#125; &#125; catch (Exception e) &#123; &#125;finally &#123; fixedThreadPool.shutdown(); &#125; &#125; &#125; 输出： 使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-4 处理业务使用线程 pool-1-thread-3 处理业务使用线程 pool-1-thread-2 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-3 处理业务使用线程 pool-1-thread-4 处理业务使用线程 pool-1-thread-5 处理业务使用线程 pool-1-thread-2 处理业务 可以看到，不管有多少个请求，最多有五个线程进行交替处理这十个请求。 Executors.newSingleThreadExecutor()一个任务一个线程执行的任务场景，线程池中只有一个线程来处理业务。 他就类似于，Executors.newFixedThreadPool(1)，不管有多少个请求，线程池内只有一个线程在执行这些请求 。 主要特点如下:1.创建一个单线程化的线程池,它只会用唯一的工作线程来执行任务,保证所有任务都按照指定顺序执行.2.newSingleThreadExecutor将corePoolSize和MaxmumPoolSize都设置为1,它使用的的LinkedBlockingQueue Executors.newCachedThreadPool()这个是一池N线程，也就是，不知道线程池中有多少个线程，当请求过来时，他会自动的创建相应的线程，线程池中的线程不是固定数量，有时候可能创建5个，也有可能创建1个，就看每个线程的执行能力，自动创建。 适用:执行很多短期异步的小程序或者负载较轻的服务器。 主要特点如下:1.创建一个可缓存线程池,如果线程池长度超过处理需要,可灵活回收空闲线程,若无可回收,则创建新线程.2.newCachedThreadPool将corePoolSize设置为0，MaxmumPoolSize设置为Integer.MAX_VALUE,它使用的是SynchronousQUeue,也就是说来了任务就创建线程运行,如果线程空闲超过60秒,就销毁线程 总结可以看到，后面三个线程池的创建，底层代码，都是通过ThreadPoolExecutor进行创建的。 线程池几个重要参数介绍 - 重要刚才我们看了fixedThreadPool、singleThreadPool、cachedThreadPool发现他们最终的实现都是ThreadPoolExecutor，只有五个参数啊？哪来的七个参数呢？ 我们再接着查看ThreadPoolExecutor构造器，发下他内部调用的this就是传递了七个参数。 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } 再接着看this /** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @param threadFactory the factory to use when the executor * creates a new thread * @param handler the handler to use when execution is blocked * because the thread bounds and queue capacities are reached * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; * or &#123;@code threadFactory&#125; or &#123;@code handler&#125; is null */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; corePoolSize:线程池中的常驻核心线程数 1.在创建了线程池后,当有请求任务来之后,就会安排池中的线程去执行请求任务,近视理解为今日当值线程 2.当线程池中的线程数目达到corePoolSize后,就会把到达的任务放入到缓存队列当中。 通俗来讲，就是，不管你用不用得到，线程池创建后，里面就有corePoolSize个线程在等着你使用。类似于银行的办事窗口，不管有没有办业务，窗口还是这么多个等着你。 maximumPoolSize:线程池能够容纳同时执行的最大线程数,此值大于等于1 也就是说，线程池中，最多最多有maximumPoolSize个线程，已经是峰值了，不能再增加了。也就是说当corePoolSize不够用了，那么可能增加到maximumPoolSize个线程。这里说的是，可能会增加，但是什么时候增加呢？ 答案是：当corePoolSize个线程已经被使用，而且，任务队列（workQueue）中等待执行的任务也已经占满了队列，那么这个时候，如果还有任务请求进来，那么这个时候就会扩展线程到maximumPoolSize个，然后先执行之前在任务队列中阻塞的任务，把后来的任务放到任务队列中继续等待。 但是可能会有个问题，这个时候，又有新的任务进来了，此刻线程已经扩展到maximumPoolSize个，任务队列也已经占满。那么为了避免其他情况的发生，这个时候就需要拒绝后来的任务。这个时候，第七个参数的重要性就来了。handler拒绝策略 keepAliveTime:多余的空闲线程存活时间。 当空间时间达到keepAliveTime值时，发现没有任务执行了，那么多余的线程会被销毁直到只剩下corePoolSize个线程为止。 默认情况下: 只有当线程池中的线程数大于corePoolSize时keepAliveTime才会起作用,直到线程中的线程数不大于corepoolSIze, ​ 这个很好理解，因为创建线程太多（最多maximumPoolSize个）会消耗内存资源，所以我们肯定要有个销毁机制，但是我们又不能全部销毁线程池中的所有线程（创建线程也需要开销），所以我们一般是销毁到corePoolSize个就停止。 unit: keepAliveTime的单位 workQueue:任务队列,被提交但尚未被执行的任务. 这个也很好理解，就是线程不够用了（达到corePoolSize个），那么后面进来的请求，就等待阻塞。这里一般是通过阻塞队列进行实现。 threadFactory:表示生成线程池中工作线程的线程工厂,用户创建新线程,一般用默认即可 handler:拒绝策略,表示当线程队列满了并且工作线程大于等于线程池的最大 数(maxnumPoolSize)时如何来拒绝. 总结换句话说，corePoolSize是线程池的初始值，如果任务上涨，那么maximumPoolSize和workQueue就是保底策略，当任务还是持续上涨，那么handler拒绝策略是最终解决方案。 反之如果任务从高峰开始下降，那么keepAliveTime和unit就是收尾工作的保证。 线程池的底层工作原理 - 重要！！！！！！！！！ 这张图就是对应了整个线程池七个参数的的使用和执行流程。 线程池用过吗?生产上你是如何设置合理参数线程池的拒绝策略请你谈谈 拒绝策略什么时候生效？ 等待队列也已经排满了,再也塞不下新的任务了，同时，线程池的max也到达了,无法继续为新任务服务，这时我们需要拒绝策略机制合理的处理这个问题。 JDK内置的拒绝策略 AbortPolicy(默认):直接抛出RejectedException异常阻止系统正常运行 CallerRunPolicy:”调用者运行”一种调节机制,该策略既不会抛弃任务,也不会抛出异常,而是将某些任务回退到调用者 DiscardOldestPolicy:抛弃队列中等待最久的任务,然后把当前任务加入队列中尝试再次提交。 DiscardPolicy:直接丢弃任务,不予任何处理也不抛出异常.如果允许任务丢失,这是最好的一种方案。 以上内置策略均实现了RejectExecutionHandler接口。 你在工作中单一的/固定数的/可变你的三种创建线程池的方法,你用哪个多?超级大坑正确答案是：一个都不用，我们生产上只能使用自定义的。 参考阿里巴巴java开发手册 【强制】线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。 说明：使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。 【强制】线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。说明：Executors返回的线程池对象的弊端如下：1）FixedThreadPool和SingleThreadPool:允许的请求队列长度为Integer.MAX_VALUE，可能会堆积大量的请求，从而导致OOM。2）CachedThreadPool和ScheduledThreadPool:允许的创建线程数量为Integer.MAX_VALUE，可能会创建大量的线程，从而导致OOM。 自定义过线程池使用 - 并使用拒绝策略public static void main(String[] args) &#123; ThreadPoolExecutor poolExecutor = new ThreadPoolExecutor( 2, 5, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); try &#123; for (int i = 0; i &lt; 9; i++) &#123; poolExecutor.execute( () -&gt; &#123; System.out.println( \"使用线程 \" +Thread.currentThread().getName() + \" 处理业务\" ); &#125; ); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; poolExecutor.shutdown(); &#125; &#125; 看上面代码，我们知道，线程池最多容纳8个任务（最大值5+阻塞队列3 = 8，5个任务在执行，3个任务在阻塞队列等待），那么执行的任务达到9个时候，很明显就会触发拒绝策略。 运行上面代码输出： 使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-3 处理业务使用线程 pool-1-thread-2 处理业务使用线程 pool-1-thread-4 处理业务java.util.concurrent.RejectedExecutionException: Task kingge.CustomThreadPool$$Lambda$1/681842940@2a84aee7 rejected from java.util.concurrent.ThreadPoolExecutor@a09ee92[Running, pool size = 5, active threads = 5, queued tasks = 0, completed tasks = 3] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) at kingge.CustomThreadPool.main(CustomThreadPool.java:19)使用线程 pool-1-thread-5 处理业务 很明显报了异常RejectedExecutionException。这个就是默认的AbortPolicy策略发出的。 使用CallerRunPolicy策略ThreadPoolExecutor poolExecutor = new ThreadPoolExecutor( 2, 5, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.CallerRunPolicy()); 输出： 使用线程 pool-1-thread-2 处理业务使用线程 main 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-4 处理业务使用线程 pool-1-thread-3 处理业务使用线程 pool-1-thread-5 处理业务 你会发现他成功执行了8个任务（这个是符合我们对于线程池的设置），但是我们发现，他并没有报异常，但是输出了这么一行日志使用线程 main 处理业务。 我们再回过头来看一下CallerRunPolicy拒绝策略的定义： 该策略既不会抛弃任务,也不会抛出异常,而是将某些任务回退到调用者。那么谁是线程调用者？很明显上诉代码中，main线程就是任务的调用者。所以这里让main新成进行了处理 DiscardOldestPolicy拒绝策略抛弃队列中等待最久的任务,然后把当前任务加入队列中尝试再次提交。 ThreadPoolExecutor poolExecutor = new ThreadPoolExecutor( 2, 5, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.DiscardOldestPolicy()); 我们发现，他每次只执行8个任务（服务设置），其余两个任务被抛弃，不会抛出异常。 输出： 使用线程 pool-1-thread-2 处理业务使用线程 pool-1-thread-5 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-3 处理业务使用线程 pool-1-thread-2 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-5 处理业务使用线程 pool-1-thread-4 处理业务 DiscardPolicy拒绝策略直接丢弃任务,不予任何处理也不抛出异常.如果允许任务丢失,这是最好的一种方案。 ThreadPoolExecutor poolExecutor = new ThreadPoolExecutor( 2, 5, 1, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.DiscardPolicy()); 他的运行结果： 使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-3 处理业务使用线程 pool-1-thread-2 处理业务使用线程 pool-1-thread-3 处理业务使用线程 pool-1-thread-4 处理业务使用线程 pool-1-thread-4 处理业务使用线程 pool-1-thread-1 处理业务使用线程 pool-1-thread-5 处理业务 他跟DiscardOldestPolicy策略类似，只不过DiscardOldestPolicy是从阻塞队列中抛弃长时间的任务，而，DiscardPolicy是从一开始就抛弃多余任务，压根就没进阻塞队列。 合理配置线程池你是如何考虑的?首先查看CPU核数 CPU密集型 System.out.println(Runtime.getRuntime().availableProcessors());查看CPU核数 IO密集型 如果线上机器突然宕机，线程池的阻塞队列中的请求怎么办？阻塞队列中的请求，都是存放在内存中的，那么如果机器宕机，那么队列中的请求必然会丢失。那么怎么解决呢？ 第一反应，应该就是，本地化，保存到本地硬盘（例如数据库等等） 解决方案： 如果你要提交一个任务到线程池之前，先把任务的信息，保存到数据库中，并更新他的状态（未提交，已提交，已完成），提交成功后，他的状态修改为已提交。 假设机器宕机，那么当机器重启后，系统启动，后台线程可以去扫描数据库中的数据，然后把未提交和已提交的任务拿出来，再次重新提交到线程池中，继续执行 死锁编码及定位分析 产生死锁的主要原因：系统资源不足、进程运行推进的顺序不合适、资源分配不当。 代码实现死锁： package kingge;import java.util.concurrent.TimeUnit;class HoldThread implements Runnable &#123; private String lockA; private String lockB; public HoldThread(String lockA, String lockB) &#123; this.lockA = lockA; this.lockB = lockB; &#125; @Override public void run() &#123; synchronized (lockA) &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t 自己持有锁&quot; + lockA + &quot;尝试获得&quot; + lockB); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (lockB) &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t 自己持有锁&quot; + lockB + &quot;尝试获得&quot; + lockA); &#125; &#125; &#125;&#125;/** * Description: * 死锁是指两个或者以上的进程在执行过程中, * 因争夺资源而造成的一种相互等待的现象, * 若无外力干涉那他们都将无法推进下去 **/public class DeadLockDemo &#123; public static void main(String[] args) &#123; String lockA = &quot;lockA&quot;; String lockB = &quot;lockB&quot;; new Thread(new HoldThread(lockA, lockB), &quot;threadAAA&quot;).start(); new Thread(new HoldThread(lockB, lockA), &quot;threadBBB&quot;).start(); &#125;&#125; 运行程序： 你会发现程序卡死在这里 解决死锁 - 重要！！！！ 1.jps命令定位进程编号 获取死锁程序的进程编号- 使用 jps -l King哥@JeremyKing MINGW64 ~/Desktop$ jps -l9268 kingge.DeadLockDemo773610908 sun.tools.jps.Jps 可以知道是9268 2.jstack找到死锁查看 使用命令 ： jstack 9268 查看输出日志可以得到： Java stack information for the threads listed above:===================================================&quot;threadBBB&quot;: at kingge.HoldThread.run(DeadLockDemo.java:25) - waiting to lock &lt;0x00000000d5c9b7e0&gt; (a java.lang.String) - locked &lt;0x00000000d5c9b818&gt; (a java.lang.String) at java.lang.Thread.run(Thread.java:745)&quot;threadAAA&quot;: at kingge.HoldThread.run(DeadLockDemo.java:25) - waiting to lock &lt;0x00000000d5c9b818&gt; (a java.lang.String) - locked &lt;0x00000000d5c9b7e0&gt; (a java.lang.String) at java.lang.Thread.run(Thread.java:745)Found 1 deadlock. 可以看到 threadBBB 锁着0x00000000d5c9b818，等待0x00000000d5c9b7e0。然而threadAAA锁着0x00000000d5c9b7e0，等待0x00000000d5c9b818。就是死锁。 缓存的重要性缓存就是利用了局部性原理实现了数据的高效读取。局部性原理包括时间局部性（temporal locality）和空间局部性（spatial locality）这两种策略。 ​ 在实际的计算机日常的开发和应用中，我们对于数据的访问总是会存在一定的局部性。有时候，这个局部性是时间局部性，就是我们最近访问过的数据还会被反复访问。有时候，这个局部性是空间局部性，就是我们最近访问过数据附近的数据很快会被访问到。 ​ 而局部性的存在，使得我们可以在应用开发中使用缓存这个有利的武器。比如，通过将热点数据加载并保留在速度更快的存储设备里面，我们可以用更低的成本来支撑服务器。 序列化和反序列化原理分析设么叫序列化：把存储在内存中的数据，保存到本地硬盘或者传输 trensiant：修饰某个属性，目的就是阻止这个属性进行序列化 那么为什么会存在writeObject和readObject？这两个方法就是序列化和反序列化调用的方法。 可以通过这个方法制订序列化和反序列化规则，例如某个字段虽然被trensiant修饰，但是我们可以在writeObejct方法中对这个字段打破trensiant的作用，也就是，可以序列化。让trensiant失效 java原生序列化，需要实现接口，Serializable。同时建议制订一个serialVersionUID，目的就是给序列化的对象加个版本号，也就是说我当前序列化这个对象的版本是serialVersionUID，那么你反序列化的时候，也要是这个serialVersionUID，一模一样，如果你修改了这个值serialVersionUID，再去反序列化那么就会报异常。，这样保证了数据的安全性。 ​ 如果不指定serialVersionUID，默认会生成一个，但是建议制订。避免后面来的开发人员，发现，咦，你这个要序列化的类竟然没有制订serialVersionUID，那么他随手给你加上了，导致，下次反序列化的时候，发现版本号不一致（默认生成的serialVersionUID跟后来人加上的serialVersionUID，不相等），反序列化失败，对象数据就拿不到了。 现在常用的序列化格式有，json，xml，hessian等等 xml的好处是：阅读性强，可以保存更多的东西，例如类名，等等。缺点是，序列化后很大，占据的空间多 json，就是比较轻量级，类似{name:”King哥”,age:12} 这样的格式，好处就是轻量级，传输所需要的带宽少，缺点就是不能够携带更多的信息。 所以具体选择哪种序列化手段，得看你具体业务场景，和你对带宽的要求","categories":[{"name":"java核心技术杂记","slug":"java核心技术杂记","permalink":"http://kingge.top/categories/java核心技术杂记/"}],"tags":[{"name":"动态代理","slug":"动态代理","permalink":"http://kingge.top/tags/动态代理/"},{"name":"juc","slug":"juc","permalink":"http://kingge.top/tags/juc/"},{"name":"对象头","slug":"对象头","permalink":"http://kingge.top/tags/对象头/"},{"name":"chm","slug":"chm","permalink":"http://kingge.top/tags/chm/"},{"name":"cas","slug":"cas","permalink":"http://kingge.top/tags/cas/"}]},{"title":"jmeter压测工具使用总结","slug":"jmeter使用总结","date":"2020-03-05T14:21:59.000Z","updated":"2020-05-08T03:02:09.621Z","comments":true,"path":"2020/03/05/jmeter使用总结/","link":"","permalink":"http://kingge.top/2020/03/05/jmeter使用总结/","excerpt":"","text":"常用压测工具比对1、loadrunner 性能稳定，压测结果及细粒度大，可以自定义脚本进行压测，但是太过于重大，功能比较繁多 2、apache ab(单接口压测最方便) 模拟多线程并发请求,ab命令对发出负载的计算机要求很低，既不会占用很多CPU，也不会占用太多的内存，但却会给目标服务器造成巨大的负载, 简单DDOS攻击等 3、webbench webbench首先fork出多个子进程，每个子进程都循环做web访问测试。子进程把访问的结果通过pipe告诉父进程，父进程做最终的统计结果。 Jmeter介绍和安装Jmeter基本介绍和使用场景 压测不同的协议和应用 ​ 1) Web - HTTP, HTTPS (Java, NodeJS, PHP, ASP.NET, …)​ 2) SOAP / REST Webservices​ 3) FTP​ 4) Database via JDBC​ 5) LDAP 轻量目录访问协议​ 6) Message-oriented middleware (MOM) via JMS​ 7) Mail - SMTP(S), POP3(S) and IMAP(S)​ 8) TCP等等 ​ 使用场景及优点 ​ 1）功能测试​ 2）压力测试​ 3）分布式压力测试​ 4）纯java开发​ 5）上手容易，高性能​ 4）提供测试数据分析​ 5）各种报表数据图形展示 Jmeter4.0安装 简介：GUI图形界面的安装 1、需要安装JDK8。或者JDK9,JDK10 2、快速下载 windows： http://mirrors.tuna.tsinghua.edu.cn/apache//jmeter/binaries/apache-jmeter-4.0.zip mac或者linux：http://mirrors.tuna.tsinghua.edu.cn/apache//jmeter/binaries/apache-jmeter-4.0.tgz 3、文档地址：http://jmeter.apache.org/usermanual/get-started.html 4、建议安装JDK环境，虽然JRE也可以，但是压测https需要JDK里面的 keytool工具 下载完成解压后 目录 bin:核心可执行文件，包含配置 jmeter.bat: windows启动文件：也就是说，在windows平台下，直接双击这个bat文件，即可启动jmeter。 ​ jmeter: mac或者linux启动文件：​ jmeter-server：mac或者Liunx分布式压测使用的启动文件​ jmeter-server.bat：mac或者Liunx分布式压测使用的启动文件​ jmeter.properties: 核心配置文件，例如可以设置GUI界面的语言（中文还是英文）​ ​ extras：插件拓展的包​ lib:核心的依赖包​ ext:核心包​ junit:单元测试包 Jmeter语言版本中英文切换 ​ 1、控制台修改​ menu -&gt; options -&gt; choose language​ 2、配置文件修改​ bin目录 -&gt; jmeter.properties​ 默认 #language=en​ 改为 language=zh_CN Jmeter使用首先我们双击jmeter.bat 启动jmeter： 创建线程组右键TestPlan，开始创建测试计划，首先创建线程组，作用是控制总体并发 接着看测试组的界面设置 这里主要介绍着三个重要的设置： 线程数：虚拟用户数。一个虚拟用户占用一个进程或线程，也就是说，你想模拟100个人访问接口，那么这里就填100。 ​ 准备时长（Ramp-Up Period(in seconds)）：全部线程启动的时长，比如100个线程，20秒，则表示20秒内100个线程都要启动完成，每秒启动5个线程。也就是说，这个参数模拟的是，用户访问数量的逐渐上涨，随着时间的增加，慢慢增长到设置的“线程数”。因为在真实的环境中，访问某个接口的用户量是慢慢上涨的，一般不会是1s内暴增很多个 用户。 - 总而言之，就是在这个时间过后达到上面设置的线程数。​​ 循环次数：每个线程发送的次数，假如值为5，100个线程，则会发送500次请求，可以勾选永远循环，那么就会一直重复发送请求。 我这里为这三个参数自上而下设置的是：10、5、1。 表示，一共模拟十个用户请求，5s内发送这十个请求，也就意味着每两秒请求两次接口，第五秒后请求结束。1表示，这进行一轮测试，也就是发送完10个请求后便停止请求。 那么如果你想模拟，每秒并发1000，持续发送5次，那么就需要由上到下设置为：1000/1/5。 创建采样器 - 模拟请求我们在上面通过线程组创建了请求的频率和请求的用户数等设置，那么我们怎么开始发送请求呢？那么这个时候就需要采样器。 可以看到，你可以在线程组下面创建多种类型的请求采样器。我这里创建的是http请求采样器。 设置完成后，我们就可以，点击上面的启动按钮，这样就可以发送请求，可以看到，请求后台的接口，每两秒打印两次输出，一共打印了五次，一共是10个请求（因为线程组设置10个请求5s内发完，只进行一轮请求） 看到这里我们，可能会有疑问，那就是，我怎么看请求返回的值或者请求的状态呢？能不能跟踪请求呢？ 创建结果收集树 - 查看测试结果我们可以右键线程组，然后创建一个结果收集器。 启动采样器发出请求后，查看结果收集树。 可以看到在线程组下面每个请求的详细信息包括返回值。 上面的结果树是在线程组上面进行添加，也就意味着，线程组下面多个采样器，那么他们都将使用同一个结果数查看（可以看到上面结果树输出了users、login接口两个的请求信息），当然你也可以专门为某一个接口创建单独的结果树（右键采样器，创建即可） 模拟一次性请求多个接口上面的演示，我们是在线程组下面只创建了一个 采样器。实际上我们可以在线程组下面创建多个采样器。那么这样就意味着，多个接口同时遵循线程组的设置信息。 例如： 如图，创建了两个采样器，那么当启动线程组的时候，是会同时请求这两个接口的，这样就可以模拟在某一秒的时候，同时去请求多个接口，这种业务场景在现实中也是很常见的。 上面线程组设置是：10、5、1。启动线程组，查看结果收集树： ​ 可以看到，确实是两个采样器使用了通个线程组的配置。1s内发送了两个请求，访问了users接口和login接口。然后一共发送5s，users和login接口各自请求了10次，只进行一轮请求。 Jmeter的断言他的作用就是，结合采样器使用。通过自定义一些判断依据，判断请求接口是否成功，是否达到我们想要的结果。例如我们可以增加一个判断响应状态是202的断言，只有当请求接口相应状态码是202，那么就说明请求成功，否则可以自定义一些错误信息。 响应断言增加断言: 线程组 -&gt; 添加 -&gt; 断言 -&gt; 响应断言 ​ apply to(应用范围):​ Main sample only: 仅当前父取样器 进行断言，一般一个请求，如果发一个请求会触发多个，则就有sub sample（比较少用） ​ 要测试的响应字段：也就是区域1的内容​ 响应文本：即响应的数据，比如json等文本​ 响应代码：http的响应状态码，比如200，302，404这些​ 响应信息：http响应代码对应的响应信息，例如：OK, Found​ Response Header: 响应头 ​ 模式匹配规则：也就是区域2的内容​ 包括：包含在里面就成功​ 匹配：响应内容完全匹配，不区分大小写​ equals：完全匹配，区分大小写 ​ Substring：判断子串 ​ ​ 匹配值：也就是区域3的内容 ​ 根据匹配规则，设定值 ​ 自定义匹配失败信息：也就是区域4的内容 ​ 当匹配失败的时候，返回的错误说明。 例子： 我们根据接口相应数据，进行判断。当响应数据等于kingge的时候，请求成功，否则请求失败，提示下面的错误信息“users接口返回值，不等于kingge” 我们知道正常访问users接口的返回值是“laojiang”，当我们启动采样器请求users接口，肯定是断言失败的。 断言失败，查看结果树任务结果颜色标红(通过结果数里面双击不通过的记录，可以看到错误信息) 每个sample采样器下面可以加单独的结果树，然后同时加多个断言，最外层可以加个结果树进行汇总，这样的架构是最常用的。 第一部分总结​ 我们一般进行接口测试，对应在jmeter需要创建一个线程组。一个接口可能调用了多个接口，那么这个时候我们就需要在线程组下面创建多个采样器，同时在线程组下面创建一个多个采样器共同使用的结果树，这样的好处是，可以查看所有采样器的请求结果。，当然我们也可以在线程组下面创建一个或者多个断言，进行某些规则的匹配从而判断采样器请求是否满足我们的需求（需要注意，这个时候线程组下面的所有采样器，统一使用创建的断言）。 ​ 我们也可以单独为某个接口采样器创建相应的结果树和断言，这样能够单独查看某个接口的请求情况和单独的断言判断。 Jmeter压测结果聚合报告新增聚合报告：1. 增加线程组整体的聚合报告，统计线程组下所有的采样器请求数据报告： ​ 线程组-&gt;添加-&gt;监听器-&gt;聚合报告（Aggregate Report） ​ 2.给单个采样器添加聚合报告： ​ sample采样器-&gt;添加-&gt;监听器-&gt;聚合报告（Aggregate Report） 下面演示的是，给users采样器添加一个聚合报告。 这里我们设置线程组的参数为30/5/2。也就是一共发送两轮请求，每轮发送30个，每轮发送5s，也就是每秒发送5个请求。 启动采样器，查看结果： ​ lable: sampler的名称​ Samples: 一共发出去多少请求，上面的60个请求​ Average: 平均响应时间，4ms​ Median: 中位数，也就是 50％ 用户的响应时间，4ms ​ 90% Line : 90％ 用户的响应不会超过该时间 （90% of the samples took no more than this time. The remaining samples at least as long as this） ​ 95% Line : 95％ 用户的响应不会超过该时间 ​ 99% Line : 99％ 用户的响应不会超过该时间 ​ min : 最小响应时间 ​ max : 最大响应时间​​ Error%：错误的请求的数量/请求的总数​ Throughput： 吞吐量——默认情况下表示每秒完成的请求数（Request per Second) 可类比为qps​ KB/Sec: 每秒接收数据量 ​ 所以在实际的压测过程中，我们会关注Throughput吞吐量，随着我们通过增加并发数，观察Throughput的值，他的最大值是多少，那么相应的并发数就是最大并发数，再增大并发数，吞吐量反而会下降了。 测试案例我们进行测试users接口，线程组设置为3000/5/10。 通过逐渐增加并发数，查看吞吐量的瓶颈。 我们发现这个时候每秒可以处理请求是593差不多600。 那么再继续加大并发数，查看qps瓶颈。 线程组设置为6000/5/10。 可以看到qps上涨为1166个，那么说明此时并发数还不是瓶颈。继续加大 线程组设置为15000/5/10。 可以看到qps上涨为1852个，那么说明此时并发数还不是瓶颈。继续加大 线程组设置为20000/5/10。 这个时候，我们发现，qps下降了，变成了1460。那么也就意味着当总并发数到达20000个的时候，程序的qps开始下降了，所以说，程序的瓶颈已经出现了，那么我们可以知道线程组设置为1500/5/10。的时候 qps是最高的。 可以通过多次次测试，得到qps最大值。上面是通过增加并发数的方式进行测试，当然了你也可以设置多少s发送这个参数（ramp-up period ），进行压测。 Jmeter压测脚本JMX讲解我们知道，上面我们设置的测试计划，他最终是保存在一个jmx后缀的文件中的。所以说，后期我们一般是通过直接修改jmx文件的方式，去修改他的并发数等等参数。 打开你会发现他实际上是一个xml文件 自定义变量和CSV可变参数实操jmeter用户自定义变量为什么使用：很多变量在全局中都有使用，或者测试数据更改，可以在一处定义，四处使用。比如服务器地址。 例如一个线程组中我们添加了一百多个采样器，那么每个采样器的服务器ip都一样，这个时候我们就可以通过自定义变量的方式，统一使用自定义好的ip变量。这样的好处是，后期如果ip或者端口号需要变动，那么我们只需要修改自定义变量的值即可，不需要单独去修改一百多个采样器的ip和端口号。 ​ 1、线程组-&gt;add -&gt; Config Element(配置原件)-&gt; User Definde Variable（用户定义的变量） ​ 定义一个自定义变量 ​ ​ 2、引用方式${XXX}，在接口中变量中使用 CSV可变参数压测它的作用就是，模拟请求数据，然后去压测。例如你要压测登录接口，就可以使用这个功能，进行模拟多个用户数据进行传入压测。 实战操作jmeter读取CSV和Txt文本文件里面的参数进行压测 ​ 1、线程组-&gt;add -&gt; Config Element(配置原件)-&gt; CSV data set config (CSV数据文件设置) ​ 这列可以传入txt文件和csv文件，下面就演示一下txt文件设置可变参数。 1.首先后台定义一个接口，如入参是用户名和密码，模拟详情接口 /** * 用户自定义变量测试 */ @RequestMapping(value = &quot;info&quot;, method = RequestMethod.GET) public @ResponseBody Object info(String name, String pwd) { List&lt;String&gt; userList = new ArrayList&lt;&gt;(); userList.add(name); userList.add(pwd); userList.add(name.length()+&quot;&quot;); System.out.println(&quot;get request, info api&quot;); return userList; } 2.创建一个txt文件保存可变参数 内容是如下，使用| 进行分割 kingge1|123kingge2|1234kingge3|1235kingge4|1236 对应的csv文件是：需要知道，默认的分隔符是, 在线程组下面创建可变参数 需要注意的是，变量名这里需要配置两个（当然你也可以配置一个），这样才能按照分隔符进行分割后，他们的值对应的变量名才能匹配上。否者采样器就无法使用了。 采样器中引用 通过结果树查看，请求url的参数拼接。 GET http://127.0.0.1:8080/info?name=kingge1&amp;pwd=123GET http://127.0.0.1:8080/info?name=kingge2&amp;pwd=1234GET http://127.0.0.1:8080/info?name=kingge3&amp;pwd=1235GET http://127.0.0.1:8080/info?name=kingge4&amp;pwd=1236//参数取完后，如果还有请求需要发送，那么就会从头再拿一遍，不断循环。GET http://127.0.0.1:8080/info?name=kingge1&amp;pwd=123GET http://127.0.0.1:8080/info?name=kingge2&amp;pwd=1234。。。。。。 Mysql数据库压测实操这里通过建立JDBC采样器的方式，进行对数据库的直接测试 测试案例1 - 简单查询新建线程组和JDBC采样器 这里的意思是一共执行两轮，一轮3次，一轮3次在两秒内完成。 建立jdbc采样器 Thread Group -&gt; add -&gt; sampler -&gt; jdbc request 绿色的说明，方框的值，需要先进行下面的《建立JDBC连接配置》后才能给出。 当前界面相关参数的值的含义，查询类型 自上而下是： 查询语句 - 也就是普通查询语句更新语句存储过程预查询语句 - 也就是带问号（动态参数）的查询语句 ，例如 select * from user where name = ？预更新语句 - 同上就是带问号，动态参数的更新语句提交回滚自动提交 1、variable name of pool declared in JDBC connection configuration（和配置文件同名） 也就是下面的jdbc_connector 3、parameter values 参数值 4、parameter types 参数类型 5、variable names sql执行结果变量名 6、result variable names 所有结果当做一个对象存储 7、query timeouts 查询超时时间 8、 handle results 处理结果集 建立JDBC连接配置JDBC request-&gt;add -&gt; config element -&gt; JDBC connection configuration ​ 核心配置​ Max Number of connections : 最大连接数​ MAX wait :最大等待时间​ Auto Commit: 是否自动提交事务 ​ DataBase URL : 数据库连接地址 jdbc:mysql://127.0.0.1:3306/blog​ JDBC Driver Class : 数据库驱动，选择对应的mysql​ username:数据库用户名​ password:数据库密码 添加jdbc连接mysqljar包 我这里因为使用的是mysql8.0，所以jdbc_url必须是com.mysql.cj.jdbc.Driver，mysql-connector.jar是mysql-connector-java-5.1.30.jar 完整连接测试 添加了聚合报告和查看结果数 运行结果查看： 查看结果树 聚合报告 带参数的复杂查询预查询语句 如果多个条件呢？ 那就用逗号分隔开 预更新语句同上 演示查询结果集handle resultset接下来的案例主要演示下面这三个参数 5、variable names sql执行结果变量名 6、result variable names 所有结果当做一个对象存储 8、 handle resultset 处理结果集 也就是说，我们把结果输出到指定的类型 需要结合debug sample采样器进行查看，查询的结果集（在线程组下面新建debug采样器） 开始执行执行程序 在结果树就可以看到 阿里云Linux服务器压测接口实战Jmeter非GUI界面 参数讲解 - 一般压测不会使用gui界面进行操作，因为比较耗性能 官方也是不推荐使用，gui窗口进行压测。**那么gui界面主要是用用来生成和调试jmx压力测试文件的，生成后，再使用jmeter命令进行命令行方式的脚本压测** jmeter 命令行 ​ -h 帮助​ -n 非GUI模式​ -t 指定要运行的 JMeter 测试脚本文件​ -l 记录结果的文件 每次运行之前，(要确保之前没有运行过,即目录下的xxx.jtl不存在，不然报错)​ -r Jmter.properties文件中指定的所有远程服务器​ -e 在脚本运行结束后生成html报告​ -o 用于存放html报告的目录（目录要为空，不然报错） 官方配置文件地址 http://jmeter.apache.org/usermanual/get-started.html 使用例子jmeter -n -t linux_users_api.jmx -l result.jtl -e -o /usr/local/softwate/jmeter/temp/ResultReport 所以整个压测流程是： 首先在windows使用gui界面，创建测试计划（jmx文件） 接着上传jmx测试脚本到需要测试服务器（一般跟项目所在服务器不同）（例如项目在147，那么测试肯定不在147上进行，可以在其他服务器，例如在148进行测试脚本的执行） 获取测试报告进行分析 压测后的jtl结果文件通过上面的测试，我们会得到一个结果文件result.jtl ，那么我们接下来就把他从linux上下载到windows本机上进行查看。 可以通过打开jmeter，新建线程组–&gt;监听器-&gt;summary report-&gt;浏览文件. 然后选择我们下载下来的result.jtl进行查看 你会发现他的测试报告就跟上面我们创建的聚合报告 是一样的。 html压测报告分析 Dashboard的核心指标 1）Test and Report informations Source file：jtl文件名 Start Time ：压测开始时间 End Time ：压测结束时间 Filter for display：过滤器 Lable:sampler采样器名称 2）APDEX(Application performance Index) apdex:应用程序性能指标,范围在0~1之间，1表示达到所有用户均满意 T(Toleration threshold)：可接受阀值 F(Frustration threshold)：失败阀值 3）Requests Summary OK:成功率 KO:失败率 4）Statistics 统计数据 lable:sampler采样器名称 samples:请求总数，并发数*循环次数 KO:失败次数 Error%:失败率 Average:平均响应时间 Min:最小响应时间 Max:最大响应时间 90th pct: 90%的用户响应时间不会超过这个值（关注这个就可以了） 举个例子，假设有10个请求，每个请求可能服务器相应的时间都不一样：2ms,3ms,4,5,2,6,8,3,9,11 那么，我们去掉极端的%10，也就是响应时间是11s，那么剩下的百分之90，最大的响应时间就是9ms，那么也就意味着，90%的用户响应时间不会超过这个9ms这个值 95th pct: 95%的用户响应时间不会超过这个值 99th pct: 99%的用户响应时间不会超过这个值 (存在极端值) throughtput:Request per Second吞吐量 qps received:每秒从服务器接收的数据量 send：每秒发送的数据量 Charts的核心指标 1、charts讲解 1)Over Time（随着时间的变化） Response Times Over Time：响应时间变化趋势（比较关注） Response Time Percentiles Over Time (successful responses)：最大，最小，平均，用户响应时间分布 Active Threads Over Time：并发用户数趋势 Bytes Throughput Over Time：每秒接收和请求字节数变化，蓝色表示发送，黄色表示接受 Latencies Over Time：平均响应延时趋势 Connect Time Over Time ：连接耗时趋势 1)Throughput Hits Per Second (excluding embedded resources):每秒点击次数 Codes Per Second (excluding embedded resources)：每秒状态码数量 Transactions Per Second：即TPS，每秒事务数 Response Time Vs Request：响应时间和请求数对比 Latency Vs Request：延迟时间和请求数对比 1)Response Times（接口整体性能） Response Time Percentiles：响应时间百分比 Response Time Overview：响应时间概述 Time Vs Threads：活跃线程数和响应时间 Response Time Distribution：响应时间分布图 Jmeter压测接口的性能优化在使用Jmeter压测减少资源使用的一些建议，即压测结果更准确： 1、使用非GUI模式：jmeter -n -t test.jmx -l result.jtl 2、少使用Listener， 如果使用-l参数，它们都可以被删除或禁用（我们还记得之前创建过的查看结果数、聚合报告等等listener，在真实测试时，最好去掉）。3、在加载测试期间不要使用“查看结果树”或“查看结果”表监听器，只能在脚本阶段使用它们来调试脚本。 4、包含控制器在这里没有帮助，因为它将文件中的所有测试元素添加到测试计划中5、不要使用功能模式,使用CSV输出而不是XML6、只保存你需要的数据,尽可能少地使用断言 7、如果测试需要大量数据，可以提前准备好测试数据放到数据文件中，以CSV Read方式读取。8、用内网压测，减少其他带宽影响压测结果9、如果压测大流量，尽量用多几个节点以非GUI模式向服务器施压 官方推荐 ：http://jakarta.apache.org/jmeter/usermanual/best-practices.html#lean_mean 分布式压测基础知识 普通压测：单台机可以对目标机器产生的压力比较小，受限因素包括CPU，网络，IO等 分布式压测：利用多台机器向目标机器产生压力，模拟几万用户并发访问 Jmeter分布式压测原理 ​ 1、总控机器的节点master，其他产生压力的机器叫“肉鸡” server​ 2、master会把压测脚本发送到 server上面 ​ 3、执行的时候，server上只需要把jmeter-server打开就可以了，不用启动jmeter​ 4、结束后，server会把压测数据回传给master,然后master汇总输出报告​ 5、配置详情 分布式压测注意事项： ​ the firewalls on the systems are turned off or correct ports are opened.​ 系统上的防火墙被关闭或正确的端口被打开。 ​ all the clients are on the same subnet.​ 所有的客户端都在同一个子网上。 ​ the server is in the same subnet, if 192.x.x.x or 10.x.x.x IP addresses are used. If the server doesn’t use 192.xx or 10.xx IP address, there shouldn’t be any problems.​ 如果使用192.x.x.x或10.x.x.x IP地址，则服务器位于同一子网中。 如果服务器不使用192.xx或10.xx IP地址，则不应该有任何问题。 ​ Make sure JMeter can access the server.​ 确保JMeter可以访问服务器。 ​ Make sure you use the same version of JMeter and Java on all the systems. Mixing versions will not work correctly.​ 确保在所有系统上使用相同版本的JMeter和Java。 混合版本将无法正常工作。 ​ You have setup SSL for RMI or disabled it.​ 您已为RMI设置SSL或将其禁用。 ​ 官网地址 http://jmeter.apache.org/usermanual/jmeter_distributed_testing_step_by_step.html ​ 压测注意事项：一定要用内网IP，不用用公网IP,用ping去检查 注意事项 （1）启动 slave ./jmeter-server 或者 nohup ./jmeter-server &amp;检查启动是否成功 ps -ef|grep jmeter-server ps aux|grep jmeter-server 启动slave的过程中可能遇到问题，那么这些问题的解决方案如下： 开始分布式压测​ 1.修改master节点信息：​ jemeter.properties 值是slave机器的ip+端口号，如果有多个，用逗号分隔 remote_hosts=192.168.0.102:8899,192.168.0.101:8899server.rmi.ssl.disable=true (前面有说到) ​ 2.启动slave机器，注意要同个网段，ip地址用内网ip ./jmeter-server Using local port: 8899 Created remote object: UnicastServerRef2 [liveRef: [endpoint:[192.168.0.102:8899](local),objID:[3a585a4d:162724586ab:-7fff, 3963132813614033916]]] 3.启动master开始压测 ./jmeter -n -t /Users/jack/Desktop/remote.jmx -r -l /Users/jack/Desktop/jtl/result.jtl -e -o /Users/jack/Desktop/result Creating summariser &lt;summary&gt; Created the tree successfully using /Users/jack/Desktop/remote.jmx Configuring remote engine: 172.20.10.3:8899 Using local port: 8899 Configuring remote engine: 172.20.10.11:8899 Starting remote engines Starting the test @ Thu Mar 29 23:21:13 CST 2018 (1522336873931) Remote engines have been started Waiting for possible Shutdown/StopTestNow/Heapdump message on port 4445 summary = 4 in 00:00:22 = 0.2/s Avg: 5582 Min: 94 Max: 21006 Err: 1 (25.00%) Tidying up remote @ Thu Mar 29 23:21:36 CST 2018 (1522336896842) ... end of run 相关资料： https://www.cnblogs.com/Fine-Chan/p/6233823.html https://blog.csdn.net/liujingqiu/article/details/52635289 https://www.cnblogs.com/puresoul/p/4844539.html","categories":[{"name":"压测工具","slug":"压测工具","permalink":"http://kingge.top/categories/压测工具/"}],"tags":[{"name":"Jmeter","slug":"Jmeter","permalink":"http://kingge.top/tags/Jmeter/"},{"name":"Jmeter压测","slug":"Jmeter压测","permalink":"http://kingge.top/tags/Jmeter压测/"}]},{"title":"zookeeper源码解析","slug":"zookeeper源码解析","date":"2020-02-02T11:12:44.000Z","updated":"2020-05-09T09:16:41.556Z","comments":true,"path":"2020/02/02/zookeeper源码解析/","link":"","permalink":"http://kingge.top/2020/02/02/zookeeper源码解析/","excerpt":"","text":"zk集群角色leader：负责进行投票的发起和决议，最终更新状态。 follower：用于接收客户请求，并返回结果。参与竞争leader的投票。 observer：可以接收客户的请求，将写请求转发给leader节点，但是observer不参与投票，只是同步leader的状态（同步leader的数据）。observer是作为系统扩展节点。 learner：和leader进行状态同步的server统称为学习者。例如上面的跟随者和观察者都是learner。 为什么需要observer​ 如果我们想增加follower来进行应对多客户的请求，那么会有个问题，那么就是follower是会参与leader的投票的，增加了服务器的数量，意味着，增加了zab协议中投票过程的压力，从而使得zk集群的不可用时间增加。 ​ 因为leader节点必须等待集群中过半server响应投票，于是节点的增加使得部分计算机运行缓慢，从而拖慢整个投票过程的可能性也随着提高。写操作效率也会下降。zk集群的不断增大，但是写操作的吞吐量反而下降，这就违背了我们的意愿。 ​ 那么我们就需要增加一种跟follower不同的server，他是不参与投票的。那么这个就是observer。 也就是说observer除了不参与投票外，他跟follower是一样的。 zk集群配置observer zoo.cfg文件 peerType=observerserver.4=0.0.0.0:2888:3888:observer 然后启动就可以了。 $ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: J:\\JavaSoftwareRuntime\\apache-zookeeper-3.5.7-bin\\conf\\zoo.cfgClient port found: 2181. Client address: localhost.Mode: observer 怎么保证一致性​ zk集群中的所有server都可以接受客户的请求，如果是读请求，那么当前server直接处理并返回结果，但是如果是写请求，那么当前server节点需要转发请求给leader进行处理。这个协议就是我们所说的zab协议。 ​ 简答来说，zab协议规定，来自客户端的所有写请求，都要转发给zk集群中的唯一一台leader处理，有leader根据该请求发起一个proposal，收到proposal的server进行投票，之后返回结果给leader，leader收集结果。当投票结果过半时leader会向所有的server发送一个通知请求。 ​ 最后server收到请求后，会把该操作更新到内存中，完成写请求。然后返回结果给客户端。 ​ zk满足了CAP原则中的CP。比如现在集群中有Leader和Follower两种角色，那么当其中任意一台服务器挂掉了，都要重新进行选举，在选举过程中，集群是不可用的，这就是牺牲了可用性。 ​ 但是，如果集群中有Leader、Follower、Observer三种角色，那么如果挂掉的是Observer，那么对于集群来说 并没有影响，集群还是可以用的，只是Observer节点的数据不同了，从这个角度考虑，Zookeeper又牺牲了 一致性，满足了AP。 需要注意的是：zk并不能够保证强一致性（在某个时刻，任意客户端读取到的数据都是一样的），它能够保证在一定时间范围内数据是一致的。他保证了顺序一致性。（至于为为什么，请看下面的源码分析） zk的ACL节点权限在实际开发中，某些zk 的节点，我们不允许其他人任意的修改和删除，所以需要个特定的节点添加权限，保证节点的安全。 其特性如下： ZooKeeper的权限控制是基于每个znode节点的，需要对每个节点设置权限。 每个znode支持设置多种权限控制方案和多个权限 子节点不会继承父节点的权限，客户端无权访问某节点，但可能可以访问它的子节点。 zk支持准确的控制某个节点的权限，权限的设置也仅仅只是对当前节点生效。 zk的权限: 权限 ACL简写 描述 CREATE c 可以创建子节点 DELETE d 可以删除子节点（仅下一级节点） READ r 可以读取节点数据及显示子节点列表 WRITE w 可以设置节点数据 ADMIN a 可以设置节点访问控制列表权限 zk权限认证方式 ACL 权限控制，使用：schema​ : id:permission 来标识，主要涵盖 3 个方面： 权限模式（Schema）：鉴权的策略 授权对象（ID） 权限（Permission） 1、schema： ZooKeeper内置了一些权限控制方案，可以用以下方案为每个节点设置权限： 方案 描述 world 只有一个用户：anyone，代表所有人都能够操作（创建的节点，默认就是这个策略） ip 使用IP地址认证 auth 使用已添加认证的用户认证。代表已经认证通过的用户(cli中可以通过addauth digest user:pwd 来添加当前上下文中的授权用户) digest 使用“用户名:密码”方式认证 Super 超级用户 2、id 权限相关命令： 命令 使用方式 描述 getAcl getAcl 读取ACL权限 setAcl setAcl 设置ACL权限 addauth addauth 添加认证用户 查看权限 默认权限 [zk: localhost:2181(CONNECTED) 11] create /kinggeCreated /kingge[zk: localhost:2181(CONNECTED) 12] getAcl /kingge'world,'anyone: cdrwa[zk: localhost:2181(CONNECTED) 13] 可以看到默认的权限是： 对于所有用户都具有cdrwa。也就是任意操作 设置acl 1.设置写和管理权限 [zk: localhost:2181(CONNECTED) 13] setAcl /kingge world:anyone:wa[zk: localhost:2181(CONNECTED) 14] getAcl /kingge'world,'anyone: wa[zk: localhost:2181(CONNECTED) 15] get /kinggeorg.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /kingge你会发现，客户端已经失去了对kingge节点的读权限。 好的文档https://blog.csdn.net/weixin_40861707/article/details/80403213 https://blog.csdn.net/liuxiao723846/article/details/79391650 zk客户端创建的三种方式原生方式也就是直接使用ZooKeeper这个类构件一个client客户端。 ZooKeeper keeper = new ZooKeeper(&quot;localhost:2181&quot;, 5000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; System.out.println( &quot;输出的事件：&quot;+ event.toString() ); &#125;&#125;);//；连接客户端//创建临时节点keeper.create(&quot;/app2&quot;, &quot;123&quot;.getBytes(), null, CreateMode.EPHEMERAL, null, null);//监听节点 Stat stat = new Stat();;keeper.getData(&quot;/app&quot;, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; System.out.println( &quot;监听kinge节点输出的事件：&quot;+event ); &#125;&#125;, stat );//只能监听一次 需要依赖的是，zookeeper的jar包，例如zookeeper-3.4.10.jar。 先看看zookeeper本身自带的客户端的问题。 1) ZooKeeper的Watcher是一次性的，用过了需要再注册（zk的特性，监听器只能够使用一次，如果需要重复监听，那么需要再注册） 2) session的超时后没有自动重连，生产环境中如果网络出现不稳定情况，那么这种情况出现的更加明显； 3) 没有领导选举机制，集群情况下可能需要实现stand by，一个服务挂了，另一个需要接替的效果； 4) 客户端只提供了存储byte数组的接口，而项目中一般都会使用对象。 5) 客户端接口需要处理的异常太多，并且通常，我们也不知道如何处理这些异常。 使用zkClient需要依赖 zkclient的jar包， &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt;&lt;/dependency&gt; ZkClient client = new ZkClient(&quot;localhost:2181&quot;, 5000, 5000);//客户端 client.createEphemeral(&quot;/app2&quot;, null);//创建临时节点 //监听节点 client.subscribeDataChanges(&quot;/app&quot;, new IZkDataListener() { @Override public void handleDataDeleted(String t) throws Exception { System.out.println( &quot; handleDataDeleted :&quot; + t); } @Override public void handleDataChange(String arg0, Object arg1) throws Exception { System.out.println( &quot; handleDataChange :&quot; + arg0 + &quot; - &quot; + arg1); } });//可以多次监听，多次调用handleDataDeleted或者handleDataChange方法 解决了zookeeper原生客户端不能够多次监听的缺点。 curator客户端 &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-client&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt;&lt;/dependency&gt; ，如果是导入jar的形式，则是依赖着三个： zookeeper中zoo.cfg详解zookeeper通常采用配置文件zoo.cfg加载配置，其可以配置的参数为： clientPort:用于配置当前服务器对客户端暴露的端口，一般配置为2181,无默认值 dataDir:用于配置zookeeper服务器存储快照文件（zookeeper 节点数据）的目录，无默认值 dataLogDir:用于配置服务器存储事务日志文件的目录，有默认值dataDir,但是建议将两个目录分别配置，防止磁盘的并发读写，影响服务器性能。可将其配置在一个单独的磁盘上。 tickTime:心跳时间，用于配置服务器最小时间的单位，默认值3000ms,心跳检测时间通常是该单位的倍数。如客户端与服务端之间的会话超时时间在2tickTime~20tickTime之间。 initLimit:用于配置leader服务器等待Follewer服务器启动，并完成数据同步的时间，默认为10，表示10*tickTime syncLimit:用于配置leader服务器和Follewer服务器之间进行心跳检测的最大延时时间，默认为5，表示5*tickTime minSessionTimeout &amp; maxSessionout:用于服务端对客户端会话超时时间的限制，也就是客户端自定义的超时时间必须在minSessionTimeout~maxSessionout内，其默认为分别为2 和 20，时间表示为2tickTime~20tickTime maxClientCnxns:从socket层面限制单个客户端和单台服务器之间的最大并发连接数，即以IP地址粒度来进行连接数的限制，如果为0，表示不作限制，默认为60 clientPortAddress:针对多网卡的机器，该参数允许为每个IP地址指定不同的监听端口 server.id=host:port:port:用于配置组成zookeeper集群的机器列表，其中id为serverId,与myid文件中的值对应。第一个端口用于指定Leader服务器和Follewer服务器进行运行时通信和数据同步所使用的端口，第二个端口用于进行Leader选举过程中的投票通信 autopurge.snapRetainCount:用于配置zookeeper在自动清理的时候需要保留的快照数据文件数量和对应的事务日志文件，默认为3，切自定义值小于3也会取值3 仅支持系统属性方式配置的参数： snapCount:用于配置相邻两次数据快照之间的事务操作次数，即zookeeper会在snapCount次事务之后进行一次数据快照，默认为100000 preAllocSize:用于配置zookeeper事务日志文件预分配的磁盘空间大小,默认为65536，单位KB,即64MB,与snapCount同时修改，如将snapCount设置为500，每次事务操作的数据量最多为1KB,则preAllocSize设置为500即可 forceSync:用于配置zookeeper是否在事务提交的时候，将日志写入操作强制刷入磁盘，默认为yes,表示强制刷盘 skipAcl:用于配置zookeeper是否跳过ACL权限检查，默认为mo，即会对每一个客户端请求进行权限检查 zoo.cfg介绍 https://www.cnblogs.com/likui360/p/5985588.html ########单机模式zk客户端源码解析使用zkCli.sh起送客户端我们来查看一下zkCli.sh文件的内容 发现他最终是通过运行org.apache.zookeeper.ZooKeeperMain 这个类来启动客户端 ZooKeeperMain源码 - 初始化zk客户端 protected void connectToZK(String newHost) throws InterruptedException, IOException &#123; if (zk != null &amp;&amp; zk.getState().isAlive()) &#123; zk.close(); &#125; host = newHost; boolean readOnly = cl.getOption(\"readonly\") != null; zk = new ZooKeeper(host, Integer.parseInt(cl.getOption(\"timeout\")), new MyWatcher(), readOnly);//3.使用zk原生客户端连接方式，进行连接 &#125; public static void main(String args[])//1.首先执行main方法 throws KeeperException, IOException, InterruptedException &#123; ZooKeeperMain main = new ZooKeeperMain(args); main.run();//1.1 while循环，解析客户端命令行，然后判断操作类型，用connectToZK构建的zk客户端，请求服务端执行相应操作指令。例如客户端输入create /app 123 ，那么就会调用zk.create（\"/app\",123）请求服务端。 &#125; public ZooKeeperMain(String args[]) throws IOException, InterruptedException &#123; cl.parseOptions(args); System.out.println(\"Connecting to \" + cl.getOption(\"server\")); connectToZK(cl.getOption(\"server\"));//2.构建zk连接 //zk = new ZooKeeper(cl.getOption(\"server\"),// Integer.parseInt(cl.getOption(\"timeout\")), new MyWatcher()); &#125; ​ 由上面可以得知，zkmain类，最终的目的是：解析命令行的参数，保存到MyCommandOptions配置类，然后通过zookeeper类，构建一个到服务端的连接。 zookeeper构建连接源码上面构建zk客户端的连接最终的调用的代码是： public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, boolean canBeReadOnly) throws IOException &#123; LOG.info(\"Initiating client connection, connectString=\" + connectString + \" sessionTimeout=\" + sessionTimeout + \" watcher=\" + watcher); watchManager.defaultWatcher = watcher;//默认的总的监听器。 //connectString可能是：localhost:2181,localhost:2182，这样的集群地址 ConnectStringParser connectStringParser = new ConnectStringParser( connectString);//1.解析zk的server地址，因为有可能是集群，所以需要根据，分割 //最后保存在ArrayList&lt;InetSocketAddress&gt; serverAddresses中 HostProvider hostProvider = new StaticHostProvider( connectStringParser.getServerAddresses());//2.将上面解析到的server地址列表，赋值给HostProvider。同时StaticHostProvider类包含了如何选出集群中server地址，以便客户端连接。 // cnxn = new ClientCnxn(connectStringParser.getChrootPath(), hostProvider, sessionTimeout, this, watchManager, getClientCnxnSocket(), canBeReadOnly);//3.chrootPat默认是null，canBeReadOnly是否是只读模式 cnxn.start();//4.启动ClientCnxn创建的两个线程 &#125;//解释一下只读模式，假设集群有三台服务器，但是挂了两台，按道理整个集群是不可用的，但是我们可以开启只读模式，然后客户端还是能够请求集群获取数据，但是更新不了数据。 ​ 代码1处，就是解析server端的地址，最后放到ConnectStringParser类的ArrayList serverAddresses 成员属性中。看里面的源码可以得知，connectString.split(“,”)，集群的地址，必须使用,分隔开。 代码2处，根据第一步解析出来的server端地址，然后打乱list里面的server地址，然后再随机取一个地址，进行连接。 代码3处，首先根据getClientCnxnSocket()构建一个客户端连接服务端的socket，这里是通过NIO的形式（默认实现是ClientCnxnSocketNIO）。 接着构建zk客户端连接 new ClientCnxn(..)。ClientCnxn可以理解创建了一个上下文，最终创建了两个线程 这两个线程很重要！！！！ public ClientCnxn(String chrootPath, HostProvider hostProvider, int sessionTimeout, ZooKeeper zooKeeper, ClientWatchManager watcher, ClientCnxnSocket clientCnxnSocket, long sessionId, byte[] sessionPasswd, boolean canBeReadOnly) &#123; this.zooKeeper = zooKeeper; this.watcher = watcher; this.sessionId = sessionId; this.sessionPasswd = sessionPasswd; this.sessionTimeout = sessionTimeout; this.hostProvider = hostProvider; this.chrootPath = chrootPath;//连接超时时间 connectTimeout = sessionTimeout / hostProvider.size(); //读超时 时间 readTimeout = sessionTimeout * 2 / 3; readOnly = canBeReadOnly; sendThread = new SendThread(clientCnxnSocket);//我们可以看到sendThread是以ClientCnxnSocketNIO为入参说明他是负责发送请求给服务端的。具体实现：他是负责从outgoingqueue队列中获取客户端要发送的请求，然后发送给服务端 - 处理客户端请求的线程 eventThread = new EventThread();//处理watcher事件的线程，在后面的代码分析中你会发现EventThread处理event事件数据来源，是在从SendThread哪里进行赋值到waitEventQueue队列中的。 &#125; 代码4，启动上面创建的两个线程，sendThread开始处理客户端发出的请求，eventThread开始处理根据服务端发出的响应事件的请求，开始处理事件。 ​ 这两个线程都是通过生产者和消费者模式，进行工作的进行，将请求都放到队列中，然后在他们的run方法中逐个拿出请求进行处理。 SendThread初始化NIO进行与server端的通信 - 负责发送请求给服务端​ 我们知道 cnxn.start()最终会启动两个线程，那么我们先看一下SendThread线程，看名字应该知道他是一个发送消息的线程，也就是发送我们客户端的操作命令，例如”ls /“。 ​ 我们直接来看他的run方法 ​ 首先判断客户端是否已经连接服务端，那么会进入这段代码。 if (!clientCnxnSocket.isConnected()) &#123;//socket是否已经连接 if(!isFirstConnect)&#123;//如果不是首次连接，那么需要随机睡一会 try &#123; Thread.sleep(r.nextInt(1000)); &#125; catch (InterruptedException e) &#123; LOG.warn(\"Unexpected exception\", e); &#125; &#125; // don't re-establish connection if we are closing if (closing || !state.isAlive()) &#123; break; &#125; if (rwServerAddress != null) &#123; serverAddress = rwServerAddress; rwServerAddress = null; &#125; else &#123; serverAddress = hostProvider.next(1000);//取下一个server连接的地址 //如果是集群那么在next方法会进行逻辑判断，然后选取出下一个连接的server //地址,一般是调用Collections.shuffle()打乱地址列表然后随机获取 &#125; startConnect(serverAddress);//代码1处，最终会执行到，这里创建一个链接，接着调用 clientCnxnSocket.connect(addr);然后通过上面创建NIOorg.apache.zookeeper.ClientCnxnSocketNIO.connect(InetSocketAddress)，进行创建NIO连接 clientCnxnSocket.updateLastSendAndHeard();//更新Socket最后一次发送以及收到消息的时间&#125; 我们查看代码1处最终调用：一下方法都在ClientCnxnSocketNIO类中 @Overridevoid connect(InetSocketAddress addr) throws IOException &#123; SocketChannel sock = createSock(); try &#123; registerAndConnect(sock, addr); &#125; catch (IOException e) &#123; LOG.error(\"Unable to open socket to \" + addr); sock.close(); throw e; &#125; initialized = false; /* * Reset incomingBuffer */ lenBuffer.clear(); incomingBuffer = lenBuffer;&#125; SocketChannel createSock() throws IOException &#123; SocketChannel sock; sock = SocketChannel.open(); sock.configureBlocking(false); sock.socket().setSoLinger(false, -1); sock.socket().setTcpNoDelay(true); return sock; &#125; void registerAndConnect(SocketChannel sock, InetSocketAddress addr) throws IOException &#123; sockKey = sock.register(selector, SelectionKey.OP_CONNECT); boolean immediateConnect = sock.connect(addr); if (immediateConnect) &#123; sendThread.primeConnection(); &#125; &#125; eventThread 处理watcher事件关于这一章节的详细内容，在下面的《为什么监听的事件只处理一次之watcher源码解析中会讲到》 ZooKeeperMain.run() - 解析命令行，处理请求经过上面初始化了zk客户端，那么接下来就是进行处理命令行的逻辑了。 ​ 通过上面的分析，我们可以构建了一个zookeeper客户端，能够与服务端取得了连接，但是我们并没有看到在那里处理或者说，解析了命令行 。 ​ 例如我们输入 create /kingge创建一个节点，那么在那里解析这一行代码呢？ 那就是根据上面创建好的ZooKeeperMain，然后调用run方法。 void run() throws KeeperException, IOException, InterruptedException &#123; if (cl.getCommand() == null) &#123; System.out.println(\"Welcome to ZooKeeper!\"); boolean jlinemissing = false; // only use jline if it's in the classpath try &#123; Class&lt;?&gt; consoleC = Class.forName(\"jline.ConsoleReader\");//这个类就是控制台 Class&lt;?&gt; completorC = Class.forName(\"org.apache.zookeeper.JLineZNodeCompletor\");//自动补齐类 System.out.println(\"JLine support is enabled\"); Object console = consoleC.getConstructor().newInstance(); Object completor = completorC.getConstructor(ZooKeeper.class).newInstance(zk); Method addCompletor = consoleC.getMethod(\"addCompletor\", Class.forName(\"jline.Completor\")); addCompletor.invoke(console, completor); String line; Method readLine = consoleC.getMethod(\"readLine\", String.class); while ((line = (String)readLine.invoke(console, getPrompt())) != null) &#123; executeLine(line);//while循环处理请求。在这里解析命令行的操作 &#125; 。。。。 我们是否还记得，当我们通过zkClie.sh连接到服务端的时候，控制台会输出这样的一行前缀： [zk: localhost:2181(CONNECTED) 1] 那么这一段输出，其实就是通过这里的getPrompt()方法进行打印的 protected String getPrompt() &#123; return \"[zk: \" + host + \"(\"+zk.getState()+\")\" + \" \" + commandCount + \"] \";&#125; 我们接着看executeLine()方法，发下他的内部实现，其实也很简单。最终调用processZKCmd()方法： ​ 发现他就是通过，各种if对于命令的判断，然后执行相应的操作，例如输入的create命令，那么接着判断创建的是临时节点，还是持久化节点，等等，然后接着调用zk.create（）原生api进行节点的创建。 if (cmd.equals(\"create\") &amp;&amp; args.length &gt;= 3) &#123; int first = 0; CreateMode flags = CreateMode.PERSISTENT; if ((args[1].equals(\"-e\") &amp;&amp; args[2].equals(\"-s\")) || (args[1]).equals(\"-s\") &amp;&amp; (args[2].equals(\"-e\"))) &#123; first+=2; flags = CreateMode.EPHEMERAL_SEQUENTIAL; &#125; else if (args[1].equals(\"-e\")) &#123; first++; flags = CreateMode.EPHEMERAL; &#125; else if (args[1].equals(\"-s\")) &#123; first++; flags = CreateMode.PERSISTENT_SEQUENTIAL; &#125; if (args.length == first + 4) &#123; acl = parseACLs(args[first+3]); &#125; path = args[first + 1]; String newPath = zk.create(path, args[first+2].getBytes(), acl, flags);//这里的zk，就是前一步 ZooKeeperMain main = new ZooKeeperMain(args);创建出来的客户端。 System.err.println(\"Created \" + newPath);//控制台打印，创建节点路径 &#125; ​ 下面我们就通过zk.create()这一种命令方式（其他的get，set命令类似的create的执行步骤，这里通过create命令进行分析），查看一下他的源码，分析一下，客户端究竟是怎么发送请求给服务端的。 构建请求到请求队列中 首先调用zookeeper.create（） public String create(final String path, byte data[], List&lt;ACL&gt; acl, CreateMode createMode) throws KeeperException, InterruptedException&#123; final String clientPath = path; PathUtils.validatePath(clientPath, createMode.isSequential()); final String serverPath = prependChroot(clientPath); RequestHeader h = new RequestHeader();//1.构建请求头 h.setType(ZooDefs.OpCode.create);//2.请求类型，每种操作命令都对应着一个状态码，例如create命令是1，delete是2，getACL是6等等 CreateRequest request = new CreateRequest();//构建请求体 CreateResponse response = new CreateResponse();//构建响应，保存请求的结果 request.setData(data); request.setFlags(createMode.toFlag()); request.setPath(serverPath); if (acl != null &amp;&amp; acl.size() == 0) &#123; throw new KeeperException.InvalidACLException(); &#125; request.setAcl(acl); ReplyHeader r = cnxn.submitRequest(h, request, response, null);//3.提交请求 if (r.getErr() != 0) &#123; throw KeeperException.create(KeeperException.Code.get(r.getErr()), clientPath); &#125; if (cnxn.chrootPath == null) &#123; return response.getPath(); &#125; else &#123; return response.getPath().substring(cnxn.chrootPath.length()); &#125;&#125; 这里着重的看一下cnxn.submitRequest()方法 public ReplyHeader submitRequest(RequestHeader h, Record request, Record response, WatchRegistration watchRegistration) throws InterruptedException &#123; ReplyHeader r = new ReplyHeader(); Packet packet = queuePacket(h, r, request, response, null, null, null, null, watchRegistration);//会把请求信息包装成一个Packet，然后把Packet放到一个队列中，LinkedList&lt;Packet&gt; outgoingQueue，接着调用 sendThread.getClientCnxnSocket().wakeupCnxn(); synchronized (packet) &#123;//阻塞，等待响应结果 while (!packet.finished) &#123;//如果packet没有处理完成则阻塞等待，什么时候完成呢？详情参见下面的doIO的获取响应结果的代码 - finishPacket(packet)。 packet.wait(); &#125; &#125; return r;&#125; sendThread.getClientCnxnSocket().wakeupCnxn(); ​ 换句话说，zk会把create命令封装的请求，放在一个outgoingQueue的链表中，然后退出queuePacket()方法，接着阻塞synchronized (packet) 等待响应结果。 ​ 看到这里你有没有发现一个问题，那就是，到这里为止，仅仅只是把请求放到outgoingQueue队列中，那么什么时候处理队列里面的数据呢？也就是什么时候处理封装好的请求呢？ ​ 那就是SendThread的run方法。 ​ 还记得我们在上面说过，在创建zk客户端的时候，创建了两个线程，并启动。其中一个就是SendThread，那么他调用run方法，内部实现是一个while循环，从outgoingqueue中拿出请求进行发送给服务端 SendThread的run方法循环处理客户端请求和服务端响应run方法内部就是通过这样一个方法，进行处理outgoingQueue队列中的请求，该方法，最终调用的是：org.apache.zookeeper.ClientCnxnSocketNIO.doTransport。 clientCnxnSocket.doTransport(to, pendingQueue, outgoingQueue, ClientCnxn.this); 然后看里面的代码，因为我们执行的是create命令，那么需要查看NIO通道是否可写。 if ((k.readyOps() &amp; (SelectionKey.OP_READ | SelectionKey.OP_WRITE)) != 0) &#123; doIO(pendingQueue, outgoingQueue, cnxn);//判断通道读或者写是否就绪，如果读就绪说明服务端已经处理请求完毕返回响应。如果写就绪说明，可以将请求发送给服务端。 &#125; 也就是说，doIO方法是处理客户端请求和服务端响应的核心方法。 发送客户端请求给服务端 接着查看doIO方法 - 关于发送请求的部分代码 写请求部分关键代码： if (sockKey.isWritable()) &#123; synchronized(outgoingQueue) &#123; Packet p = findSendablePacket(outgoingQueue, cnxn.sendThread.clientTunneledAuthenticationInProgress());//拿出队列中第一个packet请求。 if (p != null) &#123; updateLastSend(); // If we already started writing p, p.bb will already exist if (p.bb == null) &#123; if ((p.requestHeader != null) &amp;&amp; (p.requestHeader.getType() != OpCode.ping) &amp;&amp; (p.requestHeader.getType() != OpCode.auth)) &#123; p.requestHeader.setXid(cnxn.getXid()); &#125; p.createBB();//序列化请求，zk自己的序列化工具jute。没有用java的序列化工具 &#125; sock.write(p.bb);//发送请求给服务端 if (!p.bb.hasRemaining()) &#123;// sentCount++; outgoingQueue.removeFirstOccurrence(p);//从队列中移除已经处理的请求 if (p.requestHeader != null &amp;&amp; p.requestHeader.getType() != OpCode.ping &amp;&amp; p.requestHeader.getType() != OpCode.auth) &#123; synchronized (pendingQueue) &#123;//将已经处理的请求放到pendingQueue队列中 pendingQueue.add(p); &#125; &#125; &#125; &#125; 为什么要将已经处理的请求放到判定队列中呢？那是因为，上面代码仅仅只是发送请求成功， 但是服务端还没有响应数据。 但是我们已经把outgoingQueue中已经发出去的请求给删除了，那么我怎么知道请求有没有发送成功，服务端有没有响应呢？ ​ 所以需要把将已经处理的请求放到pendingQueue判定队列中，等待判定。 处理服务端返回的响应数据​ 实际上得到服务端响应结果的代码也是在doIO中。服务端的写请求对应的是客户端的读请求。所以我们查看一下这代码段 if (sockKey.isReadable()) &#123;//服务端可读，说明服务端已经返回数据 int rc = sock.read(incomingBuffer); if (rc &lt; 0) &#123; throw new EndOfStreamException( \"Unable to read additional data from server sessionid 0x\" + Long.toHexString(sessionId) + \", likely server has closed socket\"); &#125; if (!incomingBuffer.hasRemaining()) &#123; incomingBuffer.flip(); if (incomingBuffer == lenBuffer) &#123; recvCount++; readLength(); &#125; else if (!initialized) &#123; readConnectResult(); enableRead(); if (findSendablePacket(outgoingQueue, cnxn.sendThread.clientTunneledAuthenticationInProgress()) != null) &#123; // Since SASL authentication has completed (if client is configured to do so), // outgoing packets waiting in the outgoingQueue can now be sent. enableWrite(); &#125; lenBuffer.clear(); incomingBuffer = lenBuffer; updateLastHeard(); initialized = true; &#125; else &#123; sendThread.readResponse(incomingBuffer);//获取结果，然后从pendingqueue中移除已经获取响应结果的packet，然后最终 finishPacket(packet);，标记请求的packet已经完成，然后唤醒监听packet节点的线程。 p.notifyAll(); lenBuffer.clear(); incomingBuffer = lenBuffer; updateLastHeard(); &#125; &#125;&#125; sendThread.readResponse() - 读取服务端返回的信息需要注意的是，这个方法可能还额外处理服务端发出的其他请求。例如触发事件的请求。 设么意思呢？ 举个例子： 客户端A，执行命令getData(&quot;/app&quot;,true)，这里的true代表是本次getData请求，在获取app节点数据的同时，告诉服务端我要顺便绑定一个在app节点监听器（使用zk客户端默认监听器），如果app节点有什么变动你要通知我。 ​ 那么也就意味着，服务端在处理请求getData的时候，返回app节点数据给客户端，然后再readResponse()方法执行。本次请求结束。 ​ 那么当客户端紧接着在请求setData(“app”,123)，服务端在响应客户端说，我处理成功了的同时，还会再发出一个响应，就是通知客户端说你上次在我这里绑定了关于app节点的监听器，你要触发一下。 ​ 那么本次客户端的setData请求，总共会得到两个响应，那么这两个响应都是在readResponse()方法处理。 ​ 那么客户端怎么知道本次响应时普通的操作响应还是时间触发响应呢？通过xid的值进行判断，当xid==-1时说明本次响应时触发事件响应。 void readResponse(ByteBuffer incomingBuffer) throws IOException &#123; 。。。。。。。//省略部分代码 Packet packet; synchronized (pendingQueue) &#123;//获取响应结果后，然后从pendingqueue中移除已经获取响应结果的请求packet if (pendingQueue.size() == 0) &#123; throw new IOException(\"Nothing in the queue, but got \" + replyHdr.getXid()); &#125; packet = pendingQueue.remove(); &#125; if (replyHdr.getXid() == -4) &#123; // -4 is the xid for AuthPacket if(replyHdr.getErr() == KeeperException.Code.AUTHFAILED.intValue()) &#123; state = States.AUTH_FAILED; eventThread.queueEvent( new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.AuthFailed, null) ); &#125; if (LOG.isDebugEnabled()) &#123; LOG.debug(\"Got auth sessionid:0x\" + Long.toHexString(sessionId)); &#125; return; &#125; if (replyHdr.getXid() == -1) &#123;//处理服务端发出的触发事件的响应 // -1 means notification if (LOG.isDebugEnabled()) &#123; LOG.debug(\"Got notification sessionid:0x\" + Long.toHexString(sessionId)); &#125; WatcherEvent event = new WatcherEvent(); event.deserialize(bbia, \"response\"); // convert from a server path to a client path if (chrootPath != null) &#123; String serverPath = event.getPath(); if(serverPath.compareTo(chrootPath)==0) event.setPath(\"/\"); else if (serverPath.length() &gt; chrootPath.length()) event.setPath(serverPath.substring(chrootPath.length())); else &#123; LOG.warn(\"Got server path \" + event.getPath() + \" which is too short for chroot path \" + chrootPath); &#125; &#125; WatchedEvent we = new WatchedEvent(event); if (LOG.isDebugEnabled()) &#123; LOG.debug(\"Got \" + we + \" for sessionid 0x\" + Long.toHexString(sessionId)); &#125; eventThread.queueEvent( we ); return; &#125; /* * Since requests are processed in order, we better get a response * to the first request! */ try &#123; if (packet.requestHeader.getXid() != replyHdr.getXid()) &#123; packet.replyHeader.setErr( KeeperException.Code.CONNECTIONLOSS.intValue()); throw new IOException(\"Xid out of order. Got Xid \" + replyHdr.getXid() + \" with err \" + + replyHdr.getErr() + \" expected Xid \" + packet.requestHeader.getXid() + \" for a packet with details: \" + packet ); &#125; packet.replyHeader.setXid(replyHdr.getXid()); packet.replyHeader.setErr(replyHdr.getErr()); packet.replyHeader.setZxid(replyHdr.getZxid()); if (replyHdr.getZxid() &gt; 0) &#123; lastZxid = replyHdr.getZxid(); &#125; if (packet.response != null &amp;&amp; replyHdr.getErr() == 0) &#123; packet.response.deserialize(bbia, \"response\");//拿到响应，发到请求的packet中的response属性中，赋值。 &#125; if (LOG.isDebugEnabled()) &#123; LOG.debug(\"Reading reply sessionid:0x\" + Long.toHexString(sessionId) + \", packet:: \" + packet); &#125; &#125; finally &#123; finishPacket(packet);//处理正常的请求响应。 &#125;&#125; 最后处理finishPacket()请求private void finishPacket(Packet p) &#123; //首先还记得，watchRegistration是在什么时候进行初始化么？例如在客户端调用api发出例如调用exists操作命令时，调用zk.exists()时，在exists()内部初始化。 //create等命令是没有注册监听器的功能的，也就是说：实际上能够注册监听器的，也只有，exists、getDate，getChildren 这三个api才能绑定监听器。 if (p.watchRegistration != null) &#123; p.watchRegistration.register(p.replyHeader.getErr());//重要！！！！这个方法，会把客户端注册某个节点路径的监听事件，放到hashmap中。 -- 回答了下面提出的问题。保存节点监听事件的map什么时候赋值。就这这个方法赋值。 &#125; if (p.cb == null) &#123; synchronized (p) &#123; p.finished = true; p.notifyAll();//唤醒之前客户端在cnxn.submitRequest()发送请求后，阻塞等待结果的线程 //标识本次客户端请求，最终获得响应 &#125; &#125; else &#123; p.finished = true; eventThread.queuePacket(p);//然后开始处理监听事件 -- &#125;&#125; 开始注册对某个节点路径的监听到map中 p.watchRegistration.register key是path-节点路径 value是注册的监听。 public void register(int rc) &#123; if (shouldAddWatch(rc)) &#123; Map&lt;String, Set&lt;Watcher&gt;&gt; watches = getWatches(rc); synchronized(watches) &#123; Set&lt;Watcher&gt; watchers = watches.get(clientPath); if (watchers == null) &#123; watchers = new HashSet&lt;Watcher&gt;(); watches.put(clientPath, watchers); &#125; watchers.add(watcher); &#125; &#125;&#125; eventThread处理监听事件waitingEvents保存需要处理的事件。 private final LinkedBlockingQueue&lt;Object&gt; waitingEvents = new LinkedBlockingQueue&lt;Object&gt;(); //保存需要处理的事件到waitingEvents队列中。 public void queuePacket(Packet packet) &#123; if (wasKilled) &#123; synchronized (waitingEvents) &#123; if (isRunning) waitingEvents.add(packet); else processEvent(packet); &#125; &#125; else &#123; waitingEvents.add(packet); &#125;&#125; 因为他跟SendThread一样是一个线程，同时也是遵循生产者和消费者模型，那么就会去执行run方法，所以最终处理事件的逻辑还是在run方法中。 eventThread.run方法 处理事件的核心方法 public void run() &#123; try &#123; isRunning = true; while (true) &#123; Object event = waitingEvents.take();//从等待处理事件队列中获取事件并处理。拿一个就移除一个 - 这个也就是为什么事件只能够监听一次的原因 if (event == eventOfDeath) &#123; wasKilled = true; &#125; else &#123; processEvent(event);//真正调用监听事件的逻辑，然后完成监听事件的触发。 &#125; if (wasKilled) synchronized (waitingEvents) &#123; if (waitingEvents.isEmpty()) &#123; isRunning = false; break; &#125; &#125; &#125; &#125; catch (InterruptedException e) &#123; LOG.error(\"Event thread exiting due to interruption\", e); &#125; LOG.info(\"EventThread shut down for session: 0x&#123;&#125;\", Long.toHexString(getSessionId()));&#125; 那么waitingEvents事件队列在哪些地方还能赋值呢？ 在上面的sendThread.readResponse（）方法中。 对于服务端的响应类型if (replyHdr.getXid() == -1) 也会存放数据到waitingEvents事件队列中。 举个例子： ​ 假设客户端调用的api是：zk.getData(“/app”,new wahcher(){})// 在app节点上注册了监听，那么当下次客户发出请求zk.setDate(“/app”,”123”)，是不是会触发事件？是的。 ​ 那么当服务端收到setData请求类型后，调用final处理器，然后调用dataTree.setData()最后持久化数据到内存，那么此时就需要给客户端一个通知，说我修改了数据，你要触发一下你在app节点注册的事件。然后此时设置replyHdr.getXid() = -1。 ​ 客户端收到响应后，在sendThread.readResponse（）方法中处理响应数据，发现replyHdr.getXid() == -1。说明app节点数据被修改，那么需要触发事件，那么就会把赋值数据到waitingEvents事件队列中。然后等到eventThread的run方法处理。 总结​ 以上就是整个触发事件的逻辑。我们不难发现。如果客户端再某个节点上注册了监听事件，那么当他请求服务端时不会吧该监听事件一并发过去（例如客户端发起：zk.getData(“/app”,new wahcher(){})请求），只是发送了一个标志位watch==true，代表我再这个节点注册了监听。（因为最终处理监听事件的逻辑会在客户端） ​ 服务端收到请求后，然后处理调用处理链处理请求，最后在final处理器中，判断发现，客户端在这个节点上注册了监听，那么服务端就需要记录一下，把记录的值放到HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable中。然后响应客户端的请求。整个逻辑结束。1.服务端收到需要触发事件通知然后保存触发事件的节点路径和通知客户端触发事件的cnx ​ 因为我们知道：zk.getData(“/app”,new wahcher(){})这样的请求，只是去拿zpp节点的数据，然后顺便绑定一个事件。并不会触发事件，只有当下次app节点数据修改了，才会触发事件。 ​ 那么当下次客户端调用zk.setDate(“/app”,”123”)时，服务端处理流程一样，也是通过处理链最后经过final处理器，但是唯一不同的是，这次操作是个更新操作，事务操作。那么最终会执行dataTree.setData()持久化数据到zk服务器内存。 接着在dataTree.setData()最后，服务端会通知客户端触发事件，因为app节点的值修改了，你上次也绑定了一个事件。2.需要客户端触发事件 ​ 那么服务端发出请求会通知客户端触发事件，通过ServercnxNIO发送通知，这个通知的xid设置为-1，表示这个请求是个触发事件的请求，然后从watchTable中移除相应事件（这个也就是为什么事件只能触发一次的原因，触发一次，就移除一次） – 在这里我们不难发现服务端通知客户端触发事件并不能保证客户端一定能触发成功，他只是通知而已，服务端并没有确认是否触发成功机制 ​ 接着客户端再sendThread.readRespose（）中获得服务端的请求，然后判断xid，如果等于1，那么说明服务端需要客户端触发事件，那么就把往waitingEvents中存放数据，然后等待eventThread线程的run方法执行，然后触发事件。 3.客户端触发事件，触发事件流程结束 ！！为什么监听的事件只处理一次 - watcher机制源码！！在开始查看源码前，我们先思考几个问题，这样带着问题去读源码会更有效果。 问题一：为什么监听的事件只能调用一次。 ​ 我们知道，原生的zk客户端监听节点事件的时候，只能够监听一次。那么他的本质是什么呢？为什么只有一次监听。 这里先说结论： ​ 因为，服务端和客户端把对于某个节点的事件都放到一个列表中，每触发一个，就remove一个而不是take，所以只能够监听一次。（这里解释了为什么监听只会触发一次的原因） 详情代码验证，请看上面的和下面 问题二：客户端会把监听的watcher发送给服务端么？ ​ 但是这个时候存在第二个疑问？那就是如果客户端监听了某个节点，那么在请求服务端操作的时候，会把监听器也一并发送给服务端么？例如如下代码 Stat stat = new Stat();;keeper.getData(\"/app\", new Watcher() &#123;//给app节点，创建新的监听器 @Override public void process(WatchedEvent event) &#123; System.out.println( \"监听kinge节点输出的事件：\"+event ); &#125;&#125;, stat );//那么此时给服务端你发送getdata请求时，会把new出来的watcher一并发送过去么？然后由服务端来调用new出来的watcher然后出发么？是怎么样的一个操作逻辑？？ 答案是：不会发送new出来的watcher给服务端，没有必要，只会发送标志位过去（会在请求中设置watch==true的标志位） 问题三：zk客户端怎么存储监听的事件？ ​ 为了方便直接出发监听器，那么存储的数据结构，应该是一个这样的结构map&lt;string，Set&lt;Watcher&gt;&gt;，key是保存着节点的路径，value是监听这个节点路径的所有watcher。 ​ 那么这样就可以轻松的触发某个节点的所有监听器。 ​ !!!!那么这个map是什么时候赋值的呢？!!!!!! 是在调用api的时候就马上put到map还是什么时候呢？例如调用keeper.exists(“/app”, true)。的时候就直接put监听器到map中么？ 这里先说答案：不是，只有当收到服务端响应请求后，再去注册事件，然后才把事件监听放到map中。然后再去触发。（详情参见上面代码分析&lt;最后处理finishPacket()请求&gt;） zk能够注册监听的三种方式 ZooKeeper keeper = new ZooKeeper(\"localhost:2181\", 5000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123;//zk默认的总体的监听器，代码1 System.out.println( \"输出的事件：\"+ event.toString() ); &#125; &#125;);keeper.exists(\"/app\", true);//注册的事件存放到existWatcheskeeper.getData(path, watcher, stat)//注册的事件存放到dataWatcheskeeper.getChildren(path, watch, stat)//注册的事件存放到childWatches//也就是说zk只有在这三种操作的时候，才能够注册监听。如果在调用这三种api时，不指定新的watcher监听器，那么就会默认使用创建zk客户端时创建的监听器（代码1处） ​ 接下来我们就通过keeper.exists(&quot;/app&quot;, true)&lt;这里的true表示监听app节点是否存在，并且使用默认的zk监听器&gt; 来走一遍流程，探究一下watcher事件是如何触发的。 ​ 结合问题三：我们发现这三种api对应的事件监听都会分别放到一个map中 org.apache.zookeeper.ZooKeeper.ZKWatchManager private final Map&lt;String, Set&lt;Watcher&gt;&gt; dataWatches = new HashMap&lt;String, Set&lt;Watcher&gt;&gt;(); private final Map&lt;String, Set&lt;Watcher&gt;&gt; existWatches = new HashMap&lt;String, Set&lt;Watcher&gt;&gt;(); private final Map&lt;String, Set&lt;Watcher&gt;&gt; childWatches = new HashMap&lt;String, Set&lt;Watcher&gt;&gt;(); exists()源码 ‘可以看到，如果是watch===true，那么就会使用zk默认的监听器 public Stat exists(String path, boolean watch) throws KeeperException, InterruptedException&#123; return exists(path, watch ? watchManager.defaultWatcher : null);&#125;//接着调用 public Stat exists(final String path, Watcher watcher) throws KeeperException, InterruptedException &#123; final String clientPath = path; PathUtils.validatePath(clientPath); // the watch contains the un-chroot path WatchRegistration wcb = null; if (watcher != null) &#123; wcb = new ExistsWatchRegistration(watcher, clientPath); &#125; final String serverPath = prependChroot(clientPath); //下面这段代码，我们在上面的zk客户端发送create请求说过。最终发送的请求都会封装成一个request RequestHeader h = new RequestHeader(); h.setType(ZooDefs.OpCode.exists); ExistsRequest request = new ExistsRequest(); request.setPath(serverPath);//设置请求节点路径 request.setWatch(watcher != null);//很关键，如果监听器不为空，那么说明当前调用是需要触发监听器的。 SetDataResponse response = new SetDataResponse(); ReplyHeader r = cnxn.submitRequest(h, request, response, wcb);//提交请求 if (r.getErr() != 0) &#123; if (r.getErr() == KeeperException.Code.NONODE.intValue()) &#123; return null; &#125; throw KeeperException.create(KeeperException.Code.get(r.getErr()), clientPath); &#125; return response.getStat().getCzxid() == -1 ? null : response.getStat(); &#125; ​ 在这段代码中，我们不难发现，实际上发送给服务端的请求，实际上，并没有带上注册的监听器，仅仅只是设置了一个标志位 request.setWatch（true） —– 验证了我们上面提出的问题二的结论 接着调用cnx.submitRequest（）发送请求 ​ 关于这段代码的逻辑，在上面我们已经说过了，就不再具体说明。 ​ 简单说一下。在这个方法中会把请求信息包装成一个Packet，然后把Packet放到一个队列中，LinkedList outgoingQueue。然后结束。 ​ 那么什么时候发送请求给服务端呢？通过SendThread的run方法进行发送。（sendThread线程在这之前就已经启动了）。 那么我们还记得在启动SendThread线程的同时，还启动了另外一个线程：EventThread线程 那么他就是用来处理watcher事件的。 那么我们知道 调用cnx.submitRequest（）发送请求后。客户端会阻塞等待服务端响应，那么这个时候接下来就需要看server端处理请求的逻辑了 – 客户端执行逻辑到这里就结束了。 ​ 接下来的核心就是继续研究server端的处理请求逻辑了，我们跟踪到server端的final处理器，会发现，server端封装了两个hashmap来保存处理事件的监听机制(详情请看《watcher监听事件机制》) ### 客户端执行quit退出命令行操作会发生什么​ 我们知道，凡是客户端的操作，都会涉及到一个操作类型，例如是create，delete，getData，setAcl等等，那么quit操作，对应会发送一个closeSession的操作码给服务端，然后服务端，经过处理链的处理，最后在final处理器，删除session（客户端跟服务端的连接，都会维持一个session），删除在内存中的临时节点（我们知道临时节点在客户端断开连接后，自动会被删除） zookeeper集群优先选择连接节点改造本质是修改StaticHostProvider类，选出下一个连接server的地址的逻辑。 总结运行zkCli.sh文件最终发生这几部动作： 创建zookeeper客户端 创建ClientCnxnSocket，准确的说是：ClientCnxnSocketNIO 创建SendThread线程 目的是通过NIO方式构建一个到服务端的连接 创建EventThread线程 启动上面两个线程，SendThread初始化NIO的socket连接，发送请求给客户端。 解析命令行，然后发送请求给服务端 根据请求，封装成一个packet，放到outgoingqueue队列中，然后阻塞等待处理结果。 接着SendThread线程会在while循环中，不断的从outgoingqueue队列中取出请求的packet，通过初始化好的ClientCnxnSocketNIO，发送packet给服务端，然后将已经处理的packet节点从outgoingqueue中移除，并加入到pendingqueue中，等待处理结果。 根据doIO方法，处理outgoingqueue中的packet，然后把处理后的packet加入pendingqueue，等待服务端的响应 在doIO方法中处理服务端的响应，然后从pendingqueue移除已经处理的packet，然后标记当前packet节点为完成，finished=true。 然后服务端响应结果给客户端，完成请求。 EventThread处理监听事件 从waitEventQueue中获取事件，然后在run方法中触发调用。 ​ 我们可以发现，同一个客户端的请求，按其发送顺序依次执行。（根据outgoingqueue从头部逐个取出） ​ 客户端主要是通过两个线程进行主要的工作。其中SendThread主要是进行，请求的发送和处理服务端响应的结果。EventThread主要是处理触发的事件（客户端在操作节点时绑定的监听器） ​ 同时我们发现doIO方法，会在处理正常客户端请求后服务端返回的响应数据，也会处理服务端发出触发事件的请求。 换句话说，EventThread线程处理的waitEventQueue队列数据的来源，就是在doIO中进行赋值。 zk客户端连接服务端超时源码解析核心源码处理，就在SendThread线程中的run方法。而且我们知道run方法内部是一个while循环。 if (state.isConnected()) &#123; // determine whether we need to send an AuthFailed event. if (zooKeeperSaslClient != null) &#123; boolean sendAuthEvent = false; if (zooKeeperSaslClient.getSaslState() == ZooKeeperSaslClient.SaslState.INITIAL) &#123; try &#123; zooKeeperSaslClient.initialize(ClientCnxn.this); &#125; catch (SaslException e) &#123; LOG.error(\"SASL authentication with Zookeeper Quorum member failed: \" + e); state = States.AUTH_FAILED; sendAuthEvent = true; &#125; &#125; KeeperState authState = zooKeeperSaslClient.getKeeperState(); if (authState != null) &#123; if (authState == KeeperState.AuthFailed) &#123; // An authentication error occurred during authentication with the Zookeeper Server. state = States.AUTH_FAILED; sendAuthEvent = true; &#125; else &#123; if (authState == KeeperState.SaslAuthenticated) &#123; sendAuthEvent = true; &#125; &#125; &#125; if (sendAuthEvent == true) &#123; eventThread.queueEvent(new WatchedEvent( Watcher.Event.EventType.None, authState,null)); &#125; &#125; // 下一次超时时间 to = readTimeout - clientCnxnSocket.getIdleRecv(); &#125; else &#123; // 如果还没连接上 重置当前剩余可连接时间 to = connectTimeout - clientCnxnSocket.getIdleRecv(); &#125;if (to &lt;= 0) &#123;//如果小于0，那么说明已经超时连接，就抛出异常，那么在哪里处理异常呢？还是在SendThread的run方法中，异常处理完成后，然后接着再次进入while循环，然后重新从server服务地址列表中，随机取一个地址进行连接（ serverAddress = hostProvider.next(1000);） String warnInfo; warnInfo = \"Client session timed out, have not heard from server in \" + clientCnxnSocket.getIdleRecv() + \"ms\" + \" for sessionid 0x\" + Long.toHexString(sessionId); LOG.warn(warnInfo); throw new SessionTimeoutException(warnInfo); &#125; zk服务端源码解析 - 单机模式​ 我们知道，zk的服务端有单机模式和集群模式，实际上两者的区别，就是在zoo.cfg配置文件中，集群模式比单机模式多了，server.*这样的配置列表。 ​ 这里先说单机模式的服务端源码解析。 同理，启动服务端，调用的脚本文件是：zkServer.sh，最终会去执行org.apache.zookeeper.server.quorum.QuorumPeerMain#main类。 QuorumPeerMain类源码public static void main(String[] args) &#123; ZooKeeperServerMain main = new ZooKeeperServerMain(); try &#123; main.initializeAndRun(args);//根据配置zoo.cfg文件，解析出配置的信息 &#125;。。。。。&#125; 接着调用initializeAndRun方法，然后执行里面的 config.parse()，解析配置文件，最终所有配置信息，都会保存到QuorumPeerConfig类中。 解析zoo.cfg配置信息到QuorumPeerConfig并识别是否是集群模式，确定半数选举策略那么他是怎么识别是集群模式还是单机模式，就是在解析配置文件的过程中，进行判断的。 解析的方法，最终会调用org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(Properties) 里面有一行代码： else if (key.startsWith(\"server.\")) &#123;//判断配置文件中是否存在，server.这样的配置信息，如果有，说明是集群模式，并把配置信息放到servers的Map中，保存，需要注意，如果server是观察者，那么需要放到observers中。因为观察者不参与投票，servsers中的节点是需要参与投票的。 int dot = key.indexOf('.'); long sid = Long.parseLong(key.substring(dot + 1)); String parts[] = splitWithLeadingHostname(value); if ((parts.length != 2) &amp;&amp; (parts.length != 3) &amp;&amp; (parts.length !=4)) &#123; LOG.error(value + \" does not have the form host:port or host:port:port \" + \" or host:port:port:type\"); &#125; LearnerType type = null; String hostname = parts[0]; Integer port = Integer.parseInt(parts[1]); Integer electionPort = null; if (parts.length &gt; 2)&#123; electionPort=Integer.parseInt(parts[2]); &#125; if (parts.length &gt; 3)&#123; if (parts[3].toLowerCase().equals(\"observer\")) &#123; type = LearnerType.OBSERVER; &#125; else if (parts[3].toLowerCase().equals(\"participant\")) &#123; type = LearnerType.PARTICIPANT; &#125; else &#123; throw new ConfigException(\"Unrecognised peertype: \" + value); &#125; &#125; if (type == LearnerType.OBSERVER)&#123; observers.put(Long.valueOf(sid), new QuorumServer(sid, hostname, port, electionPort, type)); &#125; else &#123; servers.put(Long.valueOf(sid), new QuorumServer(sid, hostname, port, electionPort, type)); &#125; 我们往下看你会发现，一个涉及到我们经常说的，半数选举策略。 quorumVerifier = new QuorumMaj(servers.size()); 通过servers的数量获得半数数量 public QuorumMaj(int n)&#123; this.half = n/2;// 例如 13/2 == 6&#125; 判断启动的是单机版server还是集群模式配置信息解析完成后，下一步就是启动server，那么在启动之前，会涉及到单机模式还是集群模式的判断 protected void initializeAndRun(String[] args) throws ConfigException, IOException&#123; QuorumPeerConfig config = new QuorumPeerConfig(); if (args.length == 1) &#123; config.parse(args[0]); &#125; // Start and schedule the the purge task DatadirCleanupManager purgeMgr = new DatadirCleanupManager(config .getDataDir(), config.getDataLogDir(), config .getSnapRetainCount(), config.getPurgeInterval()); purgeMgr.start(); if (args.length == 1 &amp;&amp; config.servers.size() &gt; 0) &#123;//集群模式 runFromConfig(config); &#125; else &#123;//单机模式 LOG.warn(\"Either no config or no quorum defined in config, running \" + \" in standalone mode\"); // there is only server in the quorum -- run as standalone ZooKeeperServerMain.main(args); &#125;&#125; ​ 因为这里分析的是单机模式的server，所以最后最终调用的是，ZooKeeperServerMain.main(args); ， 然后紧接着最终调用的是org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ServerConfig) 启动单机模式的server。 启动单机版server我们查看一下，启动单机模式的server主要代码 public void runFromConfig(ServerConfig config) throws IOException &#123; LOG.info(\"Starting server\"); FileTxnSnapLog txnLog = null; try &#123; final ZooKeeperServer zkServer = new ZooKeeperServer();//zk服务端，对应客户端的Zookeeper,负责维护一个处理器链表processor chain final CountDownLatch shutdownLatch = new CountDownLatch(1);//使用cd，启动server后挂起，直到server出异常或者被关闭，执行countDown()，然后唤醒当前main线程，执行关闭服务操作 zkServer.registerServerShutdownHandler( new ZooKeeperServerShutdownHandler(shutdownLatch)); txnLog = new FileTxnSnapLog(new File(config.dataLogDir), new File( config.dataDir));//FileTxnSnapLog,管理FileTxLog和FileSnap txnLog.setServerStats(zkServer.serverStats()); zkServer.setTxnLogFactory(txnLog); zkServer.setTickTime(config.tickTime); zkServer.setMinSessionTimeout(config.minSessionTimeout); zkServer.setMaxSessionTimeout(config.maxSessionTimeout); // 建立socket,默认是NIOServerCnxnFactory（是一个线程） cnxnFactory = ServerCnxnFactory.createFactory();//代码1，创建一个上下文处理器,管理来自客户端的连接， cnxnFactory.configure(config.getClientPortAddress(), config.getMaxClientCnxns()); cnxnFactory.startup(zkServer); shutdownLatch.await();//启动后，等待countDown()，如果触发，那么说明，server需要关闭，那么main线程被唤醒，继续往下执行shutdown()方法 shutdown(); cnxnFactory.join(); if (zkServer.canShutdown()) &#123; zkServer.shutdown(true); &#125; &#125; catch (InterruptedException e) &#123; // warn, but generally this is ok LOG.warn(\"Server interrupted\", e); &#125; finally &#123; if (txnLog != null) &#123; txnLog.close(); &#125; &#125;&#125; 我们来看一下代码1，默认创建的也是一个NIO的上下文，负责处理客户端的连接。 static public ServerCnxnFactory createFactory() throws IOException &#123; String serverCnxnFactoryName = System.getProperty(ZOOKEEPER_SERVER_CNXN_FACTORY); if (serverCnxnFactoryName == null) &#123; serverCnxnFactoryName = NIOServerCnxnFactory.class.getName();//NIOServer &#125; try &#123; ServerCnxnFactory serverCnxnFactory = (ServerCnxnFactory) Class.forName(serverCnxnFactoryName) .getDeclaredConstructor().newInstance(); LOG.info(\"Using &#123;&#125; as server connection factory\", serverCnxnFactoryName); return serverCnxnFactory; &#125; catch (Exception e) &#123; IOException ioe = new IOException(\"Couldn't instantiate \" + serverCnxnFactoryName); ioe.initCause(e); throw ioe; &#125;&#125; 根据NIOServerCnxnFactory建立socket，监听客户端连接调用 cnxnFactory.configure(config.getClientPortAddress(), config.getMaxClientCnxns()); public void configure(InetSocketAddress addr, int maxcc) throws IOException &#123; configureSaslLogin(); // 把当前类作为线程 thread = new ZooKeeperThread(this, \"NIOServerCxn.Factory:\" + addr); // java中线程分为两种类型：用户线程和守护线程。 // 通过Thread.setDaemon(false)设置为用户线程；通过Thread.setDaemon(true)设置为守护线程。 // 如果不设置次属性，默认为用户线程。 // 守护进程（Daemon）是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。也就是说守护线程不依赖于终端，但是依赖于系统，与系统“同生共死”。 // 那Java的守护线程是什么样子的呢。当JVM中所有的线程都是守护线程的时候，JVM就可以退出了；如果还有一个或以上的非守护线程则JVM不会退出 // 垃圾回收线程就是一个经典的守护线程，当我们的程序中不再有任何运行的Thread,程序就不会再产生垃圾，垃圾回收器也就无事可做，所以当垃圾回收线程是JVM上仅剩的线程时，垃圾回收线程会自动离开。 // 它始终在低级别的状态中运行，用于实时监控和管理系统中的可回收资源。 // 所以这里的这个线程是为了和JVM生命周期绑定，只剩下这个线程时已经没有意义了，应该关闭掉。 thread.setDaemon(true); maxClientCnxns = maxcc; this.ss = ServerSocketChannel.open(); ss.socket().setReuseAddress(true); LOG.info(\"binding to port \" + addr); ss.socket().bind(addr); ss.configureBlocking(false); ss.register(selector, SelectionKey.OP_ACCEPT);//注册连接事件，监听客户端的连接&#125; 启动NIOServerCnxnFactory线程，接受客户端的请求​ cnxnFactory.startup(zkServer);主要是做三件事情：启动NIOServerCnxnFactory线程、加载快照数据到内存、初始化请求处理链。 public void startup(ZooKeeperServer zks) throws IOException,InterruptedException &#123; // 启动线程 start(); // 初始化zkServer setZooKeeperServer(zks); // 加载数据 zks.startdata(); // 设置请求处理器链 zks.startup();&#125; // 1.启动线程NIOServerCnxnFactory，监听客户端请求 public void run() &#123; while (!ss.socket().isClosed()) &#123; try &#123; selector.select(1000); Set&lt;SelectionKey&gt; selected; synchronized (this) &#123; selected = selector.selectedKeys(); &#125; ArrayList&lt;SelectionKey&gt; selectedList = new ArrayList&lt;SelectionKey&gt;( selected); Collections.shuffle(selectedList); for (SelectionKey k : selectedList) &#123; if ((k.readyOps() &amp; SelectionKey.OP_ACCEPT) != 0) &#123; // 建立连接 SocketChannel sc = ((ServerSocketChannel) k .channel()).accept(); InetAddress ia = sc.socket().getInetAddress(); int cnxncount = getClientCnxnCount(ia); if (maxClientCnxns &gt; 0 &amp;&amp; cnxncount &gt;= maxClientCnxns)&#123; LOG.warn(\"Too many connections from \" + ia + \" - max is \" + maxClientCnxns ); sc.close(); &#125; else &#123; LOG.info(\"Accepted socket connection from \" + sc.socket().getRemoteSocketAddress()); sc.configureBlocking(false); SelectionKey sk = sc.register(selector, SelectionKey.OP_READ); // 创建连接 NIOServerCnxn cnxn = createConnection(sc, sk); sk.attach(cnxn); addCnxn(cnxn); &#125; &#125; else if ((k.readyOps() &amp; (SelectionKey.OP_READ | SelectionKey.OP_WRITE)) != 0) &#123; // 接收数据,这里会间歇性的接收到客户端的ping NIOServerCnxn c = (NIOServerCnxn) k.attachment(); // 处理客户端请求 c.doIO(k); &#125; else &#123; if (LOG.isDebugEnabled()) &#123; LOG.debug(\"Unexpected ops in select \" + k.readyOps()); &#125; &#125; &#125; selected.clear(); &#125; catch (RuntimeException e) &#123; LOG.warn(\"Ignoring unexpected runtime exception\", e); &#125; catch (Exception e) &#123; LOG.warn(\"Ignoring exception\", e); &#125; &#125; closeAll(); LOG.info(\"NIOServerCnxn factory exited run method\");&#125; zks.startdata()加载数据 因为我们知道，server端可能已经存在数据，那么在启动server端后，应该重放数据，把数据加载到内存中。 zk的数据由两个部分组成：日志和快照。 他们都是保存在磁盘中。 server服务器启动时，会通过磁盘上的事务日志和快照数据文件恢复成完整的内存数据库 public void startdata() throws IOException, InterruptedException &#123; //check to see if zkDb is not null if (zkDb == null) &#123; // txnLogFactory就是存储日志文件与快照的工具类，在ZooKeeperServerMain中赋值的 zkDb = new ZKDatabase(this.txnLogFactory); &#125; if (!zkDb.isInitialized()) &#123;// 一开始为false // 没有初始化，加载数据 loadData(); &#125;&#125;public void loadData() throws IOException, InterruptedException &#123; if(zkDb.isInitialized())&#123; // 已经初始化 setZxid(zkDb.getDataTreeLastProcessedZxid()); &#125; else &#123; // 没有初始化 setZxid(zkDb.loadDataBase()); &#125; // 找到过期的session LinkedList&lt;Long&gt; deadSessions = new LinkedList&lt;Long&gt;(); for (Long session : zkDb.getSessions()) &#123; if (zkDb.getSessionWithTimeOuts().get(session) == null) &#123; deadSessions.add(session); &#125; &#125; // 设置initialized为true，表示已经初始化 zkDb.setDataTreeInit(true); // 清除过期的session for (long session : deadSessions) &#123; killSession(session, zkDb.getDataTreeLastProcessedZxid()); &#125;&#125; zks.startup()初始化请求处理链 public synchronized void startup() &#123; if (sessionTracker == null) &#123; // session跟踪器 createSessionTracker(); &#125; // 启动session跟踪器，它是一个线程 startSessionTracker(); // setupRequestProcessors()建立请求处理器链 setupRequestProcessors(); registerJMX(); setState(State.RUNNING); notifyAll();&#125;这三个请求处理器都是线程，组成一条请求链PrepRequestProcessor ———&gt;&gt; SyncRequestProcessor ———&gt;&gt; FinalRequestProcessorprotected void setupRequestProcessors() &#123; // 最后一个执行的请求处理器 RequestProcessor finalProcessor = new FinalRequestProcessor(this);// 第二个执行的请求处理器，封装了下一个执行的处理器finalProcessorRequestProcessor syncProcessor = new SyncRequestProcessor(this, finalProcessor);((SyncRequestProcessor)syncProcessor).start();// 第一个执行的请求处理器，封装了下一个执行的处理器syncProcessorfirstProcessor = new PrepRequestProcessor(this, syncProcessor);((PrepRequestProcessor)firstProcessor).start();&#125; 以上就是已经做完了，server的启动工作，已经准备好接受和处理客户端请求。 c.doIO(k)处理客户端请求这个代码在org.apache.zookeeper.server.NIOServerCnxnFactory.run() ，也就是之前启动的线程。 接下来查看org.apache.zookeeper.server.NIOServerCnxn.doIO(SelectionKey) 怎么处理客户端的请求 在doIO方法中，最终会调用 readPayload(); 处理请求 private void readPayload() throws IOException, InterruptedException &#123; if (incomingBuffer.remaining() != 0) &#123; // have we read length bytes? int rc = sock.read(incomingBuffer); // sock is non-blocking, so ok if (rc &lt; 0) &#123; throw new EndOfStreamException( \"Unable to read additional data from client sessionid 0x\" + Long.toHexString(sessionId) + \", likely client has closed socket\"); &#125; &#125; if (incomingBuffer.remaining() == 0) &#123; // have we read length bytes? // 计数 packetReceived(); incomingBuffer.flip(); if (!initialized) &#123; readConnectRequest(); &#125; else &#123; // 已连接，读取请求 readRequest(); &#125; lenBuffer.clear(); incomingBuffer = lenBuffer; &#125;&#125; 接着调用 readRequest()读取请求 private void readRequest() throws IOException &#123; /**- 这里的zkServer是在执行NIOServerCnxnFactory的run()-&gt;createConnection()- 执行构造函数初始化的*/ zkServer.processPacket(this, incomingBuffer);&#125; 使用ZooKeeperServer的processPacket处理请求​ 根据OpCode操作的类型，例如create还是delete，走相应的逻辑 // 例如create命令就会走到这里来// 构建RequestRequest si = new Request(cnxn, cnxn.getSessionId(), h.getXid(),h.getType(), incomingBuffer, cnxn.getAuthInfo());si.setOwner(ServerCnxn.me);// 提交请求。开始进入处理链处理逻辑submitRequest(si); 根据请求处理链，处理请求请求处理器PrepRequestProcessor submitRequest交由请求处理器处理数据 public void submitRequest(Request si) &#123; // 请求处理器PrepRequestProcessor，主要是做一些校验工作 if (firstProcessor == null) &#123; synchronized (this) &#123; try &#123; while (state == State.INITIAL) &#123; wait(1000); &#125; &#125; catch (InterruptedException e) &#123; LOG.warn(\"Unexpected interruption\", e); &#125; if (firstProcessor == null || state != State.RUNNING) &#123; throw new RuntimeException(\"Not started\"); &#125; &#125; &#125; try &#123; touch(si.cnxn);//客户端和服务端session会话管理功能，验证是否过期超时等等 // 校验请求是否合法 boolean validpacket = Request.isValid(si.type); if (validpacket) &#123; // 执行请求处理器，交由请求处理器处理 firstProcessor.processRequest(si); if (si.cnxn != null) &#123; incInProcess(); &#125; &#125; else &#123; LOG.warn(\"Received packet at server of unknown type \" + si.type); new UnimplementedRequestProcessor().processRequest(si); &#125; &#125; catch (MissingSessionException e) &#123; if (LOG.isDebugEnabled()) &#123; LOG.debug(\"Dropping request: \" + e.getMessage()); &#125; &#125; catch (RequestProcessorException e) &#123; LOG.error(\"Unable to process request:\" + e.getMessage(), e); &#125;&#125; 接着将请求放到队列中submittedRequests LinkedBlockingQueue&lt;Request&gt; submittedRequests = new LinkedBlockingQueue&lt;Request&gt;();public void processRequest(Request request) &#123; // request.addRQRec(\"&gt;prep=\"+zks.outstandingChanges.size()); submittedRequests.add(request);&#125; 看到队列我们第一反应，应该是生产者消费者模型，肯定有个线程是从队列中取数据进行处理。那么处理逻辑在那里呢？ prep处理器的run方法 - 处理队列中的请求 public void run() &#123; try &#123; while (true) &#123; Request request = submittedRequests.take();//从队列中拿到请求进行处理 long traceMask = ZooTrace.CLIENT_REQUEST_TRACE_MASK; if (request.type == OpCode.ping) &#123; traceMask = ZooTrace.CLIENT_PING_TRACE_MASK; &#125; if (LOG.isTraceEnabled()) &#123; ZooTrace.logRequest(LOG, traceMask, 'P', request, \"\"); &#125; if (Request.requestOfDeath == request) &#123; break; &#125; pRequest(request);//最终走到这里 &#125; &#125; catch (RequestProcessorException e) &#123; if (e.getCause() instanceof XidRolloverException) &#123; LOG.info(e.getCause().getMessage()); &#125; handleException(this.getName(), e); &#125; catch (Exception e) &#123; handleException(this.getName(), e); &#125; LOG.info(\"PrepRequestProcessor exited loop!\");&#125; 接着调用prep处理器的pRequest()方法处理请求 他会从请求中，拿到请求的操作类型，来进行相应的逻辑。最终调用pRequest2Txn()方法或者检查session，最终调用下一个处理器继续处理请求。 @SuppressWarnings(\"unchecked\")protected void pRequest(Request request) throws RequestProcessorException &#123; // LOG.info(\"Prep&gt;&gt;&gt; cxid = \" + request.cxid + \" type = \" + // request.type + \" id = 0x\" + Long.toHexString(request.sessionId)); request.hdr = null; request.txn = null; try &#123; switch (request.type) &#123; case OpCode.create: CreateRequest createRequest = new CreateRequest(); pRequest2Txn(request.type, zks.getNextZxid(), request, createRequest, true); break; case OpCode.delete: DeleteRequest deleteRequest = new DeleteRequest(); pRequest2Txn(request.type, zks.getNextZxid(), request, deleteRequest, true); break; case OpCode.setData: SetDataRequest setDataRequest = new SetDataRequest(); pRequest2Txn(request.type, zks.getNextZxid(), request, setDataRequest, true); break; case OpCode.setACL: SetACLRequest setAclRequest = new SetACLRequest(); pRequest2Txn(request.type, zks.getNextZxid(), request, setAclRequest, true); break; case OpCode.check: CheckVersionRequest checkRequest = new CheckVersionRequest(); pRequest2Txn(request.type, zks.getNextZxid(), request, checkRequest, true); break; case OpCode.multi: //。。。。。省略部分代码 break; //create/close session don't require request record case OpCode.createSession: case OpCode.closeSession: pRequest2Txn(request.type, zks.getNextZxid(), request, null, true); break; //All the rest don't need to create a Txn - just verify session //也就是说如果客户端发出的请求的操作命令是如下这些，那么不进行任何事务操作，只是检查session是否过期，然后将这些请求继续转发给下一个处理器 -- 也就是sync处理器 case OpCode.sync: case OpCode.exists: case OpCode.getData: case OpCode.getACL: case OpCode.getChildren: case OpCode.getChildren2: case OpCode.ping: case OpCode.setWatches: zks.sessionTracker.checkSession(request.sessionId, request.getOwner()); break; default: LOG.warn(\"unknown type \" + request.type); break; &#125; 。。。。。。。。//省略部分代码 request.zxid = zks.getZxid(); nextProcessor.processRequest(request);//调用下一个处理器处理请求 -- sync处理器&#125; 处理器SyncRequestProcessor持久化（快照）操作 接着调用sync处理器的processRequest（） public void processRequest(Request request) &#123; // request.addRQRec(\"&gt;sync\"); queuedRequests.add(request); &#125; 你会发现，sync处理器也是把请求放到队列中，所以很明显他跟prep处理器一样，也是生产消费模型。最终也是用过sync处理器的run方法从queuedRequests队列中拿到请求进行处理。 那么我们直接看sync处理器的run方法 sync处理器run() @Overridepublic void run() &#123; try &#123; int logCount = 0; // we do this in an attempt to ensure that not all of the servers // in the ensemble take a snapshot at the same time setRandRoll(r.nextInt(snapCount/2)); while (true) &#123; Request si = null; if (toFlush.isEmpty()) &#123; si = queuedRequests.take();//拿出请求进行处理 &#125; else &#123; si = queuedRequests.poll(); if (si == null) &#123; flush(toFlush); continue; &#125; &#125; if (si == requestOfDeath) &#123; break; &#125; if (si != null) &#123;//将请求持久化到磁盘中 // track the number of records written to the log if (zks.getZKDatabase().append(si)) &#123; logCount++; if (logCount &gt; (snapCount / 2 + randRoll)) &#123; setRandRoll(r.nextInt(snapCount/2)); // roll the log zks.getZKDatabase().rollLog(); // take a snapshot if (snapInProcess != null &amp;&amp; snapInProcess.isAlive()) &#123; LOG.warn(\"Too busy to snap, skipping\"); &#125; else &#123; snapInProcess = new ZooKeeperThread(\"Snapshot Thread\") &#123; public void run() &#123; try &#123; zks.takeSnapshot(); &#125; catch(Exception e) &#123; LOG.warn(\"Unexpected exception\", e); &#125; &#125; &#125;; snapInProcess.start(); &#125; logCount = 0; &#125; &#125; else if (toFlush.isEmpty()) &#123; if (nextProcessor != null) &#123;//然后将请求传给下一个处理器 -- final处理器 nextProcessor.processRequest(si); if (nextProcessor instanceof Flushable) &#123; ((Flushable)nextProcessor).flush(); &#125; &#125; continue; &#125; toFlush.add(si); if (toFlush.size() &gt; 1000) &#123; flush(toFlush); &#125; &#125; &#125; &#125; catch (Throwable t) &#123; handleException(this.getName(), t); running = false; &#125; LOG.info(\"SyncRequestProcessor exited!\");&#125; 处理器FinalRequestProcessor从队列中拿出数据进行内存操作这是三个处理器里面唯一一个不是线程的处理器。也是本次处理请求中，最后一个处理器。 然后接着调用 public void processRequest(Request request) {}方法处理请求 processRequest()关键代码 public void processRequest(Request request) &#123;。。。。。。。//省略部分代码 if (request.hdr != null) &#123; TxnHeader hdr = request.hdr; Record txn = request.txn; rc = zks.processTxn(hdr, txn);//1.这段代码，就是将请求处理后，加载数据到内存中也就是 - 存储在datatree中（我们zk的数据结构就是一个树形结构，那么他在代码中的对应结构就是DataTree，每个节点是DataNode） &#125; //然后根据请求的类型处理相应的逻辑代码块-- 我们这里看两个操作的代码块（create和exists） case OpCode.create: &#123; lastOp = \"CREA\"; rsp = new CreateResponse(rc.path);//构建请求响应，返回给客户端 err = Code.get(rc.err); break; &#125; case OpCode.exists: &#123; lastOp = \"EXIS\"; // TODO we need to figure out the security requirement for this! ExistsRequest existsRequest = new ExistsRequest(); ByteBufferInputStream.byteBuffer2Record(request.request, existsRequest); String path = existsRequest.getPath(); if (path.indexOf('\\0') != -1) &#123; throw new KeeperException.BadArgumentsException(); &#125; Stat stat = zks.getZKDatabase().statNode(path, existsRequest .getWatch() ? cnxn : null);//关键代码：如果客户端再请求服务端的时候，他监听了 某个节点，那么 existsRequest.getWatch() 的值肯定是true，那么如果是true，他会传入cnxn类（server端的网络处理类，通过这个类发送响应给服务端） rsp = new ExistsResponse(stat); break; &#125; //数据写入内存成功后，最后// 发送命令处理完成后的结果，默认是NIOServerCnxn，执行到这里代表服务端处理请求成功，完成。cnxn.sendResponse(hdr, rsp, \"response\"); watcher监听事件机制我们知道，服务端处理客户端发出的某个节点的监听事件的关键代码是：zks.getZKDatabase().statNode(path, existsRequest .getWatch() ? cnxn : null); 查看ZKDatabase.statNode(String, ServerCnxn) public Stat statNode(String path, ServerCnxn serverCnxn) throws KeeperException.NoNodeException &#123; return dataTree.statNode(path, serverCnxn);&#125; 接着调用dataTree.statNode() public Stat statNode(String path, Watcher watcher)//path就是节点的路径，watcher并不是监听器，他实际上就是一个网络通信类，用户处理客户端发过来的请求，他的实现是--NIOServerCnxn throws KeeperException.NoNodeException &#123; Stat stat = new Stat(); DataNode n = nodes.get(path); if (watcher != null) &#123; dataWatches.addWatch(path, watcher);//接着执行这段代码，将path和watcher存入一个中 hashmap中 &#125; if (n == null) &#123; throw new KeeperException.NoNodeException(); &#125; synchronized (n) &#123; n.copyStat(stat); return stat; &#125;&#125; 执行dataWatches.addWatch(path, watcher); ​ org.apache.zookeeper.server.WatchManager.addWatch(String, Watcher) public synchronized void addWatch(String path, Watcher watcher) &#123; HashSet&lt;Watcher&gt; list = watchTable.get(path); if (list == null) &#123; // don't waste memory if there are few watches on a node // rehash when the 4th entry is added, doubling size thereafter // seems like a good compromise list = new HashSet&lt;Watcher&gt;(4); watchTable.put(path, list); &#125; list.add(watcher); //在这之前，hashmap的key是path，value是watcher（servercnx） HashSet&lt;String&gt; paths = watch2Paths.get(watcher);//反转hashmap，key转变为watcher（servercnx），value是path if (paths == null) &#123; // cnxns typically have many watches, so use default cap here paths = new HashSet&lt;String&gt;(); watch2Paths.put(watcher, paths); &#125; paths.add(path);&#125; 最后将构造一个两个hashmap一个未反转key-value的，一个是已经反转key和value的 private final HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable = new HashMap&lt;String, HashSet&lt;Watcher&gt;&gt;(); private final HashMap&lt;Watcher, HashSet&lt;String&gt;&gt; watch2Paths = new HashMap&lt;Watcher, HashSet&lt;String&gt;&gt;(); 提醒：我们注意这里的watch并不是我们客户端的注册的监听实现，而是ServerCnx，是server端的一个通信类，所以说客户端注册的时间监听不会在服务端实现触发逻辑！！！！！ 那么什么时候从watchTable中拿数据发送呢？或者什么时候触发事件呢？ 我们先梳理一下整个流程，假设客户端调用getData(“/app”,new Watch())，那么服务端知道你会在这个app节点注册监听，那么会执行到dataWatches.addWatch()代码。 ​ 然后往watchTable存放数据，key是/app，value是servercnx。 ​ 那么当下次客户端调用setData(“/app”,”213”)，设置数据时，很明显这个时候我们需要响应客户端的事件，因为在这个节点上上次客户端创建了一个监听，所以服务端需要通知客户端，app节点修改了，你注册的事件你自己要触发一下。 那么服务端是怎么通知客户端要出发监听事件呢？ 我们接着看代码，服务端收到setDate事件后，走处理链处理请求，最终会走到final处理器，然后最终调用datatree设置内存内存数据。 org.apache.zookeeper.server.DataTree.setData(String, byte[], int, long, long) public Stat setData(String path, byte data[], int version, long zxid, long time) throws KeeperException.NoNodeException &#123; Stat s = new Stat(); DataNode n = nodes.get(path); if (n == null) &#123; throw new KeeperException.NoNodeException(); &#125; byte lastdata[] = null; synchronized (n) &#123; lastdata = n.data; n.data = data; n.stat.setMtime(time); n.stat.setMzxid(zxid); n.stat.setVersion(version); n.copyStat(s); &#125; // now update if the path is in a quota subtree. String lastPrefix; if((lastPrefix = getMaxPrefixWithQuota(path)) != null) &#123; this.updateBytes(lastPrefix, (data == null ? 0 : data.length) - (lastdata == null ? 0 : lastdata.length)); &#125; dataWatches.triggerWatch(path, EventType.NodeDataChanged);//触发监听 return s;&#125; 那么设置完成后，代表客户端的setdata请求，服务端处理成功，那么就需要通知客户端触发事件了。 最终调用： public Set&lt;Watcher&gt; triggerWatch(String path, EventType type, Set&lt;Watcher&gt; supress) &#123; WatchedEvent e = new WatchedEvent(type, KeeperState.SyncConnected, path); HashSet&lt;Watcher&gt; watchers; synchronized (this) &#123; watchers = watchTable.remove(path);//可以看到触发一个时间，就删除一个，这个就是为什么事件只能触发一次的原因。回答上面问题1的提问 if (watchers == null || watchers.isEmpty()) &#123; if (LOG.isTraceEnabled()) &#123; ZooTrace.logTraceMessage(LOG, ZooTrace.EVENT_DELIVERY_TRACE_MASK, \"No watchers for \" + path); &#125; return null; &#125; for (Watcher w : watchers) &#123; HashSet&lt;String&gt; paths = watch2Paths.get(w); if (paths != null) &#123; paths.remove(path); &#125; &#125; &#125; for (Watcher w : watchers) &#123; if (supress != null &amp;&amp; supress.contains(w)) &#123; continue; &#125; w.process(e);//触发监听事件 &#125; return watchers;&#125; 最终调用org.apache.zookeeper.server.NIOServerCnxn.process(WatchedEvent) synchronized public void process(WatchedEvent event) &#123; ReplyHeader h = new ReplyHeader(-1, -1L, 0);//重点，发送的是xid == -1，这个参数在《sendThread.readResponse() - 读取服务端返回的信息》处理服务端响应的代码中，有个判断if (replyHdr.getXid() == -1) 判断成功，说明需要客户端执行事件。那么客户端就把事件放到waitEvenQueue中等到eventThread的run方法进行处理。 if (LOG.isTraceEnabled()) &#123; ZooTrace.logTraceMessage(LOG, ZooTrace.EVENT_DELIVERY_TRACE_MASK, \"Deliver event \" + event + \" to 0x\" + Long.toHexString(this.sessionId) + \" through \" + this); &#125; // Convert WatchedEvent to a type that can be sent over the wire WatcherEvent e = event.getWrapper(); sendResponse(h, e, \"notification\");//发送通知给客户端，请求触发事件 //接着在客户端的sendThread.readResponse()中处理请求，然后判断xid，然后做响应触发事件逻辑 &#125; 完整的请求处理链 ​ 也就是说，处理请求的时候，会经过三个处理器，进行处理。其中sync处理器的目的就是，将请求写入磁盘日志，并熟悉快照。 ​ 接着使用最后一个处理器final，将请求写到内存（方便读请求直接获取内存数据并返回），触发监听器，返回结果给客户端，完成请求。 ​ 如果客户端监听了某个节点，那么这个时候就会收到服务端发出的事件（通过socket的方式发送一个通知给客户端），然后客户端再进行相应处理。 详情参见：https://blog.csdn.net/yu_kang/article/details/88573304 总结在Zookeeper中，数据存储分为两部分：内存数据存储和磁盘数据存储。 客户端的写请求，必须先写入磁盘后，才写入内存中，最后返回给客户端，整个读写请求完成。 单机模式下，客户端的请求，通过处理链进行处理，处理链上有三个处理器。 单机模式下，server保证了高并发的安全性，因为请求会放到队列中，然后逐渐经过处理链，逐个处理。保证了每个请求的顺序性 好的文档https://blog.csdn.net/yu_kang/article/details/88573304 https://www.jianshu.com/nb/33061765 - zk专栏 http://jm.taobao.org/2018/06/13/%E5%81%9A%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%EF%BC%9F/ 阿里中间件专栏 https://yq.aliyun.com/articles/227260 zk开发中遇到的坑 ########单机模式 end########集群模式集群模式下的服务端启动源码解析（如何保证数据一致性）知识储备​ 集群的选举，涉及到，myid（sid）、epoch（第几轮投票）、选举状态（LOOKING、FOLLOWING、LEADING、OBSERVERING）， 这三个值是很重要的。 在集群中，一般会存在三种类型的网络通信：所以在下面讲解集群的某些类中，你会发现某些类的io通信的作用是下面三个中的一个。 选举需要网络通信（多个server之间） 集群接受客户端请求 集群数据同步（learner同步leader数据） 集群需要实现的功能： 数据存储、持久化存储 数据同步流程 watcher监听怎么实现 所以上面这六个问题是我们在阅读源码所以带着的问题。 初始化集群配置信息其实他跟启动单机版服务端的，初始化配置信息的操作是差不多的，为了重温只是，这里再贴出来 zkServer.sh文件最终会访问这个类启动服务端 1.QuorumPeerMain类源码 public static void main(String[] args) &#123; ZooKeeperServerMain main = new ZooKeeperServerMain(); try &#123; main.initializeAndRun(args);//根据配置zoo.cfg文件，解析出配置的信息 &#125;。。。。。&#125; ​ 接着调用initializeAndRun方法，然后执行里面的 config.parse()，解析zoo.cfg配置文件，最终所有配置信息，都会保存到QuorumPeerConfig类中（包括，集群所有服务器地址，以及ob服务器，以及zk数据存储的位置，以及日志存储位置等等信息） 2.解析zoo.cfg配置信息到QuorumPeerConfig并识别是否是集群模式，确定半数数量 那么他是怎么识别是集群模式还是单机模式，就是在解析配置文件的过程中，进行判断的。 解析的方法，最终会调用org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(Properties) 里面有一行代码： else if (key.startsWith(\"server.\")) &#123;//判断配置文件中是否存在，server.这样的配置信息，如果有，说明是集群模式，并把配置信息放到servers的Map中，保存，需要注意，如果server是观察者，那么需要放到observers中。因为观察者不参与投票，servsers中的节点是需要参与投票的。 int dot = key.indexOf('.'); long sid = Long.parseLong(key.substring(dot + 1)); String parts[] = splitWithLeadingHostname(value); if ((parts.length != 2) &amp;&amp; (parts.length != 3) &amp;&amp; (parts.length !=4)) &#123; LOG.error(value + \" does not have the form host:port or host:port:port \" + \" or host:port:port:type\"); &#125; LearnerType type = null; String hostname = parts[0]; Integer port = Integer.parseInt(parts[1]); Integer electionPort = null; if (parts.length &gt; 2)&#123; electionPort=Integer.parseInt(parts[2]); &#125; if (parts.length &gt; 3)&#123; if (parts[3].toLowerCase().equals(\"observer\")) &#123; type = LearnerType.OBSERVER; &#125; else if (parts[3].toLowerCase().equals(\"participant\")) &#123; type = LearnerType.PARTICIPANT; &#125; else &#123; throw new ConfigException(\"Unrecognised peertype: \" + value); &#125; &#125; if (type == LearnerType.OBSERVER)&#123; observers.put(Long.valueOf(sid), new QuorumServer(sid, hostname, port, electionPort, type)); &#125; else &#123; servers.put(Long.valueOf(sid), new QuorumServer(sid, hostname, port, electionPort, type)); &#125; 我们往下看你会发现，一个涉及到我们经常说的，半数选举策略。 quorumVerifier = new QuorumMaj(servers.size()); 通过servers的数量获得半数数量。这里可以看到observer 并没有参与选举数量的确定，在确定半数数量后，ob最终会加到servers中（因为他也是能够处理客户端请求的）。 public QuorumMaj(int n){ this.half = n/2;// 例如 13/2 == 6 } // Now add observers to servers, once the quorums have been // figured out servers.putAll(observers);//ob最终会加到servers中 ​ 同时在parseProperties方法中，还会做一些校验工作，例如集群server必须是大于2的，而且如果集群数量是2的倍数，会提示建议使用奇数个server。 ​ runFromConfig(config)启动服务器初始化完成集群总体配置信息过后，那么就开始进入当前server服务器集群初始化和启动工作。 调用runFromConfig（）方法，QuorumPeerMain启动类的方法。 org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerConfig) public void runFromConfig(QuorumPeerConfig config) throws IOException &#123; try &#123; ManagedUtil.registerLog4jMBeans(); &#125; catch (JMException e) &#123; LOG.warn(\"Unable to register log4j JMX control\", e); &#125; LOG.info(\"Starting quorum peer\"); try &#123; ServerCnxnFactory cnxnFactory = ServerCnxnFactory.createFactory(); cnxnFactory.configure(config.getClientPortAddress(), config.getMaxClientCnxns());//跟单节点的服务端启动一样，// 建立socket,默认是NIOServerCnxnFactory（是一个线程），管理来自客户端的连接，监听ClientPortAddress。 //这个时候NIOServerCnxnFactory线程还未启动，因为没有调用start quorumPeer = getQuorumPeer(); quorumPeer.setQuorumPeers(config.getServers()); quorumPeer.setTxnFactory(new FileTxnSnapLog( new File(config.getDataLogDir()), new File(config.getDataDir()))); quorumPeer.setElectionType(config.getElectionAlg()); quorumPeer.setMyid(config.getServerId()); quorumPeer.setTickTime(config.getTickTime()); quorumPeer.setInitLimit(config.getInitLimit()); quorumPeer.setSyncLimit(config.getSyncLimit()); quorumPeer.setQuorumListenOnAllIPs(config.getQuorumListenOnAllIPs()); quorumPeer.setCnxnFactory(cnxnFactory); quorumPeer.setQuorumVerifier(config.getQuorumVerifier()); quorumPeer.setClientPortAddress(config.getClientPortAddress()); quorumPeer.setMinSessionTimeout(config.getMinSessionTimeout()); quorumPeer.setMaxSessionTimeout(config.getMaxSessionTimeout()); quorumPeer.setZKDatabase(new ZKDatabase(quorumPeer.getTxnFactory())); quorumPeer.setLearnerType(config.getPeerType());//设置当前server的状态，默认设置的是learner状态中的PARTICIPANT（参与者）。learner一共有两种状态（PARTICIPANT，OBSERVER） quorumPeer.setSyncEnabled(config.getSyncEnabled()); // sets quorum sasl authentication configurations quorumPeer.setQuorumSaslEnabled(config.quorumEnableSasl); if(quorumPeer.isQuorumSaslAuthEnabled())&#123; quorumPeer.setQuorumServerSaslRequired(config.quorumServerRequireSasl); quorumPeer.setQuorumLearnerSaslRequired(config.quorumLearnerRequireSasl); quorumPeer.setQuorumServicePrincipal(config.quorumServicePrincipal); quorumPeer.setQuorumServerLoginContext(config.quorumServerLoginContext); quorumPeer.setQuorumLearnerLoginContext(config.quorumLearnerLoginContext); &#125; quorumPeer.setQuorumCnxnThreadsSize(config.quorumCnxnThreadsSize); quorumPeer.initialize(); quorumPeer.start();//启动server服务器，做初始化工作。需要注意，这里只是启动服务器做初始化工作，例如选举leader、数据同步等等操作，并不是马上就能接受客户端请求。具体逻辑在QuorumPeer的start()方法中。 quorumPeer.join();//等待quorumPeer线程执行完毕，那么当前服务器退出。 &#125; catch (InterruptedException e) &#123; // warn, but generally this is ok LOG.warn(\"Quorum Peer interrupted\", e); &#125; &#125; ​ 接着创建一个QuorumPeer类(是一个线程)，保存当前server的配置信息（实际上就是用QuorumPeerConfig将部分配置信息赋值给QuorumPeer）。接着启动QuorumPeer.start(). quorumPeer.start() @Overridepublic synchronized void start() &#123; loadDataBase();//首先加载本地数据（根据快照和日志文件，重放数据到内存中），保持数据一致性关键 cnxnFactory.start(); //启动NIOServerCnxnFactory线程（上面创建的） startLeaderElection();//确定选举leader的策略，实际上有四种策略，但是其他三种都是过时的，目前只有FastLeaderElection策略才能够使用 super.start();//启动线程quorumPeer，执行run方法&#125; loadDataBase()重放数据到服务器内存这个在上面的单机版服务的server说过了，类似的，这里就不再说了。 cnxnFactory.start()初始化启动NIOServerCnxnFactory线程 cnxnFactory.start()，启动NIOServerCnxnFactory线程，处理客户端请求（也就是说处理客户端请求逻辑在这里） ​ 最终会去执行NIOServerCnxnFactory的run方法，在while循环中，获取客户端请求。如果nio监听到读写事件，那么就调用doIO进行处理请求。 ​ 特别注意：doIO在处理请求的时候，如果server的状态并不是 State.RUNNING，那么就会抛出异常throw new IOException(&quot;ZooKeeperServer not running&quot;); ​ 这个时候说明集群并没有准备好，那么不处理任何请求。 ​ ！！！！那么此刻很明显当前server肯定不是RUNNING状态，因为没有在这之前没有看到任何代码将当前服务器启动并设置状态为RUNNING，所以代码运行到这里，服务器是不能接受客户端请求的，就算接受了也会抛异常（从逻辑上此时server也不可能响应客户端请求，因为当前server还没有确定自己的状态，是leader还是follower还是observer，数据是否是最新的？等等之类的问题，那么他此刻肯定不能响应客户端请求）！！！！！！！！！！ ​ 总而言之，就是监听2181端口，接口客户端请求。 这方法的代码，主要的是就是讲述我们的 集群接受客户端请求 通信的内容了。 startLeaderElection（）确定选举策略和先投自己一票synchronized public void startLeaderElection() &#123; try &#123; //myid不用说了，就是我们自己配置的。getLastLoggedZxid：获取当前server最新的zxid，getCurrentEpoch（）获取当前服务器目前是在第几轮（在哪个朝代，是哪个leader领导的） currentVote = new Vote(myid, getLastLoggedZxid(), getCurrentEpoch());//你会发现这里的开始就先构建一个投票信息。表示你要投自己一票。CurrentEpoch的意思是，当前是第几轮投票。（这三个参数是在选举leader中是需要做比较的） &#125; catch(IOException e) &#123; RuntimeException re = new RuntimeException(e.getMessage()); re.setStackTrace(e.getStackTrace()); throw re; &#125; //getView()返回的是，我们在zoo.cfg中配置的集群server列表（包括observer） for (QuorumServer p : getView().values()) &#123;//通过myid，获取当前server的ip if (p.id == myid) &#123; myQuorumAddr = p.addr; break; &#125; &#125; if (myQuorumAddr == null) &#123;//如果地址为空，那么说明当前服务器不在集群中，那么启动失败。 throw new RuntimeException(\"My id \" + myid + \" not in the peer list\"); &#125; if (electionType == 0) &#123;//选举策略，默认是3，这个值的赋值是在QuorumPeerMain.runFromConfig()的方法中配置。也就是说，我们可以通过在zoo.cfg中配置electionType的值确定不通的选举策略。 try &#123; udpSocket = new DatagramSocket(myQuorumAddr.getPort()); responder = new ResponderThread(); responder.start(); &#125; catch (SocketException e) &#123; throw new RuntimeException(e); &#125; &#125; this.electionAlg = createElectionAlgorithm(electionType);//根据选举策略标志位获取选举策略，实际上除了FastLeaderElection这种策略，其他的都失效了。所以这里默认创建FastLeaderElection选举策略&#125; createElectionAlgorithm(electionType) 创建选举策略，只有case等于3的策略才能使用，其他的都已经失效了，建立投票监听。 protected Election createElectionAlgorithm(int electionAlgorithm)&#123; Election le=null; //TODO: use a factory rather than a switch switch (electionAlgorithm) &#123; case 0: le = new LeaderElection(this);//已经失效，不能使用 break; case 1: le = new AuthFastLeaderElection(this);//已经失效，不能使用 break; case 2: le = new AuthFastLeaderElection(this, true);//已经失效，不能使用 break; case 3://只能使用 qcm = createCnxnManager(); QuorumCnxManager.Listener listener = qcm.listener; if(listener != null)&#123; listener.start(); //FastLeaderElection le = new FastLeaderElection(this, qcm); &#125; else &#123; LOG.error(\"Null listener when initializing cnx manager\"); &#125; break; default: assert false; &#125; return le;&#125; 我们可以看到他创建了FastLeaderElection选举算法，但是在那里创建了其他服务器的投票监听呢？ （1）createCnxnManager()方法，最终创建QuorumCnxManager类。 - 创建投票监听listener // visible for testingpublic QuorumCnxManager(final long mySid, Map&lt;Long,QuorumPeer.QuorumServer&gt; view, QuorumAuthServer authServer, QuorumAuthLearner authLearner, int socketTimeout, boolean listenOnAllIPs, int quorumCnxnThreadsSize, boolean quorumSaslAuthEnabled, ConcurrentHashMap&lt;Long, SendWorker&gt; senderWorkerMap) &#123; this.senderWorkerMap = senderWorkerMap;//针对要发送投票信息给服务器数量，创建map，保存相应数量的SendWorker线程，每个线程都负责发送投票信息给服务器 this.recvQueue = new ArrayBlockingQueue&lt;Message&gt;(RECV_CAPACITY);//保存收到的投票信息 this.queueSendMap = new ConcurrentHashMap&lt;Long, ArrayBlockingQueue&lt;ByteBuffer&gt;&gt;();//保存要发送给每台服务器的数据，senderWorkerMap就是从这里取数据发送 this.lastMessageSent = new ConcurrentHashMap&lt;Long, ByteBuffer&gt;();//最近一次发送的数据 String cnxToValue = System.getProperty(\"zookeeper.cnxTimeout\"); if(cnxToValue != null)&#123; this.cnxTO = Integer.parseInt(cnxToValue); &#125; this.mySid = mySid; this.socketTimeout = socketTimeout; this.view = view; this.listenOnAllIPs = listenOnAllIPs; initializeAuth(mySid, authServer, authLearner, quorumCnxnThreadsSize, quorumSaslAuthEnabled); // Starts listener thread that waits for connection requests listener = new Listener();//很关键这个listnner，建立连接接受其他服务端的投票请求&#125; ​ 换言之，这个类主要创建两个map去负责发送数据，其中一个map是发送线程map，另一个map是保存线程要发送的数据。创建一个队列来接受数据。 （1.1）new Listener() - 创建投票监听 ​ 实际上是创建一个线程，但是这个listenner什么时候启动呢？在createElectionAlgorithm方法中 执行了listener.start()启动，然后执行rlistener的run方法，run方法中创建一个原生的ServerSocket，接受其他server端的投票请求。 @Overridepublic void run() &#123; int numRetries = 0; InetSocketAddress addr; while((!shutdown) &amp;&amp; (numRetries &lt; 3))&#123; try &#123; ss = new ServerSocket();//为什么不用NIO，而是用了阻塞的bio呢？因为集群中投票的节点不会太多，所以没有关系 ss.setReuseAddress(true); if (listenOnAllIPs) &#123; int port = view.get(QuorumCnxManager.this.mySid) .electionAddr.getPort(); addr = new InetSocketAddress(port); &#125; else &#123; addr = view.get(QuorumCnxManager.this.mySid) .electionAddr; &#125; LOG.info(\"My election bind port: \" + addr.toString()); setName(view.get(QuorumCnxManager.this.mySid) .electionAddr.toString()); ss.bind(addr);//绑定的addr地址，是选举地址，也就是当前服务器的选举地址。也就是监听localhost:3887。 //server.1=localhost:2887:3887 while (!shutdown) &#123;//无线遍历，处理来自其他server的投票信息 Socket client = ss.accept(); setSockOpts(client); LOG.info(\"Received connection request \" + client.getRemoteSocketAddress()); // Receive and handle the connection request // asynchronously if the quorum sasl authentication is // enabled. This is required because sasl server // authentication process may take few seconds to finish, // this may delay next peer connection requests. if (quorumSaslAuthEnabled) &#123; receiveConnectionAsync(client); &#125; else &#123; receiveConnection(client);//处理收到的请求，最终调用handleConnection处理请求 &#125; numRetries = 0; &#125; handleConnection，新建两个线程发送自己的投票数据和接受其他服务器的投票数据 也就是新建 SendWorker 和 RecvWorker 线程。 private void handleConnection(Socket sock, DataInputStream din) throws IOException &#123; Long sid = null; try &#123; // Read server id sid = din.readLong(); if (sid &lt; 0) &#123; // this is not a server id but a protocol version (see ZOOKEEPER-1633) sid = din.readLong(); // next comes the #bytes in the remainder of the message // note that 0 bytes is fine (old servers) int num_remaining_bytes = din.readInt(); if (num_remaining_bytes &lt; 0 || num_remaining_bytes &gt; maxBuffer) &#123; LOG.error(\"Unreasonable buffer length: &#123;&#125;\", num_remaining_bytes); closeSocket(sock); return; &#125; byte[] b = new byte[num_remaining_bytes]; // remove the remainder of the message from din int num_read = din.read(b); if (num_read != num_remaining_bytes) &#123; LOG.error(\"Read only \" + num_read + \" bytes out of \" + num_remaining_bytes + \" sent by server \" + sid); &#125; &#125; if (sid == QuorumPeer.OBSERVER_ID) &#123; /* * Choose identifier at random. We need a value to identify * the connection. */ sid = observerCounter.getAndDecrement(); LOG.info(\"Setting arbitrary identifier to observer: \" + sid); &#125; &#125; catch (IOException e) &#123; closeSocket(sock); LOG.warn(\"Exception reading or writing challenge: \" + e.toString()); return; &#125; // do authenticating learner LOG.debug(\"Authenticating learner server.id: &#123;&#125;\", sid); authServer.authenticate(sock, din); //If wins the challenge, then close the new connection. if (sid &lt; this.mySid) &#123;// /* * This replica might still believe that the connection to sid is * up, so we have to shut down the workers before trying to open a * new connection. */ SendWorker sw = senderWorkerMap.get(sid); if (sw != null) &#123; sw.finish(); &#125; /* * Now we start a new connection */ LOG.debug(\"Create new connection to server: \" + sid); closeSocket(sock); connectOne(sid); // Otherwise start worker threads to receive data. &#125; else &#123;//为上面创建的senderWorkerMap和queueSendMap赋值，同时启动SendWorker发送投票信息的同时，启动一个相应的RecvWorker线程接受请求。也即是为每一台server都创建一条发送和接受的专线。假设有三个sever，那么就需要三条专线，其中三个SendWorker和三个RecvWorker。 SendWorker sw = new SendWorker(sock, sid); RecvWorker rw = new RecvWorker(sock, din, sid, sw); sw.setRecv(rw); SendWorker vsw = senderWorkerMap.get(sid); if(vsw != null) vsw.finish(); senderWorkerMap.put(sid, sw);//保存数据 queueSendMap.putIfAbsent(sid, new ArrayBlockingQueue&lt;ByteBuffer&gt;(SEND_CAPACITY));//保存数据 sw.start();//启动sendWorker线程，从sendqueue中拿出自己要投票给其他服务器的数据，进行发送 //那么queueSendMap的数据是打哪儿来的呢？参加下面的《1.1sendNotifications()方法》 rw.start();//启动recvWorker线程，获取其他服务器的投票数据 //那么RecvWorker是负责接受其他服务器的投票数据，然后放到LinkedBlockingQueue&lt;Notification&gt; recvqueue;中，那么这些数据什么时候使用呢？在哪里使用嗯？ 参见后面的super.start()调用run方法中，在执行 sendNotifications();之后的逻辑，也即是下面的《1.2处理接受到其他服务端的投票数据》 return; &#125;&#125; 总结​ （1）你会发现这个方法，只是启动了其他服务器向自己投票的监听，和初始化了一个FastLeaderElection选举策略。 ​ （2）listener通过新建两个线程来负责发送自己的投票信息和接受其他服务器的投票信息。 需要注意的是：假设有a,b,c三台服务器，那么a向b,c发送投票数据，要分别为b，c服务器分别创建一对线程（SendWorker和RecvWorker），也就是一共创建两对线程，也即是两个SendWorker和两个RecvWorker。并不是b，c服务器公用一对SendWorker和RecvWorker。 保证了a与其他服务器，都有一对单独线程处理发送投票和接受投票。 ​ 也就是说，执行到这里，发送自己票据和接受其他服务器票据的服务已经启动（现在就等待往sendQueue中添加数据和从recvQueue中拿输出处理了）。 流程图： ​ 本方法并没有真正leader选举的代码逻辑。 那么leader选举的逻辑在那里实现呢？请往下看 super.start() – 根据选定策略执行leader选举-投票代码​ 真正的leader投票，在 super.start();启动之后，判断当前server的节点状态，如果是LOKING，那么说明需要进入leader选举。 选举整体思路 ​ 那么选举leader是根据什么条件选择呢？主要是三个：事务id（也就是zxid，即是比较哪台server的数据是最新的）、myid、epoch ​ 首先比较的是epoch，这里假设epoch相等。那么接着比较zxid，也就是比较那台数据最新。如果zxid一样，那么就比较myid的大小。 ​ 例如有三台server，他们的zxid和myid信息如下（假设epoch相等）： A:zxid=1,myid=1,epoch=1 B:zxid=2,myid=2,epoch=1 C:zxid=2,myid=3 ,epoch=1 一共三台server，半数是1。那么最少有两台server启动才能确定leader A先启动，发现，没有过半数，则继续LOKING。紧接着B启动，半数原则达到，然后开始选举。 首先他们都会各自投自己一票，然后把信息发给对方。假设他们吧投票信息都放到一个map中，key是zxid。 此时，A：{}，B：{}。然后把投票信息发给对方。 ​ 此时，A：{，}，B：{，}。A服务器开始判断zxid大小，发现B的zxid更大，那么就会把投自己一票的那一票给丢弃，换成投给B，此时A：{，} —&gt; A：{，} ​ 然后A把投票结果发给B，也会把，A的那票给失效，此时B：{，} —&gt; A：{，} ​ 最终确定B当选leader。 那么如果zxid一样，那么就要重新根据myid投票，投票过程是一样的。 领导选举的，关键代码在QuorumPeer线程的run方法。 源码分析 ​ 查看QuorumPeer的run方法： 首席服务器启动是，他的服务状态肯定是LOOKING。因为默认QuorumPeer默认private ServerState state = ServerState.LOOKING;. 所以进入case LOOKING的代码逻辑。 switch (getPeerState()) &#123; case LOOKING://如果服务器状态是loking，那么说明需要领导选举 setBCVote(null); //默认当前选的是自己为leader，但是随着选举的逻辑进行，可能currentVote的值就不是自己了 setCurrentVote(makeLEStrategy().lookForLeader());//投票关键代码 。。。。。。省略部分代码 查看FastLeaderElection.lookForLeader()确定选举细节 public Vote lookForLeader() throws InterruptedException &#123; 。。。。。。。。。 //收到的投票，就是投票箱的概念 HashMap&lt;Long, Vote&gt; recvset = new HashMap&lt;Long, Vote&gt;(); //发出去的投票 HashMap&lt;Long, Vote&gt; outofelection = new HashMap&lt;Long, Vote&gt;(); int notTimeout = finalizeWait; synchronized(this)&#123;//没选举一轮，那么epoch就会自增1 logicalclock.incrementAndGet();//原子long类型，增加逻辑时钟，就是epoch //更新选举提议，myid zxid epoch，这里面的信息也就是自己的myid，zxid，和currentEpoch。 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; LOG.info(\"New election. My id = \" + self.getId() + \", proposed zxid=0x\" + Long.toHexString(proposedZxid)); //（1）发送投票提议给所有的节点，需要注意，也会发送给自己.只会发送给需要参与投票的节点服务器，也即是//server.type == LearnerType.，不包括observer节点 sendNotifications(); //（2）发送完自己的提议，那么开始处理收到的票据。 //如果还是是looking状态，我们会一直去和其他节点交互信息，直到选举出leader while ((self.getPeerState() == ServerState.LOOKING) &amp;&amp; (!stop))&#123; //从接收队列中拿到投票信息，拿到其他server的投票信息。 //疑问？？ //recvQueue中的是数据是哪里来的呢？我们还记得在startLeaderElection()方法中创建的listener中创建的RecvWorker线程么？ //就是通过这个线程去接受其他服务端的投票，然后放到recvQueue中。 Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS);//然后从队列中取出一个票据 /* * Sends more notifications if haven't received enough. * Otherwise processes new notification. */ if(n == null)&#123;//如果取出的票据为空，那么说明没有其他服务器发送过来他们的票据 if(manager.haveDelivered())&#123; //检查所有的队列是否为空 sendNotifications(); //如果为空发送通知 &#125; else &#123; manager.connectAll(); //如果没有投递出去，可能是其他server还没有启动，尝试连接 &#125; /* * Exponential backoff */ int tmpTimeOut = notTimeout*2; notTimeout = (tmpTimeOut &lt; maxNotificationInterval? tmpTimeOut : maxNotificationInterval); LOG.info(\"Notification time out: \" + notTimeout); &#125; //判断收到的投票的sid, //这里判断的是收到的sid是不是属于当前集群内的 else if (validVoter(n.sid) &amp;&amp; validVoter(n.leader)) &#123; switch (n.state) &#123; //判断当前票据状态状态 - 因为是选举阶段，那么票据肯定是LOOKING状态 case LOOKING: //收到的票据中的epoch是不是比当前选举的epoch要大，如果大那么代表是新一轮选举 //那就说明，自己现在的投票轮数不是最新的，那么需要更新为最新，然后再参与投票。例如别人都已经是第三轮投票了，你现在还是第二轮。 if (n.electionEpoch &gt; logicalclock.get()) &#123; logicalclock.set(n.electionEpoch); //更新当前epoch recvset.clear(); //情况收到的投票 //进行投票 /* totalOrderPredicate方法逻辑 * We return true if one of the following three cases hold: * 1- New epoch is higher * 收到的epoch大于当前的epoch 胜出选举 * 2- New epoch is the same as current epoch, but new zxid is higher * 如果收到的epoch等于当前epoch,那么收到的zxid大于当前zxid胜出选举 * 3- New epoch is the same as current epoch, new zxid is the same * as current zxid, but server id is higher. * 如果收到的epoch等于当前epoch，zxid等于当前zxid, * 那么收到的myid大于当前myid的胜出选举 */ if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) &#123; updateProposal(n.leader, n.zxid, n.peerEpoch); //把胜出的消息更新到投票提议中 &#125; else &#123; //如果收到消息没有胜出，那么选择当前的消息更新到投票提议中 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; sendNotifications(); //发送投票消息 &#125; else if (n.electionEpoch &lt; logicalclock.get()) &#123; //如果收到的逻辑时钟小，那么表示这个投票无效 if(LOG.isDebugEnabled())&#123; LOG.debug(\"Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x\" + Long.toHexString(n.electionEpoch) + \", logicalclock=0x\" + Long.toHexString(logicalclock.get())); &#125; break; //如果收到的逻辑时钟相等，则去对比myid 、zxid、epoch &#125; else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123; updateProposal(n.leader, n.zxid, n.peerEpoch); sendNotifications(); &#125; if(LOG.isDebugEnabled())&#123; LOG.debug(\"Adding vote: from=\" + n.sid + \", proposed leader=\" + n.leader + \", proposed zxid=0x\" + Long.toHexString(n.zxid) + \", proposed election epoch=0x\" + Long.toHexString(n.electionEpoch)); &#125; //把投票结果存到本地，用来做最终判断 recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); //判断选举是否结束，默认算法过半同意 if (termPredicate(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch))) &#123; // Verify if there is any change in the proposed leader while((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null)&#123; if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch))&#123; recvqueue.put(n);//获得最新的记过 break; &#125; &#125; if (n == null) &#123; self.setPeerState((proposedLeader == self.getId()) ? ServerState.LEADING: learningState()); Vote endVote = new Vote(proposedLeader, proposedZxid, proposedEpoch); leaveInstance(endVote); return endVote; &#125; &#125; break; case OBSERVING: //如果是 LOG.debug(\"Notification from observer: \" + n.sid); break; case FOLLOWING: case LEADING: /* * Consider all notifications from the same epoch * together. */ if(n.electionEpoch == logicalclock.get())&#123; recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); if(termPredicate(recvset, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); Vote endVote = new Vote(n.leader, n.zxid, n.peerEpoch); leaveInstance(endVote); return endVote; &#125; &#125; /* * Before joining an established ensemble, verify that * a majority are following the same leader. * Only peer epoch is used to check that the votes come * from the same ensemble. This is because there is at * least one corner case in which the ensemble can be * created with inconsistent zxid and election epoch * info. However, given that only one ensemble can be * running at a single point in time and that each * epoch is used only once, using only the epoch to * compare the votes is sufficient. * * @see https://issues.apache.org/jira/browse/ZOOKEEPER-1732 */ outofelection.put(n.sid, new Vote(n.leader, IGNOREVALUE, IGNOREVALUE, n.peerEpoch, n.state)); if (termPredicate(outofelection, new Vote(n.leader, IGNOREVALUE, IGNOREVALUE, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, IGNOREVALUE)) &#123; synchronized(this)&#123; logicalclock.set(n.electionEpoch); self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); &#125; Vote endVote = new Vote(n.leader, n.zxid, n.peerEpoch); leaveInstance(endVote); return endVote; &#125; break; default: LOG.warn(\"Notification state unrecoginized: \" + n.state + \" (n.state), \" + n.sid + \" (n.sid)\"); break; &#125; &#125; else &#123; if (!validVoter(n.leader)) &#123; LOG.warn(\"Ignoring notification for non-cluster member sid &#123;&#125; from sid &#123;&#125;\", n.leader, n.sid); &#125; if (!validVoter(n.sid)) &#123; LOG.warn(\"Ignoring notification for sid &#123;&#125; from non-quorum member sid &#123;&#125;\", n.leader, n.sid); &#125; &#125; &#125; return null; &#125; finally &#123; try &#123; if(self.jmxLeaderElectionBean != null)&#123; MBeanRegistry.getInstance().unregister( self.jmxLeaderElectionBean); &#125; &#125; catch (Exception e) &#123; LOG.warn(\"Failed to unregister with JMX\", e); &#125; self.jmxLeaderElectionBean = null; LOG.debug(\"Number of connection processing threads: &#123;&#125;\", manager.getConnectionThreadCount()); &#125; &#125; 1.1sendNotifications自己的投票提议给其他server - 保存需要投票的数据到sendqueue中/** * Send notifications to all peers upon a change in our vote */private void sendNotifications() &#123; for (QuorumServer server : self.getVotingView().values()) &#123; long sid = server.id; ToSend notmsg = new ToSend(ToSend.mType.notification, proposedLeader,//当前server的myid proposedZxid,//当前server的zxid logicalclock.get(),//epoch,当前发起提议的epoch QuorumPeer.ServerState.LOOKING,//当前server的状态，肯定looking，因为是选举 sid,//当前server的zxid proposedEpoch);//当前票据的epoch，默认是currentEpoch。有可能这个跟上面的 logicalclock.get()获取的值不同。假设当前服务器是刚启动的，那么logicalclock是刚初始化，则 logicalclock.get()等于1。可能服务器砸启动之前就已经参加过几轮的选举，假设currentepoch的值是4.那么此时就不一样。 if(LOG.isDebugEnabled())&#123; LOG.debug(\"Sending Notification: \" + proposedLeader + \" (n.leader), 0x\" + Long.toHexString(proposedZxid) + \" (n.zxid), 0x\" + Long.toHexString(logicalclock.get()) + \" (n.round), \" + sid + \" (recipient), \" + self.getId() + \" (myid), 0x\" + Long.toHexString(proposedEpoch) + \" (n.peerEpoch)\"); &#125; sendqueue.offer(notmsg); &#125;&#125; ​ 最后把消息放到LinkedBlockingQueue sendqueue; 中。但是什么时候发送呢？看到阻塞队列我们肯定想到生产者和消费者模式，那么也就是说，肯定有一个线程在实时的去读取sendqueue中的队列，然后进行发送。 ​ 在那里取得sendqueue中自己的票据进行发送呢？ 答案是：WorkerSender线程。 ​ 是在startLeaderElection()方法中创建的listener中创建的WorkerSender线程。在那个时候，WorkerSender线程已经启动，并开始执行从sendqueue队列中取出自己要发给其他服务器的票据。 ​ 那么代码的执行逻辑走到 – &gt; sendNotifications（）方法之后，也就是上面的run方法（）的 sendNotifications（）方法之后。 1.2处理接受到其他服务端的投票数据 - 确定最终leader​ 首先我们要知道收到的投票数据，是放在recvQueue中的。 ​ 那么recvQueue中的数据，是打哪儿来的？是在startLeaderElection()方法中创建的listener中创建的RecvWorker线程接受后放进去的。 //如果还是是looking状态，我们会一直去和其他节点交互信息，直到选举出leader//while循环知道直到确定leader while ((self.getPeerState() == ServerState.LOOKING) &amp;&amp; (!stop))&#123; //从接收队列中拿到投票信息，拿到其他server的投票信息。 //疑问？？ //recvQueue中的是数据是哪里来的呢？我们还记得在startLeaderElection()方法中创建的listener中创建的RecvWorker线程么？ //就是通过这个线程去接受其他服务端的投票，然后放到recvQueue中。 Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS); if(n == null)&#123;//这里为空，说明，目前没有收到其他服务器发送过来的投票数据 if(manager.haveDelivered())&#123; //检查所有的队列是否为空 sendNotifications(); //如果为空发送通知 &#125; else &#123; manager.connectAll(); //如果没有投递出去，可能是其他server还没有启动，尝试连接 &#125; /* * Exponential backoff */ int tmpTimeOut = notTimeout*2; notTimeout = (tmpTimeOut &lt; maxNotificationInterval? tmpTimeOut : maxNotificationInterval); LOG.info(\"Notification time out: \" + notTimeout); &#125; //判断收到的投票的sid, //这里判断的是收到的sid是不是属于当前集群内的 else if (validVoter(n.sid) &amp;&amp; validVoter(n.leader)) &#123; switch (n.state) &#123;//判断当前票据状态状态 - 因为是选举阶段，那么票据肯定是LOOKING状态 case LOOKING: //收到其他服务的投票的epoch是不是比当前选举的epoch要大，如果大那么代表是新一轮选举 //那就说明，自己现在的投票轮数不是最新的，那么需要更新为最新，然后再参与投票。例如别人都已经是第三轮投票了，你现在还是第二轮。 if (n.electionEpoch &gt; logicalclock.get()) &#123; logicalclock.set(n.electionEpoch); //更新自己的epoch为当前最新的epoch recvset.clear(); //清空收到的投票 //进行投票 /* totalOrderPredicate方法逻辑 * 1.收到的epoch大于当前的epoch 胜出选举 * 2.如果收到的epoch等于当前epoch,那么收到的zxid大于当前zxid胜出选举 * 3.如果收到的epoch等于当前epoch，zxid等于当前zxid, * 4.那么收到的myid大于当前myid的胜出选举 */ if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) &#123; //Proposal默认就是当前服务器的myid、zxid、epoche。 //但是如果跟其他服务器发送过来的票据相比，比对没有胜出，那么就说明，我不能当选leader，胜出的票据代表的服务获得当选leader的机会，那么我就要自己放弃对自己的投票，投那台服务器一票 updateProposal(n.leader, n.zxid, n.peerEpoch); //把胜出的消息更新到投票提议中 &#125; else &#123; //如果收到消息没有胜出，那么选择当前的消息（有可能是自己的，也有可能是上一次比对胜出的）更新到投票提议中 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125;//也就是说这段代码的作用就是，决定自己要给谁投一票。也有可能是给自己投一票。 sendNotifications(); //发送最新的投票消息，开始下一次循环的判断 &#125; else if (n.electionEpoch &lt; logicalclock.get()) &#123; //如果收到的逻辑时钟（epoch）小，那么表示这个投票无效。 if(LOG.isDebugEnabled())&#123; LOG.debug(\"Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x\" + Long.toHexString(n.electionEpoch) + \", logicalclock=0x\" + Long.toHexString(logicalclock.get())); &#125; break; //如果收到的逻辑时钟epoch相等，则去对比myid 、zxid、epoch &#125; else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123; updateProposal(n.leader, n.zxid, n.peerEpoch); sendNotifications(); &#125; if(LOG.isDebugEnabled())&#123; LOG.debug(\"Adding vote: from=\" + n.sid + \", proposed leader=\" + n.leader + \", proposed zxid=0x\" + Long.toHexString(n.zxid) + \", proposed election epoch=0x\" + Long.toHexString(n.electionEpoch)); &#125; //把对方发送过来的票据存到本地，用来做最终判断 recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); //判断选举是否结束，默认算法过半同意 - 最终判断 if (termPredicate(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch))) &#123; // Verify if there is any change in the proposed leader while((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null)&#123; if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch))&#123; recvqueue.put(n);//获得最新的记过 break; &#125; &#125; /* * This predicate is true once we don't read any new * relevant message from the reception queue */ if (n == null) &#123; self.setPeerState((proposedLeader == self.getId()) ? ServerState.LEADING: learningState()); Vote endVote = new Vote(proposedLeader, proposedZxid, proposedEpoch); leaveInstance(endVote); return endVote; &#125; &#125; break; case OBSERVING: //如果是 LOG.debug(\"Notification from observer: \" + n.sid); break; case FOLLOWING: case LEADING: /* * Consider all notifications from the same epoch * together. */ if(n.electionEpoch == logicalclock.get())&#123; recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); if(termPredicate(recvset, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); Vote endVote = new Vote(n.leader, n.zxid, n.peerEpoch); leaveInstance(endVote); return endVote; &#125; &#125; outofelection.put(n.sid, new Vote(n.leader, IGNOREVALUE, IGNOREVALUE, n.peerEpoch, n.state)); if (termPredicate(outofelection, new Vote(n.leader, IGNOREVALUE, IGNOREVALUE, n.peerEpoch, n.state)) &amp;&amp; checkLeader(outofelection, n.leader, IGNOREVALUE)) &#123; synchronized(this)&#123; logicalclock.set(n.electionEpoch); self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); &#125; Vote endVote = new Vote(n.leader, n.zxid, n.peerEpoch); leaveInstance(endVote); return endVote; &#125; break; default: LOG.warn(\"Notification state unrecoginized: \" + n.state + \" (n.state), \" + n.sid + \" (n.sid)\"); break; &#125; &#125; else &#123; if (!validVoter(n.leader)) &#123; LOG.warn(\"Ignoring notification for non-cluster member sid &#123;&#125; from sid &#123;&#125;\", n.leader, n.sid); &#125; if (!validVoter(n.sid)) &#123; LOG.warn(\"Ignoring notification for sid &#123;&#125; from non-quorum member sid &#123;&#125;\", n.leader, n.sid); &#125; &#125; &#125; 通过上面的决策，我们就可以得出leader是那台服务器。 参考：https://blog.csdn.net/u010994966/article/details/95937323 如果集群中，某台server挂掉，那么怎么重新进行选举？org.apache.zookeeper.server.quorum.QuorumPeer.run() 首先挂掉，分两种情况：follower挂掉、leader挂掉。 ​ 首先，如果是leader挂掉怎么重新选举？把自己状态设置为loking，重新选举。此时follower同步leader数据会报异常。 ​ 如果是follower挂掉，怎么判断是否需要选举? leader会循环去判断，去ping所有的follower，判断是否有超过半数的follower已经ping不通，如果是，就会shutdown自己，然后把自己状态设置为loking，重新选举。 新加入的服务器怎么进行选举？怎么知道哪台是leader我们知道新机器的状态肯定是looking。 首先，如果在加入之前，集群中已经选出了leader，那么还是会走投票流程，然后确定leader（并不是重新选举，此时的投票只是为了让新加入的机器，确认哪台是leader） leader选举总结​ 在选举过程中，每台机器都会相互发送一个投票数据（Vote），如果是首轮leader 选举，那么epoch是1，每次投票epoch都会自增1. ​ 怎么决定哪个server当leader 呢？会根据vote的三个参数进行判断。 ​ 1.首先比较epoch： 收到的epoch大于当前的epoch 胜出选举2.epoch相等比较zxid：数据最新者胜出 如果收到的epoch等于当前epoch,那么收到的zxid大于当前zxid胜出选举3.zxid相等，比较myid： 如果收到的epoch等于当前epoch，zxid等于当前zxid, 那么收到的myid大于当前myid的胜出选举 ​ 举个例子，假设集群中有三台机器，那么需要至少启动两台才能够完成leader选举。这里为了方便说明，我就假设只启动两台，来看他们是怎么投票的。 ​ server 是否启动 a 是 b 是 c 否 ​ 因为是新集群，所以他们的数据都是一致的，所以zxid都是1。因为是第一轮投票，所以epoch是1. ​ a服务器启动，因为是LOOKING状态，那么就需要走选举leader流程，那么默认投自己一票，给其他server发送投票信息 vote，并监听其他服务器发送给自己的投票信息。 ​ b服务器启动，因为是LOOKING状态，那么就需要走选举leader流程，那么默认投自己一票，给其他server发送投票信息 vote，并监听其他服务器发送给自己的投票信息。此时收到a服务器的投票信息vote，开始比较投票的信息，此时一共有两个投票。那么因为epoch和zxid都是相等的，那么myid大的获胜，此时b服务胜出。所以b服务器丢弃a服务器的投票信息，还是投自己一票。给其他server发送投票信息 vote。b服务器第一轮投票结束。开始第二轮 a服务器此时收到b服务器的投票信息vote，开始比较投票的信息，此时一共有两个投票。那么因为epoch和zxid都是相等的，那么myid大的获胜，此时b服务胜出。所以a服务器丢弃自己服务器的投票信息，改投b服务器一票。给其他server发送投票信息 vote。a服务器第一轮投票结束。开始第二轮 接着进行第二轮投票，此时epoch递增为2。 ​ 同理a服务器向b发送投票信息。vote，那么b服务器收到之后，检测，发现对于自己的被选的票数已经大于半数，所以b服务器升级为leader。 ​ a服务器收到的b服务器发送的投票信息，vote，检测，发现对于b服务器被选的票数已经大于半数，所以确定b服务器升级为leader。，那么a服务器设置为follower。 接着开始同步集群数据，所有learner都向leader同步数据。（此时集群还没有启动，还不能接受客户端请求） 数据同步成功后，所有服务器，各自调用zkserver.start（）启动服务。开始接受处理客户端的请求。 整个流程图： 怎么保证数据一致性（已经确定leader）首先我们要明确一点，那就是此时，整个集群各台服务器的角色已经确定好了。接下来就是learner和leader数据同步 看到这个问题，我们首先考虑一下，什么时候会发生数据不一致？ 一台新服务器加入集群。这台服务器是没有数据的。 原先存在集群中的服务器，突然挂掉了。数据不是最新的 原先存在集群中的服务器，正在写数据（事务日志已经写了），突然挂掉了。数据不是最新的 ​ 上面这三种情况，在重启服务器后，都会向leader请求同步数据，保持数据一致性，在这期间，他们不接受任何请求。（刚启动的服务器，状态都是LOOKING，然后发起投票，从而获得现在哪台是leader，然后跟leader同步数据 – 注意这里的投票并不是重新选举leader，而是让重启的服务器确定哪台是leader） 执行loadDataBase加载本地快照数据到内存实际上就是通过 loadDataBase(); 进行再服务器启动的时候，通过日志快照重放数据到内存。 我们知道服务器启动的时候，会启动线程：quorumPeer.start() 它里面的代码就是： @Overridepublic synchronized void start() &#123; loadDataBase();//加载快照数据到内存，重点讲 cnxnFactory.start(); // startLeaderElection(); super.start();//重点讲 &#125; 接着调用 zkDb.loadDataBase(); public long loadDataBase() throws IOException &#123; long zxid = snapLog.restore(dataTree, sessionsWithTimeouts, commitProposalPlaybackListener); initialized = true; return zxid;&#125; /** * this function restores the server * database after reading from the * snapshots and transaction logs * @param dt the datatree to be restored * @param sessions the sessions to be restored * @param listener the playback listener to run on the * database restoration * @return the highest zxid restored * @throws IOException */public long restore(DataTree dt, Map&lt;Long, Integer&gt; sessions, PlayBackListener listener) throws IOException &#123; snapLog.deserialize(dt, sessions); return fastForwardFromEdits(dt, sessions, listener);&#125; 调用fastForwardFromEdits()重放日志快照数据到内存 public long fastForwardFromEdits(DataTree dt, Map&lt;Long, Integer&gt; sessions, PlayBackListener listener) throws IOException &#123; FileTxnLog txnLog = new FileTxnLog(dataDir);//获取快照日志 TxnIterator itr = txnLog.read(dt.lastProcessedZxid+1);//从最新一次事务 long highestZxid = dt.lastProcessedZxid; TxnHeader hdr; try &#123; while (true) &#123; // iterator points to // the first valid txn when initialized hdr = itr.getHeader(); if (hdr == null) &#123; //empty logs return dt.lastProcessedZxid; &#125; if (hdr.getZxid() &lt; highestZxid &amp;&amp; highestZxid != 0) &#123; LOG.error(\"&#123;&#125;(higestZxid) &gt; &#123;&#125;(next log) for type &#123;&#125;\", new Object[] &#123; highestZxid, hdr.getZxid(), hdr.getType() &#125;); &#125; else &#123; highestZxid = hdr.getZxid(); &#125; try &#123; processTransaction(hdr,dt,sessions, itr.getTxn());//更新数据到内存中 &#125; catch(KeeperException.NoNodeException e) &#123; throw new IOException(\"Failed to process transaction type: \" + hdr.getType() + \" error: \" + e.getMessage(), e); &#125; listener.onTxnLoaded(hdr, itr.getTxn()); if (!itr.next()) break; &#125; &#125; finally &#123; if (itr != null) &#123; itr.close(); &#125; &#125; return highestZxid;&#125; 启动quorumPeer线程执行run方法​ 通过上面的领导选举学习，我们知道QuorumPeer的run方法，核心的内容包括，领导选举。首先根据启动服务器的状态（LOOKING, FOLLOWING, LEADING, OBSERVING;）走相应的逻辑。 那么只有确定了leader，我们才能够往下了解follower/observer跟leader的通信过程。 通信的目的，就是为了实时同步leader的最新数据。 不然上面的loadDataBase只是加载本地的快照信息，可能不是目前集群中最新的数据 leader为每一个learner开启线程接受请求。​ 假设通过领导选举成功，当前服务器是leader，那么他就会执行。QuorumPeer的run方法的LEADER代码块。 case LEADING: LOG.info(\"LEADING\"); try &#123; setLeader(makeLeader(logFactory)); leader.lead();//执行lead方法 setLeader(null); &#125; catch (Exception e) &#123; LOG.warn(\"Unexpected exception\",e); &#125; finally &#123; if (leader != null) &#123; leader.shutdown(\"Forcing shutdown\"); setLeader(null); &#125; setPeerState(ServerState.LOOKING); &#125; break; lead()方法 void lead() throws IOException, InterruptedException &#123; 。。。。。 // Start thread that waits for connection requests from // new followers. cnxAcceptor = new LearnerCnxAcceptor();//为每一个learner开启一个处理线程 cnxAcceptor.start();//启动LearnerCnxAcceptor线程 。。。。。。忽略部分代码 （1）LearnerCnxAcceptor的run方法 @Overridepublic void run() &#123; try &#123; while (!stop) &#123; try&#123; Socket s = ss.accept();//阻塞等待learner的连接，这里使用的是bio的方式 // start with the initLimit, once the ack is processed // in LearnerHandler switch to the syncLimit s.setSoTimeout(self.tickTime * self.initLimit); s.setTcpNoDelay(nodelay); BufferedInputStream is = new BufferedInputStream( s.getInputStream()); LearnerHandler fh = new LearnerHandler(s, is, Leader.this);//learner连接后，新建LearnerHandler线程处理连接 - 也就是我们上面所说的，为每一个learner的请求创建一个单独的线程处理 fh.start(); （2）查看LearnerHandler的run方法 learner请求leader同理，当前服务器如果是follower或者observer，那么也会走。QuorumPeer的run方法的follower/observer代码块。 实际上，observer的代码块跟follower的代码块执行的逻辑差不多，那么我们这里只看一下follower的代码块 case FOLLOWING: try &#123; LOG.info(\"FOLLOWING\"); setFollower(makeFollower(logFactory)); follower.followLeader();//关键代码 &#125; catch (Exception e) &#123; LOG.warn(\"Unexpected exception\",e); &#125; finally &#123; follower.shutdown(); setFollower(null); setPeerState(ServerState.LOOKING); &#125; break; follower.followLeader()请求leader void followLeader() throws InterruptedException &#123; 。。。。。。 try &#123; QuorumServer leaderServer = findLeader();//首先找到leander相关的信息，方便后面创建连接 try &#123; connectToLeader(leaderServer.addr, leaderServer.hostname);//建立跟leader的连接 long newEpochZxid = registerWithLeader(Leader.FOLLOWERINFO);//这里会向leader发送数据 。。。。。。。。。。。 开始同步数据经过上面的leader跟learner的连接和设置目前最大投票版本号，接下来就需要开始进行数据同步。 leader怎么处理首先我们知道，所有的learner的数据肯定要以leader的为主。如果learner的数据多了（事务id比leader的大），那么leader会发送消息让learner回滚数据。 ​ 那么如果learner的数据不是最新的（zxid事务id比leader小） ​ 第一种情况， learner是刚加入集群的，什么数据都没有，那么怎么同步leader的数据呢？ 首先，leader发送他自己的快照给learner，但是leader的快照可能不是最新的，因为我们知道快照是每隔一部分client的写请求就打一次，并不是每次请求都打。所以leader仅仅只是发送他自己的快照远远不够，那么我们这个时候就想，能不能发leader已经收到的所有更新请求日志呢？也一并发给learner。 那么learner一共收到两份东西，一份是：leader快照、一份是更新日志。 这样，learner就可以根据快照和操作日志，同步数据。 疑问？那么快照我们知道在dataDir目录下，但是操作日志在哪里呢？我们还记得服务端处理client请求时经过的处理链，最后一个final处理器，他的org.apache.zookeeper.server.FinalRequestProcessor.processRequest(Request)处理方法中，有一段代码。 // do not add non quorum packets to the queue. if (Request.isQuorum(request.type)) &#123; zks.getZKDatabase().addCommittedProposal(request); &#125;会把所有的请求都保存到 protected LinkedList&lt;Proposal&gt; committedLog = new LinkedList&lt;Proposal&gt;();中。 所以committedLog就是我们需要找的更新日志 第二种情况，learner是有数据的，但是数据不是最新的（zxid事务id比leader小）。 那么这种情况，很明显就不需要发送leader快照了，我们只需要发送learner跟leader差异的部分，即可。例如learner当前zxid是20，但是leader的zxid是50，那么我们只需要从leader发送21-50的部分操作日志给learner即可。 我们接下来看同步数据代码的区域：org.apache.zookeeper.server.quorum.LearnerHandler.run()。也是在leader为每个learner分配的处理连接的线程中。 /* the default to send to the follower */ int packetToSend = Leader.SNAP;//默认传送leader快照 long zxidToSend = 0; long leaderLastZxid = 0; /** the packets that the follower needs to get updates from **/ long updates = peerLastZxid; /* we are sending the diff check if we have proposals in memory to be able to * send a diff to the */ ReentrantReadWriteLock lock = leader.zk.getZKDatabase().getLogLock(); ReadLock rl = lock.readLock(); try &#123; rl.lock(); final long maxCommittedLog = leader.zk.getZKDatabase().getmaxCommittedLog(); final long minCommittedLog = leader.zk.getZKDatabase().getminCommittedLog(); LOG.info(\"Synchronizing with Follower sid: \" + sid +\" maxCommittedLog=0x\"+Long.toHexString(maxCommittedLog) +\" minCommittedLog=0x\"+Long.toHexString(minCommittedLog) +\" peerLastZxid=0x\"+Long.toHexString(peerLastZxid)); LinkedList&lt;Proposal&gt; proposals leader.zk.getZKDatabase().getCommittedLog();//可以看到会去获取final处理器中保存的已经提交的操作日志 if (peerLastZxid == leader.zk.getZKDatabase().getDataTreeLastProcessedZxid()) &#123;//如果learner跟leader的事务id一致，那么说明数据一致，不需要同步 // Follower is already sync with us, send empty diff LOG.info(\"leader and follower are in sync, zxid=0x&#123;&#125;\", Long.toHexString(peerLastZxid)); packetToSend = Leader.DIFF; zxidToSend = peerLastZxid; &#125; else if (proposals.size() != 0) &#123;//说明当前服务器 LOG.debug(\"proposal size is &#123;&#125;\", proposals.size()); if ((maxCommittedLog &gt;= peerLastZxid) &amp;&amp; (minCommittedLog &lt;= peerLastZxid)) &#123; LOG.debug(\"Sending proposals to follower\"); // as we look through proposals, this variable keeps track of previous // proposal Id. long prevProposalZxid = minCommittedLog; // Keep track of whether we are about to send the first packet. // Before sending the first packet, we have to tell the learner // whether to expect a trunc or a diff boolean firstPacket=true; // If we are here, we can use committedLog to sync with // follower. Then we only need to decide whether to // send trunc or not packetToSend = Leader.DIFF; zxidToSend = maxCommittedLog; for (Proposal propose: proposals) &#123; // skip the proposals the peer already has if (propose.packet.getZxid() &lt;= peerLastZxid) &#123; prevProposalZxid = propose.packet.getZxid(); continue; &#125; else &#123; // If we are sending the first packet, figure out whether to trunc // in case the follower has some proposals that the leader doesn't if (firstPacket) &#123; firstPacket = false; // Does the peer have some proposals that the leader hasn't seen yet if (prevProposalZxid &lt; peerLastZxid) &#123; // send a trunc message before sending the diff packetToSend = Leader.TRUNC; zxidToSend = prevProposalZxid; updates = zxidToSend; &#125; &#125; queuePacket(propose.packet); QuorumPacket qcommit = new QuorumPacket(Leader.COMMIT, propose.packet.getZxid(), null, null); queuePacket(qcommit); &#125; &#125; &#125; else if (peerLastZxid &gt; maxCommittedLog) &#123;//如果learner的事务id大于leader的事务id，那么发送回滚删除命令给learner LOG.debug(\"Sending TRUNC to follower zxidToSend=0x&#123;&#125; updates=0x&#123;&#125;\", Long.toHexString(maxCommittedLog), Long.toHexString(updates)); packetToSend = Leader.TRUNC; zxidToSend = maxCommittedLog; updates = zxidToSend; &#125; else &#123; LOG.warn(\"Unhandled proposal scenario\"); &#125; &#125; else &#123; // just let the state transfer happen LOG.debug(\"proposals is empty\"); &#125; LOG.info(\"Sending \" + Leader.getPacketType(packetToSend)); leaderLastZxid = leader.startForwarding(this, updates); &#125; finally &#123; rl.unlock(); &#125;。。。。。 // Start sending packets new Thread() &#123; public void run() &#123; Thread.currentThread().setName( \"Sender-\" + sock.getRemoteSocketAddress()); try &#123; sendPackets(); &#125; catch (InterruptedException e) &#123; LOG.warn(\"Unexpected interruption\",e); &#125; &#125; &#125;.start();//开始发送数据给learner 经过上面的操作，leader把跟learner差异的数据发送给了learner，那么learner怎么处理呢？ learner怎么处理会在followLeader()方法中调用org.apache.zookeeper.server.quorum.Learner.syncWithLeader(long)。进行数据同步 根据 leader发送的qp.getType() 类型，处理相应的逻辑synchronized (zk) &#123; if (qp.getType() == Leader.DIFF) &#123; LOG.info(\"Getting a diff from the leader 0x&#123;&#125;\", Long.toHexString(qp.getZxid())); snapshotNeeded = false; &#125; else if (qp.getType() == Leader.SNAP) &#123; 。。。。。。 leader在哪里处理learner转发client的写请求？我们知道learner对于client客户端而言，如果发送更新请求，那么learner是会把写请求发送给leader进行处理的。 ​ 那么leader在哪里处理learner发送过来的更新请求呢？就在Learnerhandler的run方法中： case Leader.REQUEST: bb = ByteBuffer.wrap(qp.getData()); sessionId = bb.getLong(); cxid = bb.getInt(); type = bb.getInt(); bb = bb.slice(); Request si; if(type == OpCode.sync)&#123; si = new LearnerSyncRequest(this, sessionId, cxid, type, bb, qp.getAuthinfo()); &#125; else &#123; si = new Request(null, sessionId, cxid, type, bb, qp.getAuthinfo()); &#125; si.setOwner(this); leader.zk.submitRequest(si); break; ########集群模式 endzk实现分布式锁和分布式配置中心实现分布式锁实际上实现分布式锁的方式有两种： ​ 使用类似lock的规则，多个客户端同时去创建一个临时节点，创建成功者则代表他获得锁，否则就监听节点的删除操作（代表释放锁），如果节点被删除那么会通知所有客户端再去创建同样的临时节点，同理创建成功则获取锁。以此类推。 那么这种方式的好处就是实现非常简单，但是坏处就是容易发生惊群效应。锁被释放时，都会去通知所有监听该节点的客户端，然后所有客户端同时请求zk创建节点以求获取锁。那么这样对于zk而言就会造成没有必要的请求（虽然zk服务端也会把请求放到队列中一个一个的处理的，但是能够处理成功的也就只有一个，后面的创建节点都会失败。） 多个客户端同时创建多个临时顺序节点，序号最小的获取锁。然后次小的序号监听当前序号的释放。以此类推。那么只要锁被释放那么监听该节点的客户端 收到事件然后获取锁。 这样就解决了第一种实现方式的弊端。使用这种方式，每次都只通知比他序号小的那个节点获取锁。不用通知所有客户端。 他的思路就相当于juc包里面，lock或者synchronized的实现，每次都尝试获取锁–也就是判断自己创建的节点是不是序号最小的（类似于Lock的trylock方法），如果不是那么就监听比他小的那个节点的释放（类似于Lock的，tryLock失败后，那么阻塞，加入aqs阻塞队列等待唤醒）（而且在juc中每次都会从aqs阻塞队列中获取队列首节点获取锁类似于zk 的当前节点被删除后，那么就通知他的后一个节点获取锁） 也就是说，这种方式的实现，类似于Lock公平锁模式的实现。 zk 的什么特性保证了分布式锁的实现呢： ​ 监听机制 ​ 相同目录下节点名称不能相同 ​ 临时顺序节点 ​ 关于更新请求统一逐个交由leader进行处理，其他learner只能读不能处理写请求。 ​ 数据的顺序一致性 ​ 下面就粗略的实现了分布式锁的代码，但是并没有保证线程安全。只是个参考，重要的是实现的思路，因为在真是开发中，我们肯定使用的是已经封装好的，一般不会自己写。 package org.apache.zookeeper.kingge;import java.io.IOException;import java.util.Collections;import java.util.List;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType;import org.apache.zookeeper.ZooDefs;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;//可以看到整体的思路是模仿了juc中Lock的实现。public class ZKLock &#123; private String connectString = \"localhost:2181,localhost:2182\";//zk集群地址 private ZooKeeper client = null;//zk客户端 private final String PARENT_PATH = \"/lock\";//我们在这个节点下面创建临时顺序节点 private String createNodeName;//创建节点名称 public void initZKClient() &#123; if( client == null ) try &#123; client = new ZooKeeper(connectString, 2000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; System.out.println( \"zk默认监听器收到事件：\"+ event ); &#125; &#125;); Stat exists = client.exists(PARENT_PATH, false); if( exists == null ) &#123;//说明父节点不存在，则需要创建 client.create(PARENT_PATH, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // &#125; public void lock() &#123; if(tryLock()) &#123; System.out.println( \"成功获取锁\" ); &#125; &#125; public boolean tryLock() &#123; String nodeName = PARENT_PATH+\"/zk_\";//创建子节点路径 try &#123; //创建节点 createNodeName = client.create(nodeName, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);//这里返回的是创建节点的全路径，例如/lock/zk_0 //判断当前节点是否是最小节点 List&lt;String&gt; children = client.getChildren(PARENT_PATH, false);//这里返回的节点类型是 zk_0,zk_1,zk_2 Collections.sort(children); String minNodeName = children.get(0);//获得最小的节点名称 //比较节点 if( createNodeName.equals(PARENT_PATH+\"/\"+minNodeName) ) &#123; System.out.println( createNodeName +\" 是最小节点，成功获取锁\" ); return true;//说明当前创建的节点是最小节点，那么获取锁 &#125;else &#123;//否则，监听比他小的节点 //获取当前节点在children中下标位置 String substring = createNodeName.substring(createNodeName.lastIndexOf(\"/\")+1);//实际上就是截取出类似于，zk_0这样的字符串 int indexOf = children.indexOf(substring); //获取他前一个节点 String preNode = children.get(indexOf-1); final CountDownLatch countDownLatch = new CountDownLatch(1); //然后监听该节点是否存在 client.exists(PARENT_PATH+\"/\"+preNode, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; if( Event.EventType.NodeDeleted.equals(event.getType()) ) &#123;//节点删除，那么说明当前节点可以获取锁 System.out.println( createNodeName + \"，开始获取锁\" ); countDownLatch.countDown(); &#125; &#125; &#125;); System.out.println( createNodeName +\"，节点等待锁中。。。\" ); countDownLatch.await();//阻塞等待，当前节点获取锁。不阻塞的话当前方法执行完毕就直接返回了，上面监听判断获取锁的代码就不会生效了。 return true; &#125; &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return false; &#125; public void unLock() &#123; //其实这一步不用实现，因为临时节点，断开连接后自动会删除节点。但是为了代码逻辑的完整性，这里还是手动删除一下节点 try &#123; client.delete(createNodeName, -1);//这里的-1是版本号，表示在删除节点的时候不需要检查版本号。因为zk在进行删除操作的时候 //会检查客户端发送过来的版本号跟服务端节点的版本号是否一致，如果是才能删除，否则删除失败。 createNodeName = \"\"; client.close(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; ZKLock zkLock = new ZKLock(); zkLock.initZKClient(); zkLock.tryLock(); &#125; &#125; 分布式配置中心​ 目的就是，统一管理所有服务器的配置信息，这样我们只需要在配置中心修改后，所有服务器自动同步修改后的配置信息。（是不是马上就想到了监听） ​ package org.apache.zookeeper.kingge;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.concurrent.TimeUnit;import org.apache.curator.framework.CuratorFramework;import org.apache.curator.framework.CuratorFrameworkFactory;import org.apache.curator.framework.recipes.cache.PathChildrenCache;import org.apache.curator.framework.recipes.cache.PathChildrenCacheEvent;import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;import org.apache.curator.retry.RetryNTimes;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.data.Stat;public class ZKConfig &#123; private String connectString = \"localhost:2181,localhost:2182\";//zk集群地址 private Map&lt;String,String&gt; cache = new HashMap&lt;&gt;();//配置信息的缓存 private CuratorFramework client;//这里使用的是curator框架创建zk客户端 private final String PARENT_PATH = \"/config\";//存放配置信息的根节点 public ZKConfig() &#123; this.client = CuratorFrameworkFactory.newClient(connectString, new RetryNTimes(3, 1000)); client.start(); init(); &#125; public void init() &#123; try &#123; //初始化所有配置项的信息到本地缓存 List&lt;String&gt; forPath = client.getChildren().forPath(PARENT_PATH); for (String name : forPath) &#123; //根据路径获取，每个节点的配置信息 String value = new String( client.getData().forPath(PARENT_PATH + \"/\" + name) ); cache.put(name, value); &#125; //同时监听在PARENT_PATH目录下面所有孩子的 //新增，删除，修改操作。如果发生了以上三个事件，那么就需要在监听器中同步更新cache配置信息 PathChildrenCache watcher = new PathChildrenCache(client, PARENT_PATH, true); watcher.getListenable().addListener(new PathChildrenCacheListener() &#123; @Override public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception &#123; String path = event.getData().getPath();//获得节点的全路径 例如/config/zk1 if( path.startsWith(PARENT_PATH) ) &#123;//表示如果事件是发生在PARENT_PATH下面的节点之上，才会执行下面逻辑。 String key = path.replace(PARENT_PATH+\"/\", \"\");//也就是抽取出节点的后缀路径 -- /config/zk1 ---》 zk1 if( PathChildrenCacheEvent.Type.CHILD_ADDED.equals(event.getType()) ) &#123;//新增了子节点 System.out.println( \"触发了 CHILD_ADDED 事件\" ); cache.put(key, new String(event.getData().getData())); &#125; if( PathChildrenCacheEvent.Type.CHILD_UPDATED.equals(event.getType()) ) &#123;//修改了子节点 System.out.println( \"触发了 CHILD_UPDATED 事件\" ); cache.put(key, new String(event.getData().getData())); &#125; if( PathChildrenCacheEvent.Type.CHILD_REMOVED.equals(event.getType()) ) &#123;//删除了子节点 System.out.println( \"触发了 CHILD_REMOVED 事件\" ); cache.remove(key); &#125; &#125; &#125; &#125;); watcher.start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void saveNodeConfig(String name,String value) &#123;//存放配置信息到某个节点 String lookPath = PARENT_PATH + \"/\" + name; try &#123; Stat forPath = client.checkExists().forPath(lookPath); if( forPath == null ) &#123;//不存在则创建节点，并保存配置信息 client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(lookPath,value.getBytes()); &#125;else &#123;//节点存在则更新值 client.setData().forPath(lookPath,value.getBytes()); &#125; cache.put(name, value); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public String getNodeConfig(String name) &#123; return cache.get(name); &#125; public static void main(String[] args) throws InterruptedException &#123; ZKConfig config = new ZKConfig(); config.saveNodeConfig(\"erukaServers\", \"192.168.1.1,192.168.1.2\"); for (int i = 0; i &lt; 100; i++) &#123; System.out.println( \"获取配置信息：\"+config.getNodeConfig(\"erukaServers\") ); System.out.println( config.cache ); TimeUnit.SECONDS.sleep(5); &#125; &#125; &#125; 负载均衡实际上他跟配置中心的实现也是一样的 ！！！！！疑问！！！！！！！！！！！ ​ 上面说加入OB服务器，虽然不参与投票，那么zk转化为AP，那么假设ob服务器启动后，怎么同步leader的数据？？？？怎么保证数据一致性？ 答案：不会，learner启动后，必须跟leader同步数据完成后，才会调用 zk.startup()启动服务，接受客户端请求。 ​ 如果某个节点，没有同步数据，那么客户端会不会读到不是最新的数据（假设有5台机器，其中三台写入数据成功，那么就认为整个更新操作成功。也就是说，客户端的读请求会不会发送到这两台未同步最新数据的server 上？） 答案：可能会读到旧数据，因为此时，这两台可能还没有进入同步leader数据的逻辑，那么这个时候刚好客户端发来请求，那么就会访问到旧数据。所以zk是保证在一定时间范围内数据是一致性的-是弱一致性 ​ 为什么leader跟follower的通讯用的是bio而不是nio？（leader.lead（）方法，可以看到leader为每一个learner的交互使用bio且都new了一个线程） 暂无答案 数据同步过程中，集群能够接受请求么？代码哪里证明？ 答案：在进行了数据同步之后，我们发现这个时候server还只是初始化状态，state并不是Running。 Leader.lead()方法中会调用 startZkServer();启动服务无，设置无服务状态为Running，并设置处理链。 private synchronized void startZkServer() &#123; // Update lastCommitted and Db's zxid to a value representing the new epoch lastCommitted = zk.getZxid(); LOG.info(\"Have quorum of supporters, sids: [ \" + getSidSetString(newLeaderProposal.ackSet) + \" ]; starting up and setting last processed zxid: 0x&#123;&#125;\", Long.toHexString(zk.getZxid())); zk.startup();//启动服务，开始接受请求 /* * Update the election vote here to ensure that all members of the * ensemble report the same vote to new servers that start up and * send leader election notifications to the ensemble. * * @see https://issues.apache.org/jira/browse/ZOOKEEPER-1732 */ self.updateElectionVote(getEpoch()); zk.getZKDatabase().setlastProcessedZxid(zk.getZxid());&#125; public synchronized void startup() &#123; if (sessionTracker == null) &#123; createSessionTracker(); &#125; startSessionTracker(); setupRequestProcessors();//关键初始化处理链中的三个处理器 registerJMX(); setState(State.RUNNING);//关键，设置服务状态为运行状态，开始受理client业务 notifyAll(); &#125; 那么在learner中呢？ 逻辑也是如此。 follower.followLeader() – &gt; syncWithLeader（） 同步完leader数据后，马上启动。关键代码在syncWithLeader方法中： zk.startup();//跟leader一样，最终也是会调用 public synchronized void startup() &#123; if (sessionTracker == null) &#123; createSessionTracker(); &#125; startSessionTracker(); setupRequestProcessors(); registerJMX(); setState(State.RUNNING); notifyAll();&#125; ​ 所以，确保了，在leader跟learner同步数据完成后，整个集群才能够启动使用，否则在这期间是不会客户端业务的。保证了数据一致性。（而且只有这个时候，才能够初始化处理链，处理请求并把服务器状态标志位RUNning） 集群的处理链跟单机模式的处理链有何不同 ​ 我们知道单机模式下，server端是通过PrepRequestProcessor -》SyncRequestProcessor-》FinalRequestProcessor 这三个处理链进行处理客户端请求的。 ​ 那么集群模式下，也是这三个处理器么？答案：不是一样的，有些许区别。 ​ 因为leader、follower、observer对服务端（ZooKeeperServer）的实现都是不一样的，分别是LeaderZooKeeperServer，FollowerZooKeeperServer、ObserverZooKeeperServer，他们对 setupRequestProcessors()初始化处理链的实现都不同。 原生ZooKeeperServer的setupRequestProcessors（） protected void setupRequestProcessors() &#123; RequestProcessor finalProcessor = new FinalRequestProcessor(this); RequestProcessor syncProcessor = new SyncRequestProcessor(this, finalProcessor); ((SyncRequestProcessor)syncProcessor).start(); firstProcessor = new PrepRequestProcessor(this, syncProcessor); ((PrepRequestProcessor)firstProcessor).start();&#125; LeaderZooKeeperServer的setupRequestProcessors（） @Overrideprotected void setupRequestProcessors() &#123; RequestProcessor finalProcessor = new FinalRequestProcessor(this); RequestProcessor toBeAppliedProcessor = new Leader.ToBeAppliedRequestProcessor( finalProcessor, getLeader().toBeApplied); commitProcessor = new CommitProcessor(toBeAppliedProcessor, Long.toString(getServerId()), false, getZooKeeperServerListener()); commitProcessor.start(); ProposalRequestProcessor proposalProcessor = new ProposalRequestProcessor(this, commitProcessor); proposalProcessor.initialize(); firstProcessor = new PrepRequestProcessor(this, proposalProcessor); ((PrepRequestProcessor)firstProcessor).start();&#125; proposalProcessor：作用是发起投票请求的。因为集群模式下，更新操作需要投票。 ​ prep处理收到请求，然后使用proposal处理器，向learner发送操作提议，如果learner半数相应，那么说明可以提交数据，那么接着执行commit处理器，向learner发送提交数据请求。 ​ 执行commit处理器时，会阻塞，等待learner的ack确认（learner在自己服务器持久化成功后，发送ack请求），learner发送ack确认后，leader被唤醒，表示本次更新操作半数learner已经更新成功，继续执行接下来的处理器，leader最后执行final处理器持久化数据。 FollowerZooKeeperServer的setupRequestProcessors（） @Overrideprotected void setupRequestProcessors() &#123; RequestProcessor finalProcessor = new FinalRequestProcessor(this); commitProcessor = new CommitProcessor(finalProcessor, Long.toString(getServerId()), true, getZooKeeperServerListener()); commitProcessor.start(); firstProcessor = new FollowerRequestProcessor(this, commitProcessor); ((FollowerRequestProcessor) firstProcessor).start(); syncProcessor = new SyncRequestProcessor(this, new SendAckRequestProcessor((Learner)getFollower())); syncProcessor.start();&#125; ObserverZooKeeperServer的setupRequestProcessors（） @Overrideprotected void setupRequestProcessors() &#123; // We might consider changing the processor behaviour of // Observers to, for example, remove the disk sync requirements. // Currently, they behave almost exactly the same as followers. RequestProcessor finalProcessor = new FinalRequestProcessor(this); commitProcessor = new CommitProcessor(finalProcessor, Long.toString(getServerId()), true, getZooKeeperServerListener()); commitProcessor.start(); firstProcessor = new ObserverRequestProcessor(this, commitProcessor); ((ObserverRequestProcessor) firstProcessor).start(); /* * Observer should write to disk, so that the it won't request * too old txn from the leader which may lead to getting an entire * snapshot. * * However, this may degrade performance as it has to write to disk * and do periodic snapshot which may double the memory requirements */ if (syncRequestProcessorEnabled) &#123; syncProcessor = new SyncRequestProcessor(this, null); syncProcessor.start(); &#125;&#125; 压测一下zk的常用接口 例如压测一下新增节点操作，看是否存在相同节点创建等等问题。 zk的zxid zxid：事务id。 ​ 我们知道在数据同步和leader选举中，server的zxid是至关重要的，因为他表示这当前数据提交事务id，那么也就意味着， 他越大数据就越新，那么他成为leader的机会应该更大。 ​ zxid是一个64位的数字，高32位表示epoch（也就是，当前是第几轮的投票，每次有新的leader都会加1），低32位表示递增的序号（日志文件序号）。 ​ 我们在zk的存储目录中可以看到，生成的日志文件1000000001就是递增的序号，表示这是第一轮中产生的1号日志文件。其中currentEpoch表示当前是第几轮（新增一轮就表示有新的leader产生）。acceptedEpoch表示已经接受过的第几轮，一般而言，currentEpoch和acceptedEpoch是一样的。 ​ 既然每一轮都表示新的leader选出，那么就会对应着新的日志文件，然后根据低32位递增该轮的日志文件。保证了每轮都会有相应的日志（这样数据看起来就很清晰，知道这个日志文件是那一轮产生的，也方便数据同步的时候leader和learner的数据同步。leader只需要发送learner缺少的第几轮的日志即可，不需要发送之前旧轮数的数据。） 总而言之，新的leader会创建新的epoch（表示这是属于我的领导）。 zk保证了高并发的安全性​ ​ 我们知道，客户端的所有更新请求（cud，增删改），都是转发到leader服务器（无论单机模式还是集群模式），leader服务器对于请求，都会 放到一个队列中，逐个进行处理。 ​ zk是不是强一致性？是顺序一致性​ 首先理解一下强一致性的概念：要成功就一起成功，不允许存在不成功，数据必须是一致的。 ​ 很明显zk不是强一致性。zk的机制是过半提交成功，那么就认为整个更新请求是成功，此时其他没有成功ack的server是存在数据不一致的。（如果上面阐述了zk是强一致性，那么是有歧义的，望周知） ​ 那么zk是不是最终一致性呢？也不是。 zookeeper是顺序一致性。 什么叫顺序一致性：假设有一个Zookeeper集群（N&gt;=3，N为奇数），那么只有一个Leader（通过FastLeaderElection选主策略选取），所有的写操作（客户端请求Leader或Follower的写操作）都由Leader统一处理，Follower虽然对外提供读写，但写操作会提交到Leader，由Leader和Follower共同保证同一个Follower请求的顺序性，Leader会为每个请求生成一个zxid（高32位是epoch，用来标识leader选举周期，每次一个leader被选出来，都会有一个新的epoch，标识当前属于哪个leader的统治时期，低32位用于递增计数） 针对同一个Follower A提交的写请求request1、request2，某些Follower虽然可能不能在请求提交成功后立即看到（也就是强一致性），但经过自身与Leader之间的同步后，这些Follower在看到这两个请求时，一定是先看到request1，然后再看到request2，两个请求之间不会乱序，即顺序一致性。 画个图 ​ 也就是说，如果在B1读到的x是1，那么C0读到肯定也是1，那么B2肯定也是1。不会存在说，B1读到的是1，然后之后发生了回滚C0读到的x变成了0，那么这样就不满足顺序一致性。 ​ 也就是说在，时间轴上的操作，都是有顺序的。数据都是一致的，不会存在朝令夕改。 那么在zk中，使用队列和zxid保证了顺序一致性。 ​ 但是说是这么说，zk真的能够保证顺序一致性么？并不能！！！在多台客户端请求的时候并不能保证顺序一致性（也就是说可能在同步数据过程中，B1读到的x是1，但是C0读到的是0。因为存在网络延迟的原因，可能C0读到的是某一台learner还没有执行leader的commit操作，数据不是最新的） ​ 但是同一台客户端发出的请求读取到的数据肯定是一直的。例如客户端B，B2请求leader获取x数据这个时候，因为leader已经持久化诗句完毕，返回x是1。但是接着客户端B发出B3请求再次获取x数据的时候，请求到的是集群集群中的learner，那么因为这个时候这台learner还未完成数据的同步，那么此时B3请求获取到的x是0。但是客户端拿到x=0时会直接丢弃，因为他发现这个数据的zxid比上次最近获取的zxid还小，他认为不是最新的。 我们可以看官方的解释： ​ 他自己说了，zk无法保证在某个时刻，每一台zk服务，在两台客户端去连接的时候，能够获取到相同的数据。那么如果想让两个客户端读到相同的值，他提供了sync()函数帮助我们实现这个功能 也就是说，客户端B在获取某个节点数据之前，先调用sync方法，然后再去获取数据。这样就能够保证客户端B跟客户端A读到的值是一样的。 官方文档：http://zookeeper.apache.org/doc/r3.5.5/zookeeperProgrammers.html#ch_zkGuarantees zk涉及到的java基础知识 首先并发编程的wait和notify 在客户端发送请求给服务端后，会监听packet的请求是否处理完成，调用wait()阻塞等待返回。直到服务端处理完成，唤醒notify 队列相关知识 你会发现zk的客户端还是服务端，都充斥着各种队列，例如客户端发送的请求，都会封装到outgoingqueue队列中逐个处理。 服务端在处理请求的时候，会从队列中逐个拿出，然后传递给处理链，每个处理链中的处理器，都会有自己的队列，处理完成后，都会放到自己的队列中，然后交给下一个处理器处理（一共三个处理器，pre、sync、final） juc的countdownlatch ​ 服务端在启动的时候，就是利用了countdownlatch的机制，通过cd.await()实现了服务的挂起，直到cd.countdown()，那就说明server需要退出，然后才能继续处理cd.await()之后的代码，完成关闭server的后继工作。 生产者和消费者 责任链模式 我们知道不管是单机版本server还是集群版本，他们最终处理请求都是会经过一条处理链进行处理请求。 zk源码构建安装ANT 下载ant,地址：https://ant.apache.org/bindownload.cgi 下载后解压ant到硬盘目录，设置环境变量： 变量 值 备注 ANT_HOME D:\\apache-ant-1.10.2 新建变量 Path ;%ANT_HOME%\\bin 没有就新建变量，有则在内容后追加 CLASSPATH ;%ANT_HOME%\\lib 没有就新建变量，有则在内容后追加 验证ant是否安装成功 H:\\zookeeper-vip1&gt;ant -versionApache Ant(TM) version 1.10.7 compiled on September 1 2019 下载zk源码 3.下载Zookeeper源码 下载地址： https://github.com/apache/zookeeper 本次选用的是Zookeeper 3.4.14版本。 下载zip，然后解压 使用ant将Zookeeper源码编译成Eclipse工程 使用：ant eclipse命令 进入解压zk的根目录，使用ant eclipse 命令 $ ant eclipseBuildfile: E:\\workspace\\git\\zookeeper\\build.xmlant-eclipse-download: [get] Getting: http://downloads.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2 [get] To: E:\\workspace\\git\\zookeeper\\src\\java\\ant-eclipse-1.0.bin.tar.bz2 [get] http://downloads.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2 moved to https://nchc.dl.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2BUILD FAILEDE:\\workspace\\git\\zookeeper\\build.xml:1730: Redirection detected from http to https. Protocol switch unsafe, not allowed. 1.下载ant-eclipse-1.0.bin.tar.bz2失败，将源码build.xml中的 get src=”[http://downloads.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2]” 替换成如下地址 get src=”[http://ufpr.dl.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2]” 2.再次执行ant eclipse命令，等待时间有点长，等编译结束后即可导入eclipse中 如果再次执行ant eclipse命令，还是报上面的错误，那么需要把ant-eclipse-1.0.bin.tar.bz2单独下载下来 解决方案： 1、在浏览器中打开http://ufpr.dl.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2，并下载对应的文件 2、将文件copy 到 zookeeper源目录zookeeper-server/src/main/resources中 3、打开build.xml文件，找到需要下载的地方注释掉即可 再次输入命令ant eclipse，看到如下图所示，表示编译成功： 导入到eclipse中可能会提示Version类实现的Info接口不存在：https://www.dazhuanlan.com/2019/12/24/5e01b9f7e4fe0/ 启动server或clienthttps://blog.csdn.net/chinaCsdnV2/article/details/81049686","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://kingge.top/categories/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://kingge.top/tags/zookeeper/"},{"name":"zk源码，顺序一致性","slug":"zk源码，顺序一致性","permalink":"http://kingge.top/tags/zk源码，顺序一致性/"}]},{"title":"RocketMQ深入理解-源码分析","slug":"RocketMQ-深入理解","date":"2019-12-03T14:21:59.000Z","updated":"2020-05-09T09:08:09.159Z","comments":true,"path":"2019/12/03/RocketMQ-深入理解/","link":"","permalink":"http://kingge.top/2019/12/03/RocketMQ-深入理解/","excerpt":"","text":"1. 高级功能1.1 消息存储分布式队列因为有高可靠性的要求，所以数据要进行持久化存储。 消息生成者发送消息 MQ收到消息，将消息进行持久化，在存储中新增一条记录 返回ACK给生产者 MQ push 消息给对应的消费者，然后等待消费者返回ACK 如果消息消费者在指定时间内成功返回ack，那么MQ认为消息消费成功，在存储中删除消息，即执行第6步；如果MQ在指定时间内没有收到ACK，则认为消息消费失败，会尝试重新push消息,重复执行4、5、6步骤 MQ删除消息 1.1.1 存储介质 关系型数据库DB Apache下开源的另外一款MQ—ActiveMQ（默认采用的KahaDB做消息存储）可选用JDBC的方式来做消息持久化，通过简单的xml配置信息即可实现JDBC消息存储。由于，普通关系型数据库（如Mysql）在单表数据量达到千万级别的情况下，其IO读写性能往往会出现瓶颈。在可靠性方面，该种方案非常依赖DB，如果一旦DB出现故障，则MQ的消息就无法落盘存储会导致线上故障 文件系统 目前业界较为常用的几款产品（RocketMQ/Kafka/RabbitMQ）均采用的是消息刷盘至所部署虚拟机/物理机的文件系统来做持久化（刷盘一般可以分为异步刷盘和同步刷盘两种模式）。消息刷盘为消息存储提供了一种高效率、高可靠性和高性能的数据持久化方式。除非部署MQ机器本身或是本地磁盘挂了，否则一般是不会出现无法持久化的故障问题。 ###1.1.2 性能对比 文件系统&gt;关系型数据库DB 1.1.3 消息的存储和发送1）消息存储磁盘如果使用得当，磁盘的速度完全可以匹配上网络 的数据传输速度。目前的高性能磁盘，顺序写速度可以达到600MB/s， 超过了一般网卡的传输速度。但是磁盘随机写的速度只有大概100KB/s，和顺序写的性能相差6000倍！因为有如此巨大的速度差别，好的消息队列系统会比普通的消息队列系统速度快多个数量级。RocketMQ的消息用顺序写,保证了消息存储的速度。 ####2）消息发送 Linux操作系统分为【用户态】和【内核态】，文件操作、网络操作需要涉及这两种形态的切换，免不了进行数据复制。 一台服务器 把本机磁盘文件的内容发送到客户端，一般分为两个步骤： 1）read；读取本地文件内容； 2）write；将读取的内容通过网络发送出去。 这两个看似简单的操作，实际进行了4 次数据复制，分别是： 从磁盘复制数据到内核态内存； 从内核态内存复 制到用户态内存； 然后从用户态 内存复制到网络驱动的内核态内存； 最后是从网络驱动的内核态内存复 制到网卡中进行传输。 通过使用mmap的方式，可以省去向用户态的内存复制，提高速度。这种机制在Java中是通过MappedByteBuffer实现的 RocketMQ充分利用了上述特性，也就是所谓的“零拷贝”技术，提高消息存盘和网络发送的速度。 这里需要注意的是，采用MappedByteBuffer这种内存映射的方式有几个限制，其中之一是一次只能映射1.5~2G 的文件至用户态的虚拟内存，这也是为何RocketMQ默认设置单个CommitLog日志数据文件为1G的原因了 1.1.4 消息存储结构RocketMQ消息的存储是由ConsumeQueue和CommitLog配合完成 的，消息真正的物理存储文件是CommitLog，ConsumeQueue是消息的逻辑队列，类似数据库的索引文件，存储的是指向物理存储的地址。每 个Topic下的每个Message Queue都有一个对应的ConsumeQueue文件。 CommitLog：存储消息的元数据 可以看到CommitLog日志文件的大小确实是1G ConsumerQueue：存储消息在CommitLog的索引 保证了如何在CommitLog中快速找到消息，提升查询效率 可以看到，这个目录是保存了所有topic在，CommitLog中的索引。 那么每个topic目录下面，有对应的队列文件夹。保存了当前topic一共有几个队列，队列在commitlog中的位置 举个例子，生产者发送消息，那么就会在ConsumerQueue目录下创建一个文件夹TopicTest，然后里面保存有topic对应的队列。 Message msg = new Message(\"TopicTest\",\"TagA\" , (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET));SendResult sendResult = producer.send(msg); IndexFile：为了消息查询提供了一种通过key或时间区间来查询消息的方法，这种通过IndexFile来查找消息的方法不影响发送与消费消息的主流程 他的作用实际上跟consumerqueue一样，只不过是提供了另一种查询方式在CommitLog中查询消息 1.1.5 刷盘机制​ RocketMQ的消息是存储到磁盘上的，这样既能保证断电后恢复， 又可以让存储的消息量超出内存的限制。RocketMQ为了提高性能，会尽可能地保证磁盘的顺序写。消息在通过Producer写入RocketMQ的时 候，有两种写磁盘方式，分布式同步刷盘和异步刷盘。 1）同步刷盘​ 在返回写成功状态时，消息已经被写入磁盘。具体流程是，消息写入内存的PAGECACHE后，立刻通知刷盘线程刷盘， 然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，返回消息写 成功的状态。 2）异步刷盘​ 在返回写成功状态时，消息可能只是被写入了内存的PAGECACHE，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘动作，快速写入。 ####3）配置 同步刷盘还是异步刷盘，都是通过Broker配置文件里的flushDiskType 参数设置的，这个参数被配置成SYNC_FLUSH、ASYNC_FLUSH中的 一个。 1.2 高可用性机制 RocketMQ分布式集群是通过Master和Slave的配合达到高可用性的。 Master和Slave的区别：在Broker的配置文件中，参数 brokerId的值为0表明这个Broker是Master，大于0表明这个Broker是 Slave，同时brokerRole参数也会说明这个Broker是Master还是Slave。 Master角色的Broker支持读和写，Slave角色的Broker仅支持读，也就是 Producer只能和Master角色的Broker连接写入消息；Consumer可以连接 Master角色的Broker，也可以连接Slave角色的Broker来读取消息。 1.2.1 消息消费高可用​ 在Consumer的配置文件中，并不需要设置是从Master读还是从Slave 读，当Master不可用或者繁忙的时候（也就是说，默认先从master进行读消息），Consumer会被自动切换到从Slave 读。有了自动切换Consumer这种机制，当一个Master角色的机器出现故障后，Consumer仍然可以从Slave读取消息，不影响Consumer程序。这就达到了消费端的高可用性。 1.2.2 消息发送高可用​ 在创建Topic的时候，把Topic的多个Message Queue创建在多个Broker组上（相同Broker名称，不同 brokerId的机器组成一个Broker组），这样当一个Broker组的Master不可 用后，其他组的Master仍然可用，Producer仍然可以发送消息。 RocketMQ目前还不支持把Slave自动转成Master，如果机器资源不足， 需要把Slave转成Master，则要手动停止Slave角色的Broker，更改配置文 件，用新的配置文件启动Broker。 1.2.3 消息主从复制如果一个Broker组有Master和Slave，消息需要从Master复制到Slave 上，有同步和异步两种复制方式。 ####1）同步复制 同步复制方式是等Master和Slave均写 成功后才反馈给客户端写成功状态； 在同步复制方式下，如果Master出故障， Slave上有全部的备份数据，容易恢复，但是同步复制会增大数据写入 延迟，降低系统吞吐量。 ####2）异步复制 异步复制方式是只要Master写成功 即可反馈给客户端写成功状态。 在异步复制方式下，系统拥有较低的延迟和较高的吞吐量，但是如果Master出了故障，有些数据因为没有被写 入Slave，有可能会丢失； ####3）配置 同步复制和异步复制是通过Broker配置文件里的brokerRole参数进行设置的，这个参数可以被设置成ASYNC_MASTER、 SYNC_MASTER、SLAVE三个值中的一个。 ####4）总结 实际应用中要结合业务场景，合理设置刷盘方式和主从复制方式， 尤其是SYNC_FLUSH方式，由于频繁地触发磁盘写动作，会明显降低 性能。通常情况下，应该把Master和Save配置成ASYNC_FLUSH的刷盘 方式，主从之间配置成SYNC_MASTER的复制方式，这样即使有一台 机器出故障，仍然能保证数据不丢，是个不错的选择。 1.3 负载均衡1.3.1 Producer负载均衡Producer端，每个实例在发消息的时候，默认会轮询所有的message queue发送，以达到让消息平均落在不同的queue上。而由于queue可以散落在不同的broker，所以消息就发送到不同的broker下，如下图： ​ 图中箭头线条上的标号代表顺序，发布方会把第一条消息发送至 broker1-Queue 0，然后第二条消息发送至 broker1-Queue 1，第三条消息发送到broker1-Queue 2，第四条消息发送到broker2-Queue 0，第五条消息发送到broker2-Queue 1，第六条消息发送到broker2-Queue 2，第七条消息发送到broker1-Queue 0.。。。。以此类推，轮循的方式发送消息。 1.3.2 Consumer负载均衡1）集群模式在集群消费模式下，每条消息只需要投递到订阅这个topic的Consumer Group下的一个实例即可。RocketMQ采用主动拉取的方式拉取并消费消息，在拉取的时候需要明确指定拉取哪一条message queue。 而每当实例的数量有变更，都会触发一次所有实例的负载均衡，这时候会按照queue的数量和实例的数量平均分配queue给每个实例。 默认的分配算法是AllocateMessageQueueAveragely，如下图： ​ 还有另外一种平均的算法是AllocateMessageQueueAveragelyByCircle，也是平均分摊每一条queue，只是以环状轮流分queue的形式，如下图： 需要注意的是，集群模式下，queue都是只允许分配只一个实例，这是由于如果多个实例同时消费一个queue的消息，由于拉取哪些消息是consumer主动控制的，那样会导致同一个消息在不同的实例下被消费多次，所以算法上都是一个queue只分给一个consumer实例，一个consumer实例可以允许同时分到不同的queue。 通过增加consumer实例去分摊queue的消费，可以起到水平扩展的消费能力的作用。而有实例下线的时候，会重新触发负载均衡，这时候原来分配到的queue将分配到其他实例上继续消费。 但是如果consumer实例的数量比message queue的总数量还多的话，多出来的consumer实例将无法分到queue，也就无法消费到消息，也就无法起到分摊负载的作用了。所以需要控制让queue的总数量大于等于consumer的数量。（例如上面的图，如果再多一个消费者，那么就多余了。因为总共才六个queue，三个消费者刚好各自消费一个queue。再多一个消费者那么就无法分摊了） ####2）广播模式 由于广播模式下要求一条消息需要投递到一个消费组下面所有的消费者实例，所以也就没有消息被分摊消费的说法。 在实现上，其中一个不同就是在consumer分配queue的时候，所有consumer都分到所有的queue。 1.4 消息重试1.4.1 顺序消息的重试 -无线重试对于顺序消息，当消费者消费消息失败后，消息队列 RocketMQ 会自动不断进行消息重试（每次间隔时间为 1 秒），这时，应用会出现消息消费被阻塞的情况。因此，在使用顺序消息时，务必保证应用能够及时监控并处理消费失败的情况，避免阻塞现象的发生。 为什么顺序消息会无线重试呢？因为你的意图rmq已经知晓？设么意思呢？顺序消息代表着后面的消息依赖前面消息，所以为了保证业务的完整性，需要顺序执行。 所以前一个消息无法执行成功，那么我就死等。 1.4.2 无序消息的重试 - 次数重试对于无序消息（普通、定时、延时、事务消息），当消费者消费消息失败时，您可以通过设置返回状态达到消息重试的结果。 无序消息的重试只针对集群消费方式生效；广播方式不提供失败重试特性，即消费失败后，失败消息不再重试，继续消费新的消息。 1）重试次数消息队列 RocketMQ 默认允许每条消息最多重试 16 次，每次重试的间隔时间如下： 第几次重试 与上次重试的间隔时间 第几次重试 与上次重试的间隔时间 1 10 秒 9 7 分钟 2 30 秒 10 8 分钟 3 1 分钟 11 9 分钟 4 2 分钟 12 10 分钟 5 3 分钟 13 20 分钟 6 4 分钟 14 30 分钟 7 5 分钟 15 1 小时 8 6 分钟 16 2 小时 如果消息重试 16 次后仍然失败，消息将不再投递。如果严格按照上述重试时间间隔计算，某条消息在一直消费失败的前提下，将会在接下来的 4 小时 46 分钟之内进行 16 次重试，超过这个时间范围消息将不再重试投递。 ​ 这个时候，消息会被记录到死信队列。 注意： 一条消息无论重试多少次，这些重试消息的 Message ID 不会改变。 2）配置方式消费失败后，重试配置方式 集群消费方式下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置（三种方式任选一种）： 返回 Action.ReconsumeLater （推荐） 返回 Null 抛出异常 public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; //处理消息 doConsumeMessage(message); //方式1：返回 Action.ReconsumeLater，消息将重试 return Action.ReconsumeLater; //方式2：返回 null，消息将重试 return null; //方式3：直接抛出异常， 消息将重试 throw new RuntimeException(\"Consumer Message exceotion\"); &#125;&#125; 消费失败后，不重试配置方式 集群消费方式下，消息失败后期望消息不重试，需要捕获消费逻辑中可能抛出的异常，最终返回 Action.CommitMessage，此后这条消息将不会再重试。 public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; try &#123; doConsumeMessage(message); &#125; catch (Throwable e) &#123; //捕获消费逻辑中的所有异常，并返回 Action.CommitMessage; return Action.CommitMessage; &#125; //消息处理正常，直接返回 Action.CommitMessage; return Action.CommitMessage; &#125;&#125; 自定义消息最大重试次数 消息队列 RocketMQ 允许 Consumer 启动的时候设置最大重试次数，重试时间间隔将按照如下策略： 最大重试次数小于等于 16 次，则重试时间间隔同上表描述。 最大重试次数大于 16 次，超过 16 次的重试时间间隔均为每次 2 小时。 Properties properties = new Properties();//配置对应 Group ID 的最大消息重试次数为 20 次properties.put(PropertyKeyConst.MaxReconsumeTimes,\"20\");Consumer consumer =ONSFactory.createConsumer(properties); 注意： 消息最大重试次数的设置对相同 Group ID 下的所有 Consumer 实例有效。 如果只对相同 Group ID 下两个 Consumer 实例中的其中一个设置了 MaxReconsumeTimes，那么该配置对两个 Consumer 实例均生效。 配置采用覆盖的方式生效，即最后启动的 Consumer 实例会覆盖之前的启动实例的配置 获取消息重试次数 消费者收到消息后，可按照如下方式获取消息的重试次数： public class MessageListenerImpl implements MessageListener &#123; @Override public Action consume(Message message, ConsumeContext context) &#123; //获取消息的重试次数 System.out.println(message.getReconsumeTimes()); return Action.CommitMessage; &#125;&#125; 1.5 死信队列当一条消息初次消费失败，消息队列 RocketMQ 会自动进行消息重试；达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列 RocketMQ 不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中。 在消息队列 RocketMQ 中，这种正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），存储死信消息的特殊队列称为死信队列（Dead-Letter Queue）。 1.5.1 死信特性死信消息具有以下特性 不会再被消费者正常消费。 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。因此，请在死信消息产生后的 3 天内及时处理。 死信队列具有以下特性： 一个死信队列对应一个 Group ID， 而不是对应单个消费者实例。 如果一个 Group ID 未产生死信消息，消息队列 RocketMQ 不会为其创建相应的死信队列。 一个死信队列包含了对应 Group ID 产生的所有死信消息，不论该消息属于哪个 Topic。 1.5.2 查看死信信息 在控制台查询出现死信队列的主题信息 在消息界面根据主题查询死信消息 选择重新发送消息 一条消息进入死信队列，意味着某些因素导致消费者无法正常消费该消息，因此，通常需要您对其进行特殊处理。排查可疑因素并解决问题后，可以在消息队列 RocketMQ 控制台重新发送该消息，让消费者重新消费一次。 1.6 消费幂等消息队列 RocketMQ 消费者在接收到消息以后，有必要根据业务上的唯一 Key 对消息做幂等处理的必要性。 1.6.1 消费幂等的必要性在互联网应用中，尤其在网络不稳定的情况下，消息队列 RocketMQ 的消息有可能会出现重复，这个重复简单可以概括为以下情况： 发送时消息重复 当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或者客户端宕机，导致服务端对客户端应答失败。 如果此时生产者意识到消息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 投递时消息重复 消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。 为了保证消息至少被消费一次，消息队列 RocketMQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 负载均衡时消息重复（包括但不限于网络抖动、Broker 重启以及订阅方应用重启） 当消息队列 RocketMQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息。 1.6.2 处理方式因为 Message ID 有可能出现冲突（重复）的情况，所以真正安全的幂等处理，不建议以 Message ID 作为处理依据。 最好的方式是以业务唯一标识作为幂等处理的关键依据，而业务的唯一标识可以通过消息 Key （例如订单id，支付id等等业务id）进行设置： Message message = new Message();message.setKey(\"ORDERID_100\");SendResult sendResult = producer.send(message); 订阅方收到消息时可以根据消息的 Key 进行幂等处理： consumer.subscribe(\"ons_test\", \"*\", new MessageListener() &#123; public Action consume(Message message, ConsumeContext context) &#123; String key = message.getKey() // 根据业务唯一标识的 key 做幂等处理 &#125;&#125;); 2. 源码分析2.1 环境搭建依赖工具 JDK ：1.8+ Maven IntelliJ IDEA 2.1.1 源码拉取从官方仓库 https://github.com/apache/rocketmq clone或者download源码。 源码目录结构： broker: broker 模块（broke 启动进程） client ：消息客户端，包含消息生产者、消息消费者相关类 common ：公共包 dev ：开发者信息（非源代码） distribution ：部署实例文件夹（非源代码） example: RocketMQ 例代码 filter ：消息过滤相关基础类 filtersrv：消息过滤服务器实现相关类（Filter启动进程） logappender：日志实现相关类 namesrv：NameServer实现相关类（NameServer启动进程） openmessageing：消息开放标准 remoting：远程通信模块，给予Netty srcutil：服务工具类 store：消息存储实现相关类 style：checkstyle相关实现 test：测试相关类 tools：工具类，监控命令相关实现类 ###2.1.2 导入IDEA 执行安装 clean install -Dmaven.test.skip=true 2.1.3 调试在rocketqm根路径创建conf配置文件夹,从distribution拷贝broker.conf和logback_broker.xml和logback_namesrv.xml 1）启动NameServer 展开namesrv模块，右键NamesrvStartup.java 配置ROCKETMQ_HOME ROCKETMQ_HOME等于导入idea的源码的根路径 重新启动 控制台打印结果 The Name Server boot success. serializeType=JSON 2）启动Broker broker.conf配置文件内容 - broker.conf就是我们在上面创建的conf文件夹下的文件 brokerClusterName = DefaultClusterbrokerName = broker-abrokerId = 0# namesrvAddr地址namesrvAddr=127.0.0.1:9876deleteWhen = 04fileReservedTime = 48brokerRole = ASYNC_MASTERflushDiskType = ASYNC_FLUSHautoCreateTopicEnable=true# 存储路径storePathRootDir=E:\\\\RocketMQ\\\\data\\\\rocketmq\\\\dataDir# commitLog路径storePathCommitLog=E:\\\\RocketMQ\\\\data\\\\rocketmq\\\\dataDir\\\\commitlog# 消息队列存储路径storePathConsumeQueue=E:\\\\RocketMQ\\\\data\\\\rocketmq\\\\dataDir\\\\consumequeue# 消息索引存储路径storePathIndex=E:\\\\RocketMQ\\\\data\\\\rocketmq\\\\dataDir\\\\index# checkpoint文件路径storeCheckpoint=E:\\\\RocketMQ\\\\data\\\\rocketmq\\\\dataDir\\\\checkpoint# abort文件存储路径abortFile=E:\\\\RocketMQ\\\\data\\\\rocketmq\\\\dataDir\\\\abort 创建数据文件夹dataDir 启动BrokerStartup,配置broker.conf和ROCKETMQ_HOME ####3）发送消息 进入example模块的org.apache.rocketmq.example.quickstart 指定Namesrv地址 DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\");producer.setNamesrvAddr(\"127.0.0.1:9876\"); 运行main方法，发送消息 4）消费消息 进入example模块的org.apache.rocketmq.example.quickstart 指定Namesrv地址 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"please_rename_unique_group_name_4\");consumer.setNamesrvAddr(\"127.0.0.1:9876\"); 运行main方法，消费消息 2.2 NameServer2.2.1 架构设计消息中间件的设计思路一般是基于主题订阅发布的机制，消息生产者（Producer）发送某一个主题到消息服务器，消息服务器负责将消息持久化存储，消息消费者（Consumer）订阅该兴趣的主题，消息服务器根据订阅信息（路由信息）将消息推送到消费者（Push模式）或者消费者主动向消息服务器拉去（Pull模式），从而实现消息生产者与消息消费者解耦。为了避免消息服务器的单点故障导致的整个系统瘫痪，通常会部署多台消息服务器共同承担消息的存储。那消息生产者如何知道消息要发送到哪台消息服务器呢？如果某一台消息服务器宕机了，那么消息生产者如何在不重启服务情况下感知呢？ NameServer就是为了解决以上问题设计的。 ​ Broker消息服务器在启动的时向所有NameServer注册，消息生产者（Producer）在发送消息时之前先从NameServer获取Broker服务器地址列表，然后根据负载均衡算法从列表中选择一台服务器进行发送。NameServer与每台Broker保持长连接，并间隔30S检测Broker是否存活，如果检测到Broker宕机，则从路由注册表中删除。但是路由变化不会马上通知消息生产者（也就是他不会主动通知producer或者consumer）。这样设计的目的是为了降低NameServer实现的复杂度，在消息发送端提供容错机制保证消息发送的可用性。 NameServer本身的高可用是通过部署多台NameServer来实现，但彼此之间不通讯，也就是NameServer服务器之间在某一个时刻的数据并不完全相同，但这对消息发送并不会造成任何影响，这也是NameServer设计的一个亮点，总之，RocketMQ设计追求简单高效。 2.2.2 启动流程 启动类：org.apache.rocketmq.namesrv.NamesrvStartup 下面所有流程都是分析org.apache.rocketmq.namesrv.NamesrvStartup#start的方法。 ####步骤一 - 创建NamesrvController 解析配置文件，填充NameServerConfig、NettyServerConfig属性值，并创建NamesrvController 代码：NamesrvController#createNamesrvController //创建NamesrvConfigfinal NamesrvConfig namesrvConfig = new NamesrvConfig();//创建NettyServerConfig，处理其他服务器的请求final NettyServerConfig nettyServerConfig = new NettyServerConfig();//设置启动端口号，监听来自其他服务器的请求--例如生产者，消费者，brokernettyServerConfig.setListenPort(9876);//解析启动-c参数if (commandLine.hasOption('c')) &#123; String file = commandLine.getOptionValue('c'); if (file != null) &#123; InputStream in = new BufferedInputStream(new FileInputStream(file)); properties = new Properties(); properties.load(in); MixAll.properties2Object(properties, namesrvConfig); MixAll.properties2Object(properties, nettyServerConfig); namesrvConfig.setConfigStorePath(file); System.out.printf(\"load config properties file OK, %s%n\", file); in.close(); &#125;&#125;//解析启动-p参数if (commandLine.hasOption('p')) &#123; InternalLogger console = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_CONSOLE_NAME); MixAll.printObjectProperties(console, namesrvConfig); MixAll.printObjectProperties(console, nettyServerConfig); System.exit(0);&#125;//将启动参数填充到namesrvConfig,nettyServerConfigMixAll.properties2Object(ServerUtil.commandLine2Properties(commandLine), namesrvConfig);//创建NameServerControllerfinal NamesrvController controller = new NamesrvController(namesrvConfig, nettyServerConfig); NamesrvConfig属性 private String rocketmqHome = System.getProperty(MixAll.ROCKETMQ_HOME_PROPERTY, System.getenv(MixAll.ROCKETMQ_HOME_ENV));private String kvConfigPath = System.getProperty(\"user.home\") + File.separator + \"namesrv\" + File.separator + \"kvConfig.json\";private String configStorePath = System.getProperty(\"user.home\") + File.separator + \"namesrv\" + File.separator + \"namesrv.properties\";private String productEnvName = \"center\";private boolean clusterTest = false;private boolean orderMessageEnable = false; rocketmqHome：rocketmq主目录 kvConfig：NameServer存储KV配置属性的持久化路径 configStorePath：nameServer默认配置文件路径 orderMessageEnable：是否支持顺序消息 NettyServerConfig属性 private int listenPort = 8888;private int serverWorkerThreads = 8;private int serverCallbackExecutorThreads = 0;private int serverSelectorThreads = 3;private int serverOnewaySemaphoreValue = 256;private int serverAsyncSemaphoreValue = 64;private int serverChannelMaxIdleTimeSeconds = 120;private int serverSocketSndBufSize = NettySystemConfig.socketSndbufSize;private int serverSocketRcvBufSize = NettySystemConfig.socketRcvbufSize;private boolean serverPooledByteBufAllocatorEnable = true;private boolean useEpollNativeSelector = false; listenPort：NameServer监听端口，该值默认会被初始化为9876serverWorkerThreads：Netty业务线程池线程个数serverCallbackExecutorThreads：Netty public任务线程池线程个数，Netty网络设计，根据业务类型会创建不同的线程池，比如处理消息发送、消息消费、心跳检测等。如果该业务类型未注册线程池，则由public线程池执行。serverSelectorThreads：IO线程池个数，主要是NameServer、Broker端解析请求、返回相应的线程个数，这类线程主要是处理网路请求的，解析请求包，然后转发到各个业务线程池完成具体的操作，然后将结果返回给调用方;serverOnewaySemaphoreValue：send oneway消息请求并发读（Broker端参数）;serverAsyncSemaphoreValue：异步消息发送最大并发度;serverChannelMaxIdleTimeSeconds ：网络连接最大的空闲时间，默认120s。serverSocketSndBufSize：网络socket发送缓冲区大小。serverSocketRcvBufSize： 网络接收端缓存区大小。serverPooledByteBufAllocatorEnable：ByteBuffer是否开启缓存;useEpollNativeSelector：是否启用Epoll IO模型。 步骤二，初始化namesrvcontroller​ 上面第一步创建完NamesrvController，接着调用start(controller); ，在start方法内部调用boolean initResult = controller.initialize();启动初始化工作。 ​ 根据启动属性创建NamesrvController实例，并初始化该实例。NameServerController实例为NameServer核心控制器 代码：NamesrvController#initialize public boolean initialize() &#123; //加载KV配置 this.kvConfigManager.load(); //创建NettyServer网络处理对象 this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService); //开启定时任务:延迟五秒启动，然后每隔10s扫描一次brokerLiveTable（这个map保存了broker的路由信息）,移除不活跃的Broker //那么在哪里保存了broker 的信息呢？在下面的路由管理中的几个hashmap中会讲到。 // this.remotingExecutor = Executors.newFixedThreadPool(nettyServerConfig.getServerWorkerThreads(), new ThreadFactoryImpl(\"RemotingExecutorThread_\")); this.registerProcessor(); this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125; &#125;, 5, 10, TimeUnit.SECONDS); //开启定时任务:每隔10min打印一次KV配置 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.kvConfigManager.printAllPeriodically(); &#125; &#125;, 1, 10, TimeUnit.MINUTES); return true;&#125; 步骤三 启动NamesrvController在JVM进程关闭之前，先将线程池关闭，及时释放资源 代码：NamesrvStartup#start //注册JVM钩子函数代码Runtime.getRuntime().addShutdownHook(new ShutdownHookThread(log, new Callable&lt;Void&gt;() &#123; @Override public Void call() throws Exception &#123; //释放资源 controller.shutdown(); return null; &#125;&#125;)); 2.2.3 路由管理​ NameServer的主要作用是为消息的生产者和消息消费者提供关于主题Topic的路由信息，那么NameServer需要存储路由的基础信息，还要管理Broker节点，包括路由注册、路由删除等。 2.2.3.1 路由元信息代码：RouteInfoManager private final HashMap&lt;String/* topic */, List&lt;QueueData&gt;&gt; topicQueueTable;private final HashMap&lt;String/* brokerName */, BrokerData&gt; brokerAddrTable;private final HashMap&lt;String/* clusterName */, Set&lt;String/* brokerName */&gt;&gt; clusterAddrTable;private final HashMap&lt;String/* brokerAddr */, BrokerLiveInfo&gt; brokerLiveTable;private final HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable; topicQueueTable：Topic消息队列路由信息，消息发送时根据路由表进行负载均衡，保存了主题跟broker的映射关系。 brokerAddrTable：Broker基础信息，包括brokerName、所属集群名称、主备Broker地址。保存了broker所在的服务器地址信息 clusterAddrTable：Broker集群信息，存储集群中所有Broker名称 brokerLiveTable：Broker状态信息，NameServer每次收到心跳包是会替换该信息，每十秒就检查一下这个map中所有broker的存货情况。 filterServerTable：Broker上的FilterServer列表，用于类模式消息过滤。 RocketMQ基于定于发布机制，一个Topic拥有多个消息队列，一个Broker为每一个主题创建4个读队列和4个写队列。多个Broker组成一个集群，集群由brokername相同的多台Broker组成Master-Slave架构，brokerId为0代表Master，大于0为Slave。BrokerLiveInfo中的lastUpdateTimestamp存储上次收到Broker心跳包的时间。 2.2.3.2 路由注册#####1）broker发送心跳包 ​ RocketMQ路由注册是通过Broker与NameServer的心跳功能实现的。Broker启动时向集群中所有的NameServer发送心跳信息，每隔30s向集群中所有NameServer发送心跳包，NameServer收到心跳包时会更新brokerLiveTable缓存中BrokerLiveInfo的lastUpdataTimeStamp信息，然后NameServer每隔10s扫描brokerLiveTable，如果连续120S没有收到心跳包，NameServer将移除Broker的路由信息同时关闭Socket连接。 首先查看Broker启动源码 代码：org.apache.rocketmq.broker.BrokerStartup#main public static void main(String[] args) &#123; start(createBrokerController(args));//你会发现他跟namesrv启动一样，首先创建brokerController&#125; public static BrokerController createBrokerController(String[] args) &#123; 。。。。。。。 final BrokerConfig brokerConfig = new BrokerConfig();//获取解析启动broker，配置的配置信息 final NettyServerConfig nettyServerConfig = new NettyServerConfig();//这个server主要是用来处理producer的请求 final NettyClientConfig nettyClientConfig = new NettyClientConfig();//这个server是用来上报自己的状态给namesrv 。。。。 nettyServerConfig.setListenPort(10911);//设置默认监听的端口号，生产者就是通过往这个端口发送数据到broker 。。。。 &#125; 代码：BrokerController#start - //注册Broker信息this.registerBrokerAll(true, false, true);//每隔30s上报Broker信息到NameServerthis.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister()); &#125; catch (Throwable e) &#123; log.error(\"registerBrokerAll Exception\", e); &#125; &#125;&#125;, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS); 代码：BrokerOuterAPI#registerBrokerAll //获得nameServer地址信息List&lt;String&gt; nameServerAddressList = this.remotingClient.getNameServerAddressList();//遍历所有nameserver列表if (nameServerAddressList != null &amp;&amp; nameServerAddressList.size() &gt; 0) &#123; //封装请求头 final RegisterBrokerRequestHeader requestHeader = new RegisterBrokerRequestHeader(); requestHeader.setBrokerAddr(brokerAddr); requestHeader.setBrokerId(brokerId); requestHeader.setBrokerName(brokerName); requestHeader.setClusterName(clusterName); requestHeader.setHaServerAddr(haServerAddr); requestHeader.setCompressed(compressed); //封装请求体 RegisterBrokerBody requestBody = new RegisterBrokerBody(); requestBody.setTopicConfigSerializeWrapper(topicConfigWrapper); requestBody.setFilterServerList(filterServerList); final byte[] body = requestBody.encode(compressed); final int bodyCrc32 = UtilAll.crc32(body); requestHeader.setBodyCrc32(bodyCrc32); final CountDownLatch countDownLatch = new CountDownLatch(nameServerAddressList.size());//使用countDownLatch等待上报完毕 for (final String namesrvAddr : nameServerAddressList) &#123;//如果配置了namesrv集群，那么开启线程分别上报broker 信息 brokerOuterExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; //分别向NameServer注册 RegisterBrokerResult result = registerBroker(namesrvAddr,oneway, timeoutMills,requestHeader,body); if (result != null) &#123; registerBrokerResultList.add(result); &#125; log.info(\"register broker[&#123;&#125;]to name server &#123;&#125; OK\", brokerId, namesrvAddr); &#125; catch (Exception e) &#123; log.warn(\"registerBroker Exception, &#123;&#125;\", namesrvAddr, e); &#125; finally &#123; countDownLatch.countDown(); &#125; &#125; &#125;); &#125; try &#123; countDownLatch.await(timeoutMills, TimeUnit.MILLISECONDS); &#125; catch (InterruptedException e) &#123; &#125;&#125; 代码：BrokerOutAPI#registerBroker if (oneway) &#123; try &#123; this.remotingClient.invokeOneway(namesrvAddr, request, timeoutMills); &#125; catch (RemotingTooMuchRequestException e) &#123; // Ignore &#125; return null;&#125;RemotingCommand response = this.remotingClient.invokeSync(namesrvAddr, request, timeoutMills); 2）namesrv 处理心跳包 org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor网路处理类解析请求类型，如果请求类型是为REGISTER_BROKER，则将请求转发到RouteInfoManager#regiesterBroker 可以看到在维护这几个保存路由信息的map时，是加锁的。保证了并发的安全性 代码：DefaultRequestProcessor#processRequest //因为DefaultRequestProcessor的processRequest方法，是处理所有的客户端请求，那么请求可能是生产者获取路由信息，也有可能是broker向namesrv注册信息，所以需要//根据请求类型判断是注册Broker信息case RequestCode.REGISTER_BROKER: Version brokerVersion = MQVersion.value2Version(request.getVersion()); if (brokerVersion.ordinal() &gt;= MQVersion.Version.V3_0_11.ordinal()) &#123; return this.registerBrokerWithFilterServer(ctx, request); &#125; else &#123; //注册Broker信息 return this.registerBroker(ctx, request); &#125; 代码：DefaultRequestProcessor#registerBroker RegisterBrokerResult result = this.namesrvController.getRouteInfoManager().registerBroker( requestHeader.getClusterName(), requestHeader.getBrokerAddr(), requestHeader.getBrokerName(), requestHeader.getBrokerId(), requestHeader.getHaServerAddr(), topicConfigWrapper, null, ctx.channel()); 代码：RouteInfoManager#registerBroker 维护路由信息 //加锁this.lock.writeLock().lockInterruptibly();//维护clusterAddrTableSet&lt;String&gt; brokerNames = this.clusterAddrTable.get(clusterName);if (null == brokerNames) &#123; brokerNames = new HashSet&lt;String&gt;(); this.clusterAddrTable.put(clusterName, brokerNames);&#125;brokerNames.add(brokerName); //维护brokerAddrTableBrokerData brokerData = this.brokerAddrTable.get(brokerName);//第一次注册,则创建brokerDataif (null == brokerData) &#123; registerFirst = true; brokerData = new BrokerData(clusterName, brokerName, new HashMap&lt;Long, String&gt;()); this.brokerAddrTable.put(brokerName, brokerData);&#125;//非第一次注册,更新BrokerMap&lt;Long, String&gt; brokerAddrsMap = brokerData.getBrokerAddrs();Iterator&lt;Entry&lt;Long, String&gt;&gt; it = brokerAddrsMap.entrySet().iterator();while (it.hasNext()) &#123; Entry&lt;Long, String&gt; item = it.next(); if (null != brokerAddr &amp;&amp; brokerAddr.equals(item.getValue()) &amp;&amp; brokerId != item.getKey()) &#123; it.remove(); &#125;&#125;String oldAddr = brokerData.getBrokerAddrs().put(brokerId, brokerAddr);registerFirst = registerFirst || (null == oldAddr); //维护topicQueueTableif (null != topicConfigWrapper &amp;&amp; MixAll.MASTER_ID == brokerId) &#123; if (this.isBrokerTopicConfigChanged(brokerAddr, topicConfigWrapper.getDataVersion()) || registerFirst) &#123; ConcurrentMap&lt;String, TopicConfig&gt; tcTable = topicConfigWrapper.getTopicConfigTable(); if (tcTable != null) &#123; for (Map.Entry&lt;String, TopicConfig&gt; entry : tcTable.entrySet()) &#123; this.createAndUpdateQueueData(brokerName, entry.getValue()); &#125; &#125; &#125;&#125; 代码：RouteInfoManager#createAndUpdateQueueData private void createAndUpdateQueueData(final String brokerName, final TopicConfig topicConfig) &#123; //创建QueueData QueueData queueData = new QueueData(); queueData.setBrokerName(brokerName); queueData.setWriteQueueNums(topicConfig.getWriteQueueNums()); queueData.setReadQueueNums(topicConfig.getReadQueueNums()); queueData.setPerm(topicConfig.getPerm()); queueData.setTopicSynFlag(topicConfig.getTopicSysFlag()); //获得topicQueueTable中队列集合 List&lt;QueueData&gt; queueDataList = this.topicQueueTable.get(topicConfig.getTopicName()); //topicQueueTable为空,则直接添加queueData到队列集合 if (null == queueDataList) &#123; queueDataList = new LinkedList&lt;QueueData&gt;(); queueDataList.add(queueData); this.topicQueueTable.put(topicConfig.getTopicName(), queueDataList); log.info(\"new topic registered, &#123;&#125; &#123;&#125;\", topicConfig.getTopicName(), queueData); &#125; else &#123; //判断是否是新的队列 boolean addNewOne = true; Iterator&lt;QueueData&gt; it = queueDataList.iterator(); while (it.hasNext()) &#123; QueueData qd = it.next(); //如果brokerName相同,代表不是新的队列 if (qd.getBrokerName().equals(brokerName)) &#123; if (qd.equals(queueData)) &#123; addNewOne = false; &#125; else &#123; log.info(\"topic changed, &#123;&#125; OLD: &#123;&#125; NEW: &#123;&#125;\", topicConfig.getTopicName(), qd, queueData); it.remove(); &#125; &#125; &#125; //如果是新的队列,则添加队列到queueDataList if (addNewOne) &#123; queueDataList.add(queueData); &#125; &#125;&#125; //维护brokerLiveTableBrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr,new BrokerLiveInfo( System.currentTimeMillis(), topicConfigWrapper.getDataVersion(), channel, haServerAddr)); //维护filterServerListif (filterServerList != null) &#123; if (filterServerList.isEmpty()) &#123; this.filterServerTable.remove(brokerAddr); &#125; else &#123; this.filterServerTable.put(brokerAddr, filterServerList); &#125;&#125;if (MixAll.MASTER_ID != brokerId) &#123; String masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID); if (masterAddr != null) &#123; BrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.get(masterAddr); if (brokerLiveInfo != null) &#123; result.setHaServerAddr(brokerLiveInfo.getHaServerAddr()); result.setMasterAddr(masterAddr); &#125; &#125;&#125; 2.2.3.3 namesrv路由删除​ **RocketMQ有两个触发点来删除路由信息**：* NameServer定期扫描brokerLiveTable检测上次心跳包与当前系统的时间差，如果时间超过120s，则需要移除broker。* Broker在正常关闭的情况下，会执行unregisterBroker指令这两种方式路由删除的方法都是一样的，就是从相关路由表中删除与该broker相关的信息。![](rocketmq总结/路由删除.png)***代码：NamesrvController#initialize***```java//每隔10s扫描一次为活跃Brokerthis.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125;&#125;, 5, 10, TimeUnit.SECONDS); 代码：RouteInfoManager#scanNotActiveBroker public void scanNotActiveBroker() &#123; //获得brokerLiveTable Iterator&lt;Entry&lt;String, BrokerLiveInfo&gt;&gt; it = this.brokerLiveTable.entrySet().iterator(); //遍历brokerLiveTable while (it.hasNext()) &#123; Entry&lt;String, BrokerLiveInfo&gt; next = it.next(); long last = next.getValue().getLastUpdateTimestamp(); //如果收到心跳包的时间距当时时间是否超过120s if ((last + BROKER_CHANNEL_EXPIRED_TIME) &lt; System.currentTimeMillis()) &#123; //关闭连接 RemotingUtil.closeChannel(next.getValue().getChannel()); //移除broker it.remove(); //维护路由表 this.onChannelDestroy(next.getKey(), next.getValue().getChannel()); &#125; &#125;&#125; 代码：RouteInfoManager#onChannelDestroy //申请写锁,根据brokerAddress从brokerLiveTable和filterServerTable移除this.lock.writeLock().lockInterruptibly();this.brokerLiveTable.remove(brokerAddrFound);this.filterServerTable.remove(brokerAddrFound); //维护brokerAddrTableString brokerNameFound = null;boolean removeBrokerName = false;Iterator&lt;Entry&lt;String, BrokerData&gt;&gt; itBrokerAddrTable =this.brokerAddrTable.entrySet().iterator();//遍历brokerAddrTablewhile (itBrokerAddrTable.hasNext() &amp;&amp; (null == brokerNameFound)) &#123; BrokerData brokerData = itBrokerAddrTable.next().getValue(); //遍历broker地址 Iterator&lt;Entry&lt;Long, String&gt;&gt; it = brokerData.getBrokerAddrs().entrySet().iterator(); while (it.hasNext()) &#123; Entry&lt;Long, String&gt; entry = it.next(); Long brokerId = entry.getKey(); String brokerAddr = entry.getValue(); //根据broker地址移除brokerAddr if (brokerAddr.equals(brokerAddrFound)) &#123; brokerNameFound = brokerData.getBrokerName(); it.remove(); log.info(\"remove brokerAddr[&#123;&#125;, &#123;&#125;] from brokerAddrTable, because channel destroyed\", brokerId, brokerAddr); break; &#125; &#125; //如果当前主题只包含待移除的broker,则移除该topic if (brokerData.getBrokerAddrs().isEmpty()) &#123; removeBrokerName = true; itBrokerAddrTable.remove(); log.info(\"remove brokerName[&#123;&#125;] from brokerAddrTable, because channel destroyed\", brokerData.getBrokerName()); &#125;&#125; //维护clusterAddrTableif (brokerNameFound != null &amp;&amp; removeBrokerName) &#123; Iterator&lt;Entry&lt;String, Set&lt;String&gt;&gt;&gt; it = this.clusterAddrTable.entrySet().iterator(); //遍历clusterAddrTable while (it.hasNext()) &#123; Entry&lt;String, Set&lt;String&gt;&gt; entry = it.next(); //获得集群名称 String clusterName = entry.getKey(); //获得集群中brokerName集合 Set&lt;String&gt; brokerNames = entry.getValue(); //从brokerNames中移除brokerNameFound boolean removed = brokerNames.remove(brokerNameFound); if (removed) &#123; log.info(\"remove brokerName[&#123;&#125;], clusterName[&#123;&#125;] from clusterAddrTable, because channel destroyed\", brokerNameFound, clusterName); if (brokerNames.isEmpty()) &#123; log.info(\"remove the clusterName[&#123;&#125;] from clusterAddrTable, because channel destroyed and no broker in this cluster\", clusterName); //如果集群中不包含任何broker,则移除该集群 it.remove(); &#125; break; &#125; &#125;&#125; //维护topicQueueTable队列if (removeBrokerName) &#123; //遍历topicQueueTable Iterator&lt;Entry&lt;String, List&lt;QueueData&gt;&gt;&gt; itTopicQueueTable = this.topicQueueTable.entrySet().iterator(); while (itTopicQueueTable.hasNext()) &#123; Entry&lt;String, List&lt;QueueData&gt;&gt; entry = itTopicQueueTable.next(); //主题名称 String topic = entry.getKey(); //队列集合 List&lt;QueueData&gt; queueDataList = entry.getValue(); //遍历该主题队列 Iterator&lt;QueueData&gt; itQueueData = queueDataList.iterator(); while (itQueueData.hasNext()) &#123; //从队列中移除为活跃broker信息 QueueData queueData = itQueueData.next(); if (queueData.getBrokerName().equals(brokerNameFound)) &#123; itQueueData.remove(); log.info(\"remove topic[&#123;&#125; &#123;&#125;], from topicQueueTable, because channel destroyed\", topic, queueData); &#125; &#125; //如果该topic的队列为空,则移除该topic if (queueDataList.isEmpty()) &#123; itTopicQueueTable.remove(); log.info(\"remove topic[&#123;&#125;] all queue, from topicQueueTable, because channel destroyed\", topic); &#125; &#125;&#125; //释放写锁finally &#123; this.lock.writeLock().unlock();&#125; 总结：也就是说，除非是broker 通知namesrv我要下线了，然后namesrv就把跟broker 的长连接给关闭了。否则namesrv就每隔十秒检查brokerLiveTable是否存在过期未发送心跳的broker，然后再清除失效broker。 2.2.3.4 路由发现​ RocketMQ路由发现是非实时的，当Topic路由出现变化后，NameServer不会主动推送给客户端，而是由客户端定时拉取主题最新的路由。 ​ namesrv 通过DefaultRequestProcessor类来处理，客户端请求路由信息。 ​ 这个类我们在之前的namesrv处理心跳包中也讲过，也是通过这个类来处理broker的心跳包。 当org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor#processRequest收到请求时，判断请求类型。 switch (request.getCode()) &#123; ........ case RequestCode.GET_ROUTEINTO_BY_TOPIC://如果类型是这个，那么表明是来自客户端获取路由信息的请求。 return this.getRouteInfoByTopic(ctx, request);//调用getRouteInfoByTopic处理请求 ....... default: break;&#125; 代码：DefaultRequestProcessor#getRouteInfoByTopic public RemotingCommand getRouteInfoByTopic(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException &#123; final RemotingCommand response = RemotingCommand.createResponseCommand(null); final GetRouteInfoRequestHeader requestHeader = (GetRouteInfoRequestHeader) request.decodeCommandCustomHeader(GetRouteInfoRequestHeader.class); //调用RouteInfoManager的方法,从路由表topicQueueTable、brokerAddrTable、filterServerTable中分别填充TopicRouteData的List&lt;QueueData&gt;、List&lt;BrokerData&gt;、filterServer TopicRouteData topicRouteData = this.namesrvController.getRouteInfoManager().pickupTopicRouteData(requestHeader.getTopic()); //如果找到主题对应你的路由信息并且该主题为顺序消息，则从NameServer KVConfig中获取关于顺序消息相关的配置填充路由信息 if (topicRouteData != null) &#123; if (this.namesrvController.getNamesrvConfig().isOrderMessageEnable()) &#123; String orderTopicConf = this.namesrvController.getKvConfigManager().getKVConfig(NamesrvUtil.NAMESPACE_ORDER_TOPIC_CONFIG, requestHeader.getTopic()); topicRouteData.setOrderTopicConf(orderTopicConf); &#125; byte[] content = topicRouteData.encode(); response.setBody(content); response.setCode(ResponseCode.SUCCESS); response.setRemark(null); return response; &#125; response.setCode(ResponseCode.TOPIC_NOT_EXIST); response.setRemark(\"No topic route info in name server for the topic: \" + requestHeader.getTopic() + FAQUrl.suggestTodo(FAQUrl.APPLY_TOPIC_URL)); return response;&#125; 2.2.4 小结 2.3 Producer消息生产者的代码都在client模块中，相对于RocketMQ来讲，消息生产者就是客户端，也是消息的提供者。 ###2.3.1 方法和属性 ####1）主要方法介绍 //创建主题void createTopic(final String key, final String newTopic, final int queueNum) throws MQClientException; //根据时间戳从队列中查找消息偏移量long searchOffset(final MessageQueue mq, final long timestamp) //查找消息队列中最大的偏移量long maxOffset(final MessageQueue mq) throws MQClientException; //查找消息队列中最小的偏移量long minOffset(final MessageQueue mq) //根据偏移量查找消息MessageExt viewMessage(final String offsetMsgId) throws RemotingException, MQBrokerException, InterruptedException, MQClientException; //根据条件查找消息QueryResult queryMessage(final String topic, final String key, final int maxNum, final long begin, final long end) throws MQClientException, InterruptedException; //根据消息ID和主题查找消息MessageExt viewMessage(String topic,String msgId) throws RemotingException, MQBrokerException, InterruptedException, MQClientException; //启动void start() throws MQClientException; //关闭void shutdown(); //查找该主题下所有消息List&lt;MessageQueue&gt; fetchPublishMessageQueues(final String topic) throws MQClientException; //同步发送消息SendResult send(final Message msg) throws MQClientException, RemotingException, MQBrokerException, InterruptedException; //同步超时发送消息SendResult send(final Message msg, final long timeout) throws MQClientException, RemotingException, MQBrokerException, InterruptedException; //异步发送消息void send(final Message msg, final SendCallback sendCallback) throws MQClientException, RemotingException, InterruptedException; //异步超时发送消息void send(final Message msg, final SendCallback sendCallback, final long timeout) throws MQClientException, RemotingException, InterruptedException; //发送单向消息void sendOneway(final Message msg) throws MQClientException, RemotingException, InterruptedException; //选择指定队列同步发送消息SendResult send(final Message msg, final MessageQueue mq) throws MQClientException, RemotingException, MQBrokerException, InterruptedException; //选择指定队列异步发送消息void send(final Message msg, final MessageQueue mq, final SendCallback sendCallback) throws MQClientException, RemotingException, InterruptedException; //选择指定队列单项发送消息void sendOneway(final Message msg, final MessageQueue mq) throws MQClientException, RemotingException, InterruptedException; //批量发送消息SendResult send(final Collection&lt;Message&gt; msgs) throws MQClientException, RemotingException, MQBrokerException,InterruptedException; ####2）属性介绍 producerGroup：生产者所属组createTopicKey：默认TopicdefaultTopicQueueNums：默认主题在每一个Broker队列数量sendMsgTimeout：发送消息默认超时时间，默认3scompressMsgBodyOverHowmuch：消息体超过该值则启用压缩，默认4kretryTimesWhenSendFailed：同步方式发送消息重试次数，默认为2，总共执行3次retryTimesWhenSendAsyncFailed：异步方法发送消息重试次数，默认为2retryAnotherBrokerWhenNotStoreOK：消息重试时选择另外一个Broker时，是否不等待存储结果就返回，默认为falsemaxMessageSize：允许发送的最大消息长度，默认为4M 2.3.2 启动流程 代码：DefaultMQProducerImpl#start //检查生产者组名是否满足要求，不能为空，不能跟系统默认组名一致等等校验this.checkConfig();//更改当前instanceName为进程IDif (!this.defaultMQProducer.getProducerGroup().equals(MixAll.CLIENT_INNER_PRODUCER_GROUP)) &#123; this.defaultMQProducer.changeInstanceNameToPID();&#125;//获得MQ客户端实例this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook); 整个JVM中只存在一个MQClientManager实例，维护一个MQClientInstance缓存表 ConcurrentMap factoryTable = new ConcurrentHashMap(); 同一个clientId只会创建一个MQClientInstance。 MQClientInstance封装了RocketMQ网络处理API，是消息生产者和消息消费者与NameServer、Broker打交道的网络通道 代码：MQClientManager#getAndCreateMQClientInstance public MQClientInstance getAndCreateMQClientInstance(final ClientConfig clientConfig, RPCHook rpcHook) &#123; //构建客户端ID String clientId = clientConfig.buildMQClientId(); //根据客户端ID或者客户端实例 MQClientInstance instance = this.factoryTable.get(clientId); //实例如果为空就创建新的实例,并添加到实例表中 if (null == instance) &#123; instance = new MQClientInstance(clientConfig.cloneClientConfig(), this.factoryIndexGenerator.getAndIncrement(), clientId, rpcHook); MQClientInstance prev = this.factoryTable.putIfAbsent(clientId, instance); if (prev != null) &#123; instance = prev; log.warn(\"Returned Previous MQClientInstance for clientId:[&#123;&#125;]\", clientId); &#125; else &#123; log.info(\"Created new MQClientInstance for clientId:[&#123;&#125;]\", clientId); &#125; &#125; return instance;&#125; 代码：DefaultMQProducerImpl#start //注册当前生产者到到MQClientInstance管理中,方便后续调用网路请求boolean registerOK = mQClientFactory.registerProducer(this.defaultMQProducer.getProducerGroup(), this);if (!registerOK) &#123; this.serviceState = ServiceState.CREATE_JUST; throw new MQClientException(\"The producer group[\" + this.defaultMQProducer.getProducerGroup() + \"] has been created before, specify another name please.\" + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL), null);&#125;//启动生产者if (startFactory) &#123; mQClientFactory.start();&#125; 2.3.3 消息发送 代码：DefaultMQProducerImpl#send(Message msg) //发送消息public SendResult send(Message msg) &#123; return send(msg, this.defaultMQProducer.getSendMsgTimeout());&#125; 代码：DefaultMQProducerImpl#send(Message msg,long timeout) //发送消息,默认超时时间为3spublic SendResult send(Message msg,long timeout)&#123; return this.sendDefaultImpl(msg, CommunicationMode.SYNC, null, timeout);//同步发送&#125; 代码：DefaultMQProducerImpl#sendDefaultImpl //校验消息Validators.checkMessage(msg, this.defaultMQProducer); ####1）验证消息 代码：Validators#checkMessage public static void checkMessage(Message msg, DefaultMQProducer defaultMQProducer) throws MQClientException &#123; //判断是否为空 if (null == msg) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, \"the message is null\"); &#125; // 校验主题 Validators.checkTopic(msg.getTopic()); // 校验消息体 if (null == msg.getBody()) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, \"the message body is null\"); &#125; if (0 == msg.getBody().length) &#123;//消息体没有内容，那么报异常 throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, \"the message body length is zero\"); &#125; if (msg.getBody().length &gt; defaultMQProducer.getMaxMessageSize()) &#123;//判断消息体大小是否超过4M throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, \"the message body size over max value, MAX: \" + defaultMQProducer.getMaxMessageSize()); &#125;&#125; ####2）查找路由 代码：DefaultMQProducerImpl#tryToFindTopicPublishInfo private TopicPublishInfo tryToFindTopicPublishInfo(final String topic) &#123; //从缓存中获得主题的路由信息 TopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic); //路由信息为空,则从NameServer获取路由 if (null == topicPublishInfo || !topicPublishInfo.ok()) &#123; this.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo()); this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic);//请求namesrv获取路由信息 topicPublishInfo = this.topicPublishInfoTable.get(topic); &#125; if (topicPublishInfo.isHaveTopicRouterInfo() || topicPublishInfo.ok()) &#123; return topicPublishInfo; &#125; else &#123; //如果未找到当前主题的路由信息,则用默认主题继续查找 this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic, true, this.defaultMQProducer); topicPublishInfo = this.topicPublishInfoTable.get(topic); return topicPublishInfo; &#125;&#125; 代码：TopicPublishInfo public class TopicPublishInfo &#123; private boolean orderTopic = false; //是否是顺序消息 private boolean haveTopicRouterInfo = false; private List&lt;MessageQueue&gt; messageQueueList = new ArrayList&lt;MessageQueue&gt;(); //该主题消息队列 private volatile ThreadLocalIndex sendWhichQueue = new ThreadLocalIndex();//每选择一次消息队列,该值+1 private TopicRouteData topicRouteData;//关联Topic路由元信息&#125; 代码：MQClientInstance#updateTopicRouteInfoFromNameServer TopicRouteData topicRouteData;//使用默认主题从NameServer获取路由信息if (isDefault &amp;&amp; defaultMQProducer != null) &#123; topicRouteData = this.mQClientAPIImpl.getDefaultTopicRouteInfoFromNameServer(defaultMQProducer.getCreateTopicKey(), 1000 * 3); if (topicRouteData != null) &#123; for (QueueData data : topicRouteData.getQueueDatas()) &#123; int queueNums = Math.min(defaultMQProducer.getDefaultTopicQueueNums(), data.getReadQueueNums()); data.setReadQueueNums(queueNums); data.setWriteQueueNums(queueNums); &#125; &#125;&#125; else &#123; //使用指定主题从NameServer获取路由信息 topicRouteData = this.mQClientAPIImpl.getTopicRouteInfoFromNameServer(topic, 1000 * 3);&#125; 代码：MQClientInstance#updateTopicRouteInfoFromNameServer //从namesrv拿到最新路由信息后，接着判断本地路由信息是否需要更改，如果本地路由不是最新，则更新本地路由TopicRouteData old = this.topicRouteTable.get(topic);boolean changed = topicRouteDataIsChange(old, topicRouteData);if (!changed) &#123; changed = this.isNeedUpdateTopicRouteInfo(topic);&#125; else &#123; log.info(\"the topic[&#123;&#125;] route info changed, old[&#123;&#125;] ,new[&#123;&#125;]\", topic, old, topicRouteData);&#125; 代码：MQClientInstance#updateTopicRouteInfoFromNameServer if (changed) &#123; //将topicRouteData转换为发布队列 TopicPublishInfo publishInfo = topicRouteData2TopicPublishInfo(topic, topicRouteData); publishInfo.setHaveTopicRouterInfo(true); //遍历生产 Iterator&lt;Entry&lt;String, MQProducerInner&gt;&gt; it = this.producerTable.entrySet().iterator(); while (it.hasNext()) &#123; Entry&lt;String, MQProducerInner&gt; entry = it.next(); MQProducerInner impl = entry.getValue(); if (impl != null) &#123; //生产者不为空时,更新publishInfo信息 impl.updateTopicPublishInfo(topic, publishInfo); &#125; &#125;&#125; 代码：MQClientInstance#topicRouteData2TopicPublishInfo ，然后开始执行发送消息到brokerpublic static TopicPublishInfo topicRouteData2TopicPublishInfo(final String topic, final TopicRouteData route) &#123; //创建TopicPublishInfo对象 TopicPublishInfo info = new TopicPublishInfo(); //关联topicRoute info.setTopicRouteData(route); //顺序消息,更新TopicPublishInfo if (route.getOrderTopicConf() != null &amp;&amp; route.getOrderTopicConf().length() &gt; 0) &#123; String[] brokers = route.getOrderTopicConf().split(\";\"); for (String broker : brokers) &#123; String[] item = broker.split(\":\"); int nums = Integer.parseInt(item[1]); for (int i = 0; i &lt; nums; i++) &#123; MessageQueue mq = new MessageQueue(topic, item[0], i); info.getMessageQueueList().add(mq); &#125; &#125; info.setOrderTopic(true); &#125; else &#123; //非顺序消息更新TopicPublishInfo List&lt;QueueData&gt; qds = route.getQueueDatas(); Collections.sort(qds); //遍历topic队列信息 for (QueueData qd : qds) &#123; //是否是写队列 if (PermName.isWriteable(qd.getPerm())) &#123; BrokerData brokerData = null; //遍历写队列Broker for (BrokerData bd : route.getBrokerDatas()) &#123; //根据名称获得读队列对应的Broker if (bd.getBrokerName().equals(qd.getBrokerName())) &#123; brokerData = bd; break; &#125; &#125; if (null == brokerData) &#123; continue; &#125; if (!brokerData.getBrokerAddrs().containsKey(MixAll.MASTER_ID)) &#123; continue; &#125; //封装TopicPublishInfo写队列 for (int i = 0; i &lt; qd.getWriteQueueNums(); i++) &#123; MessageQueue mq = new MessageQueue(topic, qd.getBrokerName(), i); info.getMessageQueueList().add(mq); &#125; &#125; &#125; info.setOrderTopic(false); &#125; //返回TopicPublishInfo对象，然后开始选择队列发送消息 return info;&#125; 3）选择队列 默认不启用Broker故障延迟机制 代码：TopicPublishInfo#selectOneMessageQueue(lastBrokerName) public MessageQueue selectOneMessageQueue(final String lastBrokerName) &#123; //第一次选择队列 if (lastBrokerName == null) &#123; return selectOneMessageQueue(); &#125; else &#123; //sendWhichQueue int index = this.sendWhichQueue.getAndIncrement(); //遍历消息队列集合 for (int i = 0; i &lt; this.messageQueueList.size(); i++) &#123; //sendWhichQueue自增后取模 int pos = Math.abs(index++) % this.messageQueueList.size(); if (pos &lt; 0) pos = 0; //规避上次Broker队列 MessageQueue mq = this.messageQueueList.get(pos); if (!mq.getBrokerName().equals(lastBrokerName)) &#123; return mq; &#125; &#125; //如果以上情况都不满足,返回sendWhichQueue取模后的队列 return selectOneMessageQueue(); &#125;&#125; 代码：TopicPublishInfo#selectOneMessageQueue() //第一次选择队列public MessageQueue selectOneMessageQueue() &#123; //sendWhichQueue自增 int index = this.sendWhichQueue.getAndIncrement(); //对队列大小取模 int pos = Math.abs(index) % this.messageQueueList.size(); if (pos &lt; 0) pos = 0; //返回对应的队列 return this.messageQueueList.get(pos);&#125; 启用Broker故障延迟机制 public MessageQueue selectOneMessageQueue(final TopicPublishInfo tpInfo, final String lastBrokerName) &#123;//上一次发送消息的lastBrokerName //Broker故障延迟机制 if (this.sendLatencyFaultEnable) &#123; try &#123; //对sendWhichQueue自增 int index = tpInfo.getSendWhichQueue().getAndIncrement(); //对消息队列轮询获取一个队列 for (int i = 0; i &lt; tpInfo.getMessageQueueList().size(); i++) &#123; int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size(); if (pos &lt; 0) pos = 0; MessageQueue mq = tpInfo.getMessageQueueList().get(pos); //验证该队列是否可用，latencyFaultTolerance中维护了一个map，里面保存了曾经发送失败的brokername if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) &#123; //可用 if (null == lastBrokerName || mq.getBrokerName().equals(lastBrokerName)) return mq; &#125; &#125; //从规避的Broker中选择一个可用的Broker final String notBestBroker = latencyFaultTolerance.pickOneAtLeast(); //获得Broker的写队列集合 int writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker); if (writeQueueNums &gt; 0) &#123; //获得一个队列,指定broker和队列ID并返回 final MessageQueue mq = tpInfo.selectOneMessageQueue(); if (notBestBroker != null) &#123; mq.setBrokerName(notBestBroker); mq.setQueueId(tpInfo.getSendWhichQueue().getAndIncrement() % writeQueueNums); &#125; return mq; &#125; else &#123; latencyFaultTolerance.remove(notBestBroker); &#125; &#125; catch (Exception e) &#123; log.error(\"Error occurred when selecting message queue\", e); &#125; return tpInfo.selectOneMessageQueue(); &#125; return tpInfo.selectOneMessageQueue(lastBrokerName);&#125; 延迟机制接口规范 public interface LatencyFaultTolerance&lt;T&gt; &#123; //更新失败条目 void updateFaultItem(final T name, final long currentLatency, final long notAvailableDuration); //判断Broker是否可用 boolean isAvailable(final T name); //移除Fault条目 void remove(final T name); //尝试从规避的Broker中选择一个可用的Broker T pickOneAtLeast();&#125; FaultItem：失败条目 class FaultItem implements Comparable&lt;FaultItem&gt; &#123; //条目唯一键,这里为brokerName private final String name; //本次消息发送延迟 private volatile long currentLatency; //故障规避开始时间 private volatile long startTimestamp;&#125; 消息失败策略 public class MQFaultStrategy &#123; //根据currentLatency本地消息发送延迟,从latencyMax尾部向前找到第一个比currentLatency小的索引,如果没有找到,返回0 private long[] latencyMax = &#123;50L, 100L, 550L, 1000L, 2000L, 3000L, 15000L&#125;; //根据这个索引从notAvailableDuration取出对应的时间,在该时长内,Broker设置为不可用 private long[] notAvailableDuration = &#123;0L, 0L, 30000L, 60000L, 120000L, 180000L, 600000L&#125;;&#125; 原理分析 代码：DefaultMQProducerImpl#sendDefaultImpl sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime);endTimestamp = System.currentTimeMillis();this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false); 如果上述发送过程出现异常，则调用DefaultMQProducerImpl#updateFaultItem public void updateFaultItem(final String brokerName, final long currentLatency, boolean isolation) &#123; //参数一：broker名称 //参数二:本次消息发送延迟时间 //参数三:是否隔离 this.mqFaultStrategy.updateFaultItem(brokerName, currentLatency, isolation);&#125; 代码：MQFaultStrategy#updateFaultItem public void updateFaultItem(final String brokerName, final long currentLatency, boolean isolation) &#123; if (this.sendLatencyFaultEnable) &#123; //计算broker规避的时长 long duration = computeNotAvailableDuration(isolation ? 30000 : currentLatency); //更新该FaultItem规避时长 this.latencyFaultTolerance.updateFaultItem(brokerName, currentLatency, duration); &#125;&#125; 代码：MQFaultStrategy#computeNotAvailableDuration private long computeNotAvailableDuration(final long currentLatency) &#123; //遍历latencyMax for (int i = latencyMax.length - 1; i &gt;= 0; i--) &#123; //找到第一个比currentLatency的latencyMax值 if (currentLatency &gt;= latencyMax[i]) return this.notAvailableDuration[i]; &#125; //没有找到则返回0 return 0;&#125; 代码：LatencyFaultToleranceImpl#updateFaultItem public void updateFaultItem(final String name, final long currentLatency, final long notAvailableDuration) &#123; //获得原FaultItem FaultItem old = this.faultItemTable.get(name); //为空新建faultItem对象,设置规避时长和开始时间 if (null == old) &#123; final FaultItem faultItem = new FaultItem(name); faultItem.setCurrentLatency(currentLatency); faultItem.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration); old = this.faultItemTable.putIfAbsent(name, faultItem); if (old != null) &#123; old.setCurrentLatency(currentLatency); old.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration); &#125; &#125; else &#123; //更新规避时长和开始时间 old.setCurrentLatency(currentLatency); old.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration); &#125;&#125; ####4）发送消息 消息发送API核心入口DefaultMQProducerImpl#sendKernelImpl private SendResult sendKernelImpl( final Message msg, //待发送消息 final MessageQueue mq, //消息发送队列 final CommunicationMode communicationMode, //消息发送内模式 final SendCallback sendCallback, pp //异步消息回调函数 final TopicPublishInfo topicPublishInfo, //主题路由信息 final long timeout //超时时间 ) 代码：DefaultMQProducerImpl#sendKernelImpl //获得broker网络地址信息String brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());if (null == brokerAddr) &#123; //没有找到从NameServer更新broker网络地址信息 tryToFindTopicPublishInfo(mq.getTopic()); brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());&#125; //为消息分类唯一IDif (!(msg instanceof MessageBatch)) &#123; MessageClientIDSetter.setUniqID(msg);&#125;boolean topicWithNamespace = false;if (null != this.mQClientFactory.getClientConfig().getNamespace()) &#123; msg.setInstanceId(this.mQClientFactory.getClientConfig().getNamespace()); topicWithNamespace = true;&#125;//消息大小超过4K,启用消息压缩int sysFlag = 0;boolean msgBodyCompressed = false;if (this.tryToCompressMessage(msg)) &#123; sysFlag |= MessageSysFlag.COMPRESSED_FLAG; msgBodyCompressed = true;&#125;//如果是事务消息,设置消息标记MessageSysFlag.TRANSACTION_PREPARED_TYPEfinal String tranMsg = msg.getProperty(MessageConst.PROPERTY_TRANSACTION_PREPARED);if (tranMsg != null &amp;&amp; Boolean.parseBoolean(tranMsg)) &#123; sysFlag |= MessageSysFlag.TRANSACTION_PREPARED_TYPE;&#125; //如果注册了消息发送钩子函数,在执行消息发送前的增强逻辑if (this.hasSendMessageHook()) &#123; context = new SendMessageContext(); context.setProducer(this); context.setProducerGroup(this.defaultMQProducer.getProducerGroup()); context.setCommunicationMode(communicationMode); context.setBornHost(this.defaultMQProducer.getClientIP()); context.setBrokerAddr(brokerAddr); context.setMessage(msg); context.setMq(mq); context.setNamespace(this.defaultMQProducer.getNamespace()); String isTrans = msg.getProperty(MessageConst.PROPERTY_TRANSACTION_PREPARED); if (isTrans != null &amp;&amp; isTrans.equals(\"true\")) &#123; context.setMsgType(MessageType.Trans_Msg_Half); &#125; if (msg.getProperty(\"__STARTDELIVERTIME\") != null || msg.getProperty(MessageConst.PROPERTY_DELAY_TIME_LEVEL) != null) &#123; context.setMsgType(MessageType.Delay_Msg); &#125; this.executeSendMessageHookBefore(context);&#125; 代码：SendMessageHook public interface SendMessageHook &#123; String hookName(); void sendMessageBefore(final SendMessageContext context); void sendMessageAfter(final SendMessageContext context);&#125; 代码：DefaultMQProducerImpl#sendKernelImpl //构建消息发送请求包SendMessageRequestHeader requestHeader = new SendMessageRequestHeader();//生产者组requestHeader.setProducerGroup(this.defaultMQProducer.getProducerGroup());//主题requestHeader.setTopic(msg.getTopic());//默认创建主题KeyrequestHeader.setDefaultTopic(this.defaultMQProducer.getCreateTopicKey());//该主题在单个Broker默认队列树requestHeader.setDefaultTopicQueueNums(this.defaultMQProducer.getDefaultTopicQueueNums());//队列IDrequestHeader.setQueueId(mq.getQueueId());//消息系统标记requestHeader.setSysFlag(sysFlag);//消息发送时间requestHeader.setBornTimestamp(System.currentTimeMillis());//消息标记requestHeader.setFlag(msg.getFlag());//消息扩展信息requestHeader.setProperties(MessageDecoder.messageProperties2String(msg.getProperties()));//消息重试次数requestHeader.setReconsumeTimes(0);requestHeader.setUnitMode(this.isUnitMode());//是否是批量消息等requestHeader.setBatch(msg instanceof MessageBatch);if (requestHeader.getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) &#123; String reconsumeTimes = MessageAccessor.getReconsumeTime(msg); if (reconsumeTimes != null) &#123; requestHeader.setReconsumeTimes(Integer.valueOf(reconsumeTimes)); MessageAccessor.clearProperty(msg, MessageConst.PROPERTY_RECONSUME_TIME); &#125; String maxReconsumeTimes = MessageAccessor.getMaxReconsumeTimes(msg); if (maxReconsumeTimes != null) &#123; requestHeader.setMaxReconsumeTimes(Integer.valueOf(maxReconsumeTimes)); MessageAccessor.clearProperty(msg, MessageConst.PROPERTY_MAX_RECONSUME_TIMES); &#125;&#125; case ASYNC: //异步发送 Message tmpMessage = msg; boolean messageCloned = false; if (msgBodyCompressed) &#123; //If msg body was compressed, msgbody should be reset using prevBody. //Clone new message using commpressed message body and recover origin massage. //Fix bug:https://github.com/apache/rocketmq-externals/issues/66 tmpMessage = MessageAccessor.cloneMessage(msg); messageCloned = true; msg.setBody(prevBody); &#125; if (topicWithNamespace) &#123; if (!messageCloned) &#123; tmpMessage = MessageAccessor.cloneMessage(msg); messageCloned = true; &#125; msg.setTopic(NamespaceUtil.withoutNamespace(msg.getTopic(), this.defaultMQProducer.getNamespace())); &#125; long costTimeAsync = System.currentTimeMillis() - beginStartTime; if (timeout &lt; costTimeAsync) &#123; throw new RemotingTooMuchRequestException(\"sendKernelImpl call timeout\"); &#125; sendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage( brokerAddr, mq.getBrokerName(), tmpMessage, requestHeader, timeout - costTimeAsync, communicationMode, sendCallback, topicPublishInfo, this.mQClientFactory, this.defaultMQProducer.getRetryTimesWhenSendAsyncFailed(), context, this); break;case ONEWAY:case SYNC: //同步发送 long costTimeSync = System.currentTimeMillis() - beginStartTime; if (timeout &lt; costTimeSync) &#123; throw new RemotingTooMuchRequestException(\"sendKernelImpl call timeout\"); &#125; sendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage( brokerAddr, mq.getBrokerName(), msg, requestHeader, timeout - costTimeSync, communicationMode, context, this); break; default: assert false; break;&#125; //如果注册了钩子函数,则发送完毕后执行钩子函数if (this.hasSendMessageHook()) &#123; context.setSendResult(sendResult); this.executeSendMessageHookAfter(context);&#125; 2.3.4 批量消息发送 批量消息发送是将同一个主题的多条消息一起打包发送到消息服务端，减少网络调用次数，提高网络传输效率。当然，并不是在同一批次中发送的消息数量越多越好，其判断依据是单条消息的长度，如果单条消息内容比较长，则打包多条消息发送会影响其他线程发送消息的响应时间，并且单批次消息总长度不能超过DefaultMQProducer#maxMessageSize。 批量消息发送要解决的问题是如何将这些消息编码以便服务端能够正确解码出每条消息的消息内容。 代码：DefaultMQProducer#send public SendResult send(Collection&lt;Message&gt; msgs) throws MQClientException, RemotingException, MQBrokerException, InterruptedException &#123; //压缩消息集合成一条消息,然后发送出去 return this.defaultMQProducerImpl.send(batch(msgs));&#125; 代码：DefaultMQProducer#batch private MessageBatch batch(Collection&lt;Message&gt; msgs) throws MQClientException &#123; MessageBatch msgBatch; try &#123; //将集合消息封装到MessageBatch msgBatch = MessageBatch.generateFromList(msgs); //遍历消息集合,检查消息合法性,设置消息ID,设置Topic for (Message message : msgBatch) &#123; Validators.checkMessage(message, this); MessageClientIDSetter.setUniqID(message); message.setTopic(withNamespace(message.getTopic())); &#125; //压缩消息,设置消息body msgBatch.setBody(msgBatch.encode()); &#125; catch (Exception e) &#123; throw new MQClientException(\"Failed to initiate the MessageBatch\", e); &#125; //设置msgBatch的topic msgBatch.setTopic(withNamespace(msgBatch.getTopic())); return msgBatch;&#125; 2.4 消息存储###2.4.1 消息存储核心类 private final MessageStoreConfig messageStoreConfig; //消息配置属性private final CommitLog commitLog; //CommitLog文件存储的实现类private final ConcurrentMap&lt;String/* topic */, ConcurrentMap&lt;Integer/* queueId */, ConsumeQueue&gt;&gt; consumeQueueTable; //消息队列存储缓存表,按照消息主题分组private final FlushConsumeQueueService flushConsumeQueueService; //消息队列文件刷盘线程private final CleanCommitLogService cleanCommitLogService; //清除CommitLog文件服务private final CleanConsumeQueueService cleanConsumeQueueService; //清除ConsumerQueue队列文件服务private final IndexService indexService; //索引实现类private final AllocateMappedFileService allocateMappedFileService; //MappedFile分配服务private final ReputMessageService reputMessageService;//CommitLog消息分发,根据CommitLog文件构建ConsumerQueue、IndexFile文件private final HAService haService; //存储HA机制private final ScheduleMessageService scheduleMessageService; //消息服务调度线程private final StoreStatsService storeStatsService; //消息存储服务private final TransientStorePool transientStorePool; //消息堆外内存缓存private final BrokerStatsManager brokerStatsManager; //Broker状态管理器private final MessageArrivingListener messageArrivingListener; //消息拉取长轮询模式消息达到监听器private final BrokerConfig brokerConfig; //Broker配置类private StoreCheckpoint storeCheckpoint; //文件刷盘监测点private final LinkedList&lt;CommitLogDispatcher&gt; dispatcherList; //CommitLog文件转发请求 2.4.2 消息存储流程 消息存储入口：DefaultMessageStore#putMessage //判断Broker角色如果是从节点,则无需写入if (BrokerRole.SLAVE == this.messageStoreConfig.getBrokerRole()) &#123; long value = this.printTimes.getAndIncrement(); if ((value % 50000) == 0) &#123; log.warn(\"message store is slave mode, so putMessage is forbidden \"); &#125; return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null);&#125;//判断当前写入状态如果是正在写入,则不能继续if (!this.runningFlags.isWriteable()) &#123; long value = this.printTimes.getAndIncrement(); return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null);&#125; else &#123; this.printTimes.set(0);&#125;//判断消息主题长度是否超过最大限制if (msg.getTopic().length() &gt; Byte.MAX_VALUE) &#123; log.warn(\"putMessage message topic length too long \" + msg.getTopic().length()); return new PutMessageResult(PutMessageStatus.MESSAGE_ILLEGAL, null);&#125;//判断消息属性长度是否超过限制if (msg.getPropertiesString() != null &amp;&amp; msg.getPropertiesString().length() &gt; Short.MAX_VALUE) &#123; log.warn(\"putMessage message properties length too long \" + msg.getPropertiesString().length()); return new PutMessageResult(PutMessageStatus.PROPERTIES_SIZE_EXCEEDED, null);&#125;//判断系统PageCache缓存去是否占用if (this.isOSPageCacheBusy()) &#123; return new PutMessageResult(PutMessageStatus.OS_PAGECACHE_BUSY, null);&#125;//将消息写入CommitLog文件PutMessageResult result = this.commitLog.putMessage(msg); 代码：CommitLog#putMessage //记录消息存储时间msg.setStoreTimestamp(beginLockTimestamp);//判断如果mappedFile如果为空或者已满,创建新的mappedFile文件if (null == mappedFile || mappedFile.isFull()) &#123; mappedFile = this.mappedFileQueue.getLastMappedFile(0); &#125;//如果创建失败,直接返回if (null == mappedFile) &#123; log.error(\"create mapped file1 error, topic: \" + msg.getTopic() + \" clientAddr: \" + msg.getBornHostString()); beginTimeInLock = 0; return new PutMessageResult(PutMessageStatus.CREATE_MAPEDFILE_FAILED, null);&#125;//写入消息到mappedFile中result = mappedFile.appendMessage(msg, this.appendMessageCallback); 代码：MappedFile#appendMessagesInner //获得文件的写入指针int currentPos = this.wrotePosition.get();//如果指针大于文件大小则直接返回if (currentPos &lt; this.fileSize) &#123; //通过writeBuffer.slice()创建一个与MappedFile共享的内存区,并设置position为当前指针 ByteBuffer byteBuffer = writeBuffer != null ? writeBuffer.slice() : this.mappedByteBuffer.slice(); byteBuffer.position(currentPos); AppendMessageResult result = null; if (messageExt instanceof MessageExtBrokerInner) &#123; //通过回调方法写入 result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBrokerInner) messageExt); &#125; else if (messageExt instanceof MessageExtBatch) &#123; result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBatch) messageExt); &#125; else &#123; return new AppendMessageResult(AppendMessageStatus.UNKNOWN_ERROR); &#125; this.wrotePosition.addAndGet(result.getWroteBytes()); this.storeTimestamp = result.getStoreTimestamp(); return result;&#125; 代码：CommitLog#doAppend //文件写入位置long wroteOffset = fileFromOffset + byteBuffer.position();//设置消息IDthis.resetByteBuffer(hostHolder, 8);String msgId = MessageDecoder.createMessageId(this.msgIdMemory, msgInner.getStoreHostBytes(hostHolder), wroteOffset);//获得该消息在消息队列中的偏移量keyBuilder.setLength(0);keyBuilder.append(msgInner.getTopic());keyBuilder.append('-');keyBuilder.append(msgInner.getQueueId());String key = keyBuilder.toString();Long queueOffset = CommitLog.this.topicQueueTable.get(key);if (null == queueOffset) &#123; queueOffset = 0L; CommitLog.this.topicQueueTable.put(key, queueOffset);&#125;//获得消息属性长度final byte[] propertiesData =msgInner.getPropertiesString() == null ? null : msgInner.getPropertiesString().getBytes(MessageDecoder.CHARSET_UTF8);final int propertiesLength = propertiesData == null ? 0 : propertiesData.length;if (propertiesLength &gt; Short.MAX_VALUE) &#123; log.warn(\"putMessage message properties length too long. length=&#123;&#125;\", propertiesData.length); return new AppendMessageResult(AppendMessageStatus.PROPERTIES_SIZE_EXCEEDED);&#125;//获得消息主题大小final byte[] topicData = msgInner.getTopic().getBytes(MessageDecoder.CHARSET_UTF8);final int topicLength = topicData.length;//获得消息体大小final int bodyLength = msgInner.getBody() == null ? 0 : msgInner.getBody().length;//计算消息总长度final int msgLen = calMsgLength(bodyLength, topicLength, propertiesLength); 代码：CommitLog#calMsgLength protected static int calMsgLength(int bodyLength, int topicLength, int propertiesLength) &#123; final int msgLen = 4 //TOTALSIZE + 4 //MAGICCODE + 4 //BODYCRC + 4 //QUEUEID + 4 //FLAG + 8 //QUEUEOFFSET + 8 //PHYSICALOFFSET + 4 //SYSFLAG + 8 //BORNTIMESTAMP + 8 //BORNHOST + 8 //STORETIMESTAMP + 8 //STOREHOSTADDRESS + 4 //RECONSUMETIMES + 8 //Prepared Transaction Offset + 4 + (bodyLength &gt; 0 ? bodyLength : 0) //BODY + 1 + topicLength //TOPIC + 2 + (propertiesLength &gt; 0 ? propertiesLength : 0) //propertiesLength + 0; return msgLen;&#125; 代码：CommitLog#doAppend //消息长度不能超过4Mif (msgLen &gt; this.maxMessageSize) &#123; CommitLog.log.warn(\"message size exceeded, msg total size: \" + msgLen + \", msg body size: \" + bodyLength + \", maxMessageSize: \" + this.maxMessageSize); return new AppendMessageResult(AppendMessageStatus.MESSAGE_SIZE_EXCEEDED);&#125;//消息是如果没有足够的存储空间则新创建CommitLog文件if ((msgLen + END_FILE_MIN_BLANK_LENGTH) &gt; maxBlank) &#123; this.resetByteBuffer(this.msgStoreItemMemory, maxBlank); // 1 TOTALSIZE this.msgStoreItemMemory.putInt(maxBlank); // 2 MAGICCODE this.msgStoreItemMemory.putInt(CommitLog.BLANK_MAGIC_CODE); // 3 The remaining space may be any value // Here the length of the specially set maxBlank final long beginTimeMills = CommitLog.this.defaultMessageStore.now(); byteBuffer.put(this.msgStoreItemMemory.array(), 0, maxBlank); return new AppendMessageResult(AppendMessageStatus.END_OF_FILE, wroteOffset, maxBlank, msgId, msgInner.getStoreTimestamp(), queueOffset, CommitLog.this.defaultMessageStore.now() - beginTimeMills);&#125;//将消息存储到ByteBuffer中,返回AppendMessageResultfinal long beginTimeMills = CommitLog.this.defaultMessageStore.now();// Write messages to the queue bufferbyteBuffer.put(this.msgStoreItemMemory.array(), 0, msgLen);AppendMessageResult result = new AppendMessageResult(AppendMessageStatus.PUT_OK, wroteOffset, msgLen, msgId,msgInner.getStoreTimestamp(), queueOffset, CommitLog.this.defaultMessageStore.now() -beginTimeMills);switch (tranType) &#123; case MessageSysFlag.TRANSACTION_PREPARED_TYPE: case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE: break; case MessageSysFlag.TRANSACTION_NOT_TYPE: case MessageSysFlag.TRANSACTION_COMMIT_TYPE: //更新消息队列偏移量 CommitLog.this.topicQueueTable.put(key, ++queueOffset); break; default: break;&#125; 代码：CommitLog#putMessage //释放锁putMessageLock.unlock();//刷盘handleDiskFlush(result, putMessageResult, msg);//执行HA主从同步handleHA(result, putMessageResult, msg); 2.4.3 存储文件 commitLog：消息存储目录 config：运行期间一些配置信息 consumerqueue：消息消费队列存储目录 index：消息索引文件存储目录 abort：如果存在改文件寿命Broker非正常关闭 checkpoint：文件检查点，存储CommitLog文件最后一次刷盘时间戳、consumerquueue最后一次刷盘时间，index索引文件最后一次刷盘时间戳。 2.4.4 存储文件内存映射​ RocketMQ通过使用内存映射文件提高IO访问性能，无论是CommitLog、ConsumerQueue还是IndexFile，单个文件都被设计为固定长度，如果一个文件写满以后再创建一个新文件，文件名就为该文件第一条消息对应的全局物理偏移量。 ####1）MappedFileQueue - 等同于commitlog String storePath; //存储目录int mappedFileSize; // 单个文件大小，默认1GCopyOnWriteArrayList&lt;MappedFile&gt; mappedFiles; //MappedFile文件集合AllocateMappedFileService allocateMappedFileService; //创建MapFile服务类long flushedWhere = 0; //当前刷盘指针long committedWhere = 0; //当前数据提交指针,内存中ByteBuffer当前的写指针,该值大于等于flushWhere 根据存储时间查询MappedFile public MappedFile getMappedFileByTime(final long timestamp) &#123; Object[] mfs = this.copyMappedFiles(0); if (null == mfs) return null; //遍历MappedFile文件数组 for (int i = 0; i &lt; mfs.length; i++) &#123; MappedFile mappedFile = (MappedFile) mfs[i]; //MappedFile文件的最后修改时间大于指定时间戳则返回该文件 if (mappedFile.getLastModifiedTimestamp() &gt;= timestamp) &#123; return mappedFile; &#125; &#125; return (MappedFile) mfs[mfs.length - 1];&#125; 根据消息偏移量offset查找MappedFile public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) &#123; try &#123; //获得第一个MappedFile文件 MappedFile firstMappedFile = this.getFirstMappedFile(); //获得最后一个MappedFile文件 MappedFile lastMappedFile = this.getLastMappedFile(); //第一个文件和最后一个文件均不为空,则进行处理 if (firstMappedFile != null &amp;&amp; lastMappedFile != null) &#123; if (offset &lt; firstMappedFile.getFileFromOffset() || offset &gt;= lastMappedFile.getFileFromOffset() + this.mappedFileSize) &#123; &#125; else &#123; //获得文件索引 int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize)); MappedFile targetFile = null; try &#123; //根据索引返回目标文件 targetFile = this.mappedFiles.get(index); &#125; catch (Exception ignored) &#123; &#125; if (targetFile != null &amp;&amp; offset &gt;= targetFile.getFileFromOffset() &amp;&amp; offset &lt; targetFile.getFileFromOffset() + this.mappedFileSize) &#123; return targetFile; &#125; for (MappedFile tmpMappedFile : this.mappedFiles) &#123; if (offset &gt;= tmpMappedFile.getFileFromOffset() &amp;&amp; offset &lt; tmpMappedFile.getFileFromOffset() + this.mappedFileSize) &#123; return tmpMappedFile; &#125; &#125; &#125; if (returnFirstOnNotFound) &#123; return firstMappedFile; &#125; &#125; &#125; catch (Exception e) &#123; log.error(\"findMappedFileByOffset Exception\", e); &#125; return null;&#125; 获取存储文件最小偏移量 public long getMinOffset() &#123; if (!this.mappedFiles.isEmpty()) &#123; try &#123; return this.mappedFiles.get(0).getFileFromOffset(); &#125; catch (IndexOutOfBoundsException e) &#123; //continue; &#125; catch (Exception e) &#123; log.error(\"getMinOffset has exception.\", e); &#125; &#125; return -1;&#125; 获取存储文件最大偏移量 public long getMaxOffset() &#123; MappedFile mappedFile = getLastMappedFile(); if (mappedFile != null) &#123; return mappedFile.getFileFromOffset() + mappedFile.getReadPosition(); &#125; return 0;&#125; 返回存储文件当前写指针 public long getMaxWrotePosition() &#123; MappedFile mappedFile = getLastMappedFile(); if (mappedFile != null) &#123; return mappedFile.getFileFromOffset() + mappedFile.getWrotePosition(); &#125; return 0;&#125; ####2）MappedFile int OS_PAGE_SIZE = 1024 * 4; //操作系统每页大小,默认4KAtomicLong TOTAL_MAPPED_VIRTUAL_MEMORY = new AtomicLong(0); //当前JVM实例中MappedFile虚拟内存AtomicInteger TOTAL_MAPPED_FILES = new AtomicInteger(0); //当前JVM实例中MappedFile对象个数AtomicInteger wrotePosition = new AtomicInteger(0); //当前文件的写指针AtomicInteger committedPosition = new AtomicInteger(0); //当前文件的提交指针AtomicInteger flushedPosition = new AtomicInteger(0); //刷写到磁盘指针int fileSize; //文件大小FileChannel fileChannel; //文件通道 ByteBuffer writeBuffer = null; //堆外内存ByteBufferTransientStorePool transientStorePool = null; //堆外内存池String fileName; //文件名称long fileFromOffset; //该文件的处理偏移量File file; //物理文件MappedByteBuffer mappedByteBuffer; //物理文件对应的内存映射Buffervolatile long storeTimestamp = 0; //文件最后一次内容写入时间boolean firstCreateInQueue = false; //是否是MappedFileQueue队列中第一个文件 MappedFile初始化 未开启transientStorePoolEnable。transientStorePoolEnable=true为true表示数据先存储到堆外内存，然后通过Commit线程将数据提交到内存映射Buffer中，再通过Flush线程将内存映射Buffer中数据持久化磁盘。 private void init(final String fileName, final int fileSize) throws IOException &#123; this.fileName = fileName; this.fileSize = fileSize; this.file = new File(fileName); this.fileFromOffset = Long.parseLong(this.file.getName()); boolean ok = false; ensureDirOK(this.file.getParent()); try &#123; this.fileChannel = new RandomAccessFile(this.file, \"rw\").getChannel(); this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize); TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(fileSize); TOTAL_MAPPED_FILES.incrementAndGet(); ok = true; &#125; catch (FileNotFoundException e) &#123; log.error(\"create file channel \" + this.fileName + \" Failed. \", e); throw e; &#125; catch (IOException e) &#123; log.error(\"map file \" + this.fileName + \" Failed. \", e); throw e; &#125; finally &#123; if (!ok &amp;&amp; this.fileChannel != null) &#123; this.fileChannel.close(); &#125; &#125;&#125; 开启transientStorePoolEnable public void init(final String fileName, final int fileSize, final TransientStorePool transientStorePool) throws IOException &#123; init(fileName, fileSize); this.writeBuffer = transientStorePool.borrowBuffer(); //初始化writeBuffer this.transientStorePool = transientStorePool;&#125; MappedFile提交 提交数据到FileChannel，commitLeastPages为本次提交最小的页数，如果待提交数据不满commitLeastPages，则不执行本次提交操作。如果writeBuffer如果为空，直接返回writePosition指针，无需执行commit操作，表名commit操作主体是writeBuffer。 public int commit(final int commitLeastPages) &#123; if (writeBuffer == null) &#123; //no need to commit data to file channel, so just regard wrotePosition as committedPosition. return this.wrotePosition.get(); &#125; //判断是否满足提交条件 if (this.isAbleToCommit(commitLeastPages)) &#123; if (this.hold()) &#123; commit0(commitLeastPages); this.release(); &#125; else &#123; log.warn(\"in commit, hold failed, commit offset = \" + this.committedPosition.get()); &#125; &#125; // 所有数据提交后,清空缓冲区 if (writeBuffer != null &amp;&amp; this.transientStorePool != null &amp;&amp; this.fileSize == this.committedPosition.get()) &#123; this.transientStorePool.returnBuffer(writeBuffer); this.writeBuffer = null; &#125; return this.committedPosition.get();&#125; MappedFile#isAbleToCommit 判断是否执行commit操作，如果文件已满返回true；如果commitLeastpages大于0，则比较writePosition与上一次提交的指针commitPosition的差值，除以OS_PAGE_SIZE得到当前脏页的数量，如果大于commitLeastPages则返回true，如果commitLeastpages小于0表示只要存在脏页就提交。 protected boolean isAbleToCommit(final int commitLeastPages) &#123; //已经刷盘指针 int flush = this.committedPosition.get(); //文件写指针 int write = this.wrotePosition.get(); //写满刷盘 if (this.isFull()) &#123; return true; &#125; if (commitLeastPages &gt; 0) &#123; //文件内容达到commitLeastPages页数,则刷盘 return ((write / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE)) &gt;= commitLeastPages; &#125; return write &gt; flush;&#125; MappedFile#commit0 具体提交的实现，首先创建WriteBuffer区共享缓存区，然后将新创建的position回退到上一次提交的位置（commitPosition），设置limit为wrotePosition（当前最大有效数据指针），然后把commitPosition到wrotePosition的数据写入到FileChannel中，然后更新committedPosition指针为wrotePosition。commit的作用就是将MappedFile的writeBuffer中数据提交到文件通道FileChannel中。 protected void commit0(final int commitLeastPages) &#123; //写指针 int writePos = this.wrotePosition.get(); //上次提交指针 int lastCommittedPosition = this.committedPosition.get(); if (writePos - this.committedPosition.get() &gt; 0) &#123; try &#123; //复制共享内存区域 ByteBuffer byteBuffer = writeBuffer.slice(); //设置提交位置是上次提交位置 byteBuffer.position(lastCommittedPosition); //最大提交数量 byteBuffer.limit(writePos); //设置fileChannel位置为上次提交位置 this.fileChannel.position(lastCommittedPosition); //将lastCommittedPosition到writePos的数据复制到FileChannel中 this.fileChannel.write(byteBuffer); //重置提交位置 this.committedPosition.set(writePos); &#125; catch (Throwable e) &#123; log.error(\"Error occurred when commit data to FileChannel.\", e); &#125; &#125;&#125; MappedFile#flush 刷写磁盘，直接调用MappedByteBuffer或fileChannel的force方法将内存中的数据持久化到磁盘，那么flushedPosition应该等于MappedByteBuffer中的写指针；如果writeBuffer不为空，则flushPosition应该等于上一次的commit指针；因为上一次提交的数据就是进入到MappedByteBuffer中的数据；如果writeBuffer为空，数据时直接进入到MappedByteBuffer，wrotePosition代表的是MappedByteBuffer中的指针，故设置flushPosition为wrotePosition。 public int flush(final int flushLeastPages) &#123; //数据达到刷盘条件 if (this.isAbleToFlush(flushLeastPages)) &#123; //加锁，同步刷盘 if (this.hold()) &#123; //获得读指针 int value = getReadPosition(); try &#123; //数据从writeBuffer提交数据到fileChannel再刷新到磁盘 if (writeBuffer != null || this.fileChannel.position() != 0) &#123; this.fileChannel.force(false); &#125; else &#123; //从mmap刷新数据到磁盘 this.mappedByteBuffer.force(); &#125; &#125; catch (Throwable e) &#123; log.error(\"Error occurred when force data to disk.\", e); &#125; //更新刷盘位置 this.flushedPosition.set(value); this.release(); &#125; else &#123; log.warn(\"in flush, hold failed, flush offset = \" + this.flushedPosition.get()); this.flushedPosition.set(getReadPosition()); &#125; &#125; return this.getFlushedPosition();&#125; MappedFile#getReadPosition 获取当前文件最大可读指针。如果writeBuffer为空，则直接返回当前的写指针；如果writeBuffer不为空，则返回上一次提交的指针。在MappedFile设置中,只有提交了的数据（写入到MappedByteBuffer或FileChannel中的数据）才是安全的数据 public int getReadPosition() &#123; //如果writeBuffer为空,刷盘的位置就是应该等于上次commit的位置,如果为空则为mmap的写指针 return this.writeBuffer == null ? this.wrotePosition.get() : this.committedPosition.get();&#125; MappedFile#selectMappedBuffer 查找pos到当前最大可读之间的数据，由于在整个写入期间都未曾改MappedByteBuffer的指针，如果mappedByteBuffer.slice()方法返回的共享缓存区空间为整个MappedFile，然后通过设置ByteBuffer的position为待查找的值，读取字节长度当前可读最大长度，最终返回的ByteBuffer的limit为size。整个共享缓存区的容量为（MappedFile#fileSize-pos）。故在操作SelectMappedBufferResult不能对包含在里面的ByteBuffer调用filp方法。 public SelectMappedBufferResult selectMappedBuffer(int pos) &#123; //获得最大可读指针 int readPosition = getReadPosition(); //pos小于最大可读指针,并且大于0 if (pos &lt; readPosition &amp;&amp; pos &gt;= 0) &#123; if (this.hold()) &#123; //复制mappedByteBuffer读共享区 ByteBuffer byteBuffer = this.mappedByteBuffer.slice(); //设置读指针位置 byteBuffer.position(pos); //获得可读范围 int size = readPosition - pos; //设置最大刻度范围 ByteBuffer byteBufferNew = byteBuffer.slice(); byteBufferNew.limit(size); return new SelectMappedBufferResult(this.fileFromOffset + pos, byteBufferNew, size, this); &#125; &#125; return null;&#125; MappedFile#shutdown MappedFile文件销毁的实现方法为public boolean destory(long intervalForcibly)，intervalForcibly表示拒绝被销毁的最大存活时间。 public void shutdown(final long intervalForcibly) &#123; if (this.available) &#123; //关闭MapedFile this.available = false; //设置当前关闭时间戳 this.firstShutdownTimestamp = System.currentTimeMillis(); //释放资源 this.release(); &#125; else if (this.getRefCount() &gt; 0) &#123; if ((System.currentTimeMillis() - this.firstShutdownTimestamp) &gt;= intervalForcibly) &#123; this.refCount.set(-1000 - this.getRefCount()); this.release(); &#125; &#125;&#125; 3）TransientStorePool短暂的存储池。RocketMQ单独创建一个MappedByteBuffer内存缓存池，用来临时存储数据，数据先写入该内存映射中，然后由commit线程定时将数据从该内存复制到与目标物理文件对应的内存映射中。RocketMQ引入该机制主要的原因是提供一种内存锁定，将当前堆外内存一直锁定在内存中，避免被进程将内存交换到磁盘。 private final int poolSize; //availableBuffers个数private final int fileSize; //每隔ByteBuffer大小private final Deque&lt;ByteBuffer&gt; availableBuffers; //ByteBuffer容器。双端队列 初始化 public void init() &#123; //创建poolSize个堆外内存 for (int i = 0; i &lt; poolSize; i++) &#123; ByteBuffer byteBuffer = ByteBuffer.allocateDirect(fileSize); final long address = ((DirectBuffer) byteBuffer).address(); Pointer pointer = new Pointer(address); //使用com.sun.jna.Library类库将该批内存锁定,避免被置换到交换区,提高存储性能 LibC.INSTANCE.mlock(pointer, new NativeLong(fileSize)); availableBuffers.offer(byteBuffer); &#125;&#125; 2.4.5 实时更新消息消费队列与索引文件消息消费队文件、消息属性索引文件都是基于CommitLog文件构建的，当消息生产者提交的消息存储在CommitLog文件中，ConsumerQueue、IndexFile需要及时更新，否则消息无法及时被消费，根据消息属性查找消息也会出现较大延迟。RocketMQ通过开启一个线程ReputMessageService来准实时转发CommitLog文件更新事件，相应的任务处理器根据转发的消息及时更新ConsumerQueue、IndexFile文件。 代码：DefaultMessageStore：start //设置CommitLog内存中最大偏移量this.reputMessageService.setReputFromOffset(maxPhysicalPosInLogicQueue);//启动this.reputMessageService.start(); 代码：DefaultMessageStore：run public void run() &#123; DefaultMessageStore.log.info(this.getServiceName() + \" service started\"); //每隔1毫秒就继续尝试推送消息到消息消费队列和索引文件 while (!this.isStopped()) &#123; try &#123; Thread.sleep(1); this.doReput(); &#125; catch (Exception e) &#123; DefaultMessageStore.log.warn(this.getServiceName() + \" service has exception. \", e); &#125; &#125; DefaultMessageStore.log.info(this.getServiceName() + \" service end\");&#125; 代码：DefaultMessageStore：deReput //从result中循环遍历消息,一次读一条,创建DispatherRequest对象。for (int readSize = 0; readSize &lt; result.getSize() &amp;&amp; doNext; ) &#123; DispatchRequest dispatchRequest = DefaultMessageStore.this.commitLog.checkMessageAndReturnSize(result.getByteBuffer(), false, false); int size = dispatchRequest.getBufferSize() == -1 ? dispatchRequest.getMsgSize() : dispatchRequest.getBufferSize(); if (dispatchRequest.isSuccess()) &#123; if (size &gt; 0) &#123; DefaultMessageStore.this.doDispatch(dispatchRequest); &#125; &#125;&#125; DispatchRequest String topic; //消息主题名称int queueId; //消息队列IDlong commitLogOffset; //消息物理偏移量int msgSize; //消息长度long tagsCode; //消息过滤tag hashCodelong storeTimestamp; //消息存储时间戳long consumeQueueOffset; //消息队列偏移量String keys; //消息索引keyboolean success; //是否成功解析到完整的消息String uniqKey; //消息唯一键int sysFlag; //消息系统标记long preparedTransactionOffset; //消息预处理事务偏移量Map&lt;String, String&gt; propertiesMap; //消息属性byte[] bitMap; //位图 1）转发到ConsumerQueue class CommitLogDispatcherBuildConsumeQueue implements CommitLogDispatcher &#123; @Override public void dispatch(DispatchRequest request) &#123; final int tranType = MessageSysFlag.getTransactionValue(request.getSysFlag()); switch (tranType) &#123; case MessageSysFlag.TRANSACTION_NOT_TYPE: case MessageSysFlag.TRANSACTION_COMMIT_TYPE: //消息分发 DefaultMessageStore.this.putMessagePositionInfo(request); break; case MessageSysFlag.TRANSACTION_PREPARED_TYPE: case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE: break; &#125; &#125;&#125; 代码：DefaultMessageStore#putMessagePositionInfo public void putMessagePositionInfo(DispatchRequest dispatchRequest) &#123; //获得消费队列 ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId()); //消费队列分发消息 cq.putMessagePositionInfoWrapper(dispatchRequest);&#125; 代码：DefaultMessageStore#putMessagePositionInfo //依次将消息偏移量、消息长度、tag写入到ByteBuffer中this.byteBufferIndex.flip();this.byteBufferIndex.limit(CQ_STORE_UNIT_SIZE);this.byteBufferIndex.putLong(offset);this.byteBufferIndex.putInt(size);this.byteBufferIndex.putLong(tagsCode);//获得内存映射文件MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile(expectLogicOffset);if (mappedFile != null) &#123; //将消息追加到内存映射文件,异步输盘 return mappedFile.appendMessage(this.byteBufferIndex.array());&#125; 2）转发到Index class CommitLogDispatcherBuildIndex implements CommitLogDispatcher &#123; @Override public void dispatch(DispatchRequest request) &#123; if (DefaultMessageStore.this.messageStoreConfig.isMessageIndexEnable()) &#123; DefaultMessageStore.this.indexService.buildIndex(request); &#125; &#125;&#125; 代码：DefaultMessageStore#buildIndex public void buildIndex(DispatchRequest req) &#123; //获得索引文件 IndexFile indexFile = retryGetAndCreateIndexFile(); if (indexFile != null) &#123; //获得文件最大物理偏移量 long endPhyOffset = indexFile.getEndPhyOffset(); DispatchRequest msg = req; String topic = msg.getTopic(); String keys = msg.getKeys(); //如果该消息的物理偏移量小于索引文件中的最大物理偏移量,则说明是重复数据,忽略本次索引构建 if (msg.getCommitLogOffset() &lt; endPhyOffset) &#123; return; &#125; final int tranType = MessageSysFlag.getTransactionValue(msg.getSysFlag()); switch (tranType) &#123; case MessageSysFlag.TRANSACTION_NOT_TYPE: case MessageSysFlag.TRANSACTION_PREPARED_TYPE: case MessageSysFlag.TRANSACTION_COMMIT_TYPE: break; case MessageSysFlag.TRANSACTION_ROLLBACK_TYPE: return; &#125; //如果消息ID不为空,则添加到Hash索引中 if (req.getUniqKey() != null) &#123; indexFile = putKey(indexFile, msg, buildKey(topic, req.getUniqKey())); if (indexFile == null) &#123; return; &#125; &#125; //构建索引key,RocketMQ支持为同一个消息建立多个索引,多个索引键空格隔开. if (keys != null &amp;&amp; keys.length() &gt; 0) &#123; String[] keyset = keys.split(MessageConst.KEY_SEPARATOR); for (int i = 0; i &lt; keyset.length; i++) &#123; String key = keyset[i]; if (key.length() &gt; 0) &#123; indexFile = putKey(indexFile, msg, buildKey(topic, key)); if (indexFile == null) &#123; return; &#125; &#125; &#125; &#125; &#125; else &#123; log.error(\"build index error, stop building index\"); &#125;&#125; 2.4.6 消息队列和索引文件恢复由于RocketMQ存储首先将消息全量存储在CommitLog文件中，然后异步生成转发任务更新ConsumerQueue和Index文件。如果消息成功存储到CommitLog文件中，转发任务未成功执行，此时消息服务器Broker由于某个愿意宕机，导致CommitLog、ConsumerQueue、IndexFile文件数据不一致。如果不加以人工修复的话，会有一部分消息即便在CommitLog中文件中存在，但由于没有转发到ConsumerQueue，这部分消息将永远复发被消费者消费。 ####1）存储文件加载 代码：DefaultMessageStore#load 判断上一次是否异常退出。实现机制是Broker在启动时创建abort文件，在退出时通过JVM钩子函数删除abort文件。如果下次启动时存在abort文件。说明Broker时异常退出的，CommitLog与ConsumerQueue数据有可能不一致，需要进行修复。 //判断临时文件是否存在boolean lastExitOK = !this.isTempFileExist();//根据临时文件判断当前Broker是否异常退出private boolean isTempFileExist() &#123; String fileName = StorePathConfigHelper .getAbortFile(this.messageStoreConfig.getStorePathRootDir()); File file = new File(fileName); return file.exists();&#125; 代码：DefaultMessageStore#load //加载延时队列if (null != scheduleMessageService) &#123; result = result &amp;&amp; this.scheduleMessageService.load();&#125;// 加载CommitLog文件result = result &amp;&amp; this.commitLog.load();// 加载消费队列文件result = result &amp;&amp; this.loadConsumeQueue();if (result) &#123; //加载存储监测点,监测点主要记录CommitLog文件、ConsumerQueue文件、Index索引文件的刷盘点 this.storeCheckpoint =new StoreCheckpoint(StorePathConfigHelper.getStoreCheckpoint(this.messageStoreConfig.getStorePathRootDir())); //加载index文件 this.indexService.load(lastExitOK); //根据Broker是否异常退出,执行不同的恢复策略 this.recover(lastExitOK);&#125; 代码：MappedFileQueue#load 加载CommitLog到映射文件 //指向CommitLog文件目录File dir = new File(this.storePath);//获得文件数组File[] files = dir.listFiles();if (files != null) &#123; // 文件排序 Arrays.sort(files); //遍历文件 for (File file : files) &#123; //如果文件大小和配置文件不一致,退出 if (file.length() != this.mappedFileSize) &#123; return false; &#125; try &#123; //创建映射文件 MappedFile mappedFile = new MappedFile(file.getPath(), mappedFileSize); mappedFile.setWrotePosition(this.mappedFileSize); mappedFile.setFlushedPosition(this.mappedFileSize); mappedFile.setCommittedPosition(this.mappedFileSize); //将映射文件添加到队列 this.mappedFiles.add(mappedFile); log.info(\"load \" + file.getPath() + \" OK\"); &#125; catch (IOException e) &#123; log.error(\"load file \" + file + \" error\", e); return false; &#125; &#125;&#125;return true; 代码：DefaultMessageStore#loadConsumeQueue 加载消息消费队列 //执行消费队列目录File dirLogic = new File(StorePathConfigHelper.getStorePathConsumeQueue(this.messageStoreConfig.getStorePathRootDir()));//遍历消费队列目录File[] fileTopicList = dirLogic.listFiles();if (fileTopicList != null) &#123; for (File fileTopic : fileTopicList) &#123; //获得子目录名称,即topic名称 String topic = fileTopic.getName(); //遍历子目录下的消费队列文件 File[] fileQueueIdList = fileTopic.listFiles(); if (fileQueueIdList != null) &#123; //遍历文件 for (File fileQueueId : fileQueueIdList) &#123; //文件名称即队列ID int queueId; try &#123; queueId = Integer.parseInt(fileQueueId.getName()); &#125; catch (NumberFormatException e) &#123; continue; &#125; //创建消费队列并加载到内存 ConsumeQueue logic = new ConsumeQueue( topic, queueId, StorePathConfigHelper.getStorePathConsumeQueue(this.messageStoreConfig.getStorePathRootDir()), this.getMessageStoreConfig().getMapedFileSizeConsumeQueue(), this); this.putConsumeQueue(topic, queueId, logic); if (!logic.load()) &#123; return false; &#125; &#125; &#125; &#125;&#125;log.info(\"load logics queue all over, OK\");return true; 代码：IndexService#load 加载索引文件 public boolean load(final boolean lastExitOK) &#123; //索引文件目录 File dir = new File(this.storePath); //遍历索引文件 File[] files = dir.listFiles(); if (files != null) &#123; //文件排序 Arrays.sort(files); //遍历文件 for (File file : files) &#123; try &#123; //加载索引文件 IndexFile f = new IndexFile(file.getPath(), this.hashSlotNum, this.indexNum, 0, 0); f.load(); if (!lastExitOK) &#123; //索引文件上次的刷盘时间小于该索引文件的消息时间戳,该文件将立即删除 if (f.getEndTimestamp() &gt; this.defaultMessageStore.getStoreCheckpoint() .getIndexMsgTimestamp()) &#123; f.destroy(0); continue; &#125; &#125; //将索引文件添加到队列 log.info(\"load index file OK, \" + f.getFileName()); this.indexFileList.add(f); &#125; catch (IOException e) &#123; log.error(\"load file &#123;&#125; error\", file, e); return false; &#125; catch (NumberFormatException e) &#123; log.error(\"load file &#123;&#125; error\", file, e); &#125; &#125; &#125; return true;&#125; 代码：DefaultMessageStore#recover 文件恢复，根据Broker是否正常退出执行不同的恢复策略 private void recover(final boolean lastExitOK) &#123; //获得最大的物理便宜消费队列 long maxPhyOffsetOfConsumeQueue = this.recoverConsumeQueue(); if (lastExitOK) &#123; //正常恢复 this.commitLog.recoverNormally(maxPhyOffsetOfConsumeQueue); &#125; else &#123; //异常恢复 this.commitLog.recoverAbnormally(maxPhyOffsetOfConsumeQueue); &#125; //在CommitLog中保存每个消息消费队列当前的存储逻辑偏移量 this.recoverTopicQueueTable();&#125; 代码：DefaultMessageStore#recoverTopicQueueTable 恢复ConsumerQueue后，将在CommitLog实例中保存每隔消息队列当前的存储逻辑偏移量，这也是消息中不仅存储主题、消息队列ID、还存储了消息队列的关键所在。 public void recoverTopicQueueTable() &#123; HashMap&lt;String/* topic-queueid */, Long/* offset */&gt; table = new HashMap&lt;String, Long&gt;(1024); //CommitLog最小偏移量 long minPhyOffset = this.commitLog.getMinOffset(); //遍历消费队列,将消费队列保存在CommitLog中 for (ConcurrentMap&lt;Integer, ConsumeQueue&gt; maps : this.consumeQueueTable.values()) &#123; for (ConsumeQueue logic : maps.values()) &#123; String key = logic.getTopic() + \"-\" + logic.getQueueId(); table.put(key, logic.getMaxOffsetInQueue()); logic.correctMinOffset(minPhyOffset); &#125; &#125; this.commitLog.setTopicQueueTable(table);&#125; ####2）正常恢复 代码：CommitLog#recoverNormally public void recoverNormally(long maxPhyOffsetOfConsumeQueue) &#123; final List&lt;MappedFile&gt; mappedFiles = this.mappedFileQueue.getMappedFiles(); if (!mappedFiles.isEmpty()) &#123; //Broker正常停止再重启时,从倒数第三个开始恢复,如果不足3个文件,则从第一个文件开始恢复。 int index = mappedFiles.size() - 3; if (index &lt; 0) index = 0; MappedFile mappedFile = mappedFiles.get(index); ByteBuffer byteBuffer = mappedFile.sliceByteBuffer(); long processOffset = mappedFile.getFileFromOffset(); //代表当前已校验通过的offset long mappedFileOffset = 0; while (true) &#123; //查找消息 DispatchRequest dispatchRequest = this.checkMessageAndReturnSize(byteBuffer, checkCRCOnRecover); //消息长度 int size = dispatchRequest.getMsgSize(); //查找结果为true,并且消息长度大于0,表示消息正确.mappedFileOffset向前移动本消息长度 if (dispatchRequest.isSuccess() &amp;&amp; size &gt; 0) &#123; mappedFileOffset += size; &#125; //如果查找结果为true且消息长度等于0,表示已到该文件末尾,如果还有下一个文件,则重置processOffset和MappedFileOffset重复查找下一个文件,否则跳出循环。 else if (dispatchRequest.isSuccess() &amp;&amp; size == 0) &#123; index++; if (index &gt;= mappedFiles.size()) &#123; // Current branch can not happen break; &#125; else &#123; //取出每个文件 mappedFile = mappedFiles.get(index); byteBuffer = mappedFile.sliceByteBuffer(); processOffset = mappedFile.getFileFromOffset(); mappedFileOffset = 0; &#125; &#125; // 查找结果为false，表明该文件未填满所有消息，跳出循环，结束循环 else if (!dispatchRequest.isSuccess()) &#123; log.info(\"recover physics file end, \" + mappedFile.getFileName()); break; &#125; &#125; //更新MappedFileQueue的flushedWhere和committedWhere指针 processOffset += mappedFileOffset; this.mappedFileQueue.setFlushedWhere(processOffset); this.mappedFileQueue.setCommittedWhere(processOffset); //删除offset之后的所有文件 this.mappedFileQueue.truncateDirtyFiles(processOffset); if (maxPhyOffsetOfConsumeQueue &gt;= processOffset) &#123; this.defaultMessageStore.truncateDirtyLogicFiles(processOffset); &#125; &#125; else &#123; this.mappedFileQueue.setFlushedWhere(0); this.mappedFileQueue.setCommittedWhere(0); this.defaultMessageStore.destroyLogics(); &#125;&#125; 代码：MappedFileQueue#truncateDirtyFiles public void truncateDirtyFiles(long offset) &#123; List&lt;MappedFile&gt; willRemoveFiles = new ArrayList&lt;MappedFile&gt;(); //遍历目录下文件 for (MappedFile file : this.mappedFiles) &#123; //文件尾部的偏移量 long fileTailOffset = file.getFileFromOffset() + this.mappedFileSize; //文件尾部的偏移量大于offset if (fileTailOffset &gt; offset) &#123; //offset大于文件的起始偏移量 if (offset &gt;= file.getFileFromOffset()) &#123; //更新wrotePosition、committedPosition、flushedPosistion file.setWrotePosition((int) (offset % this.mappedFileSize)); file.setCommittedPosition((int) (offset % this.mappedFileSize)); file.setFlushedPosition((int) (offset % this.mappedFileSize)); &#125; else &#123; //offset小于文件的起始偏移量,说明该文件是有效文件后面创建的,释放mappedFile占用内存,删除文件 file.destroy(1000); willRemoveFiles.add(file); &#125; &#125; &#125; this.deleteExpiredFile(willRemoveFiles);&#125; ####3）异常恢复 Broker异常停止文件恢复的实现为CommitLog#recoverAbnormally。异常文件恢复步骤与正常停止文件恢复流程基本相同，其主要差别有两个。首先，正常停止默认从倒数第三个文件开始进行恢复，而异常停止则需要从最后一个文件往前走，找到第一个消息存储正常的文件。其次，如果CommitLog目录没有消息文件，如果消息消费队列目录下存在文件，则需要销毁。 代码：CommitLog#recoverAbnormally if (!mappedFiles.isEmpty()) &#123; // Looking beginning to recover from which file int index = mappedFiles.size() - 1; MappedFile mappedFile = null; for (; index &gt;= 0; index--) &#123; mappedFile = mappedFiles.get(index); //判断消息文件是否是一个正确的文件 if (this.isMappedFileMatchedRecover(mappedFile)) &#123; log.info(\"recover from this mapped file \" + mappedFile.getFileName()); break; &#125; &#125; //根据索引取出mappedFile文件 if (index &lt; 0) &#123; index = 0; mappedFile = mappedFiles.get(index); &#125; //...验证消息的合法性,并将消息转发到消息消费队列和索引文件 &#125;else&#123; //未找到mappedFile,重置flushWhere、committedWhere都为0，销毁消息队列文件 this.mappedFileQueue.setFlushedWhere(0); this.mappedFileQueue.setCommittedWhere(0); this.defaultMessageStore.destroyLogics();&#125; 2.4.7 刷盘机制RocketMQ的存储是基于JDK NIO的内存映射机制（MappedByteBuffer）的，消息存储首先将消息追加到内存，再根据配置的刷盘策略在不同时间进行刷写磁盘。 同步刷盘消息追加到内存后，立即将数据刷写到磁盘文件 代码：CommitLog#handleDiskFlush //刷盘服务final GroupCommitService service = (GroupCommitService) this.flushCommitLogService;if (messageExt.isWaitStoreMsgOK()) &#123; //封装刷盘请求 GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes()); //提交刷盘请求 service.putRequest(request); //线程阻塞5秒，等待刷盘结束 boolean flushOK = request.waitForFlush(this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout()); if (!flushOK) &#123; putMessageResult.setPutMessageStatus(PutMessageStatus.FLUSH_DISK_TIMEOUT); &#125; GroupCommitRequest long nextOffset; //刷盘点偏移量CountDownLatch countDownLatch = new CountDownLatch(1); //倒计树锁存器volatile boolean flushOK = false; //刷盘结果;默认为false 代码：GroupCommitService#run public void run() &#123; CommitLog.log.info(this.getServiceName() + \" service started\"); while (!this.isStopped()) &#123; try &#123; //线程等待10ms this.waitForRunning(10); //执行提交 this.doCommit(); &#125; catch (Exception e) &#123; CommitLog.log.warn(this.getServiceName() + \" service has exception. \", e); &#125; &#125; ...&#125; 代码：GroupCommitService#doCommit private void doCommit() &#123; //加锁 synchronized (this.requestsRead) &#123; if (!this.requestsRead.isEmpty()) &#123; //遍历requestsRead for (GroupCommitRequest req : this.requestsRead) &#123; // There may be a message in the next file, so a maximum of // two times the flush boolean flushOK = false; for (int i = 0; i &lt; 2 &amp;&amp; !flushOK; i++) &#123; flushOK = CommitLog.this.mappedFileQueue.getFlushedWhere() &gt;= req.getNextOffset(); //刷盘 if (!flushOK) &#123; CommitLog.this.mappedFileQueue.flush(0); &#125; &#125; //唤醒发送消息客户端 req.wakeupCustomer(flushOK); &#125; //更新刷盘监测点 long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp(); if (storeTimestamp &gt; 0) &#123; CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp); &#125; this.requestsRead.clear(); &#125; else &#123; // Because of individual messages is set to not sync flush, it // will come to this process CommitLog.this.mappedFileQueue.flush(0); &#125; &#125;&#125; 异步刷盘在消息追加到内存后，立即返回给消息发送端。如果开启transientStorePoolEnable，RocketMQ会单独申请一个与目标物理文件（commitLog）同样大小的堆外内存，该堆外内存将使用内存锁定，确保不会被置换到虚拟内存中去，消息首先追加到堆外内存，然后提交到物理文件的内存映射中，然后刷写到磁盘。如果未开启transientStorePoolEnable，消息直接追加到物理文件直接映射文件中，然后刷写到磁盘中。 开启transientStorePoolEnable后异步刷盘步骤: 将消息直接追加到ByteBuffer（堆外内存） CommitRealTimeService线程每隔200ms将ByteBuffer新追加内容提交到MappedByteBuffer中 MappedByteBuffer在内存中追加提交的内容，wrotePosition指针向后移动 commit操作成功返回，将committedPosition位置恢复 FlushRealTimeService线程默认每500ms将MappedByteBuffer中新追加的内存刷写到磁盘 代码：CommitLog$CommitRealTimeService#run 提交线程工作机制 //间隔时间,默认200msint interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitIntervalCommitLog();//一次提交的至少页数int commitDataLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitCommitLogLeastPages();//两次真实提交的最大间隔,默认200msint commitDataThoroughInterval =CommitLog.this.defaultMessageStore.getMessageStoreConfig().getCommitCommitLogThoroughInterval();//上次提交间隔超过commitDataThoroughInterval,则忽略提交commitDataThoroughInterval参数,直接提交long begin = System.currentTimeMillis();if (begin &gt;= (this.lastCommitTimestamp + commitDataThoroughInterval)) &#123; this.lastCommitTimestamp = begin; commitDataLeastPages = 0;&#125;//执行提交操作,将待提交数据提交到物理文件的内存映射区boolean result = CommitLog.this.mappedFileQueue.commit(commitDataLeastPages);long end = System.currentTimeMillis();if (!result) &#123; this.lastCommitTimestamp = end; // result = false means some data committed. //now wake up flush thread. //唤醒刷盘线程 flushCommitLogService.wakeup();&#125;if (end - begin &gt; 500) &#123; log.info(\"Commit data to file costs &#123;&#125; ms\", end - begin);&#125;this.waitForRunning(interval); 代码：CommitLog$FlushRealTimeService#run 刷盘线程工作机制 //表示await方法等待,默认falseboolean flushCommitLogTimed = CommitLog.this.defaultMessageStore.getMessageStoreConfig().isFlushCommitLogTimed();//线程执行时间间隔int interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushIntervalCommitLog();//一次刷写任务至少包含页数int flushPhysicQueueLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogLeastPages();//两次真实刷写任务最大间隔int flushPhysicQueueThoroughInterval =CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogThoroughInterval();...//距离上次提交间隔超过flushPhysicQueueThoroughInterval,则本次刷盘任务将忽略flushPhysicQueueLeastPages,直接提交long currentTimeMillis = System.currentTimeMillis();if (currentTimeMillis &gt;= (this.lastFlushTimestamp + flushPhysicQueueThoroughInterval)) &#123; this.lastFlushTimestamp = currentTimeMillis; flushPhysicQueueLeastPages = 0; printFlushProgress = (printTimes++ % 10) == 0;&#125;...//执行一次刷盘前,先等待指定时间间隔if (flushCommitLogTimed) &#123; Thread.sleep(interval);&#125; else &#123; this.waitForRunning(interval);&#125;...long begin = System.currentTimeMillis();//刷写磁盘CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages);long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp();if (storeTimestamp &gt; 0) &#123;//更新存储监测点文件的时间戳CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp); 2.4.8 过期文件删除机制由于RocketMQ操作CommitLog、ConsumerQueue文件是基于内存映射机制并在启动的时候回加载CommitLog、ConsumerQueue目录下的所有文件，为了避免内存与磁盘的浪费，不可能将消息永久存储在消息服务器上，所以要引入一种机制来删除已过期的文件。RocketMQ顺序写CommitLog、ConsumerQueue文件，所有写操作全部落在最后一个CommitLog或者ConsumerQueue文件上，之前的文件在下一个文件创建后将不会再被更新。RocketMQ清除过期文件的方法时：如果当前文件在在一定时间间隔内没有再次被消费，则认为是过期文件，可以被删除，RocketMQ不会关注这个文件上的消息是否全部被消费。默认每个文件的过期时间为72小时，通过在Broker配置文件中设置fileReservedTime来改变过期时间，单位为小时。 代码：DefaultMessageStore#addScheduleTask private void addScheduleTask() &#123; //每隔10s调度一次清除文件 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; DefaultMessageStore.this.cleanFilesPeriodically(); &#125; &#125;, 1000 * 60, this.messageStoreConfig.getCleanResourceInterval(), TimeUnit.MILLISECONDS); ...&#125; 代码：DefaultMessageStore#cleanFilesPeriodically private void cleanFilesPeriodically() &#123; //清除存储文件 this.cleanCommitLogService.run(); //清除消息消费队列文件 this.cleanConsumeQueueService.run();&#125; 代码：DefaultMessageStore#deleteExpiredFiles private void deleteExpiredFiles() &#123; //删除的数量 int deleteCount = 0; //文件保留的时间 long fileReservedTime = DefaultMessageStore.this.getMessageStoreConfig().getFileReservedTime(); //删除物理文件的间隔 int deletePhysicFilesInterval = DefaultMessageStore.this.getMessageStoreConfig().getDeleteCommitLogFilesInterval(); //线程被占用,第一次拒绝删除后能保留的最大时间,超过该时间,文件将被强制删除 int destroyMapedFileIntervalForcibly = DefaultMessageStore.this.getMessageStoreConfig().getDestroyMapedFileIntervalForcibly();boolean timeup = this.isTimeToDelete();boolean spacefull = this.isSpaceToDelete();boolean manualDelete = this.manualDeleteFileSeveralTimes &gt; 0;if (timeup || spacefull || manualDelete) &#123; ...执行删除逻辑&#125;else&#123; ...无作为&#125; 删除文件操作的条件 指定删除文件的时间点，RocketMQ通过deleteWhen设置一天的固定时间执行一次删除过期文件操作，默认4点 磁盘空间如果不充足，删除过期文件 预留，手工触发。 代码：CleanCommitLogService#isSpaceToDelete 当磁盘空间不足时执行删除过期文件 private boolean isSpaceToDelete() &#123; //磁盘分区的最大使用量 double ratio = DefaultMessageStore.this.getMessageStoreConfig().getDiskMaxUsedSpaceRatio() / 100.0; //是否需要立即执行删除过期文件操作 cleanImmediately = false; &#123; String storePathPhysic = DefaultMessageStore.this.getMessageStoreConfig().getStorePathCommitLog(); //当前CommitLog目录所在的磁盘分区的磁盘使用率 double physicRatio = UtilAll.getDiskPartitionSpaceUsedPercent(storePathPhysic); //diskSpaceWarningLevelRatio:磁盘使用率警告阈值,默认0.90 if (physicRatio &gt; diskSpaceWarningLevelRatio) &#123; boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskFull(); if (diskok) &#123; DefaultMessageStore.log.error(\"physic disk maybe full soon \" + physicRatio + \", so mark disk full\"); &#125; //diskSpaceCleanForciblyRatio:强制清除阈值,默认0.85 cleanImmediately = true; &#125; else if (physicRatio &gt; diskSpaceCleanForciblyRatio) &#123; cleanImmediately = true; &#125; else &#123; boolean diskok = DefaultMessageStore.this.runningFlags.getAndMakeDiskOK(); if (!diskok) &#123; DefaultMessageStore.log.info(\"physic disk space OK \" + physicRatio + \", so mark disk ok\"); &#125; &#125; if (physicRatio &lt; 0 || physicRatio &gt; ratio) &#123; DefaultMessageStore.log.info(\"physic disk maybe full soon, so reclaim space, \" + physicRatio); return true; &#125;&#125; 代码：MappedFileQueue#deleteExpiredFileByTime 执行文件销毁和删除 for (int i = 0; i &lt; mfsLength; i++) &#123; //遍历每隔文件 MappedFile mappedFile = (MappedFile) mfs[i]; //计算文件存活时间 long liveMaxTimestamp = mappedFile.getLastModifiedTimestamp() + expiredTime; //如果超过72小时,执行文件删除 if (System.currentTimeMillis() &gt;= liveMaxTimestamp || cleanImmediately) &#123; if (mappedFile.destroy(intervalForcibly)) &#123; files.add(mappedFile); deleteCount++; if (files.size() &gt;= DELETE_FILES_BATCH_MAX) &#123; break; &#125; if (deleteFilesInterval &gt; 0 &amp;&amp; (i + 1) &lt; mfsLength) &#123; try &#123; Thread.sleep(deleteFilesInterval); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125; else &#123; break; &#125; &#125; else &#123; //avoid deleting files in the middle break; &#125;&#125; 2.4.9 小结RocketMQ的存储文件包括消息文件（Commitlog）、消息消费队列文件（ConsumerQueue）、Hash索引文件（IndexFile）、监测点文件（checkPoint）、abort（关闭异常文件）。单个消息存储文件、消息消费队列文件、Hash索引文件长度固定以便使用内存映射机制进行文件的读写操作。RocketMQ组织文件以文件的起始偏移量来命令文件，这样根据偏移量能快速定位到真实的物理文件。RocketMQ基于内存映射文件机制提供了同步刷盘和异步刷盘两种机制，异步刷盘是指在消息存储时先追加到内存映射文件，然后启动专门的刷盘线程定时将内存中的文件数据刷写到磁盘。 CommitLog，消息存储文件，RocketMQ为了保证消息发送的高吞吐量，采用单一文件存储所有主题消息，保证消息存储是完全的顺序写，但这样给文件读取带来了不便，为此RocketMQ为了方便消息消费构建了消息消费队列文件，基于主题与队列进行组织，同时RocketMQ为消息实现了Hash索引，可以为消息设置索引键，根据所以能够快速从CommitLog文件中检索消息。 当消息达到CommitLog后，会通过ReputMessageService线程接近实时地将消息转发给消息消费队列文件与索引文件。为了安全起见，RocketMQ引入abort文件，记录Broker的停机是否是正常关闭还是异常关闭，在重启Broker时为了保证CommitLog文件，消息消费队列文件与Hash索引文件的正确性，分别采用不同策略来恢复文件。 RocketMQ不会永久存储消息文件、消息消费队列文件，而是启动文件过期机制并在磁盘空间不足或者默认凌晨4点删除过期文件，文件保存72小时并且在删除文件时并不会判断该消息文件上的消息是否被消费。 2.5 Consumer2.5.1 消息消费概述​ 消息消费以组的模式开展，一个消费组内可以包含多个消费者，每一个消费者组可订阅多个主题，消费组之间有集群模式和广播模式两种消费模式。集群模式，主题下的同一条消息只允许被其中一个消费者消费。广播模式，主题下的同一条消息，将被集群内的所有消费者消费一次。消息服务器与消费者之间的消息传递也有两种模式：推模式、拉模式。所谓的拉模式，是消费端主动拉起拉消息请求，而推模式是消息达到消息服务器后，推送给消息消费者。RocketMQ消息推模式的实现基于拉模式，在拉模式上包装一层，一个拉取任务完成后开始下一个拉取任务。 集群模式下，多个消费者如何对消息队列进行负载呢？消息队列负载机制遵循一个通用思想：一个消息队列同一个时间只允许被一个消费者消费，一个消费者可以消费多个消息队列。 RocketMQ支持局部顺序消息消费，也就是保证同一个消息队列上的消息顺序消费。不支持消息全局顺序消费，如果要实现某一个主题的全局顺序消费，可以将该主题的队列数设置为1，牺牲高可用性。 ###2.5.2 消息消费初探 消息推送模式 消息消费重要方法 void sendMessageBack(final MessageExt msg, final int delayLevel, final String brokerName)：发送消息确认Set&lt;MessageQueue&gt; fetchSubscribeMessageQueues(final String topic) :获取消费者对主题分配了那些消息队列void registerMessageListener(final MessageListenerConcurrently messageListener)：注册并发事件监听器void registerMessageListener(final MessageListenerOrderly messageListener)：注册顺序消息事件监听器void subscribe(final String topic, final String subExpression)：基于主题订阅消息，消息过滤使用表达式void subscribe(final String topic, final String fullClassName,final String filterClassSource)：基于主题订阅消息，消息过滤使用类模式void subscribe(final String topic, final MessageSelector selector) ：订阅消息，并指定队列选择器void unsubscribe(final String topic)：取消消息订阅 DefaultMQPushConsumer //消费者组private String consumerGroup; //消息消费模式private MessageModel messageModel = MessageModel.CLUSTERING; //指定消费开始偏移量（最大偏移量、最小偏移量、启动时间戳）开始消费private ConsumeFromWhere consumeFromWhere = ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET;//集群模式下的消息队列负载策略private AllocateMessageQueueStrategy allocateMessageQueueStrategy;//订阅信息private Map&lt;String /* topic */, String /* sub expression */&gt; subscription = new HashMap&lt;String, String&gt;();//消息业务监听器private MessageListener messageListener;//消息消费进度存储器private OffsetStore offsetStore;//消费者最小线程数量private int consumeThreadMin = 20;//消费者最大线程数量private int consumeThreadMax = 20;//并发消息消费时处理队列最大跨度private int consumeConcurrentlyMaxSpan = 2000;//每1000次流控后打印流控日志private int pullThresholdForQueue = 1000;//推模式下任务间隔时间private long pullInterval = 0;//推模式下任务拉取的条数,默认32条private int pullBatchSize = 32;//每次传入MessageListener#consumerMessage中消息的数量private int consumeMessageBatchMaxSize = 1;//是否每次拉取消息都订阅消息private boolean postSubscriptionWhenPull = false;//消息重试次数,-1代表16次private int maxReconsumeTimes = -1;//消息消费超时时间private long consumeTimeout = 15; 2.5.3 消费者启动流程 代码：DefaultMQPushConsumerImpl#start public synchronized void start() throws MQClientException &#123; switch (this.serviceState) &#123; case CREATE_JUST: this.defaultMQPushConsumer.getMessageModel(), this.defaultMQPushConsumer.isUnitMode()); this.serviceState = ServiceState.START_FAILED; //检查消息者是否合法 this.checkConfig(); //构建主题订阅信息 this.copySubscription(); //设置消费者客户端实例名称为进程ID if (this.defaultMQPushConsumer.getMessageModel() == MessageModel.CLUSTERING) &#123; this.defaultMQPushConsumer.changeInstanceNameToPID(); &#125; //创建MQClient实例 this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQPushConsumer, this.rpcHook); //构建rebalanceImpl this.rebalanceImpl.setConsumerGroup(this.defaultMQPushConsumer.getConsumerGroup()); this.rebalanceImpl.setMessageModel(this.defaultMQPushConsumer.getMessageModel()); this.rebalanceImpl.setAllocateMessageQueueStrategy(this.defaultMQPushConsumer.getAllocateMessageQueueStrategy()); this.rebalanceImpl.setmQClientFactory(this.mQClientFactor this.pullAPIWrapper = new PullAPIWrapper( mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup(), isUnitMode()); this.pullAPIWrapper.registerFilterMessageHook(filterMessageHookLis if (this.defaultMQPushConsumer.getOffsetStore() != null) &#123; this.offsetStore = this.defaultMQPushConsumer.getOffsetStore(); &#125; else &#123; switch (this.defaultMQPushConsumer.getMessageModel()) &#123; case BROADCASTING: //消息消费广播模式,将消费进度保存在本地 this.offsetStore = new LocalFileOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup()); break; case CLUSTERING: //消息消费集群模式,将消费进度保存在远端Broker this.offsetStore = new RemoteBrokerOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup()); break; default: break; &#125; this.defaultMQPushConsumer.setOffsetStore(this.offsetStore); &#125; this.offsetStore.load //创建顺序消息消费服务 if (this.getMessageListenerInner() instanceof MessageListenerOrderly) &#123; this.consumeOrderly = true; this.consumeMessageService = new ConsumeMessageOrderlyService(this, (MessageListenerOrderly) this.getMessageListenerInner()); //创建并发消息消费服务 &#125; else if (this.getMessageListenerInner() instanceof MessageListenerConcurrently) &#123; this.consumeOrderly = false; this.consumeMessageService = new ConsumeMessageConcurrentlyService(this, (MessageListenerConcurrently) this.getMessageListenerInner()); &#125; //消息消费服务启动 this.consumeMessageService.start(); //注册消费者实例 boolean registerOK = mQClientFactory.registerConsumer(this.defaultMQPushConsumer.getConsumerGroup(), this); if (!registerOK) &#123; this.serviceState = ServiceState.CREATE_JUST; this.consumeMessageService.shutdown(); throw new MQClientException(\"The consumer group[\" + this.defaultMQPushConsumer.getConsumerGroup() + \"] has been created before, specify another name please.\" + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL), null); //启动消费者客户端 mQClientFactory.start(); log.info(\"the consumer [&#123;&#125;] start OK.\", this.defaultMQPushConsumer.getConsumerGroup()); this.serviceState = ServiceState.RUNNING; break; case RUNNING: case START_FAILED: case SHUTDOWN_ALREADY: throw new MQClientException(\"The PushConsumer service state not OK, maybe started once, \" + this.serviceState + FAQUrl.suggestTodo(FAQUrl.CLIENT_SERVICE_NOT_OK), null); default: break; &#125; this.updateTopicSubscribeInfoWhenSubscriptionChanged(); this.mQClientFactory.checkClientInBroker(); this.mQClientFactory.sendHeartbeatToAllBrokerWithLock(); this.mQClientFactory.rebalanceImmediately();&#125; 2.5.4 消息拉取消息消费模式有两种模式：广播模式与集群模式。广播模式比较简单，每一个消费者需要拉取订阅主题下所有队列的消息。本文重点讲解集群模式。在集群模式下，同一个消费者组内有多个消息消费者，同一个主题存在多个消费队列，消费者通过负载均衡的方式消费消息。 消息队列负载均衡，通常的作法是一个消息队列在同一个时间只允许被一个消费消费者消费，一个消息消费者可以同时消费多个消息队列。 1）PullMessageService实现机制从MQClientInstance的启动流程中可以看出，RocketMQ使用一个单独的线程PullMessageService来负责消息的拉取。 代码：PullMessageService#run public void run() &#123; log.info(this.getServiceName() + \" service started\"); //循环拉取消息 while (!this.isStopped()) &#123; try &#123; //从请求队列中获取拉取消息请求 PullRequest pullRequest = this.pullRequestQueue.take(); //拉取消息 this.pullMessage(pullRequest); &#125; catch (InterruptedException ignored) &#123; &#125; catch (Exception e) &#123; log.error(\"Pull Message Service Run Method exception\", e); &#125; &#125; log.info(this.getServiceName() + \" service end\");&#125; PullRequest private String consumerGroup; //消费者组private MessageQueue messageQueue; //待拉取消息队列private ProcessQueue processQueue; //消息处理队列private long nextOffset; //待拉取的MessageQueue偏移量private boolean lockedFirst = false; //是否被锁定 代码：PullMessageService#pullMessage private void pullMessage(final PullRequest pullRequest) &#123; //获得消费者实例 final MQConsumerInner consumer = this.mQClientFactory.selectConsumer(pullRequest.getConsumerGroup()); if (consumer != null) &#123; //强转为推送模式消费者 - 所以说，拉取方式本质上就是push模式的封装 DefaultMQPushConsumerImpl impl = (DefaultMQPushConsumerImpl) consumer; //推送消息 impl.pullMessage(pullRequest); &#125; else &#123; log.warn(\"No matched consumer for the PullRequest &#123;&#125;, drop it\", pullRequest); &#125;&#125; ####2）ProcessQueue实现机制 ProcessQueue是MessageQueue在消费端的重现、快照。PullMessageService从消息服务器默认每次拉取32条消息，按照消息的队列偏移量顺序存放在ProcessQueue中，PullMessageService然后将消息提交到消费者消费线程池，消息成功消费后从ProcessQueue中移除。 属性 //消息容器private final TreeMap&lt;Long, MessageExt&gt; msgTreeMap = new TreeMap&lt;Long, MessageExt&gt;();//读写锁private final ReadWriteLock lockTreeMap = new ReentrantReadWriteLock();//ProcessQueue总消息树private final AtomicLong msgCount = new AtomicLong();//ProcessQueue队列最大偏移量private volatile long queueOffsetMax = 0L;//当前ProcessQueue是否被丢弃private volatile boolean dropped = false;//上一次拉取时间戳private volatile long lastPullTimestamp = System.currentTimeMillis();//上一次消费时间戳private volatile long lastConsumeTimestamp = System.currentTimeMillis(); 方法 //移除消费超时消息public void cleanExpiredMsg(DefaultMQPushConsumer pushConsumer)//添加消息public boolean putMessage(final List&lt;MessageExt&gt; msgs)//获取消息最大间隔public long getMaxSpan()//移除消息public long removeMessage(final List&lt;MessageExt&gt; msgs)//将consumingMsgOrderlyTreeMap中消息重新放在msgTreeMap,并清空consumingMsgOrderlyTreeMap public void rollback() //将consumingMsgOrderlyTreeMap消息清除,表示成功处理该批消息public long commit()//重新处理该批消息public void makeMessageToCosumeAgain(List&lt;MessageExt&gt; msgs) //从processQueue中取出batchSize条消息public List&lt;MessageExt&gt; takeMessags(final int batchSize) 3）消息拉取基本流程#####1.客户端发起拉取请求 代码：DefaultMQPushConsumerImpl#pullMessage public void pullMessage(final PullRequest pullRequest) &#123; //从pullRequest获得ProcessQueue final ProcessQueue processQueue = pullRequest.getProcessQueue(); //如果处理队列被丢弃,直接返回 if (processQueue.isDropped()) &#123; log.info(\"the pull request[&#123;&#125;] is dropped.\", pullRequest.toString()); return; &#125; //如果处理队列未被丢弃,更新时间戳 pullRequest.getProcessQueue().setLastPullTimestamp(System.currentTimeMillis()); try &#123; this.makeSureStateOK(); &#125; catch (MQClientException e) &#123; log.warn(\"pullMessage exception, consumer state not ok\", e); this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); return; &#125; //如果处理队列被挂起,延迟1s后再执行 if (this.isPause()) &#123; log.warn(\"consumer was paused, execute pull request later. instanceName=&#123;&#125;, group=&#123;&#125;\", this.defaultMQPushConsumer.getInstanceName(), this.defaultMQPushConsumer.getConsumerGroup()); this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_SUSPEND); return; &#125; //获得最大待处理消息数量 long cachedMessageCount = processQueue.getMsgCount().get(); //获得最大待处理消息大小 long cachedMessageSizeInMiB = processQueue.getMsgSize().get() / (1024 * 1024); //从数量进行流控 if (cachedMessageCount &gt; this.defaultMQPushConsumer.getPullThresholdForQueue()) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); if ((queueFlowControlTimes++ % 1000) == 0) &#123; log.warn( \"the cached message count exceeds the threshold &#123;&#125;, so do flow control, minOffset=&#123;&#125;, maxOffset=&#123;&#125;, count=&#123;&#125;, size=&#123;&#125; MiB, pullRequest=&#123;&#125;, flowControlTimes=&#123;&#125;\", this.defaultMQPushConsumer.getPullThresholdForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes); &#125; return; &#125; //从消息大小进行流控 if (cachedMessageSizeInMiB &gt; this.defaultMQPushConsumer.getPullThresholdSizeForQueue()) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); if ((queueFlowControlTimes++ % 1000) == 0) &#123; log.warn( \"the cached message size exceeds the threshold &#123;&#125; MiB, so do flow control, minOffset=&#123;&#125;, maxOffset=&#123;&#125;, count=&#123;&#125;, size=&#123;&#125; MiB, pullRequest=&#123;&#125;, flowControlTimes=&#123;&#125;\", this.defaultMQPushConsumer.getPullThresholdSizeForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes); &#125; return; &#125; //获得订阅信息 final SubscriptionData subscriptionData = this.rebalanceImpl.getSubscriptionInner().get(pullRequest.getMessageQueue().getTopic()); if (null == subscriptionData) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); log.warn(\"find the consumer's subscription failed, &#123;&#125;\", pullRequest); return; //与服务端交互,获取消息 this.pullAPIWrapper.pullKernelImpl( pullRequest.getMessageQueue(), subExpression, subscriptionData.getExpressionType(), subscriptionData.getSubVersion(), pullRequest.getNextOffset(), this.defaultMQPushConsumer.getPullBatchSize(), sysFlag, commitOffsetValue, BROKER_SUSPEND_MAX_TIME_MILLIS, CONSUMER_TIMEOUT_MILLIS_WHEN_SUSPEND, CommunicationMode.ASYNC, pullCallback ); &#125; #####2.消息服务端Broker组装消息 代码：PullMessageProcessor#processRequest //构建消息过滤器MessageFilter messageFilter;if (this.brokerController.getBrokerConfig().isFilterSupportRetry()) &#123; messageFilter = new ExpressionForRetryMessageFilter(subscriptionData, consumerFilterData, this.brokerController.getConsumerFilterManager());&#125; else &#123; messageFilter = new ExpressionMessageFilter(subscriptionData, consumerFilterData, this.brokerController.getConsumerFilterManager());&#125;//调用MessageStore.getMessage查找消息final GetMessageResult getMessageResult = this.brokerController.getMessageStore().getMessage( requestHeader.getConsumerGroup(), //消费组名称 requestHeader.getTopic(), //主题名称 requestHeader.getQueueId(), //队列ID requestHeader.getQueueOffset(), //待拉取偏移量 requestHeader.getMaxMsgNums(), //最大拉取消息条数 messageFilter //消息过滤器 ); 代码：DefaultMessageStore#getMessage GetMessageStatus status = GetMessageStatus.NO_MESSAGE_IN_QUEUE;long nextBeginOffset = offset; //查找下一次队列偏移量long minOffset = 0; //当前消息队列最小偏移量long maxOffset = 0; //当前消息队列最大偏移量GetMessageResult getResult = new GetMessageResult();final long maxOffsetPy = this.commitLog.getMaxOffset(); //当前commitLog最大偏移量//根据主题名称和队列编号获取消息消费队列ConsumeQueue consumeQueue = findConsumeQueue(topic, queueId);...minOffset = consumeQueue.getMinOffsetInQueue();maxOffset = consumeQueue.getMaxOffsetInQueue();//消息偏移量异常情况校对下一次拉取偏移量if (maxOffset == 0) &#123; //表示当前消息队列中没有消息 status = GetMessageStatus.NO_MESSAGE_IN_QUEUE; nextBeginOffset = nextOffsetCorrection(offset, 0);&#125; else if (offset &lt; minOffset) &#123; //待拉取消息的偏移量小于队列的其实偏移量 status = GetMessageStatus.OFFSET_TOO_SMALL; nextBeginOffset = nextOffsetCorrection(offset, minOffset);&#125; else if (offset == maxOffset) &#123; //待拉取偏移量为队列最大偏移量 status = GetMessageStatus.OFFSET_OVERFLOW_ONE; nextBeginOffset = nextOffsetCorrection(offset, offset);&#125; else if (offset &gt; maxOffset) &#123; //偏移量越界 status = GetMessageStatus.OFFSET_OVERFLOW_BADLY; if (0 == minOffset) &#123; nextBeginOffset = nextOffsetCorrection(offset, minOffset); &#125; else &#123; nextBeginOffset = nextOffsetCorrection(offset, maxOffset); &#125;&#125;...//根据偏移量从CommitLog中拉取32条消息SelectMappedBufferResult selectResult = this.commitLog.getMessage(offsetPy, sizePy); 代码：PullMessageProcessor#processRequest //根据拉取结果填充responseHeaderresponse.setRemark(getMessageResult.getStatus().name());responseHeader.setNextBeginOffset(getMessageResult.getNextBeginOffset());responseHeader.setMinOffset(getMessageResult.getMinOffset());responseHeader.setMaxOffset(getMessageResult.getMaxOffset());//判断如果存在主从同步慢,设置下一次拉取任务的ID为主节点switch (this.brokerController.getMessageStoreConfig().getBrokerRole()) &#123; case ASYNC_MASTER: case SYNC_MASTER: break; case SLAVE: if (!this.brokerController.getBrokerConfig().isSlaveReadEnable()) &#123; response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY); responseHeader.setSuggestWhichBrokerId(MixAll.MASTER_ID); &#125; break;&#125;...//GetMessageResult与Response的Code转换switch (getMessageResult.getStatus()) &#123; case FOUND: //成功 response.setCode(ResponseCode.SUCCESS); break; case MESSAGE_WAS_REMOVING: //消息存放在下一个commitLog中 response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY); //消息重试 break; case NO_MATCHED_LOGIC_QUEUE: //未找到队列 case NO_MESSAGE_IN_QUEUE: //队列中未包含消息 if (0 != requestHeader.getQueueOffset()) &#123; response.setCode(ResponseCode.PULL_OFFSET_MOVED); requestHeader.getQueueOffset(), getMessageResult.getNextBeginOffset(), requestHeader.getTopic(), requestHeader.getQueueId(), requestHeader.getConsumerGroup() ); &#125; else &#123; response.setCode(ResponseCode.PULL_NOT_FOUND); &#125; break; case NO_MATCHED_MESSAGE: //未找到消息 response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY); break; case OFFSET_FOUND_NULL: //消息物理偏移量为空 response.setCode(ResponseCode.PULL_NOT_FOUND); break; case OFFSET_OVERFLOW_BADLY: //offset越界 response.setCode(ResponseCode.PULL_OFFSET_MOVED); // XXX: warn and notify me log.info(\"the request offset: &#123;&#125; over flow badly, broker max offset: &#123;&#125;, consumer: &#123;&#125;\", requestHeader.getQueueOffset(), getMessageResult.getMaxOffset(), channel.remoteAddress()); break; case OFFSET_OVERFLOW_ONE: //offset在队列中未找到 response.setCode(ResponseCode.PULL_NOT_FOUND); break; case OFFSET_TOO_SMALL: //offset未在队列中 response.setCode(ResponseCode.PULL_OFFSET_MOVED); requestHeader.getConsumerGroup(), requestHeader.getTopic(), requestHeader.getQueueOffset(), getMessageResult.getMinOffset(), channel.remoteAddress()); break; default: assert false; break;&#125;...//如果CommitLog标记可用,并且当前Broker为主节点,则更新消息消费进度boolean storeOffsetEnable = brokerAllowSuspend;storeOffsetEnable = storeOffsetEnable &amp;&amp; hasCommitOffsetFlag;storeOffsetEnable = storeOffsetEnable &amp;&amp; this.brokerController.getMessageStoreConfig().getBrokerRole() != BrokerRole.SLAVE;if (storeOffsetEnable) &#123; this.brokerController.getConsumerOffsetManager().commitOffset(RemotingHelper.parseChannelRemoteAddr(channel), requestHeader.getConsumerGroup(), requestHeader.getTopic(), requestHeader.getQueueId(), requestHeader.getCommitOffset());&#125; #####3.消息拉取客户端处理消息 代码：MQClientAPIImpl#processPullResponse private PullResult processPullResponse( final RemotingCommand response) throws MQBrokerException, RemotingCommandException &#123; PullStatus pullStatus = PullStatus.NO_NEW_MSG; //判断响应结果 switch (response.getCode()) &#123; case ResponseCode.SUCCESS: pullStatus = PullStatus.FOUND; break; case ResponseCode.PULL_NOT_FOUND: pullStatus = PullStatus.NO_NEW_MSG; break; case ResponseCode.PULL_RETRY_IMMEDIATELY: pullStatus = PullStatus.NO_MATCHED_MSG; break; case ResponseCode.PULL_OFFSET_MOVED: pullStatus = PullStatus.OFFSET_ILLEGAL; break; default: throw new MQBrokerException(response.getCode(), response.getRemark()); &#125; //解码响应头 PullMessageResponseHeader responseHeader = (PullMessageResponseHeader) response.decodeCommandCustomHeader(PullMessageResponseHeader.class); //封装PullResultExt返回 return new PullResultExt(pullStatus, responseHeader.getNextBeginOffset(), responseHeader.getMinOffset(), responseHeader.getMaxOffset(), null, responseHeader.getSuggestWhichBrokerId(), response.getBody());&#125; PullResult类 private final PullStatus pullStatus; //拉取结果private final long nextBeginOffset; //下次拉取偏移量private final long minOffset; //消息队列最小偏移量private final long maxOffset; //消息队列最大偏移量private List&lt;MessageExt&gt; msgFoundList; //拉取的消息列表 代码：DefaultMQPushConsumerImpl$PullCallback#OnSuccess //将拉取到的消息存入processQueueboolean dispatchToConsume = processQueue.putMessage(pullResult.getMsgFoundList());//将processQueue提交到consumeMessageService中供消费者消费DefaultMQPushConsumerImpl.this.consumeMessageService.submitConsumeRequest( pullResult.getMsgFoundList(), processQueue, pullRequest.getMessageQueue(), dispatchToConsume);//如果pullInterval大于0,则等待pullInterval毫秒后将pullRequest对象放入到PullMessageService中的pullRequestQueue队列中if (DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval() &gt; 0) &#123; DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest, DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval());&#125; else &#123; DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest);&#125; 4.消息拉取总结 4）消息拉取长轮询机制分析RocketMQ未真正实现消息推模式，而是消费者主动向消息服务器拉取消息，RocketMQ推模式是循环向消息服务端发起消息拉取请求，如果消息消费者向RocketMQ拉取消息时，消息未到达消费队列时，如果不启用长轮询机制，则会在服务端等待shortPollingTimeMills时间后（挂起）再去判断消息是否已经到达指定消息队列，如果消息仍未到达则提示拉取消息客户端PULL—NOT—FOUND（消息不存在）；如果开启长轮询模式，RocketMQ一方面会每隔5s轮询检查一次消息是否可达，同时一有消息达到后立马通知挂起线程再次验证消息是否是自己感兴趣的消息，如果是则从CommitLog文件中提取消息返回给消息拉取客户端，否则直到挂起超时，超时时间由消息拉取方在消息拉取是封装在请求参数中，PUSH模式为15s，PULL模式通过DefaultMQPullConsumer#setBrokerSuspendMaxTimeMillis设置。RocketMQ通过在Broker客户端配置longPollingEnable为true来开启长轮询模式。 代码：PullMessageProcessor#processRequest //当没有拉取到消息时，通过长轮询方式继续拉取消息case ResponseCode.PULL_NOT_FOUND: if (brokerAllowSuspend &amp;&amp; hasSuspendFlag) &#123; long pollingTimeMills = suspendTimeoutMillisLong; if (!this.brokerController.getBrokerConfig().isLongPollingEnable()) &#123; pollingTimeMills = this.brokerController.getBrokerConfig().getShortPollingTimeMills(); &#125; String topic = requestHeader.getTopic(); long offset = requestHeader.getQueueOffset(); int queueId = requestHeader.getQueueId(); //构建拉取请求对象 PullRequest pullRequest = new PullRequest(request, channel, pollingTimeMills, this.brokerController.getMessageStore().now(), offset, subscriptionData, messageFilter); //处理拉取请求 this.brokerController.getPullRequestHoldService().suspendPullRequest(topic, queueId, pullRequest); response = null; break; &#125; PullRequestHoldService方式实现长轮询 代码：PullRequestHoldService#suspendPullRequest //将拉取消息请求，放置在ManyPullRequest集合中public void suspendPullRequest(final String topic, final int queueId, final PullRequest pullRequest) &#123; String key = this.buildKey(topic, queueId); ManyPullRequest mpr = this.pullRequestTable.get(key); if (null == mpr) &#123; mpr = new ManyPullRequest(); ManyPullRequest prev = this.pullRequestTable.putIfAbsent(key, mpr); if (prev != null) &#123; mpr = prev; &#125; &#125; mpr.addPullRequest(pullRequest);&#125; 代码：PullRequestHoldService#run public void run() &#123; log.info(\"&#123;&#125; service started\", this.getServiceName()); while (!this.isStopped()) &#123; try &#123; //如果开启长轮询每隔5秒判断消息是否到达 if (this.brokerController.getBrokerConfig().isLongPollingEnable()) &#123; this.waitForRunning(5 * 1000); &#125; else &#123; //没有开启长轮询,每隔1s再次尝试 this.waitForRunning(this.brokerController.getBrokerConfig().getShortPollingTimeMills()); &#125; long beginLockTimestamp = this.systemClock.now(); this.checkHoldRequest(); long costTime = this.systemClock.now() - beginLockTimestamp; if (costTime &gt; 5 * 1000) &#123; log.info(\"[NOTIFYME] check hold request cost &#123;&#125; ms.\", costTime); &#125; &#125; catch (Throwable e) &#123; log.warn(this.getServiceName() + \" service has exception. \", e); &#125; &#125; log.info(\"&#123;&#125; service end\", this.getServiceName());&#125; 代码：PullRequestHoldService#checkHoldRequest //遍历拉取任务private void checkHoldRequest() &#123; for (String key : this.pullRequestTable.keySet()) &#123; String[] kArray = key.split(TOPIC_QUEUEID_SEPARATOR); if (2 == kArray.length) &#123; String topic = kArray[0]; int queueId = Integer.parseInt(kArray[1]); //获得消息偏移量 final long offset = this.brokerController.getMessageStore().getMaxOffsetInQueue(topic, queueId); try &#123; //通知有消息达到 this.notifyMessageArriving(topic, queueId, offset); &#125; catch (Throwable e) &#123; log.error(\"check hold request failed. topic=&#123;&#125;, queueId=&#123;&#125;\", topic, queueId, e); &#125; &#125; &#125;&#125; 代码：PullRequestHoldService#notifyMessageArriving //如果拉取消息偏移大于请求偏移量,如果消息匹配调用executeRequestWhenWakeup处理消息if (newestOffset &gt; request.getPullFromThisOffset()) &#123; boolean match = request.getMessageFilter().isMatchedByConsumeQueue(tagsCode, new ConsumeQueueExt.CqExtUnit(tagsCode, msgStoreTime, filterBitMap)); // match by bit map, need eval again when properties is not null. if (match &amp;&amp; properties != null) &#123; match = request.getMessageFilter().isMatchedByCommitLog(null, properties); &#125; if (match) &#123; try &#123; this.brokerController.getPullMessageProcessor().executeRequestWhenWakeup(request.getClientChannel(), request.getRequestCommand()); &#125; catch (Throwable e) &#123; log.error(\"execute request when wakeup failed.\", e); &#125; continue; &#125;&#125;//如果过期时间超时,则不继续等待将直接返回给客户端消息未找到if (System.currentTimeMillis() &gt;= (request.getSuspendTimestamp() + request.getTimeoutMillis())) &#123; try &#123; this.brokerController.getPullMessageProcessor().executeRequestWhenWakeup(request.getClientChannel(), request.getRequestCommand()); &#125; catch (Throwable e) &#123; log.error(\"execute request when wakeup failed.\", e); &#125; continue;&#125; 如果开启了长轮询机制，PullRequestHoldService会每隔5s被唤醒去尝试检测是否有新的消息的到来才给客户端响应，或者直到超时才给客户端进行响应，消息实时性比较差，为了避免这种情况，RocketMQ引入另外一种机制：当消息到达时唤醒挂起线程触发一次检查。 DefaultMessageStore$ReputMessageService机制 代码：DefaultMessageStore#start //长轮询入口this.reputMessageService.setReputFromOffset(maxPhysicalPosInLogicQueue);this.reputMessageService.start(); 代码：DefaultMessageStore$ReputMessageService#run public void run() &#123; DefaultMessageStore.log.info(this.getServiceName() + \" service started\"); while (!this.isStopped()) &#123; try &#123; Thread.sleep(1); //长轮询核心逻辑代码入口 this.doReput(); &#125; catch (Exception e) &#123; DefaultMessageStore.log.warn(this.getServiceName() + \" service has exception. \", e); &#125; &#125; DefaultMessageStore.log.info(this.getServiceName() + \" service end\");&#125; 代码：DefaultMessageStore$ReputMessageService#deReput //当新消息达到是,进行通知监听器进行处理if (BrokerRole.SLAVE != DefaultMessageStore.this.getMessageStoreConfig().getBrokerRole() &amp;&amp; DefaultMessageStore.this.brokerConfig.isLongPollingEnable()) &#123; DefaultMessageStore.this.messageArrivingListener.arriving(dispatchRequest.getTopic(), dispatchRequest.getQueueId(), dispatchRequest.getConsumeQueueOffset() + 1, dispatchRequest.getTagsCode(), dispatchRequest.getStoreTimestamp(), dispatchRequest.getBitMap(), dispatchRequest.getPropertiesMap());&#125; 代码：NotifyMessageArrivingListener#arriving public void arriving(String topic, int queueId, long logicOffset, long tagsCode, long msgStoreTime, byte[] filterBitMap, Map&lt;String, String&gt; properties) &#123; this.pullRequestHoldService.notifyMessageArriving(topic, queueId, logicOffset, tagsCode, msgStoreTime, filterBitMap, properties);&#125; 2.5.5 消息队列负载与重新分布机制RocketMQ消息队列重新分配是由RebalanceService线程来实现。一个MQClientInstance持有一个RebalanceService实现，并随着MQClientInstance的启动而启动。 代码：RebalanceService#run public void run() &#123; log.info(this.getServiceName() + \" service started\"); //RebalanceService线程默认每隔20s执行一次mqClientFactory.doRebalance方法 while (!this.isStopped()) &#123; this.waitForRunning(waitInterval); this.mqClientFactory.doRebalance(); &#125; log.info(this.getServiceName() + \" service end\");&#125; 代码：MQClientInstance#doRebalance public void doRebalance() &#123; //MQClientInstance遍历以注册的消费者,对消费者执行doRebalance()方法 for (Map.Entry&lt;String, MQConsumerInner&gt; entry : this.consumerTable.entrySet()) &#123; MQConsumerInner impl = entry.getValue(); if (impl != null) &#123; try &#123; impl.doRebalance(); &#125; catch (Throwable e) &#123; log.error(\"doRebalance exception\", e); &#125; &#125; &#125;&#125; 代码：RebalanceImpl#doRebalance //遍历订阅消息对每个主题的订阅的队列进行重新负载public void doRebalance(final boolean isOrder) &#123; Map&lt;String, SubscriptionData&gt; subTable = this.getSubscriptionInner(); if (subTable != null) &#123; for (final Map.Entry&lt;String, SubscriptionData&gt; entry : subTable.entrySet()) &#123; final String topic = entry.getKey(); try &#123; this.rebalanceByTopic(topic, isOrder); &#125; catch (Throwable e) &#123; if (!topic.startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) &#123; log.warn(\"rebalanceByTopic Exception\", e); &#125; &#125; &#125; &#125; this.truncateMessageQueueNotMyTopic();&#125; 代码：RebalanceImpl#rebalanceByTopic //从主题订阅消息缓存表中获取主题的队列信息Set&lt;MessageQueue&gt; mqSet = this.topicSubscribeInfoTable.get(topic);//查找该主题订阅组所有的消费者IDList&lt;String&gt; cidAll = this.mQClientFactory.findConsumerIdList(topic, consumerGroup);//给消费者重新分配队列if (mqSet != null &amp;&amp; cidAll != null) &#123; List&lt;MessageQueue&gt; mqAll = new ArrayList&lt;MessageQueue&gt;(); mqAll.addAll(mqSet); Collections.sort(mqAll); Collections.sort(cidAll); AllocateMessageQueueStrategy strategy = this.allocateMessageQueueStrategy; List&lt;MessageQueue&gt; allocateResult = null; try &#123; allocateResult = strategy.allocate( this.consumerGroup, this.mQClientFactory.getClientId(), mqAll, cidAll); &#125; catch (Throwable e) &#123; log.error(\"AllocateMessageQueueStrategy.allocate Exception. allocateMessageQueueStrategyName=&#123;&#125;\", strategy.getName(), e); return; &#125; RocketMQ默认提供5中负载均衡分配算法 AllocateMessageQueueAveragely:平均分配举例:8个队列q1,q2,q3,q4,q5,a6,q7,q8,消费者3个:c1,c2,c3分配如下:c1:q1,q2,q3c2:q4,q5,a6c3:q7,q8AllocateMessageQueueAveragelyByCircle:平均轮询分配举例:8个队列q1,q2,q3,q4,q5,a6,q7,q8,消费者3个:c1,c2,c3分配如下:c1:q1,q4,q7c2:q2,q5,a8c3:q3,q6 注意：消息队列的分配遵循一个消费者可以分配到多个队列，但同一个消息队列只会分配给一个消费者，故如果出现消费者个数大于消息队列数量，则有些消费者无法消费消息。 2.5.6 消息消费过程PullMessageService负责对消息队列进行消息拉取，从远端服务器拉取消息后将消息存储ProcessQueue消息队列处理队列中，然后调用ConsumeMessageService#submitConsumeRequest方法进行消息消费，使用线程池来消费消息，确保了消息拉取与消息消费的解耦。ConsumeMessageService支持顺序消息和并发消息，核心类图如下： 并发消息消费 代码：ConsumeMessageConcurrentlyService#submitConsumeRequest //消息批次单次final int consumeBatchSize = this.defaultMQPushConsumer.getConsumeMessageBatchMaxSize();//msgs.size()默认最多为32条。//如果msgs.size()小于consumeBatchSize,则直接将拉取到的消息放入到consumeRequest,然后将consumeRequest提交到消费者线程池中if (msgs.size() &lt;= consumeBatchSize) &#123; ConsumeRequest consumeRequest = new ConsumeRequest(msgs, processQueue, messageQueue); try &#123; this.consumeExecutor.submit(consumeRequest); &#125; catch (RejectedExecutionException e) &#123; this.submitConsumeRequestLater(consumeRequest); &#125;&#125;else&#123; //如果拉取的消息条数大于consumeBatchSize,则对拉取消息进行分页 for (int total = 0; total &lt; msgs.size(); ) &#123; List&lt;MessageExt&gt; msgThis = new ArrayList&lt;MessageExt&gt;(consumeBatchSize); for (int i = 0; i &lt; consumeBatchSize; i++, total++) &#123; if (total &lt; msgs.size()) &#123; msgThis.add(msgs.get(total)); &#125; else &#123; break; &#125; ConsumeRequest consumeRequest = new ConsumeRequest(msgThis, processQueue, messageQueue); try &#123; this.consumeExecutor.submit(consumeRequest); &#125; catch (RejectedExecutionException e) &#123; for (; total &lt; msgs.size(); total++) &#123; msgThis.add(msgs.get(total)); this.submitConsumeRequestLater(consumeRequest); &#125; &#125;&#125; 代码：ConsumeMessageConcurrentlyService$ConsumeRequest#run //检查processQueue的dropped,如果为true,则停止该队列消费。if (this.processQueue.isDropped()) &#123; log.info(\"the message queue not be able to consume, because it's dropped. group=&#123;&#125; &#123;&#125;\", ConsumeMessageConcurrentlyService.this.consumerGroup, this.messageQueue); return;&#125;...//执行消息处理的钩子函数if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) &#123; consumeMessageContext = new ConsumeMessageContext(); consumeMessageContext.setNamespace(defaultMQPushConsumer.getNamespace()); consumeMessageContext.setConsumerGroup(defaultMQPushConsumer.getConsumerGroup()); consumeMessageContext.setProps(new HashMap&lt;String, String&gt;()); consumeMessageContext.setMq(messageQueue); consumeMessageContext.setMsgList(msgs); consumeMessageContext.setSuccess(false); ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookBefore(consumeMessageContext);&#125;...//调用应用程序消息监听器的consumeMessage方法,进入到具体的消息消费业务处理逻辑status = listener.consumeMessage(Collections.unmodifiableList(msgs), context);//执行消息处理后的钩子函数if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) &#123; consumeMessageContext.setStatus(status.toString()); consumeMessageContext.setSuccess(ConsumeConcurrentlyStatus.CONSUME_SUCCESS == status); ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookAfter(consumeMessageContext);&#125; 2.5.7 定时消息机制定时消息是消息发送到Broker后，并不立即被消费者消费而是要等到特定的时间后才能被消费，RocketMQ并不支持任意的时间精度，如果要支持任意时间精度定时调度，不可避免地需要在Broker层做消息排序，再加上持久化方面的考量，将不可避免的带来巨大的性能消耗，所以RocketMQ只支持特定级别的延迟消息。消息延迟级别在Broker端通过messageDelayLevel配置，默认为“1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h”，delayLevel=1表示延迟消息1s,delayLevel=2表示延迟5s,依次类推。 RocketMQ定时消息实现类为ScheduleMessageService，该类在DefaultMessageStore中创建。通过在DefaultMessageStore中调用load方法加载该类并调用start方法启动。 代码：ScheduleMessageService#load //加载延迟消息消费进度的加载与delayLevelTable的构造。延迟消息的进度默认存储路径为/store/config/delayOffset.jsonpublic boolean load() &#123; boolean result = super.load(); result = result &amp;&amp; this.parseDelayLevel(); return result;&#125; 代码：ScheduleMessageService#start //遍历延迟队列创建定时任务,遍历延迟级别，根据延迟级别level从offsetTable中获取消费队列的消费进度。如果不存在，则使用0for (Map.Entry&lt;Integer, Long&gt; entry : this.delayLevelTable.entrySet()) &#123; Integer level = entry.getKey(); Long timeDelay = entry.getValue(); Long offset = this.offsetTable.get(level); if (null == offset) &#123; offset = 0L; &#125; if (timeDelay != null) &#123; this.timer.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME); &#125;&#125;//每隔10s持久化一次延迟队列的消息消费进度this.timer.scheduleAtFixedRate(new TimerTask() &#123; @Override public void run() &#123; try &#123; if (started.get()) ScheduleMessageService.this.persist(); &#125; catch (Throwable e) &#123; log.error(\"scheduleAtFixedRate flush exception\", e); &#125; &#125;&#125;, 10000, this.defaultMessageStore.getMessageStoreConfig().getFlushDelayOffsetInterval()); 调度机制 ScheduleMessageService的start方法启动后，会为每一个延迟级别创建一个调度任务，每一个延迟级别对应SCHEDULE_TOPIC_XXXX主题下的一个消息消费队列。定时调度任务的实现类为DeliverDelayedMessageTimerTask，核心实现方法为executeOnTimeup 代码：ScheduleMessageService$DeliverDelayedMessageTimerTask#executeOnTimeup //根据队列ID与延迟主题查找消息消费队列ConsumeQueue cq = ScheduleMessageService.this.defaultMessageStore.findConsumeQueue(SCHEDULE_TOPIC, delayLevel2QueueId(delayLevel));...//根据偏移量从消息消费队列中获取当前队列中所有有效的消息SelectMappedBufferResult bufferCQ = cq.getIndexBuffer(this.offset);...//遍历ConsumeQueue,解析消息队列中消息for (; i &lt; bufferCQ.getSize(); i += ConsumeQueue.CQ_STORE_UNIT_SIZE) &#123; long offsetPy = bufferCQ.getByteBuffer().getLong(); int sizePy = bufferCQ.getByteBuffer().getInt(); long tagsCode = bufferCQ.getByteBuffer().getLong(); if (cq.isExtAddr(tagsCode)) &#123; if (cq.getExt(tagsCode, cqExtUnit)) &#123; tagsCode = cqExtUnit.getTagsCode(); &#125; else &#123; //can't find ext content.So re compute tags code. log.error(\"[BUG] can't find consume queue extend file content!addr=&#123;&#125;, offsetPy=&#123;&#125;, sizePy=&#123;&#125;\", tagsCode, offsetPy, sizePy); long msgStoreTime = defaultMessageStore.getCommitLog().pickupStoreTimestamp(offsetPy, sizePy); tagsCode = computeDeliverTimestamp(delayLevel, msgStoreTime); &#125; &#125; long now = System.currentTimeMillis(); long deliverTimestamp = this.correctDeliverTimestamp(now, tagsCode); ... //根据消息偏移量与消息大小,从CommitLog中查找消息. MessageExt msgExt = ScheduleMessageService.this.defaultMessageStore.lookMessageByOffset( offsetPy, sizePy);&#125; 2.5.8 顺序消息顺序消息实现类是org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService 代码：ConsumeMessageOrderlyService#start public void start() &#123; //如果消息模式为集群模式，启动定时任务，默认每隔20s执行一次锁定分配给自己的消息消费队列 if (MessageModel.CLUSTERING.equals(ConsumeMessageOrderlyService.this.defaultMQPushConsumerImpl.messageModel())) &#123; this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; ConsumeMessageOrderlyService.this.lockMQPeriodically(); &#125; &#125;, 1000 * 1, ProcessQueue.REBALANCE_LOCK_INTERVAL, TimeUnit.MILLISECONDS); &#125;&#125; 代码：ConsumeMessageOrderlyService#submitConsumeRequest //构建消息任务,并提交消费线程池中public void submitConsumeRequest( final List&lt;MessageExt&gt; msgs, final ProcessQueue processQueue, final MessageQueue messageQueue, final boolean dispathToConsume) &#123; if (dispathToConsume) &#123; ConsumeRequest consumeRequest = new ConsumeRequest(processQueue, messageQueue); this.consumeExecutor.submit(consumeRequest); &#125;&#125; 代码：ConsumeMessageOrderlyService$ConsumeRequest#run //如果消息队列为丢弃,则停止本次消费任务if (this.processQueue.isDropped()) &#123; log.warn(\"run, the message queue not be able to consume, because it's dropped. &#123;&#125;\", this.messageQueue); return;&#125;//从消息队列中获取一个对象。然后消费消息时先申请独占objLock锁。顺序消息一个消息消费队列同一时刻只会被一个消费线程池处理final Object objLock = messageQueueLock.fetchLockObject(this.messageQueue);synchronized (objLock) &#123; ...&#125; 2.5.9 小结RocketMQ消息消费方式分别为集群模式、广播模式。 消息队列负载由RebalanceService线程默认每隔20s进行一次消息队列负载，根据当前消费者组内消费者个数与主题队列数量按照某一种负载算法进行队列分配，分配原则为同一个消费者可以分配多个消息消费队列，同一个消息消费队列同一个时间只会分配给一个消费者。 消息拉取由PullMessageService线程根据RebalanceService线程创建的拉取任务进行拉取，默认每次拉取32条消息，提交给消费者消费线程后继续下一次消息拉取。如果消息消费过慢产生消息堆积会触发消息消费拉取流控。 并发消息消费指消费线程池中的线程可以并发对同一个消息队列的消息进行消费，消费成功后，取出消息队列中最小的消息偏移量作为消息消费进度偏移量存储在于消息消费进度存储文件中，集群模式消息消费进度存储在Broker（消息服务器），广播模式消息消费进度存储在消费者端。 RocketMQ不支持任意精度的定时调度消息，只支持自定义的消息延迟级别，例如1s、2s、5s等，可通过在broker配置文件中设置messageDelayLevel。 顺序消息一般使用集群模式，是指对消息消费者内的线程池中的线程对消息消费队列只能串行消费。并并发消息消费最本质的区别是消息消费时必须成功锁定消息消费队列，在Broker端会存储消息消费队列的锁占用情况。","categories":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://kingge.top/categories/RocketMQ/"}],"tags":[{"name":"rmq","slug":"rmq","permalink":"http://kingge.top/tags/rmq/"},{"name":"RocketMQ源码分析","slug":"RocketMQ源码分析","permalink":"http://kingge.top/tags/RocketMQ源码分析/"}]},{"title":"RocketMQ基础知识","slug":"RocketMQ基础知识","date":"2019-11-13T02:21:59.000Z","updated":"2020-05-09T09:07:54.262Z","comments":true,"path":"2019/11/13/RocketMQ基础知识/","link":"","permalink":"http://kingge.top/2019/11/13/RocketMQ基础知识/","excerpt":"","text":"1. MQ介绍##1.1 为什么要用MQ 消息队列是一种“先进先出”的数据结构 其应用场景主要包含以下3个方面 应用解耦 系统的耦合性越高，容错性就越低。以电商应用为例，用户创建订单后，如果耦合调用库存系统、物流系统、支付系统，任何一个子系统出了故障或者因为升级等原因暂时不可用，都会造成下单操作异常，影响用户使用体验。 使用消息队列解耦合，系统的耦合性就会提高了。比如物流系统发生故障，需要几分钟才能来修复，在这段时间内，物流系统要处理的数据被缓存到消息队列中，用户的下单操作正常完成。当物流系统回复后，补充处理存在消息队列中的订单消息即可，终端系统感知不到物流系统发生过几分钟故障。 流量削峰 应用系统如果遇到系统请求流量的瞬间猛增，有可能会将系统压垮。有了消息队列可以将大量请求缓存起来，分散到很长一段时间处理，这样可以大大提到系统的稳定性和用户体验。 一般情况，为了保证系统的稳定性，如果系统负载超过阈值，就会阻止用户请求，这会影响用户体验，而如果使用消息队列将请求缓存起来，等待系统处理完毕后通知用户下单完毕，这样总不能下单体验要好。 处于经济考量目的： 业务系统正常时段的QPS如果是1000，流量最高峰是10000，为了应对流量高峰配置高性能的服务器显然不划算，这时可以使用消息队列对峰值流量削峰 数据分发 通过消息队列可以让数据在多个系统更加之间进行流通。数据的产生方不需要关心谁来使用数据，只需要将数据发送到消息队列，数据使用方直接在消息队列中直接获取数据即可 1.2 MQ的优点和缺点优点：解耦、削峰、数据分发 缺点包含以下几点： 系统可用性降低 系统引入的外部依赖越多，系统稳定性越差。一旦MQ宕机，就会对业务造成影响。 如何保证MQ的高可用？ 系统复杂度提高 MQ的加入大大增加了系统的复杂度，以前系统间是同步的远程调用，现在是通过MQ进行异步调用。 如何保证消息没有被重复消费？怎么处理消息丢失情况？那么保证消息传递的顺序性？ 一致性问题 A系统处理完业务，通过MQ给B、C、D三个系统发消息数据，如果B系统、C系统处理成功，D系统处理失败。 如何保证消息数据处理的一致性？ 1.3 各种MQ产品的比较常见的MQ产品包括Kafka、ActiveMQ、RabbitMQ、RocketMQ。 2. RocketMQ快速入门RocketMQ是阿里巴巴2016年MQ中间件，使用Java语言开发，在阿里内部，RocketMQ承接了例如“双11”等高并发场景的消息流转，能够处理万亿级别的消息。 2.1 准备工作2.1.1 下载RocketMQRocketMQ最新版本：4.5.1 下载地址 2.2.2 环境要求 Linux64位系统 JDK1.8(64位) 源码安装需要安装Maven 3.2.x 2.2 安装RocketMQ2.2.1 安装步骤本次以二进制包方式安装 解压安装包 unzip rocketmq-all-4.4.0-bin-release.zip 进入安装目录 2.2.2 目录介绍 bin：启动脚本，包括shell脚本和CMD脚本 conf：实例配置文件 ，包括broker配置文件、logback配置文件等 lib：依赖jar包，包括Netty、commons-lang、FastJSON等 2.3 启动RocketMQ进入bin目录启动。 启动NameServer # 1.启动NameServer - 后台的方式启动nohup sh bin/mqnamesrv &amp;# 2.查看启动日志tail -f ~/logs/rocketmqlogs/namesrv.log 启动Broker # 1.启动Brokernohup sh bin/mqbroker -n localhost:9876 &amp;# 2.查看启动日志tail -f ~/logs/rocketmqlogs/broker.log 问题描述： RocketMQ默认的虚拟机内存较大，启动Broker如果因为内存不足失败，需要编辑如下两个配置文件，修改JVM内存大小 # 编辑runbroker.sh和runserver.sh修改默认JVM大小vi runbroker.shvi runserver.sh 参考设置： vi runbroker.sh文件修改为： -server -Xms256m -Xmx256m -Xmn128m\"```vi runserver.sh文件修改为：```JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot; 修改完成后，如果之前已经启动了nameserver，那么需要关闭重启，然后再启动broker。 2.4 测试RocketMQ2.4.1 发送消息# 1.设置环境变量export NAMESRV_ADDR=localhost:9876# 2.使用安装包的Demo发送消息sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer 2.4.2 接收消息# 1.设置环境变量export NAMESRV_ADDR=localhost:9876# 2.接收消息sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 2.5 关闭RocketMQ# 1.关闭NameServersh bin/mqshutdown namesrv# 2.关闭Brokersh bin/mqshutdown broker 3. RocketMQ集群搭建3.1 各角色介绍 Producer：消息的发送者，发送前需要询问nameser到底要发到哪个broker，broker地址在哪里 Consumer：消息接收者，订阅消息时，也要询问namesesr需要订阅的消息在那个borker Broker：暂存和传输消息，启动后也需要向nameser汇报自己的状况信息。 NameServer：管理Broker。接受生产者和消费者的请求 Topic：区分消息的种类；一个发送者可以发送消息给一个或者多个Topic；一个消息的接收者可以订阅一个或者多个Topic消息 Message Queue：相当于是Topic的分区；用于并行发送和接收消息 3.2 集群搭建方式3.2.1 集群特点 NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 每一个nameser节点的数据都是一样的，但是他们之间是没有进行相互数据同步的，那么只能说明，broker启动后需要都给所有的nameser通信，汇报自己的情况。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer。 master和slave的出现目的是为了，进行读写分离。主节点是负责写。从节点负责读，他们之间进行数据同步（类似zk的集群架构）。 Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。 Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。 3.2.3 集群模式1）单Master模式这种方式风险较大，一旦Broker重启或者宕机时，会导致整个服务不可用。不建议线上环境使用,可以用于本地测试。 2）多Master模式一个集群无Slave，全是Master，例如2个Master或者3个Master，这种模式的优缺点如下： 优点：配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10时，即使机器宕机不可恢复情况下，由于RAID10磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢），性能最高； 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响。 3）多Master多Slave模式（消息异步更新）每个Master配置一个Slave，有多对Master-Slave，HA采用异步复制方式（master和broker之间数据同步采用异步方式），主备有短暂消息延迟（毫秒级），这种模式的优缺点如下： 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时Master宕机后，消费者仍然可以从Slave消费，而且此过程对应用透明，不需要人工干预，性能同多Master模式几乎一样； 缺点：Master宕机，磁盘损坏情况下会丢失少量消息。 4）多Master多Slave模式（消息同步更新）每个Master配置一个Slave，有多对Master-Slave，HA采用同步双写方式，即只有主备都写成功，才向应用返回成功，这种模式的优缺点如下： 优点：数据与服务都无单点故障，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高； 缺点：性能比异步复制模式略低（大约低10%左右），发送单个消息的RT会略高，且目前版本在主节点宕机后，备机不能自动切换为主机。 3.3 双主双从集群搭建3.3.1 总体架构消息高可用采用2m-2s（同步双写）方式 3.3.2 集群工作流程 启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。 Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。 收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。 Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。 Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。 3.3.3 服务器环境 序号 IP 角色 架构模式 1 192.168.25.135 nameserver、brokerserver Master1、Slave2 2 192.168.25.138 nameserver、brokerserver Master2、Slave1 3.3.4 Host添加信息vim /etc/hosts 配置如下: # nameserver192.168.25.135 rocketmq-nameserver1192.168.25.138 rocketmq-nameserver2# broker192.168.25.135 rocketmq-master1192.168.25.135 rocketmq-slave2192.168.25.138 rocketmq-master2192.168.25.138 rocketmq-slave1 配置完成后, 重启网卡 systemctl restart network 3.3.5 防火墙配置宿主机需要远程访问虚拟机的rocketmq服务和web服务，需要开放相关的端口号，简单粗暴的方式是直接关闭防火墙 # 关闭防火墙systemctl stop firewalld.service # 查看防火墙的状态firewall-cmd --state # 禁止firewall开机启动systemctl disable firewalld.service 或者为了安全，只开放特定的端口号，RocketMQ默认使用3个端口：9876 、10911 、11011 。如果防火墙没有关闭的话，那么防火墙就必须开放这些端口： nameserver 默认使用 9876 端口 master 默认使用 10911 端口 slave 默认使用11011 端口 执行以下命令： # 开放name server默认端口firewall-cmd --remove-port=9876/tcp --permanent# 开放master默认端口firewall-cmd --remove-port=10911/tcp --permanent# 开放slave默认端口 (当前集群模式可不开启)firewall-cmd --remove-port=11011/tcp --permanent # 重启防火墙firewall-cmd --reload 3.3.6 环境变量配置vim /etc/profile 在profile文件的末尾加入如下命令 #set rocketmqROCKETMQ_HOME=/usr/local/rocketmq/rocketmq-all-4.4.0-bin-releasePATH=$PATH:$ROCKETMQ_HOME/binexport ROCKETMQ_HOME PATH 输入:wq! 保存并退出， 并使得配置立刻生效： source /etc/profile 3.3.7 创建消息存储路径mkdir /usr/local/rocketmqmkdir /usr/local/rocketmq/storemkdir /usr/local/rocketmq/store/commitlogmkdir /usr/local/rocketmq/store/consumequeuemkdir /usr/local/rocketmq/store/index 3.3.8 broker配置文件我们可以打开rmq的conf配置目录： 可以看到他本身就提供了三种样例的配置目录，从上往下分别是：双master和双slave的消息异步同步，双master和双slave的消息同步同步，双master。分别对应了我们上面所讲的三种集群模式。 因为本次搭建的是双m双s的消息同步的方式，所以只需要修改2m-2s-sync目录下的配置文件即可。 1）master1服务器：192.168.25.135 vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-a.properties 修改配置如下： #所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-a#0 表示 Master，&gt;0 表示 SlavebrokerId=0#显式声明当前master所在的ip地址brokerIP1=192.168.25.135#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=SYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 2）slave2服务器：192.168.25.135 vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-b-s.properties 修改配置如下： #所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-b#0 表示 Master，&gt;0 表示 SlavebrokerId=1#显式声明当前slave所在的ip地址brokerIP1=192.168.25.135#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=11011#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SLAVE#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 3）master2服务器：192.168.25.138 vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-b.properties 修改配置如下： #所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-b#0 表示 Master，&gt;0 表示 SlavebrokerId=0#显式声明当前master所在的ip地址brokerIP1=192.168.25.138#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=SYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 4）slave1服务器：192.168.25.138 vi /usr/soft/rocketmq/conf/2m-2s-sync/broker-a-s.properties 修改配置如下： #所属集群名字brokerClusterName=rocketmq-cluster#broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-a#0 表示 Master，&gt;0 表示 SlavebrokerId=1#显式声明当前slave所在的ip地址brokerIP1=192.168.25.138#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口listenPort=11011#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径storePathRootDir=/usr/local/rocketmq/store#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SLAVE#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128 3.3.81同理把其余两个服务器还没有配置的配置文件，根据上面的broker配置，在配置即可 3.3.9 修改启动脚本文件1）runbroker.shvi /usr/local/rocketmq/bin/runbroker.sh 需要根据内存大小进行适当的对JVM参数进行调整： #===================================================# 开发环境配置 JVM ConfigurationJAVA_OPT=\"$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m\" ####2）runserver.sh vim /usr/local/rocketmq/bin/runserver.sh JAVA_OPT=\"$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m\" 3.3.10 服务启动1）启动NameServe集群分别在192.168.25.135和192.168.25.138启动NameServer cd /usr/local/rocketmq/binnohup sh mqnamesrv &amp; 2）启动Broker集群 在192.168.25.135上启动master1和slave2 master1： cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-2s-sync/broker-a.properties &amp; slave2： cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-2s-sync/broker-b-s.properties &amp; 在192.168.25.138上启动master2和slave2 master2 cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-2s-sync/broker-b.properties &amp; slave1 cd /usr/local/rocketmq/binnohup sh mqbroker -c /usr/local/rocketmq/conf/2m-2s-sync/broker-a-s.properties &amp; 3.3.11 查看进程状态启动后通过JPS查看启动进程 3.3.12 查看日志# 查看nameServer日志tail -500f ~/logs/rocketmqlogs/namesrv.log# 查看broker日志tail -500f ~/logs/rocketmqlogs/broker.log 3.6 重要提示！！！！！如果在云服务器上部署，配置跟以上是一样的，但是需要注意的是，为了避免启动的master或者slave使用内网ip进行通信，所以我们在配置《3.3.8broker配置文件中》 在每个properties配置文件中，需要显式声明master或者salve所属的ip地址： brokerIP1=106.13.13.172 3.4 mqadmin管理工具3.4.1 使用方式进入RocketMQ安装位置，在bin目录下执行&#123;command&#125; &#123;args&#125;``` ###3.4.2 命令介绍####1）Topic相关&lt;table border=0 cellpadding=0 cellspacing=0 width=714&gt; &lt;col width=177&gt; &lt;col width=175&gt; &lt;col width=177&gt; &lt;col width=185&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl63 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;名称&lt;/td&gt; &lt;td class=xl64 width=175 style=&apos;width:131pt&apos;&gt;含义&lt;/td&gt; &lt;td class=xl64 width=177 style=&apos;width:133pt&apos;&gt;命令选项&lt;/td&gt; &lt;td class=xl64 width=185 style=&apos;width:139pt&apos;&gt;说明&lt;/td&gt; &lt;/tr&gt; &lt;tr height=132 style=&apos;height:99.0pt&apos;&gt; &lt;td rowspan=8 height=593 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:444.0pt;border-top:none;width:122pt&apos;&gt;updateTopic&lt;/td&gt; &lt;td rowspan=8 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;创建更新Topic配置&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;Broker 地址，表示 topic 所在 Broker，只支持单台Broker，地址为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=132 style=&apos;height:99.0pt&apos;&gt; &lt;td height=132 class=xl65 width=149 style=&apos;height:99.0pt;width:112pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;cluster 名称，表示 topic 所在集群（集群可通过 clusterList 查询）&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-h-&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=76 style=&apos;height:57.0pt&apos;&gt; &lt;td height=76 class=xl65 width=149 style=&apos;height:57.0pt;width:112pt&apos;&gt;-p&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;指定新topic的读写权限( W=2|R=4|WR=6 )&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl65 width=149 style=&apos;height:29.0pt;width:112pt&apos;&gt;-r&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;可读队列数（默认为 8）&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl65 width=149 style=&apos;height:29.0pt;width:112pt&apos;&gt;-w&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;可写队列数（默认为 8）&lt;/td&gt; &lt;/tr&gt; &lt;tr height=95 style=&apos;height:71.0pt&apos;&gt; &lt;td height=95 class=xl65 width=149 style=&apos;height:71.0pt;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;topic 名称（名称只能使用字符 ^[a-zA-Z0-9_-]+$ ）&lt;/td&gt; &lt;/tr&gt; &lt;tr height=132 style=&apos;height:99.0pt&apos;&gt; &lt;td rowspan=4 height=307 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:230.0pt;border-top:none;width:122pt&apos;&gt;deleteTopic&lt;/td&gt; &lt;td rowspan=4 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;删除Topic&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;cluster 名称，表示删除某集群下的某个 topic （集群 可通过 clusterList 查询）&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=95 style=&apos;height:71.0pt&apos;&gt; &lt;td height=95 class=xl65 width=149 style=&apos;height:71.0pt;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;topic 名称（名称只能使用字符 ^[a-zA-Z0-9_-]+$ ）&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=3 height=287 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:215.0pt;border-top:none;width:122pt&apos;&gt;topicList&lt;/td&gt; &lt;td rowspan=3 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;查看 Topic 列表信息&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=207 style=&apos;height:155.0pt&apos;&gt; &lt;td height=207 class=xl65 width=149 style=&apos;height:155.0pt;width:112pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;不配置-c只返回topic列表，增加-c返回clusterName, topic, consumerGroup信息，即topic的所属集群和订阅关系，没有参数&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=3 height=103 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:77.0pt;border-top:none;width:122pt&apos;&gt;topicRoute&lt;/td&gt; &lt;td rowspan=3 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;查看 Topic 路由信息&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;topic 名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=3 height=103 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:77.0pt;border-top:none;width:122pt&apos;&gt;topicStatus&lt;/td&gt; &lt;td rowspan=3 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;查看 Topic 消息队列offset&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;topic 名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=3 height=103 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:77.0pt;border-top:none;width:122pt&apos;&gt;topicClusterList&lt;/td&gt; &lt;td rowspan=3 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;查看 Topic 所在集群列表&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;topic 名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=6 height=518 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:380pt;border-top:none;width:122pt&apos;&gt;updateTopicPerm&lt;/td&gt; &lt;td rowspan=6 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;更新 Topic 读写权限&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;topic 名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=132 style=&apos;height:99.0pt&apos;&gt; &lt;td height=132 class=xl65 width=149 style=&apos;height:99.0pt;width:112pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;Broker 地址，表示 topic 所在 Broker，只支持单台Broker，地址为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=76 style=&apos;height:57.0pt&apos;&gt; &lt;td height=76 class=xl65 width=149 style=&apos;height:57.0pt;width:112pt&apos;&gt;-p&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;指定新 topic 的读写权限( W=2|R=4|WR=6 )&lt;/td&gt; &lt;/tr&gt; &lt;tr height=207 style=&apos;height:155.0pt&apos;&gt; &lt;td height=207 class=xl65 width=149 style=&apos;height:155.0pt;width:112pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;cluster 名称，表示 topic 所在集群（集群可通过 clusterList 查询），-b优先，如果没有-b，则对集群中所有Broker执行命令&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=5 height=199 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:149.0pt;border-top:none;width:122pt&apos;&gt;updateOrderConf&lt;/td&gt; &lt;td rowspan=5 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;从NameServer上创建、删除、获取特定命名空间的kv配置，目前还未启用&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;topic，键&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl65 width=149 style=&apos;height:29.0pt;width:112pt&apos;&gt;-v&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;orderConf，值&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-m&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;method，可选get、put、delete&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=4 height=198 class=xl68 width=163 style=&apos;border-bottom:1.0pt; height:140pt;border-top:none;width:122pt&apos;&gt;allocateMQ&lt;/td&gt; &lt;td rowspan=4 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;以平均负载算法计算消费者列表负载消息队列的负载结果&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;topic 名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=95 style=&apos;height:71.0pt&apos;&gt; &lt;td height=95 class=xl65 width=149 style=&apos;height:71.0pt;width:112pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;ipList，用逗号分隔，计算这些ip去负载Topic的消息队列&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=4 height=142 class=xl68 width=163 style=&apos;border-bottom:1.0pt solid black; height:106.0pt;border-top:1.0pt;width:122pt&apos;&gt;statsAll&lt;/td&gt; &lt;td rowspan=4 class=xl70 width=135 style=&apos;border-bottom:1.0pt; border-top:none;width:101pt&apos;&gt;打印Topic订阅关系、TPS、积累量、24h读写总量等信息&lt;/td&gt; &lt;td class=xl65 width=149 style=&apos;width:112pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=149 style=&apos;height:43.0pt;width:112pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl65 width=149 style=&apos;height:29.0pt;width:112pt&apos;&gt;-a&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;是否只打印活跃topic&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=149 style=&apos;height:17.0pt;width:112pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl66 width=159 style=&apos;width:119pt&apos;&gt;指定topic&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;####2）集群相关&lt;table border=0 cellpadding=0 cellspacing=0 width=714&gt; &lt;col width=177&gt; &lt;col width=175&gt; &lt;col width=177&gt; &lt;col width=185&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl63 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;名称&lt;/td&gt; &lt;td class=xl64 width=175 style=&apos;width:131pt&apos;&gt;含义&lt;/td&gt; &lt;td class=xl64 width=177 style=&apos;width:133pt&apos;&gt;命令选项&lt;/td&gt; &lt;td class=xl64 width=185 style=&apos;width:139pt&apos;&gt;说明&lt;/td&gt; &lt;/tr&gt; &lt;tr height=207 style=&apos;height:155.0pt&apos;&gt; &lt;td rowspan=4 height=326 class=xl67 width=177 style=&apos;border-bottom:1.0pt; height:244.0pt;border-top:none;width:133pt&apos;&gt;&lt;span style=&apos;mso-spacerun:yes&apos;&gt; &lt;/span&gt;clusterList&lt;/td&gt; &lt;td rowspan=4 class=xl70 width=175 style=&apos;border-bottom:1.0pt; border-top:none;width:131pt&apos;&gt;查看集群信息，集群、BrokerName、BrokerId、TPS等信息&lt;/td&gt; &lt;td class=xl65 width=177 style=&apos;width:133pt&apos;&gt;-m&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;打印更多信息 (增加打印出如下信息 #InTotalYest, #OutTotalYest, #InTotalToday ,#OutTotalToday)&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=177 style=&apos;height:43.0pt;width:133pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl65 width=177 style=&apos;height:29.0pt;width:133pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;打印间隔，单位秒&lt;/td&gt; &lt;/tr&gt; &lt;tr height=95 style=&apos;height:71.0pt&apos;&gt; &lt;td rowspan=8 height=391 class=xl67 width=177 style=&apos;border-bottom:1.0pt; height:292.0pt;border-top:none;width:133pt&apos;&gt;clusterRT&lt;/td&gt; &lt;td rowspan=8 class=xl70 width=175 style=&apos;border-bottom:1.0pt; border-top:none;width:131pt&apos;&gt;发送消息检测集群各Broker RT。消息发往$&#123;BrokerName&#125; Topic。&lt;/td&gt; &lt;td class=xl65 width=177 style=&apos;width:133pt&apos;&gt;-a&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;amount，每次探测的总数，RT = 总时间 / amount&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl65 width=177 style=&apos;height:29.0pt;width:133pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;消息大小，单位B&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;探测哪个集群&lt;/td&gt; &lt;/tr&gt; &lt;tr height=76 style=&apos;height:57.0pt&apos;&gt; &lt;td height=76 class=xl65 width=177 style=&apos;height:57.0pt;width:133pt&apos;&gt;-p&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;是否打印格式化日志，以|分割，默认不打印&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl65 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl65 width=177 style=&apos;height:29.0pt;width:133pt&apos;&gt;-m&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;所属机房，打印使用&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl65 width=177 style=&apos;height:29.0pt;width:133pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;发送间隔，单位秒&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl65 width=177 style=&apos;height:43.0pt;width:133pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl66 width=185 style=&apos;width:139pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;####3）Broker相关&lt;table border=0 cellpadding=0 cellspacing=0 width=714&gt; &lt;col width=177&gt; &lt;col width=175&gt; &lt;col width=177&gt; &lt;col width=185&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl63 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;名称&lt;/td&gt; &lt;td class=xl64 width=175 style=&apos;width:131pt&apos;&gt;含义&lt;/td&gt; &lt;td class=xl64 width=177 style=&apos;width:133pt&apos;&gt;命令选项&lt;/td&gt; &lt;td class=xl64 width=185 style=&apos;width:139pt&apos;&gt;说明&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=6 height=206 class=xl69 width=191 style=&apos;border-bottom:1.0pt; height:154.0pt;border-top:none;width:143pt&apos;&gt;updateBrokerConfig&lt;/td&gt; &lt;td rowspan=6 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;更新 Broker 配置文件，会修改Broker.conf&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker 地址，格式为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;cluster 名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-k&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;key 值&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-v&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;value 值&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=3 height=137 class=xl69 width=191 style=&apos;border-bottom:1.0pt; height:103.0pt;border-top:none;width:143pt&apos;&gt;brokerStatus&lt;/td&gt; &lt;td rowspan=3 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;查看 Broker 统计信息、运行状态（你想要的信息几乎都在里面）&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker 地址，地址为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=6 height=256 class=xl69 width=191 style=&apos;border-bottom:1.0pt; height:192.0pt;border-top:none;width:143pt&apos;&gt;brokerConsumeStats&lt;/td&gt; &lt;td rowspan=6 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;Broker中各个消费者的消费情况，按Message Queue维度返回Consume Offset，Broker Offset，Diff，TImestamp等信息&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker 地址，地址为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;请求超时时间&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-l&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;diff阈值，超过阈值才打印&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-o&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否为顺序topic，一般为false&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=2 height=114 class=xl69 width=191 style=&apos;border-bottom:1.0pt; height:86.0pt;border-top:none;width:143pt&apos;&gt;getBrokerConfig&lt;/td&gt; &lt;td rowspan=2 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;获取Broker配置&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker 地址，地址为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=3 height=137 class=xl69 width=191 style=&apos;border-bottom:1.0pt; height:103.0pt;border-top:none;width:143pt&apos;&gt;wipeWritePerm&lt;/td&gt; &lt;td rowspan=3 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;从NameServer上清除 Broker写权限&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker 地址，地址为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=4 height=160 class=xl69 width=191 style=&apos;border-bottom:1.0pt; height:120.0pt;border-top:none;width:143pt&apos;&gt;cleanExpiredCQ&lt;/td&gt; &lt;td rowspan=4 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;清理Broker上过期的Consume Queue，如果手动减少对列数可能产生过期队列&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker 地址，地址为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;集群名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=88 style=&apos;mso-height-source:userset;height:66.0pt&apos;&gt; &lt;td rowspan=4 height=191 class=xl69 width=191 style=&apos;border-bottom:1.0pt; height:143.0pt;border-top:none;width:143pt&apos;&gt;cleanUnusedTopic&lt;/td&gt; &lt;td rowspan=4 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;清理Broker上不使用的Topic，从内存中释放Topic的Consume Queue，如果手动删除Topic会产生不使用的Topic&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker 地址，地址为ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;集群名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=5 height=199 class=xl69 width=191 style=&apos;border-bottom:1.0pt; height:149.0pt;border-top:none;width:143pt&apos;&gt;sendMsgStatus&lt;/td&gt; &lt;td rowspan=5 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;向Broker发消息，返回发送状态和RT&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;BrokerName，注意不同于Broker地址&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消息大小，单位B&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;发送次数&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;####4）消息相关&lt;table border=0 cellpadding=0 cellspacing=0 width=714&gt; &lt;col width=177&gt; &lt;col width=175&gt; &lt;col width=177&gt; &lt;col width=185&gt;&lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl63 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;名称&lt;/td&gt; &lt;td class=xl64 width=175 style=&apos;width:131pt&apos;&gt;含义&lt;/td&gt; &lt;td class=xl64 width=177 style=&apos;width:133pt&apos;&gt;命令选项&lt;/td&gt; &lt;td class=xl64 width=185 style=&apos;width:139pt&apos;&gt;说明&lt;/td&gt; &lt;/tr&gt; &lt;tr height=128 style=&apos;height:96.0pt&apos;&gt; &lt;td rowspan=3 height=208 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:156.0pt;border-top:none;width:65pt&apos;&gt;queryMsgById&lt;/td&gt; &lt;td rowspan=3 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;根据offsetMsgId查询msg，如果使用开源控制台，应使用offsetMsgId，此命令还有其他参数，具体作用请阅读QueryMsgByIdSubCommand。&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;msgId&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=4 height=126 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:94.0pt;border-top:none;width:65pt&apos;&gt;queryMsgByKey&lt;/td&gt; &lt;td rowspan=4 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;根据消息 Key 查询消息&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-k&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;msgKey&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Topic 名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=225 style=&apos;height:169.0pt&apos;&gt; &lt;td rowspan=6 height=390 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:292.0pt;border-top:none;width:65pt&apos;&gt;queryMsgByOffset&lt;/td&gt; &lt;td rowspan=6 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;根据 Offset 查询消息&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker 名称，（这里需要注意 填写的是 Broker 的名称，不是 Broker 的地址，Broker 名称可以在 clusterList 查到）&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;query 队列 id&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-o&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;offset 值&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic 名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=47&gt; &lt;td rowspan=6 height=209 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:156.0pt;border-top:none;width:65pt&apos;&gt;queryMsgByUniqueKey&lt;/td&gt; &lt;td rowspan=6 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;根据msgId查询，msgId不同于offsetMsgId，区别详见常见运维问题。-g，-d配合使用，查到消息后尝试让特定的消费者消费消息并返回消费结果&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;uniqe msg id&lt;/td&gt; &lt;/tr&gt; &lt;tr height=36 style=&apos;height:27.0pt&apos;&gt; &lt;td height=36 class=xl67 width=87 style=&apos;height:27.0pt;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;consumerGroup&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-d&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;clientId&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=5 height=149 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:111.0pt;border-top:none;width:65pt&apos;&gt;checkMsgSendRT&lt;/td&gt; &lt;td rowspan=5 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;检测向topic发消息的RT，功能类似clusterRT&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-a&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;探测次数&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消息大小&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=8 height=218 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:162.0pt;border-top:none;width:65pt&apos;&gt;sendMessage&lt;/td&gt; &lt;td rowspan=8 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;发送一条消息，可以根据配置发往特定Message Queue，或普通发送。&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-p&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;body，消息体&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-k&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;keys&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;tags&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;BrokerName&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;queueId&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=10 height=312 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:232.0pt;border-top:none;width:65pt&apos;&gt;consumeMessage&lt;/td&gt; &lt;td rowspan=10 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;消费消息。可以根据offset、开始&amp;amp;结束时间戳、消息队列消费消息，配置不同执行不同消费逻辑，详见ConsumeMessageCommand。&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;BrokerName&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-o&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;从offset开始消费&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;queueId&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消费者分组&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;开始时间戳，格式详见-h&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-d&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;结束时间戳&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消费多少条消息&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=8 height=282 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:210.0pt;border-top:none;width:65pt&apos;&gt;printMsg&lt;/td&gt; &lt;td rowspan=8 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;从Broker消费消息并打印，可选时间段&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;字符集，例如UTF-8&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;subExpress，过滤表达式&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;开始时间戳，格式参见-h&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-e&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;结束时间戳&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-d&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否打印消息体&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=12 height=390 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:290.0pt;border-top:none;width:65pt&apos;&gt;printMsgByQueue&lt;/td&gt; &lt;td rowspan=12 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;类似printMsg，但指定Message Queue&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;queueId&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-a&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;BrokerName&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;字符集，例如UTF-8&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;subExpress，过滤表达式&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;开始时间戳，格式参见-h&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-e&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;结束时间戳&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-p&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否打印消息&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-d&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否打印消息体&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-f&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否统计tag数量并打印&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=7 height=410 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:307.0pt;border-top:none;width:65pt&apos;&gt;resetOffsetByTime&lt;/td&gt; &lt;td rowspan=7 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;按时间戳重置offset，Broker和consumer都会重置&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消费者分组&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;重置为此时间戳对应的offset&lt;/td&gt; &lt;/tr&gt; &lt;tr height=188 style=&apos;height:141.0pt&apos;&gt; &lt;td height=188 class=xl67 width=87 style=&apos;height:141.0pt;width:65pt&apos;&gt;-f&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否强制重置，如果false，只支持回溯offset，如果true，不管时间戳对应offset与consumeOffset关系&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否重置c++客户端offset&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;#### 5）消费者、消费组相关&lt;table border=0 cellpadding=0 cellspacing=0 width=714&gt; &lt;col width=177&gt; &lt;col width=175&gt; &lt;col width=177&gt; &lt;col width=185&gt;&lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl63 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;名称&lt;/td&gt; &lt;td class=xl64 width=175 style=&apos;width:131pt&apos;&gt;含义&lt;/td&gt; &lt;td class=xl64 width=177 style=&apos;width:133pt&apos;&gt;命令选项&lt;/td&gt; &lt;td class=xl64 width=185 style=&apos;width:139pt&apos;&gt;说明&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td rowspan=4 height=158 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:110pt;border-top:none;width:65pt&apos;&gt;consumerProgress&lt;/td&gt; &lt;td rowspan=4 class=xl72 width=87 style=&apos;border-bottom:1.0pt; border-top:none;width:65pt&apos;&gt;查看订阅组消费状态，可以查看具体的client IP的消息积累量&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消费者所属组名&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否打印client IP&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=105 style=&apos;mso-height-source:userset;height:79.0pt&apos;&gt; &lt;td rowspan=5 height=260 class=xl69 width=87 style=&apos;border-bottom:1.0pt; height:195.0pt;border-top:none;width:65pt&apos;&gt;consumerStatus&lt;/td&gt; &lt;td rowspan=5 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;查看消费者状态，包括同一个分组中是否都是相同的订阅，分析Process Queue是否堆积，返回消费者jstack结果，内容较多，使用者参见ConsumerStatusSubCommand&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=36 style=&apos;height:27.0pt&apos;&gt; &lt;td height=36 class=xl67 width=87 style=&apos;height:27.0pt;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;consumer group&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;clientId&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否执行jstack&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td rowspan=5 height=181 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:135.0pt;border-top:none;width:65pt&apos;&gt;getConsumerStatus&lt;/td&gt; &lt;td rowspan=5 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;获取 Consumer 消费进度&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消费者所属组名&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;查询主题&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Consumer 客户端 ip&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=13 height=761 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:569.0pt;border-top:none;width:65pt&apos;&gt;updateSubGroup&lt;/td&gt; &lt;td rowspan=13 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;更新或创建订阅关系&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker地址&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;集群名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消费者分组名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;分组是否允许消费&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-m&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否从最小offset开始消费&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-d&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;是否是广播模式&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-q&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;重试队列数量&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-r&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;最大重试次数&lt;/td&gt; &lt;/tr&gt; &lt;tr height=207 style=&apos;height:155.0pt&apos;&gt; &lt;td height=207 class=xl67 width=87 style=&apos;height:155.0pt;width:65pt&apos;&gt;-i&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;当slaveReadEnable开启时有效，且还未达到从slave消费时建议从哪个BrokerId消费，可以配置备机id，主动从备机消费&lt;/td&gt; &lt;/tr&gt; &lt;tr height=132 style=&apos;height:99.0pt&apos;&gt; &lt;td height=132 class=xl67 width=87 style=&apos;height:99.0pt;width:65pt&apos;&gt;-w&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;如果Broker建议从slave消费，配置决定从哪个slave消费，配置BrokerId，例如1&lt;/td&gt; &lt;/tr&gt; &lt;tr height=76 style=&apos;height:57.0pt&apos;&gt; &lt;td height=76 class=xl67 width=87 style=&apos;height:57.0pt;width:65pt&apos;&gt;-a&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;当消费者数量变化时是否通知其他消费者负载均衡&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=5 height=165 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:123.0pt;border-top:none;width:65pt&apos;&gt;deleteSubGroup&lt;/td&gt; &lt;td rowspan=5 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;从Broker删除订阅关系&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-b&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;Broker地址&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-c&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;集群名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td height=39 class=xl67 width=87 style=&apos;height:29.0pt;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消费者分组名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=6 height=172 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:120pt;border-top:none;width:65pt&apos;&gt;cloneGroupOffset&lt;/td&gt; &lt;td rowspan=6 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;在目标群组中使用源群组的offset&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;源消费者组&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-d&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;目标消费者组&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;topic名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-o&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;暂未使用&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;#### 6）连接相关&lt;table border=0 cellpadding=0 cellspacing=0 width=714&gt; &lt;col width=177&gt; &lt;col width=175&gt; &lt;col width=177&gt; &lt;col width=185&gt;&lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl63 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;名称&lt;/td&gt; &lt;td class=xl64 width=175 style=&apos;width:131pt&apos;&gt;含义&lt;/td&gt; &lt;td class=xl64 width=177 style=&apos;width:133pt&apos;&gt;命令选项&lt;/td&gt; &lt;td class=xl64 width=185 style=&apos;width:139pt&apos;&gt;说明&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td rowspan=3 height=119 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:89.0pt;border-top:none;width:65pt&apos;&gt;consumerConnec tion&lt;/td&gt; &lt;td rowspan=3 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;查询 Consumer 的网络连接&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;消费者所属组名&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=39 style=&apos;height:29.0pt&apos;&gt; &lt;td rowspan=4 height=142 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:106.0pt;border-top:none;width:65pt&apos;&gt;producerConnec tion&lt;/td&gt; &lt;td rowspan=4 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;查询 Producer 的网络连接&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-g&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;生产者所属组名&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-t&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;主题名称&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;#### 7）NameServer相关&lt;table border=0 cellpadding=0 cellspacing=0 width=714&gt; &lt;col width=177&gt; &lt;col width=175&gt; &lt;col width=177&gt; &lt;col width=185&gt;&lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl63 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;名称&lt;/td&gt; &lt;td class=xl64 width=175 style=&apos;width:131pt&apos;&gt;含义&lt;/td&gt; &lt;td class=xl64 width=177 style=&apos;width:133pt&apos;&gt;命令选项&lt;/td&gt; &lt;td class=xl64 width=185 style=&apos;width:139pt&apos;&gt;说明&lt;/td&gt; &lt;/tr&gt; &lt;tr height=21 style=&apos;height:16.0pt&apos;&gt; &lt;td rowspan=5 height=143 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:100pt;border-top:none;width:65pt&apos;&gt;updateKvConfig&lt;/td&gt; &lt;td rowspan=5 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;更新NameServer的kv配置，目前还未使用&lt;/td&gt; &lt;td class=xl75 width=87 style=&apos;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl76 width=87 style=&apos;width:65pt&apos;&gt;命名空间&lt;/td&gt; &lt;/tr&gt; &lt;tr height=21 style=&apos;height:16.0pt&apos;&gt; &lt;td height=21 class=xl75 width=87 style=&apos;height:16.0pt;width:65pt&apos;&gt;-k&lt;/td&gt; &lt;td class=xl75 width=87 style=&apos;width:65pt&apos;&gt;key&lt;/td&gt; &lt;/tr&gt; &lt;tr height=21 style=&apos;height:16.0pt&apos;&gt; &lt;td height=21 class=xl75 width=87 style=&apos;height:16.0pt;width:65pt&apos;&gt;-v&lt;/td&gt; &lt;td class=xl75 width=87 style=&apos;width:65pt&apos;&gt;value&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td rowspan=4 height=126 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:94.0pt;border-top:none;width:65pt&apos;&gt;deleteKvConfig&lt;/td&gt; &lt;td rowspan=4 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;删除NameServer的kv配置&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-s&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;命名空间&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-k&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;key&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td height=57 class=xl67 width=87 style=&apos;height:43.0pt;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=2 height=80 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:60.0pt;border-top:none;width:65pt&apos;&gt;getNamesrvConfig&lt;/td&gt; &lt;td rowspan=2 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;获取NameServer配置&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=4 height=126 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:94.0pt;border-top:none;width:65pt&apos;&gt;updateNamesrvConfig&lt;/td&gt; &lt;td rowspan=4 class=xl72 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;修改NameServer配置&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-k&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;key&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-v&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;value&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;#### 8）其他&lt;table border=0 cellpadding=0 cellspacing=0 width=714&gt; &lt;col width=177&gt; &lt;col width=175&gt; &lt;col width=177&gt; &lt;col width=185&gt;&lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl63 width=177 style=&apos;height:17.0pt;width:133pt&apos;&gt;名称&lt;/td&gt; &lt;td class=xl64 width=175 style=&apos;width:131pt&apos;&gt;含义&lt;/td&gt; &lt;td class=xl64 width=177 style=&apos;width:133pt&apos;&gt;命令选项&lt;/td&gt; &lt;td class=xl64 width=185 style=&apos;width:139pt&apos;&gt;说明&lt;/td&gt; &lt;/tr&gt; &lt;tr height=57 style=&apos;height:43.0pt&apos;&gt; &lt;td rowspan=2 height=80 class=xl69 width=87 style=&apos;border-bottom:1.0pt height:60.0pt;border-top:none;width:65pt&apos;&gt;startMonitoring&lt;/td&gt; &lt;td rowspan=2 class=xl71 width=87 style=&apos;border-bottom:1.0pt border-top:none;width:65pt&apos;&gt;开启监控进程，监控消息误删、重试队列消息数等&lt;/td&gt; &lt;td class=xl67 width=87 style=&apos;width:65pt&apos;&gt;-n&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;NameServer 服务地址，格式 ip:port&lt;/td&gt; &lt;/tr&gt; &lt;tr height=23 style=&apos;height:17.0pt&apos;&gt; &lt;td height=23 class=xl67 width=87 style=&apos;height:17.0pt;width:65pt&apos;&gt;-h&lt;/td&gt; &lt;td class=xl68 width=87 style=&apos;width:65pt&apos;&gt;打印帮助&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;### 3.4.3 注意事项* 几乎所有命令都需要配置-n表示NameServer地址，格式为ip:port* 几乎所有命令都可以通过-h获取帮助* 如果既有Broker地址（-b）配置项又有clusterName（-c）配置项，则优先以Broker地址执行命令；如果不配置Broker地址，则对集群中所有主机执行命令## 3.5 集群监控平台搭建### 3.5.1 概述`RocketMQ`有一个对其扩展的开源项目[incubator-rocketmq-externals](https://github.com/apache/rocketmq-externals)，这个项目中有一个子模块叫`rocketmq-console`，这个便是管理控制台项目了，先将[incubator-rocketmq-externals](https://github.com/apache/rocketmq-externals)拉到本地，因为我们需要自己对`rocketmq-console`进行编译打包运行。![](rocketmq总结/rocketmq-console.png)### 3.5.2 下载并编译打包```shgit clone https://github.com/apache/rocketmq-externalscd rocketmq-consolemvn clean package -Dmaven.test.skip=true 注意：打包前在```shrocketmq.config.namesrvAddr=192.168.25.135:9876;192.168.25.138:9876 启动rocketmq-console： java -jar rocketmq-console-ng-1.0.0.jar 启动成功后，我们就可以通过浏览器访问http://localhost:8080进入控制台界面了，如下图： 集群状态： 4. 消息发送样例 导入MQ客户端依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.4.0&lt;/version&gt;&lt;/dependency&gt; 消息发送者步骤分析 1.创建消息生产者producer，并制定生产者组名2.指定Nameserver地址3.启动producer4.创建消息对象，指定主题Topic、Tag和消息体5.发送消息6.关闭生产者producer 消息消费者步骤分析 1.创建消费者Consumer，制定消费者组名2.指定Nameserver地址3.订阅主题Topic和Tag4.设置回调函数，处理消息5.启动消费者consumer 4.1 基本样例4.1.1 消息发送1）发送同步消息意思就是：生产者发送消息后，阻塞等待rmq的ack通知。 这种可靠性同步地发送方式使用的比较广泛，比如：重要的消息通知，短信通知。 public class SyncProducer &#123; public static void main(String[] args) throws Exception &#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\"); // 设置NameServer的地址 producer.setNamesrvAddr(\"localhost:9876\"); // 启动Producer实例 producer.start(); for (int i = 0; i &lt; 100; i++) &#123; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message(\"TopicTest\" /* Topic */, \"TagA\" /* Tag */, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); // 发送消息到一个Broker SendResult sendResult = producer.send(msg);//阻塞等待rmq的ack // 通过sendResult返回消息是否成功送达 System.out.printf(\"%s%n\", sendResult); &#125; // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 2）发送异步消息​ 异步消息通常用在对响应时间敏感（不用阻塞等待rmq响应）的业务场景，即发送端不能容忍长时间地等待Broker的响应。 ​ 提供了通过回调函数的方式，触发broker响应的结果 public class AsyncProducer &#123; public static void main(String[] args) throws Exception &#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\"); // 设置NameServer的地址 producer.setNamesrvAddr(\"localhost:9876\"); // 启动Producer实例 producer.start(); producer.setRetryTimesWhenSendAsyncFailed(0); for (int i = 0; i &lt; 100; i++) &#123; final int index = i; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message(\"TopicTest\", \"TagA\", \"OrderID188\", \"Hello world\".getBytes(RemotingHelper.DEFAULT_CHARSET)); // SendCallback接收异步返回结果的回调 producer.send(msg, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; System.out.printf(\"%-10d OK %s %n\", index, sendResult.getMsgId()); &#125; @Override public void onException(Throwable e) &#123; System.out.printf(\"%-10d Exception %s %n\", index, e); e.printStackTrace(); &#125; &#125;); &#125; // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 3）单向发送消息这种方式主要用在不特别关心发送结果的场景，例如日志发送。 public class OnewayProducer &#123; public static void main(String[] args) throws Exception&#123; // 实例化消息生产者Producer DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\"); // 设置NameServer的地址 producer.setNamesrvAddr(\"localhost:9876\"); // 启动Producer实例 producer.start(); for (int i = 0; i &lt; 100; i++) &#123; // 创建消息，并指定Topic，Tag和消息体 Message msg = new Message(\"TopicTest\" /* Topic */, \"TagA\" /* Tag */, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); // 发送单向消息，没有任何返回结果 producer.sendOneway(msg); &#125; // 如果不再发送消息，关闭Producer实例。 producer.shutdown(); &#125;&#125; 4.1.2 消费消息消息消费模式由消费者来决定，可以由消费者设置MessageModel来决定消息模式。 消息模式默认为集群消费模式 consumer.setMessageModel(MessageModel.BROADCASTING);consumer.setMessageModel(MessageModel.CLUSTERING); 1）负载均衡/集群模式 - 默认模式消费者采用负载均衡方式消费消息，多个消费者共同消费队列消息，每个消费者处理的消息不同 public static void main(String[] args) throws Exception &#123; // 实例化消息生产者,指定组名 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"group1\"); // 指定Namesrv地址信息. consumer.setNamesrvAddr(\"localhost:9876\"); // 订阅Topic consumer.subscribe(\"Test\", \"*\"); //负载均衡模式消费 consumer.setMessageModel(MessageModel.CLUSTERING); // 注册回调函数，处理消息 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; System.out.printf(\"%s Receive New Messages: %s %n\", Thread.currentThread().getName(), msgs); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;//消费完成后需要返回值，这里标识消费成功 &#125; &#125;); //启动消息者 consumer.start(); System.out.printf(\"Consumer Started.%n\");&#125; 架构图 集群消息是指集群化部署消费者 当使用集群消费模式时，MQ 认为任意一条消息只需要被集群内的任意一个消费者处理即可。 特点 每条消息只需要被处理一次，broker只会把消息发送给消费集群中的一个消费者 在消息重投时（为什么会重投，那是因为你没有返回ack给rmq，所以rmq认为你没收到会再重发），但是不能保证路由到同一台机器上 消费状态由broker维护 2）广播模式消费者采用广播的方式消费消息，每个消费者消费的消息都是相同的。 public static void main(String[] args) throws Exception &#123; // 实例化消息生产者,指定组名 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"group1\"); // 指定Namesrv地址信息. consumer.setNamesrvAddr(\"localhost:9876\"); // 订阅Topic consumer.subscribe(\"Test\", \"*\"); //广播模式消费 consumer.setMessageModel(MessageModel.BROADCASTING); // 注册回调函数，处理消息 consumer.registerMessageListener(new MessageListenerConcurrently() &#123;//一个queue开启多个线程同时消费 @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; System.out.printf(\"%s Receive New Messages: %s %n\", Thread.currentThread().getName(), msgs); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); //启动消息者 consumer.start(); System.out.printf(\"Consumer Started.%n\");&#125; ​ 当使用广播消费模式时，MQ 会将每条消息推送给集群内所有注册过的客户端，保证消息至少被每台机器消费一次。 特点 消费进度由consumer维护 保证每个消费者消费一次消息 消费失败的消息不会重投 为什么要定义组名！！！！我们可以看到在创建生产者或者消费者的时候，构造函数都需要传递一个组名； DefaultMQProducer producer = new DefaultMQProducer(&quot;please_rename_unique_group_name&quot;);DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;group1&quot;); 我们可以回过头看一下rmq集群架构图： 可以看到生产者和消费者，都会构建一个集群。那么例如生产者构建集群的目的到底有设么用？生产者不就负责发送消息就可以了么？ ​ 答案是：事务消息的时候会使用。我们是否还记得，rmq会回查生产者的接口，确认本次半消息是否投递commit。 而消费者定义组名，那么就可以做负载均衡。相同组内的所有消费者在消费消息时可以做负载均衡。 4.2 顺序消息消息有序指的是可以按照消息的发送顺序来消费(FIFO)。RocketMQ可以严格的保证消息有序，可以分为分区有序或者全局有序。 顺序消费的原理解析，在默认的情况下消息发送会采取Round Robin轮询方式把消息发送到不同的queue(分区队列)（broker中 可能存在多个队列）；而消费消息的时候从多个queue上拉取消息（消费者多线程消费消息），这种情况发送和消费是不能保证顺序。但是如果控制发送的顺序消息只依次发送到同一个queue中，消费的时候只从这个queue上依次拉取，则就保证了顺序。当发送和消费参与的queue只有一个，则是全局有序；如果多个queue参与，则为分区有序，即相对每个queue，消息都是有序的。 ​ 下面用订单进行分区有序的示例。一个订单的顺序流程是：创建、付款、推送、完成。订单号相同的消息会被先后发送到同一个队列中，消费时，同一个OrderId获取到的肯定是同一个队列。 4.2.1 顺序消息生产/*** Producer，发送顺序消息*/public class Producer &#123; public static void main(String[] args) throws Exception &#123; DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\"); producer.setNamesrvAddr(\"127.0.0.1:9876\"); producer.start(); String[] tags = new String[]&#123;\"TagA\", \"TagC\", \"TagD\"&#125;; // 订单列表 List&lt;OrderStep&gt; orderList = new Producer().buildOrders(); Date date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); String dateStr = sdf.format(date); for (int i = 0; i &lt; 10; i++) &#123; // 加个时间前缀 String body = dateStr + \" Hello RocketMQ \" + orderList.get(i); Message msg = new Message(\"TopicTest\", tags[i % tags.length], \"KEY\" + i, body.getBytes()); /** * 参数一：消息对象 * 参数二：消息队列的选择器 * 参数三：选择队列的业务标识（订单ID） */ SendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; /** * * @param mqs：队列集合，TopicTest主题下的所有消息队列 * @param msg：消息对象 * @param arg：业务标识的参数，，也就是在执行send方法时的传递第三个参数，他会回传到select方法 * @return */ @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Long id = (Long) arg; //根据订单id选择发送queue long index = id % mqs.size(); return mqs.get((int) index); &#125; &#125;, orderList.get(i).getOrderId());//订单id System.out.println(String.format(\"SendResult status:%s, queueId:%d, body:%s\", sendResult.getSendStatus(), sendResult.getMessageQueue().getQueueId(), body)); &#125; producer.shutdown(); &#125; /** * 订单的步骤 */ private static class OrderStep &#123; private long orderId; private String desc; public long getOrderId() &#123; return orderId; &#125; public void setOrderId(long orderId) &#123; this.orderId = orderId; &#125; public String getDesc() &#123; return desc; &#125; public void setDesc(String desc) &#123; this.desc = desc; &#125; @Override public String toString() &#123; return \"OrderStep&#123;\" + \"orderId=\" + orderId + \", desc='\" + desc + '\\'' + '&#125;'; &#125; &#125; /** * 生成模拟订单数据 */ private List&lt;OrderStep&gt; buildOrders() &#123; List&lt;OrderStep&gt; orderList = new ArrayList&lt;OrderStep&gt;(); OrderStep orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc(\"创建\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc(\"创建\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc(\"付款\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc(\"创建\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc(\"付款\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc(\"付款\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111065L); orderDemo.setDesc(\"完成\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc(\"推送\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103117235L); orderDemo.setDesc(\"完成\"); orderList.add(orderDemo); orderDemo = new OrderStep(); orderDemo.setOrderId(15103111039L); orderDemo.setDesc(\"完成\"); orderList.add(orderDemo); return orderList; &#125;&#125; 4.2.2 顺序消费消息/*** 顺序消息消费，带事务方式（应用可控制Offset什么时候提交）*/public class ConsumerInOrder &#123; public static void main(String[] args) throws Exception &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"please_rename_unique_group_name_3\"); consumer.setNamesrvAddr(\"127.0.0.1:9876\"); /** * 设置Consumer第一次启动是从队列头部开始消费还是队列尾部开始消费&lt;br&gt; * 如果非第一次启动，那么按照上次消费的位置继续消费 */ consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.subscribe(\"TopicTest\", \"TagA || TagC || TagD\"); consumer.registerMessageListener(new MessageListenerOrderly() &#123;//注册顺序消费监听 Random random = new Random(); @Override public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; context.setAutoCommit(true); for (MessageExt msg : msgs) &#123; // 可以看到每个queue有唯一的consume线程来消费, 订单对每个queue(分区)有序 System.out.println(\"consumeThread=\" + Thread.currentThread().getName() + \"queueId=\" + msg.getQueueId() + \", content:\" + new String(msg.getBody())); &#125; try &#123; //模拟业务逻辑处理中... TimeUnit.SECONDS.sleep(random.nextInt(10)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return ConsumeOrderlyStatus.SUCCESS; &#125; &#125;); consumer.start(); System.out.println(\"Consumer Started.\"); &#125;&#125; 输出： 消费者启动线程名称：【ConsumeMessageThread_1】:OrderStep&#123;orderId=1065, desc=&apos;创建&apos;&#125;线程名称：【ConsumeMessageThread_1】:OrderStep&#123;orderId=1065, desc=&apos;付款&apos;&#125;线程名称：【ConsumeMessageThread_1】:OrderStep&#123;orderId=1065, desc=&apos;完成&apos;&#125;线程名称：【ConsumeMessageThread_2】:OrderStep&#123;orderId=7235, desc=&apos;创建&apos;&#125;线程名称：【ConsumeMessageThread_2】:OrderStep&#123;orderId=7235, desc=&apos;付款&apos;&#125;线程名称：【ConsumeMessageThread_2】:OrderStep&#123;orderId=7235, desc=&apos;完成&apos;&#125;线程名称：【ConsumeMessageThread_3】:OrderStep&#123;orderId=1039, desc=&apos;创建&apos;&#125;线程名称：【ConsumeMessageThread_3】:OrderStep&#123;orderId=1039, desc=&apos;付款&apos;&#125;线程名称：【ConsumeMessageThread_3】:OrderStep&#123;orderId=1039, desc=&apos;推送&apos;&#125;线程名称：【ConsumeMessageThread_3】:OrderStep&#123;orderId=1039, desc=&apos;完成&apos;&#125; 同一个队列，都是用相同的线程消费（一个queue开启一个线程消费）。 4.3 延时消息比如电商里，提交了一个订单就可以发送一个延时消息，1h后去检查这个订单的状态，如果还是未付款就取消订单释放库存。 4.3.1 启动消息消费者public class ScheduledMessageConsumer &#123; public static void main(String[] args) throws Exception &#123; // 实例化消费者 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"ExampleConsumer\"); // 订阅Topics consumer.subscribe(\"TestTopic\", \"*\"); // 注册消息监听者 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; messages, ConsumeConcurrentlyContext context) &#123; for (MessageExt message : messages) &#123; // Print approximate delay time period System.out.println(\"Receive message[msgId=\" + message.getMsgId() + \"] \" + (System.currentTimeMillis() - message.getStoreTimestamp()) + \"ms later\");//当前时间减去，消息保存到队列时间，就可以验证生产者设置的延时是否生效 &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); // 启动消费者 consumer.start(); &#125;&#125; 4.3.2 发送延时消息public class ScheduledMessageProducer &#123; public static void main(String[] args) throws Exception &#123; // 实例化一个生产者来产生延时消息 DefaultMQProducer producer = new DefaultMQProducer(\"ExampleProducerGroup\"); // 启动生产者 producer.start(); int totalMessagesToSend = 100; for (int i = 0; i &lt; totalMessagesToSend; i++) &#123; Message message = new Message(\"TestTopic\", (\"Hello scheduled message \" + i).getBytes()); // 设置延时等级3,这个消息将在10s之后发送(现在只支持固定的几个时间,详看delayTimeLevel) message.setDelayTimeLevel(3); // 发送消息 producer.send(message); &#125; // 关闭生产者 producer.shutdown(); &#125;&#125; ###4.3.3 验证 您将会看到消息的消费比存储时间晚10秒 4.3.4 使用限制// org/apache/rocketmq/store/config/MessageStoreConfig.javaprivate String messageDelayLevel = \"1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h\";使用方式：message.setDelayTimeLevel(3);表示延时10s 同理：message.setDelayTimeLevel(0)，表示延时1s 现在RocketMq并不支持任意时间的延时，需要设置几个固定的延时等级，从1s到2h分别对应着等级1到18 4.4 批量消息批量发送消息能显著提高传递小消息的性能。限制是这些批量消息应该有相同的topic，相同的waitStoreMsgOK，而且不能是延时消息。此外，这一批消息的总大小不应超过4MB。 4.4.1 发送批量消息如果您每次只发送不超过4MB的消息，则很容易使用批处理，样例如下： String topic = \"BatchTest\";List&lt;Message&gt; messages = new ArrayList&lt;&gt;();messages.add(new Message(topic, \"TagA\", \"OrderID001\", \"Hello world 0\".getBytes()));messages.add(new Message(topic, \"TagA\", \"OrderID002\", \"Hello world 1\".getBytes()));messages.add(new Message(topic, \"TagA\", \"OrderID003\", \"Hello world 2\".getBytes()));try &#123; producer.send(messages);&#125; catch (Exception e) &#123; e.printStackTrace(); //处理error&#125; 如果消息的总长度可能大于4MB时，这时候最好把消息进行分割 public class ListSplitter implements Iterator&lt;List&lt;Message&gt;&gt; &#123; private final int SIZE_LIMIT = 1024 * 1024 * 4; private final List&lt;Message&gt; messages; private int currIndex; public ListSplitter(List&lt;Message&gt; messages) &#123; this.messages = messages; &#125; @Override public boolean hasNext() &#123; return currIndex &lt; messages.size(); &#125; @Override public List&lt;Message&gt; next() &#123; int nextIndex = currIndex; int totalSize = 0; for (; nextIndex &lt; messages.size(); nextIndex++) &#123; Message message = messages.get(nextIndex); int tmpSize = message.getTopic().length() + message.getBody().length; Map&lt;String, String&gt; properties = message.getProperties(); for (Map.Entry&lt;String, String&gt; entry : properties.entrySet()) &#123; tmpSize += entry.getKey().length() + entry.getValue().length(); &#125; tmpSize = tmpSize + 20; // 增加日志的开销20字节 if (tmpSize &gt; SIZE_LIMIT) &#123; //单个消息超过了最大的限制 //忽略,否则会阻塞分裂的进程 if (nextIndex - currIndex == 0) &#123; //假如下一个子列表没有元素,则添加这个子列表然后退出循环,否则只是退出循环 nextIndex++; &#125; break; &#125; if (tmpSize + totalSize &gt; SIZE_LIMIT) &#123; break; &#125; else &#123; totalSize += tmpSize; &#125; &#125; List&lt;Message&gt; subList = messages.subList(currIndex, nextIndex); currIndex = nextIndex; return subList; &#125;&#125;//把大的消息分裂成若干个小的消息ListSplitter splitter = new ListSplitter(messages);while (splitter.hasNext()) &#123; try &#123; List&lt;Message&gt; listItem = splitter.next(); producer.send(listItem); &#125; catch (Exception e) &#123; e.printStackTrace(); //处理error &#125;&#125; 4.5 过滤消息在大多数情况下，TAG是一个简单而有用的设计，其可以来选择您想要的消息。例如： DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"CID_EXAMPLE\");consumer.subscribe(\"TOPIC\", \"TAGA || TAGB || TAGC\"); 消费者将接收包含TAGA或TAGB或TAGC的消息。但是限制是一个消息只能有一个标签，这对于复杂的场景可能不起作用。在这种情况下，可以使用SQL表达式筛选消息。SQL特性可以通过发送消息时的属性来进行计算。在RocketMQ定义的语法下，可以实现一些简单的逻辑。下面是一个例子： ------------| message ||----------| a &gt; 5 AND b = &apos;abc&apos;| a = 10 | --------------------&gt; Gotten| b = &apos;abc&apos;|| c = true |------------------------| message ||----------| a &gt; 5 AND b = &apos;abc&apos;| a = 1 | --------------------&gt; Missed| b = &apos;abc&apos;|| c = true |------------ 4.5.1 SQL基本语法RocketMQ只定义了一些基本语法来支持这个特性。你也可以很容易地扩展它。 数值比较，比如：&gt;，&gt;=，&lt;，&lt;=，BETWEEN，=； 字符比较，比如：=，&lt;&gt;，IN； IS NULL 或者 IS NOT NULL； 逻辑符号 AND，OR，NOT； 常量支持类型为： 数值，比如：123，3.1415； 字符，比如：‘abc’，必须用单引号包裹起来； NULL，特殊的常量 布尔值，TRUE 或 FALSE 只有使用push模式的消费者才能用使用SQL92标准的sql语句，接口如下： public void subscribe(finalString topic, final MessageSelector messageSelector) 4.5.2 消息生产者发送消息时，你能通过putUserProperty来设置消息的属性 DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\");producer.start();Message msg = new Message(\"TopicTest\", tag, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET));// 设置一些属性msg.putUserProperty(\"a\", String.valueOf(i));SendResult sendResult = producer.send(msg);producer.shutdown(); 4.5.3 消息消费者用MessageSelector.bySql来使用sql筛选消息 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"please_rename_unique_group_name_4\");// 只有订阅的消息有这个属性a, a &gt;=0 and a &lt;= 3consumer.subscribe(\"TopicTest\", MessageSelector.bySql(\"a between 0 and 3\");consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;&#125;);consumer.start(); 4.6 事务消息###4.6.1 流程分析 上图说明了事务消息的大致方案，其中分为两个流程：正常事务消息的发送及提交、事务消息的补偿流程。 分布式系统中的事务可以使用TCC（Try、Confirm、Cancel）、2pc来解决分布式系统中的消息原子性 rmq的事务消息默认使用，2PC。 ####1）事务消息发送及提交 (1) 发送消息（half消息）（预处理消息，当broker收到此类消息后，会存储到RMQ_SYS_TRANS_HALF_TOPIC的消息消费队列中）。 (2) 服务端响应消息写入结果。 (3) 根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）。 (4) 根据本地事务状态执行Commit或者Rollback（Commit操作生成消息索引，消息对消费者可见） 2）事务补偿(1) 对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次“回查” (2) Producer收到回查消息，检查回查消息对应的本地事务的状态 (3) 根据本地事务状态，重新Commit或者Rollback 其中，补偿阶段用于解决消息Commit或者Rollback发生超时或者失败的情况。 3）事务消息状态事务消息共有三种状态，提交状态、回滚状态、中间状态： TransactionStatus.CommitTransaction: 提交事务，它允许消费者消费此消息。 TransactionStatus.RollbackTransaction: 回滚事务，它代表该消息将被删除，不允许被消费。broker端会删除半消息 TransactionStatus.Unknown: 中间状态，它代表需要检查消息队列来确定状态。 RocketMQ实现方式Half Message：预处理消息，当broker收到此类消息后，会存储到RMQ_SYS_TRANS_HALF_TOPIC的消息消费队列中 检查事务状态：Broker会开启一个定时任务，消费RMQ_SYS_TRANS_HALF_TOPIC队列中的消息，每次执行任务会向消息发送者确认事务执行状态（提交、回滚、未知），如果是未知，等待下一次回调。 超时：如果超过回查次数，默认回滚消息 TransactionListener的两个方法executeLocalTransaction 半消息发送成功触发此方法来执行本地事务 checkLocalTransaction broker将发送检查消息来检查事务状态，并将调用此方法来获取本地事务状态 ###4.6.1 发送事务消息 1) 创建事务性生产者使用 TransactionMQProducer类创建生产者，并指定唯一的 ProducerGroup，就可以设置自定义线程池来处理这些检查请求。执行本地事务后、需要根据执行结果对消息队列进行回复。回传的事务状态在请参考前一节。 public class Producer &#123; public static void main(String[] args) throws MQClientException, InterruptedException &#123; //创建事务监听器 TransactionListener transactionListener = new TransactionListenerImpl(); //创建消息生产者 TransactionMQProducer producer = new TransactionMQProducer(\"group6\"); producer.setNamesrvAddr(\"192.168.25.135:9876;192.168.25.138:9876\"); //生产者设置监听器 producer.setTransactionListener(transactionListener); //启动消息生产者 producer.start(); String[] tags = new String[]&#123;\"TagA\", \"TagB\", \"TagC\"&#125;; for (int i = 0; i &lt; 3; i++) &#123; try &#123; Message msg = new Message(\"TransactionTopic\", tags[i % tags.length], \"KEY\" + i, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET)); SendResult sendResult = producer.sendMessageInTransaction(msg, null); System.out.printf(\"%s%n\", sendResult); TimeUnit.SECONDS.sleep(1); &#125; catch (MQClientException | UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; //producer.shutdown(); &#125;&#125; 2）实现事务的监听接口当发送半消息成功时，我们使用 executeLocalTransaction 方法来执行本地事务。它返回前一节中提到的三个事务状态之一。checkLocalTranscation 方法用于检查本地事务状态，并回应消息队列的检查请求。它也是返回前一节中提到的三个事务状态之一。 public class TransactionListenerImpl implements TransactionListener &#123; @Override public LocalTransactionState executeLocalTransaction(Message msg, Object arg) &#123; System.out.println(\"执行本地事务\"); if (StringUtils.equals(\"TagA\", msg.getTags())) &#123; return LocalTransactionState.COMMIT_MESSAGE; &#125; else if (StringUtils.equals(\"TagB\", msg.getTags())) &#123; return LocalTransactionState.ROLLBACK_MESSAGE; &#125; else &#123; return LocalTransactionState.UNKNOW; &#125; &#125; //提供给mq回查本次本地事务执行的状态，然后返回响应的消息的事务状态 @Override public LocalTransactionState checkLocalTransaction(MessageExt msg) &#123; System.out.println(\"MQ检查消息Tag【\"+msg.getTags()+\"】的本地事务执行结果\"); return LocalTransactionState.COMMIT_MESSAGE; &#125;&#125; 4.6.2 使用限制 事务消息不支持延时消息和批量消息。 为了避免单个消息被检查太多次而导致半队列消息累积，我们默认将单个消息的检查次数限制为 15 次，但是用户可以通过 Broker 配置文件的 transactionCheckMax参数来修改此限制。如果已经检查某条消息超过 N 次的话（ N = transactionCheckMax ） 则 Broker 将丢弃此消息，并在默认情况下同时打印错误日志。用户可以通过重写 AbstractTransactionCheckListener 类来修改这个行为。 事务消息将在 Broker 配置文件中的参数 transactionMsgTimeout 这样的特定时间长度之后被检查。当发送事务消息时，用户还可以通过设置用户属性 CHECK_IMMUNITY_TIME_IN_SECONDS 来改变这个限制，该参数优先于 transactionMsgTimeout 参数。 事务性消息可能不止一次被检查或消费。 提交给用户的目标主题消息可能会失败，目前这依日志的记录而定。它的高可用性通过 RocketMQ 本身的高可用性机制来保证，如果希望确保事务消息不丢失、并且事务完整性得到保证，建议使用同步的双重写入机制。 事务消息的生产者 ID 不能与其他类型消息的生产者 ID 共享。与其他类型的消息不同，事务消息允许反向查询、MQ服务器能通过它们的生产者 ID 查询到消费者。 kafka和rmqkafka的定位是：日志处理，那么他对消息的丢失是可以容忍的。 rmq的定位是：消息中间件，对于消息的丢失和处理要求更高。rmq他是借鉴与kafka产生的。 ​ rmq中，broker的master之间是没有进行数据同步的，生产者发送的消息会路由到其中某一个master。也就是说，2m-2s架构中两个master的作用就相当于负载均衡的效果，就是为了承担生产者的写请求压力。两个slave就是承担消费者的读压力。 ​ 也就是说，整个rmq的集群你会发现，nameserver之间也是不进行通信的，broker之间也是不进行通信的。仅仅只有master和slave才会存在通信（需要做数据同步） ​ rmq做到了读写分离，master负责写，slave负责读。而且假设master挂掉了，slave也不会重新选举master，挂了就挂了。但是此时与挂掉的master相关联的slave还是可以接受读请求的。 错误详解RocketMQ 同步复制 SLAVE_NOT_AVAILABLE 异常源码分析解决方案：https://blog.csdn.net/zchdjb/article/details/102889760","categories":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://kingge.top/categories/RocketMQ/"}],"tags":[{"name":"rmq","slug":"rmq","permalink":"http://kingge.top/tags/rmq/"},{"name":"RocketMQ基础知识","slug":"RocketMQ基础知识","permalink":"http://kingge.top/tags/RocketMQ基础知识/"}]},{"title":"king-spring","slug":"kingspring总结文档","date":"2019-06-12T02:21:59.000Z","updated":"2020-05-04T01:32:48.237Z","comments":true,"path":"2019/06/12/kingspring总结文档/","link":"","permalink":"http://kingge.top/2019/06/12/kingspring总结文档/","excerpt":"","text":"编写king-spring的目的旨在能够了解spring整个架构核心的脉络，从而更好的理解spring的架构精神和底层原理 A tiny IoC container refer to Spring king-spring的结构逻辑 可以看到整个spring的简化版逻辑就是这样子的，核心就是IOC/DI，MVC。 那么我们接下来就模拟spring实现我们的king-spring 代码实现配置阶段-做准备工作配置web.xml和配置控制类 - KDispatcherServlet 创建web请求控制类 - KDispatcherServlet 配置到web.xml中 &lt;!DOCTYPE web-app PUBLIC \"-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN\" \"http://java.sun.com/dtd/web-app_2_3.dtd\" &gt;&lt;web-app&gt; &lt;display-name&gt;king-srping project&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;kc&lt;/servlet-name&gt; &lt;servlet-class&gt;com.kingge.spring.web.servlet.KDispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:application.properties&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;kc&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 配置application.properties这里采用的是properties的方式进行spring项目的配置（后期可以能会扩展成xml方式） scanPackage=com.kingge.spring 目前就配置一项，项目扫描根目录 实现扫描注解​ 我们应该知道，spring是通过校验某个bean是否加了特定的注解，从而决定是否加入到IOC容器中，也即是：bean的生命周期管理是否托管给spring。 ​ 常用的注解有这几个：@Component、@Service、@Controller、@Repository，后三个实际上都是基于Component的实现，所以他们的效果等同@Component，但是后三个出现的目的是为了标识业务层次，所以在使用过程中，我们最好是按照不同的业务层次使用不同的注解，而且在spring官网上提到，后面这三个注解在以后可能还会赋予其他的含义。 逐层分为两个层次(包)： 业务层和web控制层 总结​ 做完这一步，我们基本把king-spring的整体架构搭建出来了，而且某些注解类和控制类也已经建立，但是某些细节实现还是没有完成，例如KDispatcherServlet内部还没有实现。 ​ 接下来就实现，具体的解析配置信息（application.properties） 初始化IOC容器-解析配置类首先我们把这段代码逻辑是声明在了KDispatcherServlet的init方法中，也即是项目启动过程中就会去执行 这个过程分为四步 根据web.xml配置的contextConfigLocation解析配置类//1.加载配置文件 loadConfig(config.getInitParameter(CONFIG_PROPERTIES_LOCATION));//CONFIG_PROPERTIES_LOCATION的值等于\"contextConfigLocation\"//2. 将配置类映射到Properties中//保存解析application.properties 的配置文件信息 private Properties contextConfig = new Properties(); 根据配置类获取扫描包的路径//2.根据配置文件配置的扫描路径，扫描类 scanPackage(contextConfig.getProperty(\"scanPackages\"));//经过这个方法会把递归把包路径下的所有类都放到下面的list中 //3.保存扫描到的所有bean的全类名（还未过滤，可能包含不需要注入到ioc容器的bean） private List&lt;String&gt; classNames = new ArrayList&lt;String&gt;(); 需要注意的是，扫描到的类，可能存在某些类需要过滤掉，并不是都需要加到IOC容器中，判断是否加入IOC容器中的依据是：是否被我们上面定义的注解所修饰 解析需要注入到ioc容器中的bean//3.筛选上面扫描的所有类，将标有注入注解的类，放到IOC容器中 //类似于解析xml的bean标签 initContext();//最终根据注解筛选出需要放到IOC容器中的bean。classNames经过了过滤后得到的bean放入下面的map中private Map&lt;String,Object&gt; beanDefinitionMap = new ConcurrentHashMap&lt;String,Object&gt;(256); 解决IOC容器中bean 的依赖问题也就是DI依赖注入阶段 //4.解析IOC容器中的bean，并设置依赖 - DI阶段 diBean(); 控制层的url绑定处理方法​ 实际上就是处理IOC容器中被@KController注解修饰的类，然后再根据@KRequestMapping获取映射的url，然后再绑定url对应的处理方法。 //5.初始化handlerMapping，解析所有控制类的url地址和相关联的处理method initHandlerMapping();//解析出来的映射关系，保存到下面的handlerMapping中 //保存控制层，url跟处理方法的映射关系 private Map&lt;String,Method&gt; handlerMapping = new ConcurrentHashMap&lt;String,Method&gt;(); 处理请求实际上就是根据请求的url，然后从上面的handlerMapping中，获取url对应的处理方法，然后执行，返回结果。 //客户端处理请求doDispatcher(req, resp); 总结​ IOC容器在上面我们是使用private Map&lt;String,Object&gt; beanDefinitionMap = new ConcurrentHashMap&lt;String,Object&gt;(256); key是beanName（默认是类名首字母小写），value是bean的实例 ​ ​ url映射关系我们使用的是handlerMapping，他的类型是：private Map&lt;String,Method&gt; handlerMapping = new ConcurrentHashMap&lt;String,Method&gt;(); key就是请求url，value就是url映射的method 自我实现的king-spring 使用了那些设计模式 模板模式：init方法，把初始化分为了多个步骤，每个步骤自己实现自己的逻辑 策略模式：根据注解类型选择不同的操作方式。 委托模式：doGet()、doPost(). 单例模式：Constants类 项目地址 https://github.com/JeremyKinge/king-spring.git //项目地址 回过头来查看spring源码spring的ioc容器怎么实现的呢？跟我们自己的实现有何不同通过查看我们发现，spring在启动时，假设是注解版本那么，使用的context是 AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();那么也就意味着，最终扫描包下的类，然后放到IOC容器中的逻辑肯定在其中 最后跟踪到，DefaultListableBeanFactory的registerBeanDefinition()方法，最后调用了this.beanDefinitionMap.put(beanName, beanDefinition); 也就是说BeanDefinitionMap就是我们需要查找的IOC容器 /** Map of bean definition objects, keyed by bean name */private final Map&lt;String, BeanDefinition&gt; beanDefinitionMap = new ConcurrentHashMap&lt;&gt;(256); 可以看到，他的key跟我们的定义的一样，也是通过beanName方式，但是value跟我们却不同。 BeanDefinition ​ 他实际上就是spring对于注解的描述类。我们还记不记得，在使用@Service这样的注解时，我们是可以配置作用域，生命周期，是否懒加载等等信息。那么这些信息就是保存在BeanDefinition中。 ​ 换句话说，spring的BeanDefinition实现，更加全面，而且更加面对对对象。 我们上面的做法是直接把实例化的bean当做value，那么这样势必会造成内存的浪费。 spring的handlermapping是怎么实现的呢？那么这个属性的定义肯定是在DispatcherServlet中的。 /** List of HandlerMappings used by this servlet */@Nullableprivate List&lt;HandlerMapping&gt; handlerMappings; 可以看到spring他的实现，是一个list，值得类型是HandlerMapping，在handlerMapping维护URl跟method 的映射关系。 我们的实现是map数据结构，key是url，value是url映射的method 那为什么不用我们的map结构呢？map架构取数据不是更加清晰么？ 我觉得spring应该是考虑到冗余原则，因为如果用map方式，那么key的值就只能是url。为了保证能够获取更多的信息，那么就封装成对象，然后使用list保存 - 满足单一原则，就是我所有信息都可以用一个对象保存，干嘛还要分开成key-value的形式呢？。其实我觉得map也很不错，选择不同而已 那为什么不用map呢？其实也可以。 目前存在的缺点1.在doDispatcher方法中，调用处理方法时，请求参数的拼接是静态写死的，后期需要修改成动态拼接 思考题spring中的bean是线程安全的么？​ 首先，我们要知道，spring的bean是在spring启动时，通过反射实例化出来，然后放到IOC容器中的，也就是说spring中的bean只是帮你管理而已，并没有做什么增强或者修改工作。 ​ 也就是说，bean的是不是线程安全的，那么取决于你对于bean的实现，跟spring没有任何关系。 所以这个思考题的答案是：spring中的bean是否线程安全，是这个bean的问题，如果你在bean中有操作共享资源的操作，那么就有线程安全的问题，如果没有那么就是跟线程安全的。所以bean是否是线程安全的取决于bean自身的实现，而不是spring的问题。 ​ 虽然IOC容器是基于chm实现的。 应该说，spring存取bean的操作是线程安全的。这样的因为会比较好点。 spring中的bean什么时候被回收这个跟bean的生命周期 有关系，spring的bean生命周期有：singleton、prototype、request、session。 所以什么时候被回收，取决于，你设定的生命周期类型","categories":[{"name":"king-spring","slug":"king-spring","permalink":"http://kingge.top/categories/king-spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://kingge.top/tags/spring/"},{"name":"spring源码","slug":"spring源码","permalink":"http://kingge.top/tags/spring源码/"}]},{"title":"项目全局异常处理","slug":"项目全局异常处理","date":"2019-05-11T02:21:59.000Z","updated":"2019-09-03T13:24:12.946Z","comments":true,"path":"2019/05/11/项目全局异常处理/","link":"","permalink":"http://kingge.top/2019/05/11/项目全局异常处理/","excerpt":"","text":"起因 这是service层的一个业务方法，业务功能是：查询某些二维码数据。可以看到我们在这个方法里面除了主要的业务逻辑，还有数据非空判断，以及异常处理等等。 如果我们不仅仅是在service层添加try catch异常处理，在controller层调用service方法的地方也添加了try catch这样的异常处理代码，那么代码冗余严重且不易维护 总结-上诉代码存在两个问题 1、上边的代码只要操作不成功仅向用户返回“错误代码：-1，失败信息：操作失败”，无法区别具体的错误信息。–参见第三处代码2、service方法在执行过程出现异常在哪捕获？在service中需要都加try/catch，如果在controller也需要添加try/catch，代码冗余严重且不易维护。 以下项目是 springboot2.0.1 下开发 解决方式1、在Service方法中的编码顺序是先校验判断，有问题则抛出具体的异常信息，最后执行具体的业务操作，返回成功信息。 2、在统一异常处理类中去捕获异常，无需controller捕获异常，向用户返回统一规范的响应信息。 异常处理流程 系统对异常的处理使用统一的异常处理流程： 1、自定义异常类型。 2、自定义错误代码及错误信息。 3、对于可预知的异常由程序员在代码中主动抛出，由SpringMVC统一捕获。可预知异常是程序员在代码中手动抛出本系统定义的特定异常类型，由于是程序员抛出的异常，通常异常信息比较齐全，程序员在抛出时会指定错误代码及错误信息，获取异常信息也比较方便。 4、对于不可预知的异常（运行时异常）由SpringMVC统一捕获Exception类型的异常。不可预知异常通常是由于系统出现bug、或一些不要抗拒的错误（比如网络中断、服务器宕机等），异常类型为RuntimeException类型（运行时异常）。 5、可预知的异常及不可预知的运行时异常最终会采用统一的信息格式（错误代码+错误信息）来表示，最终也会随请求响应给客户端。 1、在controller、service、dao中程序员抛出自定义异常；springMVC框架抛出框架异常类型2、统一由异常捕获类捕获异常，并进行处理3、捕获到自定义异常则直接取出错误代码及错误信息，响应给用户。 4、捕获到非自定义异常类型首先从Map中找该异常类型是否对应具体的错误代码，如果有则取出错误代码和错误信息并响应给用户，如果从Map中找不到异常类型所对应的错误代码则统一为99999错误代码并响应给用户。 5、将错误代码及错误信息以Json格式响应给用户。 特别解释 不可知异常我们在上图中有两个分支，左边的分支表示的是有些不可知异常我们是知道的（这个可能比较拗口），例如SQLException，HttpMessageNotReadableException（消息转化异常），这些异常我们是可以特别处理的，可以给其标注响应的错误信息，而不用走右边的99999默认消息异常处理分支。 实现定义常用错误代码接口-错误信息格式public interface ResultCode &#123; //操作是否成功,true为成功，false操作失败 boolean success(); //操作代码 int code(); //提示信息 String message();&#125; 根据这个信息格式我们可以自定义我们的错误实现。例如通用的消息模块CommonCode里面包含了常用的错误消息例子、文件上传消息模块FileSystemCode、认证消息模块AuthCode等等。满足我们自己业务的相关提示和异常提示。 定义通用的消息模块CommonCodepublic enum CommonCode implements ResultCode&#123; INVALID_PARAM(false,10003,&quot;非法参数！&quot;), SUCCESS(true,10000,&quot;操作成功！&quot;), FAIL(false,11111,&quot;操作失败！&quot;), UNAUTHENTICATED(false,10001,&quot;此操作需要登陆系统！&quot;), UNAUTHORISE(false,10002,&quot;权限不足，无权操作！&quot;), SERVER_ERROR(false,99999,&quot;抱歉，系统繁忙，请稍后重试！&quot;); //操作是否成功 boolean success; //操作代码 int code; //提示信息 String message; private CommonCode(boolean success,int code, String message)&#123; this.success = success; this.code = code; this.message = message; &#125; @Override public boolean success() &#123; return success; &#125; @Override public int code() &#123; return code; &#125; @Override public String message() &#123; return message; &#125;&#125; 文件上传消息模块FileSystemCode-可选public enum FileSystemCode implements ResultCode &#123; FS_UPLOADFILE_FILEISNULL(false,25001,&quot;上传文件为空！&quot;), FS_UPLOADFILE_BUSINESSISNULL(false,25002,&quot;业务Id为空！&quot;), FS_UPLOADFILE_SERVERFAIL(false,25003,&quot;上传文件服务器失败！&quot;), FS_DELETEFILE_NOTEXISTS(false,25004,&quot;删除的文件不存在！&quot;), FS_DELETEFILE_DBFAIL(false,25005,&quot;删除文件信息失败！&quot;), FS_DELETEFILE_SERVERFAIL(false,25006,&quot;删除文件失败！&quot;), FS_UPLOADFILE_METAERROR(false,25007,&quot;上传文件的元信息请使用json格式！&quot;), FS_UPLOADFILE_USERISNULL(false,25008,&quot;上传文件用户为空！&quot;); //操作代码 boolean success; //操作代码 int code; //提示信息 String message; private FileSystemCode(boolean success, int code, String message)&#123; this.success = success; this.code = code; this.message = message; &#125; @Override public boolean success() &#123; return success; &#125; @Override public int code() &#123; return code; &#125; @Override public String message() &#123; return message; &#125;&#125; 基础的消息格式我们已经做完了，那么接下来就是定义我们的异常 可预知异常和不可预知异常处理（1）自定义异常类 /** * 自定义异常类型 **/public class CustomException extends RuntimeException &#123; //错误代码 ResultCode resultCode; public CustomException(ResultCode resultCode)&#123; this.resultCode = resultCode; &#125; public ResultCode getResultCode()&#123; return resultCode; &#125;&#125; （2）异常抛出类 /** * 异常抛出类，业务端代码直接调用即可 * 避免在service或者controller端书写throw new CustomException(resultCode);这样的重复性代码 *可以直接使用ExceptionCast.cast（）的方式直接抛出异常 **/public class ExceptionCast &#123; public static void cast(ResultCode resultCode)&#123; throw new CustomException(resultCode); &#125;&#125; （3）异常捕获类 使用 @ControllerAdvice和@ExceptionHandler注解来捕获指定类型的异常 /** * 统一异常捕获类 **/@ControllerAdvice//控制器增强public class ExceptionCatch &#123; private static final Logger LOGGER = LoggerFactory.getLogger(ExceptionCatch.class); //定义map，配置异常类型所对应的错误代码 - 这个就是存储我们在不可预知异常的左边分支的异常 private static ImmutableMap&lt;Class&lt;? extends Throwable&gt;,ResultCode&gt; EXCEPTIONS; //定义map的builder对象，去构建ImmutableMap protected static ImmutableMap.Builder&lt;Class&lt;? extends Throwable&gt;,ResultCode&gt; builder = ImmutableMap.builder(); //捕获CustomException此类异常 -处理可预知异常关键方法（1） @ExceptionHandler(CustomException.class) @ResponseBody//返回给controller的json串格式的异常消息格式 public ResponseResult customException(CustomException customException)&#123; customException.printStackTrace(); //记录日志 LOGGER.error(&quot;catch exception:&#123;&#125;&quot;,customException.getMessage()); ResultCode resultCode = customException.getResultCode(); return new ResponseResult(resultCode); &#125; //捕获Exception此类异常 - 处理不可预知异常关键方法（2） //里面包含我们在异常处理流程中&lt;处理不可预知异常&gt;的左右两个分支 //根据异常的全类名在我们自定义的异常map中查找是否存在已经自定义的不可预知异常，存在直接返回，不存在则当做99999全局异常处理 @ExceptionHandler(Exception.class) @ResponseBody public ResponseResult exception(Exception exception)&#123; exception.printStackTrace(); //记录日志 LOGGER.error(&quot;catch exception:&#123;&#125;&quot;,exception.getMessage()); if(EXCEPTIONS == null)&#123; EXCEPTIONS = builder.build();//EXCEPTIONS构建成功 &#125; //从EXCEPTIONS中找异常类型所对应的错误代码，如果找到了将错误代码响应给用户，如果找不到给用户响应99999异常 ResultCode resultCode = EXCEPTIONS.get(exception.getClass()); if(resultCode !=null)&#123; return new ResponseResult(resultCode); &#125;else&#123; //返回99999异常 return new ResponseResult(CommonCode.SERVER_ERROR); &#125; &#125; static &#123; //定义异常类型所对应的错误代码 builder.put(HttpMessageNotReadableException.class,CommonCode.INVALID_PARAM); &#125;&#125; 测试使用改造上诉代码，try catch语句可以去掉，而且可以返回我们精准自定义的错误信息 总结实现的关键点在于，使用 @ControllerAdvice和@ExceptionHandler注解来捕获指定类型的异常 @ExceptionHandler //该注解作用对象为方法@Target(&#123;ElementType.METHOD&#125;)//在运行时有效@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface ExceptionHandler &#123; //value()可以指定异常类 Class&lt;? extends Throwable&gt;[] value() default &#123;&#125;;&#125; @ControllerAdvice 增强版控制器 @Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface ControllerAdvice &#123; @AliasFor(&quot;basePackages&quot;) String[] value() default &#123;&#125;; @AliasFor(&quot;value&quot;) String[] basePackages() default &#123;&#125;; Class&lt;?&gt;[] basePackageClasses() default &#123;&#125;; Class&lt;?&gt;[] assignableTypes() default &#123;&#125;; Class&lt;? extends Annotation&gt;[] annotations() default &#123;&#125;;&#125;","categories":[{"name":"springboot项目常用技术实现","slug":"springboot项目常用技术实现","permalink":"http://kingge.top/categories/springboot项目常用技术实现/"}],"tags":[{"name":"异常处理","slug":"异常处理","permalink":"http://kingge.top/tags/异常处理/"},{"name":"ControllerAdvice注解","slug":"ControllerAdvice注解","permalink":"http://kingge.top/tags/ControllerAdvice注解/"},{"name":"ExceptionHandler注解","slug":"ExceptionHandler注解","permalink":"http://kingge.top/tags/ExceptionHandler注解/"}]},{"title":"SpringCloud个人总结","slug":"SpringCloud个人总结","date":"2019-05-01T02:21:59.000Z","updated":"2019-09-03T13:09:54.499Z","comments":true,"path":"2019/05/01/SpringCloud个人总结/","link":"","permalink":"http://kingge.top/2019/05/01/SpringCloud个人总结/","excerpt":"","text":"一下内容就是个人学习sc微服务架构中的学习总结，整个架构的东西很多，大家可以在需要某个组件时再去学习。 一、为什么需要微服务 我么那首先思考下面这些问题，为什么需要微服务，微服务能够解决什么痛点，它有什么优缺点？微服务和微服务架构是什么关系？什么是分布式？什么是集群？为了解决这些问题我们得从实现一个系统的架构的到底发生了那些演变说起。 1.1分布式的演化 1.1.1单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 优点： 适用于小型网站，小型管理系统，将所有功能都部署到一个工程里，简单易用，易于开发 缺点： 1、性能扩展比较难 2、协同开发问题 3、不利于升级维护 4、 只能采用同一种技术，很难用不同的语言或者语言不同版本开发不同模块； 5、系统耦合性强，一旦其中一个模块有问题，整个系统就瘫痪了；一旦升级其中一个模块，整个系统就停机了； 6、 集群只能是复制整个系统，即使只是其中一个模块压力大。（可能整个订单处理，仅仅是支付模块压力过大，按道理只需要升级支付模块，但是在单一场景里面是不能的） 那么这个时候我么那肯定产生了想法，就是把所有功能模块切开，分而治之，那么就演变成了下面的架构。 1.1.2 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用**拆成互不相干**的几个应用，以提升效率，**这样就可以单独修改某个模块而不用重启或者影响其他模块，同时也可以给某个访问量剧增的模块，单独添加服务器部署集群**。此时，用于加速前端页面开发的Web框架(MVC)是关键。 通过切分业务来实现各个模块独立部署，降低了维护和部署的难度，团队各司其职更易管理，性能扩展也更方便，更有针对性。 缺点： 公用模块无法重复利用，开发性的浪费（存在重复开发的问题） 面对突变的应用场景，可能某个模块对于web界面会频繁修改，但是模块业务功能没有变化，这样会造成单个应用频繁修改。所以需要界面+业务逻辑的实现分离。 没有处理好应用之间的交互问题，系统之间相互独立，例如订单模块可能会需要查询商品模块的信息。 这个时候，虽然切分了各个模块，但是没有很好地考虑到服务之间的引用等等问题。 1.1.3 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 例如我们常见的springcloud和dubbo就是属于分布式服务架构，但是严格上来讲dubbo并不是属于分布式架构，因为他并不具备分布式架构的某些特性，例如服务的分布式配置，服务网关，数据流，批量任务等等。一般认为Dubbo只是相当于SpringCloud中的Eureka模块（服务注册中心） 分布式服务框架很好的解决了垂直应用架构的缺点，实现界面和服务的分离，实现界面和服务，以及服务与服务之间的调度。 但是存在问题，那就是没有一个**统一管理服务的机制和基于访问压力的调度中心(服务注册中心，负载均衡)**，容易造成资源浪费，什么意思呢？假设用户服务部署了200台服务器，但是在某个时间段，他的访问压力很小，订单服务的访问压力剧增，服务器不够用。那么就会造成资源浪费和倾斜，存在服务器闲置或者请求量少的情况。 1.1.4 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)Service Oriented Architecture]是关键。 以前出现了什么问题？ 服务越来越多，需要管理每个服务的地址 调用关系错综复杂，难以理清依赖关系 服务过多，服务状态难以管理，无法根据服务情况动态管理 服务治理要做什么？ 服务注册中心，实现服务自动注册和发现，无需人为记录服务地址 服务自动订阅，服务列表自动推送，服务调用透明化，无需关心依赖关系 动态监控服务状态监控报告，人为控制服务状态 缺点： 服务间会有依赖关系，一旦某个环节出错会影响较大 服务关系复杂，运维、测试部署困难，不符合DevOps思想  1.1.5 微服务架构前面说的SOA，英文翻译过来是面向服务。微服务，似乎也是服务，都是对系统进行拆分。因此两者非常容易混淆，但其实却有一些差别： 微服务的特点： 单一职责：微服务中每一个服务都对应唯一的业务能力，做到单一职责 微：微服务的服务拆分粒度很小，例如一个用户管理就可以作为一个服务。每个服务虽小，但“五脏俱全”。 面向服务：面向服务是说每个服务都要对外暴露Rest风格服务接口API。并不关心服务的技术实现，做到与平台和语言无关，也不限定用什么技术实现，只要提供Rest的接口即可。 自治：自治是说服务间互相独立，互不干扰 团队独立：每个服务都是一个独立的开发团队，人数不能过多。 技术独立：因为是面向服务，提供Rest接口，使用什么技术没有别人干涉 前后端分离：采用前后端分离开发，提供统一Rest接口，后端不用再为PC、移动段开发不同接口 数据库分离：每个服务都使用自己的数据源 部署独立，服务间虽然有调用，但要做到服务重启不影响其它服务。有利于持续集成和持续交付。每个服务都是独立的组件，可复用，可替换，降低耦合，易维护 微服务结构图： 1.2 基本概念梳理 分布式：一个业务分拆多个子业务，部署在不同的服务器上 集群： 同一个业务，部署在多个服务器上 微服务： 微服务化的核心就是将传统的一站式应用,根据业务拆分成一个一个的服务,彻底地去耦合,每一个微服务提供单个业务功能的服务,一个服务做一件事,从技术角度看就是一种小而独立的处理过程,类似进程概念,能够自行单独启动或销毁，可以拥有自己独立的数据库。（我们之前使用springboot开发的项目就是属于一个微服务，他是单一进程，处理单一服务。-他关注的是单一业务的实现细节） 微服务架构：微服务架构是一种架构模式,它提倡将单一应用程序划分成一组小的服务,服务之间互相协调、互相配合,为用户提供最终价值.每个服务运行在其独立的进程中,服务与服务间采用轻量级的通信机制互相协作(通常是基于HTTP协议的RESTful API).每个服务都围绕着具体业务进行构建,并且能够被独立的部署到生产环境、类生产环境等.另外,应当尽量避免统一的、集中式的服务管理机制,对具体的一个服务而言,应根据业务上下文,选择合适的语言、工具对其进行构建.（springcloud就是一个微服务架构，通过一系列措施，管理微服务，实现系统整体的功能-他关注的是整体项目的实现和架构） 微服务提出者：马丁.福勒(Martin Fowler) 论文网址:https://martinfowler.com/articles/microservices.html ​ 1.3 微服务优缺点优点： 每个服务足够内聚,足够小,代码容易理解这样能聚焦一个指定的业务功能或业务需求 开发简单、开发效率提高,一个服务可能就是专一的只干一件事. 微服务能够被小团队单独开服,这个小团队是2到5人的开发人员组成 微服务是松耦合的,是有功能意义的服务,无论是在开发阶段或部署阶段都是独立的. 微服务能试用不同的语言开发 易于和第三方集成,微服务允许容易且灵活的方式集成自动部署,通过持续集成工具,如Jenkins,Hudson,bamboo. 微服务易于被一个开发人员理解,修改和维护,这样小团队能够更关注自己的工作成果.无需通过合作才能体现价值 微服务允许你利用融合最新技术. 微服务只是业务逻辑的代码,不会和HTML,CSS或其他界面组件混合. 每个微服务都有自己的存储能力,可以有自己的数据库.也可以有统一的数据库 缺点： 开发人员要处理分布式系统的复杂性 多服务运维难度,随着服务的增加,运维的压力也在增大 系统部署依赖 服务间通信成本 数据一致性 系统集成测试 性能监控 1.4 常见微服务架构 微服务条目 落地技术 服务开发 SpringBoot,Spring,SpringMVC 服务配置与管理 Netflix公司的Archaius、阿里的Diamond等 服务注册与发现 Eureka、Consul、Zookeeper等 服务调用 Rest、RPC、gRPC 服务熔断器 Hystrix、Envoy等 负载均衡 Ribbon、Nginx等 服务接口调用（客户端调用服务的简化工具） Feign等 消息队列 Kafka、RabbitMQ、ActiveMQ等 服务配置中心管理 SpringCloudConfig、Chef等 服务路由（API网关） Zuul等 服务监控 Zabbix、Nagios、Metrics、Specatator等 全链路追踪 Zipkin、Brave、Dapper等 服务部署 Docker、OpenStack、Kubernetes等 数据流操作开发包 SpringCloud Stream(封装与Redis，Rabbit，Kafka等发送接收消息) 事件消息总线 SpringCloud Bus 二.springcloud和dubbo为什么现在流行的微服务架构是springcloud而不是dubbo，最主要的是dubbo在这之前停止更新过几年的时间，这个时候springcloud异军突起，很好地抢占了先机，整体解决方案和框架成熟度，社区热度，可维护性，学习曲线也是它更加火爆的原因： 最主要的是，Dubbo 的定位始终是一款 RPC 框架，目的是提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案 然而：Spring Cloud 的目标是微服务架构下的一站式解决方案，换句话说，dubbo更像是springcloud的Eureka模块。 接下来我么你看一段关于Dubbo目前负责人刘军的一段采访 https://www.oschina.net/question/2896879_2272652?sort=time 当前各大IT公司用的微服务架构有哪些 阿里Dubbo/HSF京东JSF新浪微博Motan当当网DubboX 1.各微服务的框架对比 功能点/服务框架 Netflix/SpringCloud Motan gRPC Thrift Dubbo/DubboX 功能定位 完整的微服务架构 RPC框架，但整合了ZK或Consul，实现集群环境的基本服务注册/发现 RPC框架 RPC框架 服务框架 支持Rest 是，Ribbon支持多种可插拔的序列化选择 否 否 否 否 支持RPC 否 是 是 是 是 支持多语言 是（Rest形式） 否 是 是 否 服务注册/发现 是（Eureka） Eureka服务注册表，Karyon服务端框架支持服务自注册和健康检查 是（zookeeper/consul） 否 否 是 负载均衡 是（服务端zuul+客户端Ribbon） zuul-服务，动态路由 云端负载均衡 Eureka（针对中间层服务器） 是（客户端） 否 否 是（客户端） 配置服务 Netflix Archaius SpringCloud Config Server集中配置 是（zookeeper提供） 否 否 否 服务调用链监控 是（zuul） Zuul提供边缘服务，API网关 否 否 否 否 高可用/容错 是（服务端Hystrix+客户端Ribbon） 是（客户端） 否 否 是（客户端） 典型应用案例 Netflix Sina Google Facebook 社区活跃度 高 一般 高 一般 2017年7月才重启 学习难度 中等 一般 高 一般 低 文档丰富度 高 一般 一般 一般 高 其他 Spring Cloud Bus为我们应用程序带来了更多管理端点 支持降级 Netflix内部在开发集成gRPC IDL定义 实践公司比较多 2. springcloud VS Dubbo社区活跃度 https://github.com/dubbohttps://github.com/springcloud 功能对比 Dubbo Spring 服务注册中心 Zookeeper Spring Cloud Netfilx Eureka 服务调用方式 RPC REST API 服务监控 Dubbo-monitor Spring Boot Admin 断路器 不完善 Spring Cloud Netflix Hystrix 服务网关 无 Spring Cloud Netflix Zuul 分布式配置 无 Spring Cloud Config 服务跟踪 无 Spring Cloud Sleuth 消息总线 无 Spring Cloud Bus 数据流 无 Spring Cloud Stream 批量任务 无 Spring Cloud Task 最大区别： Spring Cloud抛弃了RPC通讯，采用基于HTTP的REST方式。Spring Cloud牺牲了服务调用的性能，但是同时也避免了原生RPC带来的问题。REST比RPC更为灵活，不存在代码级别的强依赖，在强调快速演化的微服务环境下，显然更合适。 ==一句话：Dubbo像组装机，Spring Cloud像一体机== 社区的支持与力度：Dubbo曾经停运了5年，虽然重启了，但是对于技术发展的新需求，还是需要开发者自行去拓展，对于中小型公司，显然显得比较费时费力，也不一定有强大的实力去修改源码 总结 解决的问题域不一样：Dubbo的定位是一款RPC框架，Spring Cloud的目标是微服务架构下的一站式解决方案 三.服务调用方式1.RPC和HTTP无论是微服务还是SOA，都面临着服务间的远程调用。那么服务间的远程调用方式有哪些呢？ 常见的远程调用方式有以下2种： RPC：Remote Produce Call远程过程调用，类似的还有RMI。自定义数据格式，基于原生TCP通信，速度快，效率高。早期的webservice，现在热门的dubbo，都是RPC的典型代表 Http：http其实是一种网络传输协议，基于TCP，规定了数据传输的格式。现在客户端浏览器与服务端通信基本都是采用Http协议，也可以用来进行远程服务调用。缺点是消息封装臃肿，优势是对服务的提供和调用方没有任何技术限定，自由灵活，更符合微服务理念。 现在热门的Rest风格，就可以通过http协议来实现。 如果你们公司全部采用Java技术栈，那么使用Dubbo作为微服务架构是一个不错的选择。 相反，如果公司的技术栈多样化，而且你更青睐Spring家族，那么SpringCloud搭建微服务是不二之选。在我们的项目中，我们会选择SpringCloud套件，因此我们会使用Http方式来实现服务间调用。 2.Http客户端工具既然微服务选择了Http，那么我们就需要考虑自己来实现对请求和响应的处理。不过开源世界已经有很多的http客户端工具，能够帮助我们做这些事情，例如： HttpClient OKHttp URLConnection 接下来，不过这些不同的客户端，API各不相同 3.Spring的RestTemplateSpring提供了一个RestTemplate模板工具类，对基于Http的客户端进行了封装，并且实现了对象与json的序列化和反序列化，非常方便。RestTemplate并没有限定Http的客户端类型，而是进行了抽象，目前常用的3种都有支持： HttpClient OkHttp JDK原生的URLConnection（默认的） RestTemplate简单使用 首先在项目中注册一个RestTemplate对象，可以在启动类位置注册： @SpringBootApplicationpublic class HttpDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(HttpDemoApplication.class, args); &#125; @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 在测试类中直接@Autowired注入： @RunWith(SpringRunner.class)@SpringBootTest(classes = HttpDemoApplication.class)public class HttpDemoApplicationTests &#123; @Autowired private RestTemplate restTemplate; @Test public void httpGet() &#123; // 调用springboot案例中的rest接口 User user = this.restTemplate.getForObject(\"http://localhost/user/1\", User.class); System.out.println(user); &#125;&#125; 通过RestTemplate的getForObject()方法，传递url地址及实体类的字节码，RestTemplate会自动发起请求，接收响应，并且帮我们对响应结果进行反序列化。 学习完了Http客户端工具，接下来就可以正式学习微服务了。 四.初识SpringCloud微服务是一种架构方式，最终肯定需要技术架构去实施。 微服务的实现方式很多，但是最火的莫过于Spring Cloud了。为什么？ 后台硬：作为Spring家族的一员，有整个Spring全家桶靠山，背景十分强大。 技术强：Spring作为Java领域的前辈，可以说是功力深厚。有强力的技术团队支撑，一般人还真比不了 群众基础好：可以说大多数程序员的成长都伴随着Spring框架，试问：现在有几家公司开发不用Spring？SpringCloud与Spring的各个框架无缝整合，对大家来说一切都是熟悉的配方，熟悉的味道。 使用方便：相信大家都体会到了SpringBoot给我们开发带来的便利，而SpringCloud完全支持SpringBoot的开发，用很少的配置就能完成微服务框架的搭建 1.简介SpringCloud是Spring旗下的项目之一，官网地址：http://projects.spring.io/spring-cloud/ 官网介绍:https://spring.io/ SpringCloud,基于springboot提供了一套为服务解决方案. Spring最擅长的就是集成，把世界上最好的框架拿过来，集成到自己的项目中。 SpringCloud也是一样，它将现在非常流行的一些技术整合到一起，实现了诸如：配置管理，服务发现，智能路由，负载均衡，熔断器，控制总线，集群状态等等功能。其主要涉及的组件包括： Eureka：服务治理组件，包含服务注册中心，服务注册与发现机制的实现。（服务治理，服务注册/发现） Zuul：网关组件，提供智能路由，访问过滤功能 Ribbon：客户端负载均衡的服务调用组件（客户端负载） Feign：服务调用，给予Ribbon和Hystrix的声明式服务调用组件 （声明式服务调用） Hystrix：容错管理组件，实现断路器模式，帮助服务依赖中出现的延迟和为故障提供强大的容错能力。(熔断、断路器，容错) 架构图： 以上只是其中一部分。 SpringCloud,基于springboot提供了一套为服务解决方案,包括服务注册与发现,配置中心,全链路监控,服务网关,负载均衡,熔断器等组件,除了基于NetFlix的开源组件做高度抽象封装之外,还有一些选型中立的开源组件. SpringCloud利用springboot的开发便利性巧妙地简化了分布式系统基础设施的开发,SpringCloud为开发人员提供了快速构建分布式系统的一些工具,包括配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等,它们都可以利用Springboot的开发风格做到一键启动和部署. SpringBoot并没有重复制造轮子,它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来,通过Springboot风格进行再封装屏蔽掉了复杂的配置和实现原理,最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包. 2.SpringCloud和springboot是什么关系Springboot专注于快速方便的开发单个个体微服务. SpringCloud是关注全局的微服务协调整理治理框架,它将Springboot开发的一个个单体微服务整合并管理起来,为各个微服务质检提供,配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务 Springboot可以离开SpringCloud独立使用开发项目,但是SpringCloud离不开Springboot,属于依赖的关系.Springboot专注于快速、方便的开发单个微服务个体,SpringCloud关注全局的服务治理框架. 3.springcloud的版本因为Spring Cloud不同其他独立项目，它拥有很多子项目的大项目。所以它的版本是版本名+版本号 （如Angel.SR6）。 版本名：是伦敦的地铁名 版本号：SR（Service Releases）是固定的 ,大概意思是稳定版本。后面会有一个递增的数字。 所以 Edgware.SR3就是Edgware的第3个Release版本。 我们在项目中，会是以Finchley的版本。 其中包含的组件，也都有各自的版本，如下表： Component Edgware.SR3 Finchley.RC1 Finchley.BUILD-SNAPSHOT spring-cloud-aws 1.2.2.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-bus 1.3.2.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-cli 1.4.1.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-commons 1.3.3.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-contract 1.2.4.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-config 1.4.3.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-netflix 1.4.4.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-security 1.2.2.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-cloudfoundry 1.1.1.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-consul 1.3.3.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-sleuth 1.3.3.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-stream Ditmars.SR3 Elmhurst.RELEASE Elmhurst.BUILD-SNAPSHOT spring-cloud-zookeeper 1.2.1.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-boot 1.5.10.RELEASE 2.0.1.RELEASE 2.0.0.BUILD-SNAPSHOT spring-cloud-task 1.2.2.RELEASE 2.0.0.RC1 2.0.0.RELEASE spring-cloud-vault 1.1.0.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-gateway 1.0.1.RELEASE 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT spring-cloud-openfeign 2.0.0.RC1 2.0.0.BUILD-SNAPSHOT 4.SpringCloud的参考资料五.springcloud的实现准备为了下面使用springcloud微服务架构各个优秀的组件，我们先搭建一个基本的分布式项目工程。 案例使用的springcloud和springboot版本分别是： &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Dalston.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt; 1 新建父工程-microservicecloudmicroservicecloud 打包方式设置为pom 设置pom.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;lombok.version&gt;1.16.18&lt;/lombok.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Dalston.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.0.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.31&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 查看项目 新建父工程目的：定义pom文件，统一各个子模块的jar依赖版本，方面管理，避免每个子模块使用相同组件不同版本，造成项目测试运行出现问题。 2 根据父工程，新建api公共模块-microservicecloud-api目的：抽取出所有子项目公共的bean或者方法。例如在下面的创建的服务提供者和服务消费者都是用到了Person这个javabean，那么我们就需要把这个bean放在这里。然后服务提供者和消费者就可以依赖这个api公共模块，从而实用Person实体类。 创建完毕 2.1 查看父工程pom 发现多了这一行，因为我们是在父工程microserviceproject下新建的，表示microservicecloud-api工程为父工程子模块。 2.2 修改microservicecloud-api的pom文件修改内容如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;dependencies&gt;&lt;!-- 当前Module需要用到的jar包，按自己需求添加，如果父类已经包含了，可以不用写版本号 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt;//这个组件可用可不用 &lt;/dependencies&gt;&lt;/project&gt; Lombok组件在真实项目中不建议使用，虽然他简化了javabean的开发，但是代码的可读性也产生了严重的影响。 2.3 新建公共bean-Person 生成get/set方法 2.4 项目结构 3 根据父工程，新建微服务提供者-providermicroservicecloud-provider-person-8001 –&gt; 8001表示服务暴露的端口号 新建方法同新建-microservicecloud-api 模块 3.1 修改pom文件&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;microservicecloud-provider-person-8001&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!-- 引入自己定义的api通用包，可以使用Person用户Entity --&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 3.2 新建application.yml文件 内容是：详细的参见代码 server: port: 8001mybatis: config-location: classpath:mybatis/mybatis.cfg.xml # mybatis配置文件所在路径 type-aliases-package: com.kingge.entity # 所有Entity别名类所在包 mapper-locations: - classpath:mybatis/mapper/**/*.xml # mapper映射文件spring: application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://127.0.0.1:3306/test # 数据库名称 username: root password: 123 dbcp2: min-idle: 5 # 数据库连接池的最小维持连接数 initial-size: 5 # 初始化连接数 max-total: 5 # 最大连接数 max-wait-millis: 200 注意每个属性后面必须是有空格-yml文件的格式 3.3 新建person数据表DROP TABLE IF EXISTS `person`;CREATE TABLE `person` ( `deptno` int(255) NOT NULL AUTO_INCREMENT, `dname` varchar(255) DEFAULT NULL, `db_source` varchar(255) DEFAULT NULL, //这和字段标识当前数据来源于那个数据库，后面讲解Eureka集群时会使用到 PRIMARY KEY (`deptno`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;-- ------------------------------ Records of person-- ----------------------------INSERT INTO `person` VALUES (&apos;1&apos;, &apos;开发部&apos;, &apos;test&apos;);INSERT INTO `person` VALUES (&apos;2&apos;, &apos;人事部&apos;, &apos;test&apos;);INSERT INTO `person` VALUES (&apos;3&apos;, &apos;集成部&apos;, &apos;test&apos;);INSERT INTO `person` VALUES (&apos;4&apos;, &apos;市场部&apos;, &apos;test&apos;); 3.4 新建dao和mapper 内容 import com.kingge.entity.Person;import org.apache.ibatis.annotations.Mapper;import java.util.List;@Mapperpublic interface PersonDao &#123; public boolean addDept(Person dept); public Person findById(Long id); public List&lt;Person&gt; findAll();&#125; 3.4.1 新建mybatis.cfg.xml实际上我们不需要这个xml文件，因为我们的配置一般都是已经放置在了application.yml文件中，但是为了整体架构的扩展性，这里也新建了改文件 3.5 新建PersonMapper.xml 存放mapper.xml文件的位置我们在上面application.yml中已经声明 内容 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.kingge.dao.PersonDao&quot;&gt; &lt;select id=&quot;findById&quot; resultType=&quot;Person&quot; parameterType=&quot;Long&quot;&gt; select deptno,dname,db_source from person where deptno=#&#123;deptno&#125;; &lt;/select&gt; &lt;select id=&quot;findAll&quot; resultType=&quot;Person &quot;&gt; select deptno,dname,db_source from person; &lt;/select&gt; &lt;insert id=&quot;addDept&quot; parameterType=&quot;Person &quot;&gt; INSERT INTO person(dname,db_source) VALUES(#&#123;dname&#125;,DATABASE()); &lt;/insert&gt;&lt;/mapper&gt; 3.6 新建PersonService接口和接口实现类impl PersonService 接口 public interface PersonService&#123; publicboolean add(Person dept); public Personget(Long id); publicList&lt;Person&gt; list();&#125; PersonServiceImpl 实现类 @Servicepublic class PersonServiceImpl implements PersonService&#123; @Autowired private PersonDao dao; @Override public boolean add(Person person)&#123; return dao.addDept(person); &#125; @Override public Person get(Long id)&#123; return dao.findById(id); &#125; @Override public List&lt;Person&gt; list()&#123; return dao.findAll(); &#125;&#125; 3.7 controller控制层-PersonController代码实现： @RestControllerpublic class PersonController&#123; @Autowired private PersonService service;//全部使用restful风格，返回json字符串 @RequestMapping(value = &quot;/person/add&quot;, method = RequestMethod.POST) public boolean add(@RequestBody Person person) &#123; return service.add(person); &#125; @RequestMapping(value = &quot;/person/get/&#123;id&#125;&quot;, method = RequestMethod.GET) public Person get(@PathVariable(&quot;id&quot;) Long id) &#123; return service.get(id); &#125; @RequestMapping(value = &quot;/person/list&quot;, method = RequestMethod.GET) public List&lt;Person&gt; list() &#123; return service.list(); &#125;&#125; 3.8 springboot启动类@SpringBootApplicationpublic class ApplicationBootStart &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart.class, args); &#125;&#125; 启动服务并测试访问。 3.9 项目整体结构 4 根据父工程，新建微服务消费者-consumermicroservicecloud-consumer-person-80 方法同上 4.1 修改pom文件&lt;dependencies&gt; &lt;dependency&gt;&lt;!-- 自己定义的api --&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;version&gt;2.1.7.RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.2 添加application.yml文件添加端口配置 server: port: 80 4.3 新建@Configuration注解的配置类@Configurationpublic class ConfigBean&#123; @Bean public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125;&#125; 它的作用就是我们整合SSM的时候，对应的applicationContext.xml 4.4 新建消费者控制器，访问服务提供者获取数据通过RestTemplate获取消息提供者暴露的服务信息。 @RestControllerpublic class ConsumerController &#123; @Autowired private RestTemplate restTemplate; private static final String REST_URL_PREFIX = &quot;http://localhost:8001&quot;;//这里通过书写固定的服务提供者的地址，后面我们学习到了Eureka服务注册中心，那么注重修改这里 @RequestMapping(value = &quot;/consumer/person/add&quot;) public boolean add(Person person) &#123; return restTemplate.postForObject(REST_URL_PREFIX + &quot;/person/add&quot;, person, Boolean.class); &#125; @RequestMapping(value = &quot;/consumer/person/get/&#123;id&#125;&quot;) public Person get(@PathVariable(&quot;id&quot;) Long id) &#123; return restTemplate.getForObject(REST_URL_PREFIX + &quot;/person/get/&quot; + id, Person.class); &#125; @SuppressWarnings(&quot;unchecked&quot;) @RequestMapping(value = &quot;/consumer/person/list&quot;) public List&lt;Person&gt; list() &#123; return restTemplate.getForObject(REST_URL_PREFIX + &quot;/person/list&quot;, List.class); &#125;&#125; 4.5 新建启动类@SpringBootApplicationpublic class ApplicationBootStart &#123; public static void main(String[] args)&#123; SpringApplication.run(ApplicationBootStart.class, args); &#125;&#125; 4.6 完整项目结构 5 同时启动服务提供者和服务消费者也就是运行ApplicationBootStart8001和ApplicationBootStart80启动类即可 测试服务是否可以消费，访问消费者 测试成功,以上就是成功的搭建了一个简单的微服务架构，下面我们会逐步的加入springcloud的其他组件，完善这个架构 5.上面的项目存在什么问题存在什么问题？ 在consumer中，我们把url地址硬编码到了代码中，不方便后期维护 consumer需要记忆provider的地址，如果出现变更，可能得不到通知，地址将失效 consumer不清楚provider的状态，服务宕机也不知道 provider只有1台服务，不具备高可用性 即便provider形成集群，consumer还需自己实现负载均衡 其实上面说的问题，概括一下就是分布式服务必然要面临的问题： 服务管理 如何自动注册和发现 如何实现状态监管 如何实现动态路由 服务如何实现负载均衡 服务如何解决容灾问题 服务如何实现统一配置 以上的问题，我们都将在SpringCloud中得到答案。 六.Eureka注册中心**首先我们来解决第一问题，服务的管理 - 利用Eureka解决服务治理问题** 1.Eureka概念 Eureka是Netflix的一个子模块，也是核心模块之一。Eureka是一个基于REST的服务，用于定位服务，以实现云端中间层服务发现和故障转移。**服务注册与发现对于微服务架构来说是非常重要的，有了服务发现与注册，只需要使用服务的标识符，就可以访问到服务(解决上诉案例在consumer中，我们把服务提供者的url地址硬编码到了代码中)**，而不需要修改服务调用的配置文件了。功能类似于dubbo的注册中心，比如Zookeeper。 Netflix在设计Eureka时遵守的就是CAP规则中的AP原则。 CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得 特别提示：同样作为dubbo服务注册中心的zookeeper遵守的是CP原则。 2.Eureka架构和原理Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务注册和发现(请对比Zookeeper)。 Eureka 采用了 C-S 的设计架构。Eureka Server 作为服务注册功能的服务器，它是服务注册中心。 而系统中的其他微服务，使用 Eureka 的客户端（上诉案例中的，服务提供者和服务消费者相对于EurekaServer都是属于客户端，前者是向EurekaServer注册服务，后者是向EurekaServer获取服务）连接到 Eureka Server并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。SpringCloud 的一些其他模块（比如Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关的逻辑。 基本架构： Eureka包含两个组件：Eureka Server和Eureka Client Eureka Server提供服务注册服务各个节点启动后，会在EurekaServer中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观的看到 EurekaClient是一个Java客户端，用于简化Eureka Server的交互，客户端同时也具备一个内置的、使用轮询(round-robin)负载算法的负载均衡器。在应用启动后，将会向Eureka Server发送心跳(默认周期为30秒)。如果Eureka Server在多个心跳周期内没有接收到某个节点的心跳，EurekaServer将会从服务注册表中把这个服务节点移除（默认90秒） Eureka：就是服务注册中心（可以是一个集群），对外暴露自己的地址 提供者：启动后向Eureka注册自己信息（地址，提供什么服务） 消费者：向Eureka订阅服务，Eureka会将对应服务的所有提供者地址列表发送给消费者，并且定期更新 心跳(续约)：提供者定期通过http方式向Eureka刷新自己的状态 3. 实现EurekaServer接下来我们通过加入springcloud的Eureka服务治理组件，改造之前的例子，解决动态路由（上个例子中我们是把服务提供者的url硬编码到消费者中）、注册发现，动态监管的问题 3.1 根据父工程实现单节点Eureka服务注册中心根据父工程microservicecloud 创建 microservicecloud-eureka-7001模块 3.1.1 修改pom文件导入EurekaServer依赖&lt;dependencies&gt; &lt;!--eureka-server服务端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies 3.1.2 修改application.yml配置文件server: port: 7001eureka: instance: hostname: localhost #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ # #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 #等同于 http://localhost:7001/eureka/ 3.1.3 创建启动类并添加@EnableEurekaServer注解package com.kingge.entity;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServer ////EurekaServer服务器端启动类,接受其它微服务注册进来public class ApplicationBootStart7001 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart7001.class, args); &#125;&#125; 3.1.4 完整项目结构 3.1.5 运行启动类，启动EurekaServer访问网址：http://localhost:7001/ No application available 没有服务被发现 —- 因为没有注册服务进来当然不可能有服务被发现 接下来我门把8001模块服务注册进来 3.2 修改 服务提供者也就是修改上面我们实现的：microservicecloud-provider-person-8001 模块，将人员服务注册进EurekaServer中 3.2.1 修改pom文件修改内容： &lt;!-- 将微服务provider端注册进eureka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; 3.2.2 修改application.yml配置文件eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://localhost:7001/eureka #这个地址就是我们在3.1.2定义的EurekaServer对外暴露的连接地址。 3.2.3 修改启动类添加@EnableEurekaClient注解@SpringBootApplication@EnableEurekaClient////本服务启动后会自动注册进 eureka服务中public class ApplicationBootStart8001 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart8001.class, args); &#125;&#125; 3.3 修改服务消费者通过访问服务名称的方式消费服务，解决硬编码服务提供者url的问题 也就是修改上面我们实现的：microservicecloud-consumer-person-80 模块 3.3.1 修改pom文件修改内容： &lt;!-- 将微服务provider端注册进eureka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; 3.3.2 修改application.yml配置文件eureka: client: register-with-eureka: false service-url: defaultZone: http://localhost:7001/eureka/ 3.3.3 修改启动类添加@EnableEurekaClient注解@SpringBootApplication@EnableEurekaClient//public class ApplicationBootStart80 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80.class, args); &#125;&#125; 3.3.4修改ConsumerController代码修改内容如下 // private static final String REST_URL_PREFIX = &quot;http://localhost:8001&quot;; private static final String REST_URL_PREFIX = &quot;http://MICROSERVICECLOUD-PERSON&quot;; 把url更改为服务名称。 3.4 启动EurekaServer和服务提供者，以及服务消费者运行ApplicationBootStart7001和ApplicationBootStart8001、ApplicationBootStart80 查看地址：http://localhost:7001/ 注册成功！！！ 服务名称就是我们在服务提供者的application.yml配置文件中配置的： spring: application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 我们发现生成的实例名称是没有任何意义的，而且实例的介绍地址不是ip的形式 下面就讲解修改这些小细节 4.actuator与注册微服务信息完善4.1 修改服务实例名和实例名访问路径显示ip修改microservicecloud-provider-person-8001 模块的配置文件，添加以下内容 instance: instance-id: microservicecloud-person8001 prefer-ip-address: true #访问路径可以显示IP地址 注意instance属性是eureka的子属性 重启服务提供者ApplicationBootStart8001 查看EurekaServer 这里涉及到了Eureka 的自我保护机制，下一章节我们会讲到。 4.2 修改服务实例的详情页info默认我么你点击服务实例名，跳转到的界面是： 接下来我们要定制一下这界面，显示一下当前服务实例的一些说明信息。 （1）修改microservicecloud-provider-person-8001 模块的pom文件，添加以下内容 &lt;!-- actuator监控信息完善 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改microservicecloud-provider-person-8001 模块的配置文件，添加以下内容 info: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个提供查询部门人员信息的服务 （3）修改父工程的pom文件，添加以下内容 &lt;build&gt; &lt;finalName&gt;microservicecloud&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;delimiters&gt; &lt;delimit&gt;$&lt;/delimit&gt; &lt;/delimiters&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 如果不添加那么就无法解析像这样的动态赋值${spring.application.name} 重启EurekaServer和服务提供者 再次查看服务实例名的info界面 5.Eureka详解5.1.基础架构Eureka架构中的三个核心角色： 服务注册中心 Eureka的服务端应用，提供服务注册和发现功能，就是刚刚我们建立的7001模块。 服务提供者 提供服务的应用，可以是SpringBoot应用，也可以是其它任意技术实现，只要对外提供的是Rest风格服务即可。本例中就是我们实现的8001模块。 服务消费者 消费应用从注册中心获取服务列表，从而得知每个服务方的信息，知道去哪里调用服务方。本例中就是我们实现的80模块。 服务提供和服务消费者相对于服务注册中心，他们都是客户端。所以他们访问EurekaServer导入的依赖是相同的都是spring-cloud-starter-netflix-eureka-client 5.2.服务提供者服务提供者要向EurekaServer注册服务，并且完成服务续约等工作。 服务注册 服务提供者在启动时，会检测配置属性中的：eureka.client.register-with-eureka=true参数是否正确，事实上默认就是true。如果值确实为true，则会向EurekaServer发起一个Rest请求，并携带自己的元数据信息，Eureka Server会把这些信息保存到一个双层Map结构中。 第一层Map的Key就是服务id，一般是配置中的spring.application.name属性 第二层Map的key是服务的实例id。一般host+ serviceId + port，例如：locahost:service-provider:8081 值则是服务的实例对象，也就是说一个服务，可以同时启动多个不同实例，形成集群。 服务续约 在注册服务完成以后，服务提供者会维持一个心跳（定时向EurekaServer发起Rest请求），告诉EurekaServer：“我还活着”。这个我们称为服务的续约（renew）； 有两个重要参数可以修改服务续约的行为： eureka: instance: lease-expiration-duration-in-seconds: 90 lease-renewal-interval-in-seconds: 30 lease-renewal-interval-in-seconds：服务续约(renew)的间隔，默认为30秒 lease-expiration-duration-in-seconds：服务失效时间，默认值90秒 也就是说，默认情况下每个30秒服务会向注册中心发送一次心跳，证明自己还活着。如果超过90秒没有发送心跳，EurekaServer就会认为该服务宕机，会从服务列表中移除，这两个值在生产环境不要修改，默认即可。 但是在开发时，这个值有点太长了，经常我们关掉一个服务，会发现Eureka依然认为服务在活着。所以我们在开发阶段可以适当调小。 eureka: instance: lease-expiration-duration-in-seconds: 10 # 10秒即过期 lease-renewal-interval-in-seconds: 5 # 5秒一次心跳 5.3.服务消费者 获取服务列表 当服务消费者启动时，会检测eureka.client.fetch-registry=true参数的值，如果为true，则会拉取Eureka Server服务的列表只读备份，然后缓存在本地。并且每隔30秒会重新获取并更新数据。我们可以通过下面的参数来修改： eureka: client: registry-fetch-interval-seconds: 5 生产环境中，我们不需要修改这个值。 但是为了开发环境下，能够快速得到服务的最新状态，我们可以将其设置小一点。 5.4.失效剔除和自我保护 服务下线 当服务进行正常关闭操作时，它会触发一个服务下线的REST请求给Eureka Server，告诉服务注册中心：“我要下线了”。服务中心接受到请求之后，将该服务置为下线状态。 失效剔除 有些时候，我们的服务提供方并不一定会正常下线，可能因为内存溢出、网络故障等原因导致服务无法正常工作。Eureka Server需要将这样的服务剔除出服务列表。因此它会开启一个定时任务，每隔60秒对所有失效的服务（超过90秒未响应）进行剔除。 可以通过eureka.server.eviction-interval-timer-in-ms参数对其进行修改，单位是毫秒，生产环境不要修改。 这个会对我们开发带来极大的不变，你对服务重启，隔了60秒Eureka才反应过来。开发阶段可以适当调整，比如：10秒 自我保护 我们关停一个服务，就会在Eureka面板看到一条警告： 这是触发了Eureka的自我保护机制。当一个服务未按时进行心跳续约时，Eureka会统计最近15分钟心跳失败的服务实例的比例是否超过了85%。在生产环境下，因为网络延迟等原因，心跳失败实例的比例很有可能超标，但是此时就把服务剔除列表并不妥当，因为服务可能没有宕机。Eureka就会把当前实例的注册信息保护起来，不予剔除。生产环境下这很有效，保证了大多数服务依然可用。 也就是好死不如赖活着，这个就是用EurekaServer的AP原则，保证可用性 但是这给我们的开发带来了麻烦， 因此开发阶段我们都会关闭自我保护模式：（itcast-eureka） eureka: server: enable-self-preservation: false # 关闭自我保护模式（缺省为打开） eviction-interval-timer-in-ms: 1000 # 扫描失效服务的间隔时间（缺省为60*1000ms） 综上，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留），也不盲目注销任何健康的微服务。使用自我保护模式，可以让Eureka集群更加的健壮、稳定。 一句话：某时刻某一个微服务不可用了，eureka不会立刻清理，依旧会对该微服务的信息进行保存 6.消费者获取服务信息如果我们想要在消费者端获取服务者提供的服务实例列表，那么应该怎么做？对于注册进eureka里面的微服务，可以通过服务发现来获得该服务的信息 既然是消费者端想查看服务端暴露的服务信息，那么就需要在服务提供者实现一个查询暴露服务实例的列表的接口 1.修改服务端PersonController 添加如下代码，查询服务名称为MICROSERVICECLOUD-PERSON的服务实例列表信息 @Autowiredprivate DiscoveryClient client;@RequestMapping(value = &quot;/person/discovery&quot;, method = RequestMethod.GET)public Object discovery()&#123; List&lt;String&gt; list = client.getServices(); System.out.println(&quot;**********&quot; + list); List&lt;ServiceInstance&gt; srvList = client.getInstances(&quot;MICROSERVICECLOUD-PERSON&quot;); for (ServiceInstance element : srvList) &#123; System.out.println(element.getServiceId() + &quot;\\t&quot; + element.getHost() + &quot;\\t&quot; + element.getPort() + &quot;\\t&quot; + element.getUri()); &#125; return this.client;&#125; 2.服务提供者启动类添加注解@EnableDiscoveryClient （后来测试发现其实这一步是多余的） 因为@EnableEurekaClient注解已经包含了@EnableDiscoveryClient 注解 也就是说当服务注册中心是Eureka的时候那么官方已经为了包装了一个注解替代了@EnableDiscoveryClient，但是如果注册中心不是Eureka的话，那么建议使用@EnableDiscoveryClient注解实现服务发现，因为这里注册中心是Eureka那么就是用官方推荐的@EnableEurekaClient 3.修改ConsumerController代码，访问服务提供者提供的接口： // 测试@EnableDiscoveryClient,消费端可以调用服务发现@RequestMapping(value = \"/consumer/person/discovery\")public Object discovery()&#123; return restTemplate.getForObject(REST_URL_PREFIX + \"/person/discovery\", Object.class);&#125; 4.启动服务提供者和服务消费者 消费者访问接口 7.EurekaServer集群单个的EurekaServer很明显是不符合HA，高可用原则，所以下面再加两台EurekaServer-8002和8003构成EurekaServer集群 基本原理 上图是来自eureka的官方架构图，这是基于集群配置的eureka； - 处于不同节点的eureka通过Replicate进行数据同步 - Application Service为服务提供者 - Application Client为服务消费者 - Make Remote Call完成一次服务调用 服务启动后向Eureka注册，Eureka Server会将注册信息向其他Eureka Server进行同步，当服务消费者要调用服务提供者，则向服务注册中心获取服务提供者地址，然后会将服务提供者地址缓存在本地，下次再调用时，则直接从本地缓存中取，完成一次调用。 当服务注册中心Eureka Server检测到服务提供者因为宕机、网络原因不可用时，则在服务注册中心将服务置为DOWN状态，并把当前服务提供者状态向订阅者发布，订阅过的服务消费者更新本地缓存。 服务提供者在启动后，周期性（默认30秒）向Eureka Server发送心跳，以证明当前服务是可用状态。Eureka Server在一定的时间（默认90秒）未收到客户端的心跳，则认为服务宕机，注销该实例。 7.1 根据microservicecloud-eureka-7001创建两个相同的工程分别是microservicecloud-eureka-7002和microservicecloud-eureka-7003 按照7001为模板粘贴POM 修改7002和7003的主启动类 完整工程如下 7.2 修改映射配置-实现唯一的eureka服务端的实例名称为了模拟EurekaServer集群，不同的EurekaServer在不同的机器，而且拥有不同的实例名称 找到C:\\Windows\\System32\\drivers\\etc路径下的hosts文件 修改映射配置添加进hosts文件 127.0.0.1 peer1 127.0.0.1 peer2 127.0.0.1 peer3 7.3 修改7001-7003三台EurekaServer的配置文件7001 修改内容如下： server: port: 7001eureka: instance: hostname: peer1 #peer1 #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://peer2:7002/eureka/,http://peer3:7003/eureka/ #注册7002和7003 自己不用声明 7002 修改内容如下： server: port: 7002eureka: instance: hostname: peer2 #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://peer1:7001/eureka/,http://peer3:7003/eureka/ #注册7001和7003 自己不用声明 #等同于http://localhost:7001/eureka/ 7003修改内容如下 server: port: 7003eureka: instance: hostname: peer3 #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/ #注册7001和7002 自己不用声明 #等同于http://localhost:7001/eureka/ 需要注意的是service-url:defaultZone的值，都是包含其他EurekaServer的值，不用书写自己的。 7.4 修改服务提供者的配置文件也就是修改microservicecloud-provider-person-8001模块修改内容如下 server: port: 8001mybatis: config-location: classpath:mybatis/mybatis.cfg.xml # mybatis配置文件所在路径 type-aliases-package: com.kingge.entity # 所有Entity别名类所在包 mapper-locations: - classpath:mybatis/mapper/**/*.xml # mapper映射文件spring: application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://127.0.0.1:3306/test # 数据库名称 username: root password: 123 dbcp2: min-idle: 5 # 数据库连接池的最小维持连接数 initial-size: 5 # 初始化连接数 max-total: 5 # 最大连接数 max-wait-millis: 200 # 等待连接获取的最大超时时间#eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/# http://localhost:7001/eureka #单机版本使用# defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ instance: instance-id: microservicecloud-person8001 #自定义服务实例名 prefer-ip-address: true #访问路径可以显示IP地址#info: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个提供查询部门人员信息的服务 实际上就是修改了service-url:defaultZone的值，修改为了EurekaServer集群的地址，其他配置没有改变 7.5重启7001-7003服务器也就是分别运行ApplicationBootStart7001、ApplicationBootStart7002、ApplicationBootStart7003 访问查看 部署成功！！！！！！！ 8.Eureka和Zookeeper-服务注册中心比较著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性P在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。 那么分布式系统，必然要求分区容错性，也就是P原则，那么Zookeeper选择了C，Eureka选择了A 因此Zookeeper保证的是CP,Eureka则是AP。 8.1 Zookeeper保证CP当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。 8.2 Eureka保证APEureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的 ，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用) 当网络稳定时，当前实例新的注册信息会被同步到其它节点中 因此， Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。 那么既然保证了保证了可用性，那么数据的一致性肯定是不能够保证了，所以这个就是自我保护的机制。所以到底是AP还是CP，又或者是AC（数据库），要看业务场景来定。 七.Ribbon负载均衡接下来解决第二个问题，那就是假设在多个服务提供者提供服务的情况下，怎么做到负载均衡，解决需要把服务提供者url硬编码到消费者端的问题。 7.1 Ribbon概念Spring Cloud Ribbon是基于Netflix Ribbon实现的一套客户端 负载均衡的工具 。 简单的说，Ribbon是Netflix发布的开源项目，主要功能是提供**客户端**的**软件负载**均衡算法，将Netflix的中间层服务连接在一起。Ribbon客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出Load Balancer（简称LB）后面所有的机器，Ribbon会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器。我们也很容易使用Ribbon**实现自定义的负载均衡算法**。 7.2 什么叫LB（负载均衡） LB，即负载均衡(Load Balance)，在微服务或分布式集群中经常用的一种应用。 负载均衡简单的说就是将用户的请求平摊的分配到多个服务上，从而达到系统的HA。 常见的负载均衡有软件Nginx，LVS，硬件 F5等。 相应的在中间件，例如：dubbo和SpringCloud中均给我们提供了负载均衡，SpringCloud的负载均衡算法可以自定义。 两种负载均衡： 集中式LB：偏硬件，服务的消费方和提供方之间使用独立的LB设施，由该设施负责把访问请求以某种策略转发至服务的提供方。 进程内LB：偏软件， 将LB逻辑集成到消费方，消费方从服务注册中心指导哪些地址可用，再自己选择一个合适的服务器。 Ribbon就属于进程内LB，它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。 7.3 Ribbon负载均衡实现因为在上面的例子中只存在一个8001模块在提供服务，那么为了能够演示负载均衡的例子，这里需要再增加两个服务提供者8002和8003 7.3.1 根据8001模块复制新建两份分别命名为8002和8003请看完整项目结构图 8002模块 8003模块 7.3.2 新建数据库test2、test3，让各自微服务分别连各自的数据库我们知道一个微服务可能是一套完整的系统，那么也就意味着他可能拥有自己的数据库。而且为了方便测试负载均衡，我们让8001-8003这三个服务提供者各自连接自己的数据库，也方便验证负载均衡是否实现。 给test2数据库导入数据： DROP TABLE IF EXISTS `person`;CREATE TABLE `person` ( `deptno` int(255) NOT NULL AUTO_INCREMENT, `db_source` varchar(255) DEFAULT NULL, `dname` varchar(255) DEFAULT NULL, PRIMARY KEY (`deptno`)) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8;-- ------------------------------ Records of person-- ----------------------------INSERT INTO `person` VALUES (&apos;5&apos;, &apos;test2&apos;, &apos;开发部&apos;);INSERT INTO `person` VALUES (&apos;6&apos;, &apos;test2&apos;, &apos;人事部&apos;);INSERT INTO `person` VALUES (&apos;7&apos;, &apos;test2&apos;, &apos;集成部&apos;);INSERT INTO `person` VALUES (&apos;8&apos;, &apos;test2&apos;, &apos;市场部&apos;);INSERT INTO `person` VALUES (&apos;9&apos;, &apos;test2&apos;, &apos;hr&apos;); db_source字段标识，数据来源那个数据库 给test3数据库导入数据： DROP TABLE IF EXISTS `person`;CREATE TABLE `person` ( `deptno` int(255) NOT NULL AUTO_INCREMENT, `db_source` varchar(255) DEFAULT NULL, `dname` varchar(255) DEFAULT NULL, PRIMARY KEY (`deptno`)) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8;-- ------------------------------ Records of person-- ----------------------------INSERT INTO `person` VALUES (&apos;5&apos;, &apos;test3&apos;, &apos;开发部&apos;);INSERT INTO `person` VALUES (&apos;6&apos;, &apos;test3&apos;, &apos;人事部&apos;);INSERT INTO `person` VALUES (&apos;7&apos;, &apos;test3&apos;, &apos;集成部&apos;);INSERT INTO `person` VALUES (&apos;8&apos;, &apos;test3&apos;, &apos;市场部&apos;);INSERT INTO `person` VALUES (&apos;9&apos;, &apos;test3&apos;, &apos;hr&apos;); 7.3.3 修改8002/8003各自application.yml配置文件实际上只需要修改三个地方：服务暴露的端口号，服务连接的数据库，服务的实例名 注意：服务名称不能够修改 因为这三个服务都是提供同样的业务，那么就不会归属到一个服务组下，也就是说我们想要的是：microservicecloud-person 这个服务名称（服务组）下面有三个服务实例（8001-8003）提供服务，这样负载均衡才能够演示 8002模块修改如下 8003模块修改如下 7.3.4 启动EurekaServer集群和8001-8003服务模块 查看EurekaServer http://peer1:7001/ http://peer2:7002/ http://peer3:7003/ 服务提供者集群创建成功 7.3.5 自测启动的服务是否可用 7.3.6 修改服务消费者-采取负载均衡方式访问服务也就是修改microservicecloud-consumer-person-80模块 （1）修改pom文件添加Robbin依赖 &lt;!-- Ribbon相关 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改application.yml文件 server: port: 80 #eureka: client: register-with-eureka: false service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ 也就是修改服务注册中心地址为集群地址 （3）修改ConfigBean配置类添加@LoadBalanced注解，获取rest服务的时候添加ribbon @Configurationpublic class ConfigBean&#123; @Bean @LoadBalanced public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125;&#125; （4）主启动类ApplicationBootStart80添加@EnableEurekaClient @SpringBootApplication@EnableEurekaClient//这里建议使用@EnableDiscoveryClient 替换@EnableEurekaClientpublic class ApplicationBootStart80 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80.class, args); &#125;&#125; 7.3.7 修改服务消费者修改ConsumerController代码 修改内容如下 // private static final String REST_URL_PREFIX = &quot;http://localhost:8001&quot;; private static final String REST_URL_PREFIX = &quot;http://MICROSERVICECLOUD-PERSON&quot;; 把url更改为服务名称 Ribbon和Eureka整合后Consumer可以直接调用服务而不用再关心地址和端口号 7.3.8 启动7001-7003,8001-8003，80 启动完成后测试： 第一次访问 第二次访问： 第三次访问： 第四次访问： 我们注意db_source的值是变换的，说明负载均衡成功。但是当我们访问一轮后，发现他又从头开始： test2-&gt;test3-&gt;test-&gt;test2-&gt;test3-&gt;test 说明Ribbon默认采用的是轮询的负载均衡策略 7.3.9 总结 Ribbon在工作时分成两步 第一步先选择 EurekaServer ,它优先选择在同一个区域内负载较少的server. 第二步再根据用户指定的策略，在从server取到的服务注册列表中选择一个地址。 其中Ribbon提供了多种策略：比如轮询、随机和根据响应时间加权。 Ribbon其实就是一个软负载均衡的客户端组件， 他可以和其他所需请求的客户端结合使用，和eureka结合只是其中的一个实例。 7.4 Ribbon负载均衡实现核心接口IRule7.4.1.源码跟踪为什么我们只输入了service名称就可以访问了呢？之前还要获取ip和端口。 显然有人帮我们根据service名称，获取到了服务实例的ip和端口。它就是LoadBalancerInterceptor 在consumer的ConsumerController如下代码打断点： 一路源码跟踪：RestTemplate.getForObject –&gt; RestTemplate.execute –&gt; RestTemplate.doExecute： 点击进入AbstractClientHttpRequest.execute –&gt; AbstractBufferingClientHttpRequest.executeInternal –&gt; InterceptingClientHttpRequest.executeInternal –&gt; InterceptingClientHttpRequest.execute: 继续跟入：LoadBalancerInterceptor.intercept方法 获取请求的服务名称，我们发现执行this.loadBalancer.execute()方法的loadBalancer是一个接口LoadBalancerClient，那么很明显执行execute（）方法的只能是LoadBalancerClient的实现类。 继续跟入execute方法发现执行该方法的类是：RibbonLoadBalancerClient负载均衡客户端类： 发现获取了8003端口的服务 我们查看一下获取的负载均衡器的信息： 7.4.2.负载均衡策略Ribbon默认的负载均衡策略是简单的轮询，我们可以测试一下： 编写测试类，在刚才的源码中我们看到拦截中是使用RibbonLoadBalanceClient来进行负载均衡的，其中有一个choose方法，找到choose方法的接口方法，是这样介绍的： 现在这个就是负载均衡获取实例的方法。 我们注入这个类的对象，然后对其测试： 测试内容： @RunWith(SpringRunner.class)@SpringBootTest(classes = ApplicationBootStart80.class)public class LoadBalanceTest &#123; @Autowired private RibbonLoadBalancerClient client; @Test public void testLoadBalance()&#123; for (int i = 0; i &lt; 100; i++) &#123; ServiceInstance instance = this.client.choose(\"MICROSERVICECLOUD-PERSON\"); System.out.println(instance.getHost() + \":\" +instance.getPort()); &#125; &#125;&#125; 结果： 符合了我们的预期推测，确实是轮询方式。 我们是否可以修改负载均衡的策略呢？ 继续跟踪源码，发现这么一段代码： 我们看看这个rule是谁： 这里的rule默认值是一个RoundRobinRule，看类的介绍： 这不就是轮询的意思嘛。 我们注意到，这个类其实是实现了接口IRule的，查看一下： 定义负载均衡的规则接口。 它有以下实现： 七大方法IRule是一个接口，七大方法是其实现类 RoundRobinRule：轮询（默认方法） RandomRule：随机 AvailabilityFilteringRule：先过滤掉由于多次访问故障而处于断路器跳闸状态的服务，还有并发的连接数量超过阈值的服务，然后对剩余的服务进行轮询 WeightedResponseTimeRule：根据平均响应时间计算服务的权重。统计信息不足时会按照轮询，统计信息足够会按照响应的时间选择服务 RetryRule：正常时按照轮询选择服务，若过程中有服务出现故障，在轮询一定次数后依然故障，则会跳过故障的服务继续轮询。 BestAvailableRule：先过滤掉由于多次访问故障而处于断路器跳闸状态的服务，然后选择一个并发量最小的服务 ZoneAvoidanceRule：默认规则，符合判断server所在的区域的性能和server的可用性选择服务 7.4.3 负载均衡自定义1.修改某个服务的负载均衡策略例如我们只想修改 MICROSERVICECLOUD-PERSON服务的负载均衡策略 这里一共有两种方法实现一种是使用yml配置的方式声明-一种是使用注解的方式声明 第一种使用配置方式（建议使用）（1）修改消费者端（microservicecloud-consumer-person-80）的application.yml配置文件 修改内容如下： MICROSERVICECLOUD-PERSON: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 格式是：{服务名称}.ribbon.NFLoadBalancerRuleClassName，值就是IRule的实现类。 （2）运行上面的LoadBalanceTest 第二种使用配置方式前提：注释掉第一种方式实现的 配置信息（不注释掉也可以，因为第一种方式跟第二种方式同时存在时，以第二种方式为主） （1）修改消费者端启动类 @SpringBootApplication#@EnableEurekaClient@EnableDiscoveryClient //服务发现@RibbonClient(name = &quot;MICROSERVICECLOUD-PERSON&quot;,configuration = OwnRule.class)public class ApplicationBootStart80 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80.class, args); &#125;&#125; 添加一下注解 @RibbonClient(name = &quot;MICROSERVICECLOUD-PERSON&quot;,configuration = OwnRule.class) （2）新建OwnRule自定义侧略类 首先新建包com.myrule @Configurationpublic class OwnRule &#123; @Bean public IRule getIuIRule()&#123; System.out.println(&quot;进入了自定义负载均衡策略&quot;); return new RandomRule(); &#125;&#125; 注意： 官方文档明确给出了警告： 这个自定义配置类不能放在@ComponentScan所扫描的当前包下以及子包下， 否则我们自定义的这个配置类就会被所有的Ribbon客户端所共享，也就是说 我们达不到特殊化定制的目的了。 所以上面的OwnRule类是不在启动类同级包或者子包下的。 所以我们也可以利用这个特性，修改全局的所有服务的获取策略为某个策略 （3）运行上面的LoadBalanceTest 2. 修改全局的服务访问策略（替换默认的轮询策略）很简单，直接在IOC容器注入想要替换成的负载均衡策略即可 @Configurationpublic class ConfigBeanapplicationContext.xml&#123; @Bean @LoadBalanced public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125; @Bean public IRule getIuIRule()&#123; return new RandomRule(); &#125;&#125; 3.自定义负载均衡策略上上面的两个章节实际上使用的都是Ribbon提供的负载均衡策略，所以接下来我们要实现一个负载均衡策略 提出需求： 问题：修改MICROSERVICECLOUD-PERSON服务的负载均衡策略：依旧使用轮询策略，但是加上新需求，每个服务器（现在有三台8001-8003）要求被调用5次。也即 以前是每台机器一次，现在是每台机器5次 在实现之前拜读一下官方的RandomRule 源代码，然后再修改出符合我们需求的策略类 https://github.com/Netflix/ribbon/blob/master/ribbon-loadbalancer/src/main/java/com/netflix/loadbalancer/RandomRule.java public class RandomRule extends AbstractLoadBalancerRule &#123; /** * Randomly choose from all living servers */ @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = &quot;RCN_REDUNDANT_NULLCHECK_OF_NULL_VALUE&quot;) public Server choose(ILoadBalancer lb, Object key) &#123; if (lb == null) &#123; return null; &#125; Server server = null;//需要返回的服务 while (server == null) &#123;//使用while循环知道获取服务 if (Thread.interrupted()) &#123;//如果当前线程已经中断，那么直接返回null return null; &#125; List&lt;Server&gt; upList = lb.getReachableServers();//获取所有可达的服务列表 List&lt;Server&gt; allList = lb.getAllServers();//获取所有服务列表 int serverCount = allList.size();//得到服务列表里服务实例的数量 if (serverCount == 0) &#123; /* * No servers. End regardless of pass, because subsequent passes * only get more restrictive. */ return null; &#125; int index = chooseRandomInt(serverCount);//根据服务数量所及获取服务下标 等同于Random.rand.nextInt(serverCount); server = upList.get(index);//根据随机获取到的下标，从可用服务列表实例中获取服务 if (server == null) &#123;//获取不到时，暂停当前正在执行的线程对象(及放弃当前拥有的cup资源),并执行其他线程 //然后继续while循环获取 /* * The only time this should happen is if the server list were * somehow trimmed. This is a transient condition. Retry after * yielding. */ Thread.yield(); continue; &#125; if (server.isAlive()) &#123; return (server); &#125; // Shouldn&apos;t actually happen.. but must be transient or a bug. server = null; Thread.yield(); &#125; return server; &#125; protected int chooseRandomInt(int serverCount) &#123; return ThreadLocalRandom.current().nextInt(serverCount); &#125; @Override public Server choose(Object key) &#123; return choose(getLoadBalancer(), key); &#125;&#125; 代码其实很简单 （1）新建RandomRuleModify类package com.myrule;import java.util.List; import com.netflix.client.config.IClientConfig; import com.netflix.loadbalancer.AbstractLoadBalancerRule; import com.netflix.loadbalancer.ILoadBalancer; import com.netflix.loadbalancer.Server;public class RandomRuleModify extends AbstractLoadBalancerRule&#123; // total = 0 // 当total==5以后，我们指针才能往下走， // index = 0 // 当前对外提供服务的服务器地址， // total需要重新置为零，但是已经达到过一个5次，我们的index = 1 // 分析：我们5次，但是微服务只有8001 8002 8003 三台，OK？ // private int total = 0; // 总共被调用的次数，目前要求每台被调用5次 private int currentIndex = 0; // 当前提供服务的机器号 public Server choose(ILoadBalancer lb, Object key) &#123; if (lb == null) &#123; return null; &#125; Server server = null; while (server == null) &#123; if (Thread.interrupted()) &#123; return null; &#125; List&lt;Server&gt; upList = lb.getReachableServers(); List&lt;Server&gt; allList = lb.getAllServers(); int serverCount = allList.size(); if (serverCount == 0) &#123; return null; &#125; if(total &lt; 5) &#123; server = upList.get(currentIndex); total++; &#125;else &#123; total = 0; currentIndex++; if(currentIndex &gt;= upList.size()) &#123; currentIndex = 0; &#125; &#125; if (server == null) &#123; Thread.yield(); continue; &#125; if (server.isAlive()) &#123; return (server); &#125; // Shouldn&apos;t actually happen.. but must be transient or a bug. server = null; Thread.yield(); &#125; return server; &#125; @Override public Server choose(Object key) &#123; return choose(getLoadBalancer(), key); &#125; @Override public void initWithNiwsConfig(IClientConfig clientConfig) &#123; // TODO Auto-generated method stub &#125;&#125; 这个类不能够放在启动类的同级或者子包下，否则将被设置为全局的负载均衡策略，起不到为MICROSERVICECLOUD-PERSON服务定制策略的作用 （2）OwnRule返回我们实现的RandomRuleModify@Configurationpublic class OwnRule &#123; @Bean public IRule getIuIRule()&#123; System.out.println(&quot;进入了自定义负载均衡策略&quot;); return new RandomRuleModify();//RandomRule(); &#125;&#125; （3）修改启动类@SpringBootApplication//@EnableEurekaClient@EnableDiscoveryClient //服务发现@RibbonClient(name = &quot;MICROSERVICECLOUD-PERSON&quot;,configuration = OwnRule.class)public class ApplicationBootStart80 &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80.class, args); &#125;&#125; （4）运行测试类LoadBalanceTest 轮询的同时每个服务器调用五次！！！ （5）总结我们也可以使用配置类的方式为某个服务配置特定的负载均衡策略实现类（参考7.4.3.1 章节的第一种配置方式），这样就可以省略上面的第二和第三步骤 通过配置的方式更加，灵活 然后运行测试类LoadBalanceTest，结果一模一样。 八. Feign负载均衡8.1.简介Feign出现的原因是什么，既然他也是提供负载均衡的功能，那么他跟Ribbon有什么区别？ 项目主页： https://github.com/OpenFeign/feign 官网解释： http://projects.spring.io/spring-cloud/spring-cloud.html#spring-cloud-feign Feign是一个声明式WebService客户端。使用Feign能让编写Web Service客户端更加简单, 它的使用方法是定义一个接口，然后在上面添加注解，同时也支持JAX-RS标准的注解。Feign也支持可拔插式的编码器和解码器。Spring Cloud对Feign进行了封装，使其支持了Spring MVC标准注解和HttpMessageConverters。Feign可以与Eureka和Ribbon组合使用以支持负载均衡。 Feign的出现的目的是：为了迎合我们平时面向接口编程和调用的习惯。例如我们在Controller通过注入Service层的接口调用相关的业务。但是在上面的Ribbon例子中我们是通过RestTemplate和URL的方式调用某个服务： Feign的实现其实也很简单：只需要创建一个接口，然后在上面添加注解即可。 有道词典的英文解释： 为什么叫伪装？ Feign可以把Rest的请求进行隐藏，伪装成类似SpringMVC的Controller一样。你不用再自己拼接url，拼接参数等等操作，一切都交给Feign去做。 总的来说：Feign通过封装Ribbon实现了我们常用的面向接口编程 8.2 实现Feign其实就是复制microservicecloud-consumer-person-80工程代码，在做一些修改。 （1）根据父工程新建microservicecloud-consumer-person-80-feign模块（2）复制80模块代码到80-feign模块 修改启动类的名字。 （3）修改80-feign模块的pom文件，添加Feign依赖添加如下内容： &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; （4）修改api公共模块也就是修改microservicecloud-api，因为我们知道我们抽象出来的服务，有可能其他模块也会调用，不仅仅是80-feign模块。所以公共的东西我们都放在api模块 1.修改pom文件，添加feign依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; 2.新建PersonClientService接口并新增注解@FeignClient @FeignClient(value = &quot;MICROSERVICECLOUD-PERSON&quot;)public interface PersonClientService&#123; @RequestMapping(value = &quot;/person/get/&#123;id&#125;&quot;, method = RequestMethod.GET) public Person get(@PathVariable(&quot;id&quot;) long id); @RequestMapping(value = &quot;/person/list&quot;, method = RequestMethod.GET) public List&lt;Person&gt; list(); @RequestMapping(value = &quot;/person/add&quot;, method = RequestMethod.POST) public boolean add(Person person);&#125; • 首先这是一个接口，Feign会通过动态代理，帮我们生成实现类。这点跟mybatis的mapper很像 • @FeignClient，声明这是一个Feign客户端，类似@Mapper注解。同时通过value属性指定服务名称 • 接口中的定义方法，完全采用SpringMVC的注解，Feign会根据注解帮我们生成URL，并访问获取结果 （5）feign工程修改Controller，添加上一步新建的PersonClientService接口@RestControllerpublic class ConsumerController &#123;// @Autowired// private RestTemplate restTemplate;// private static final String REST_URL_PREFIX = &quot;http://localhost:8001&quot;;// private static final String REST_URL_PREFIX = &quot;http://MICROSERVICECLOUD-PERSON&quot;;@Autowiredprivate PersonClientService personClientService; @RequestMapping(value = &quot;/consumer/person/add&quot;) public boolean add(Person person) &#123;// return restTemplate.postForObject(REST_URL_PREFIX + &quot;/person/add&quot;, person, Boolean.class); return personClientService.add(person); &#125; @RequestMapping(value = &quot;/consumer/person/list&quot;) public List&lt;Person&gt; list() &#123;// return restTemplate.getForObject(REST_URL_PREFIX + &quot;/person/list&quot;, List.class); return personClientService.list(); &#125; @RequestMapping(value = &quot;/consumer/person/get/&#123;id&#125;&quot;) public Person get(@PathVariable(&quot;id&quot;) Long id) &#123; return this.personClientService.get(id); &#125;&#125; 可以看到我们注释掉了以往的RestTemplate+URL请求服务的方式，通过注入接口调用的方式，实现了面向借口编程。 （6）feign工程修改主启动类添加@EnableFeignClients // 开启feign客户端 @SpringBootApplication@EnableDiscoveryClient //服务发现@EnableFeignClients // 开启feign客户端public class ApplicationBootStart80feign &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80feign.class, args); &#125;&#125; （7）启动EurekaServer7001-7003和服务提供集群8001-8003，启动feign工程进行测试 请求 默认使用轮询的负载均衡方式 8.3 总结 Feign通过接口的方法调用Rest服务（之前是Ribbon+RestTemplate） ， 该请求发送给Eureka服务器（http://MICROSERVICECLOUD-PERSON/person/list）, 通过Feign直接找到服务接口，由于在进行服务调用的时候融合了Ribbon技术，所以也支持负载均衡作用。 也就是说Feign只是封装了Ribbon，改造成了我们习惯的面向接口编程的方式。 自定义负载均衡的方式跟之前使用Ribbon 的一样，也就是支持注解和配置两种方式实现某个服务或者全局服务的负载均衡策略自定义 例如： @SpringBootApplication@EnableDiscoveryClient //服务发现@EnableFeignClients // 开启feign客户端@RibbonClient(name = &quot;MICROSERVICECLOUD-PERSON&quot;,configuration = OwnRule.class)public class ApplicationBootStart80feign &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart80feign.class, args); &#125;&#125; 8.4.请求压缩(了解)Spring Cloud Feign 支持对请求和响应进行GZIP压缩，以减少通信过程中的性能损耗。通过下面的参数即可开启请求与响应的压缩功能： feign: compression: request: enabled: true # 开启请求压缩 response: enabled: true # 开启响应压缩 同时，我们也可以对请求的数据类型，以及触发压缩的大小下限进行设置： feign: compression: request: enabled: true # 开启请求压缩 mime-types: text/html,application/xml,application/json # 设置压缩的数据类型 min-request-size: 2048 # 设置触发压缩的大小下限 注：上面的数据类型、压缩大小下限均为默认值。 8.5.日志级别(了解)前面讲过，通过logging.level.xx=debug来设置日志级别。然而这个对Fegin客户端而言不会产生效果。因为@FeignClient注解修改的客户端在被代理时，都会创建一个新的Fegin.Logger实例。我们需要额外指定这个日志的级别才可以。 1）设置com.kingge包下的日志级别都为debug logging: level: com.kingge: debug 2）新建FeignLogConfiguration配置类，定义日志级别 内容： @Configurationpublic class FeignLogConfiguration &#123; @Bean Logger.Level feignLoggerLevel()&#123; return Logger.Level.FULL; &#125;&#125; 这里指定的Level级别是FULL，Feign支持4种级别： • NONE：不记录任何日志信息，这是默认值。 • BASIC：仅记录请求的方法，URL以及响应状态码和执行时间 • HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息 • FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。 3）在FeignClient中指定配置类： @FeignClient(value = “MICROSERVICECLOUD-PERSON” configuration = FeignLogConfiguration.class) public interface UserFeignClient { @GetMapping(“/user/{id}”) User queryUserById(@PathVariable(“id”) Long id); } 4）重启项目，即可看到每次访问的日志： 8.6.Hystrix支持参加下面服务降级的案例 九.Hystrix断路器**首先我们来解决第三问题，服务的容灾处理** 跟Ribbon和Feign是客户端技术不同的是Hystrix是服务端的技术，也就是他是作用在服务提供端 1.分布式系统面临的问题复杂分布式体系结构中的应用程序有数十个依赖关系，每个依赖关系在某些时候将不可避免地失败 1.雪崩问题 微服务中，服务间调用关系错综复杂，一个请求，可能需要调用多个微服务接口才能实现，会形成非常复杂的调用链路： 如图，一次业务请求，需要调用A、P、H、I四个服务，这四个服务又可能调用其它服务（这个就是所谓的扇出）。 如果此时，某个服务出现异常： 例如微服务I发生异常，请求阻塞，用户不会得到响应，则tomcat的这个线程不会释放，于是越来越多的用户请求到来，越来越多的线程会阻塞： 服务器支持的线程和并发数有限，请求一直阻塞，会导致服务器资源耗尽，从而导致所有其它服务都不可用，形成雪崩效应，所以我们需要阻断故障的传播，这个就是断路器。 这就好比，一个汽车生产线，生产不同的汽车，需要使用不同的零件，如果某个零件因为种种原因无法使用，那么就会造成整台车无法装配，陷入等待零件的状态，直到零件到位，才能继续组装。 此时如果有很多个车型都需要这个零件，那么整个工厂都将陷入等待的状态，导致所有生产都陷入瘫痪。一个零件的波及范围不断扩大。 备注：一般情况对于服务依赖的保护主要有3中解决方案： （1）熔断模式：这种模式主要是参考电路熔断，如果一条线路电压过高，保险丝会熔断，防止火灾。放到我们的系统中，如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。 （2）（降级）隔离模式：这种模式就像对系统请求按类型划分成一个个小岛的一样，当某个小岛被火少光了，不会影响到其他的小岛。例如可以对不同类型的请求使用线程池来资源隔离，每种类型的请求互不影响，如果一种类型的请求线程资源耗尽，则对后续的该类型请求直接返回，不再调用后续资源。这种模式使用场景非常多，例如将一个服务拆开，对于重要的服务使用单独服务器来部署，再或者公司最近推广的多中心。 （3）限流模式：上述的熔断模式和隔离模式都属于出错后的容错处理机制，而限流模式则可以称为预防模式。限流模式主要是提前对各个类型的请求设置最高的QPS阈值，若高于设置的阈值则对该请求直接返回，不再调用后续资源。这种模式不能解决服务依赖的问题，只能解决系统整体资源分配问题，因为没有被限流的请求依然有可能造成雪崩效应。 springcloud的Hystrix就是提供了前两种解决方式： Hystix解决雪崩问题的手段有两个： • 线程隔离（降级） • 服务熔断 2.Hystrix简介Hystrix是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统里，许多依赖不可避免的会调用失败，比如超时、异常等，Hystrix能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性。 Hystix是Netflix开源的一个延迟和容错库，用于隔离访问远程服务、第三方库，防止出现级联失败。 “断路器”本身是一种开关装置，当某个服务单元发生故障之后，**通过断路器的故障监控（类似熔断保险丝），向调用方返回一个符合预期的、可处理的备选响应（FallBack）（解决思路）** ，**而不是长时间的等待或者抛出调用方无法处理的异常** ，这样就保证了服务调用方的线程不会被长时间、不必要地占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。 官网资料 https://github.com/Netflix/Hystrix/wiki/How-To-Use 3.服务端的服务熔断熔断机制是应对雪崩效应的一种微服务链路保护机制。他是在服务端实现。 **当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回&quot;错误&quot;的响应信息**。 当检测到该节点微服务调用响应正常后恢复调用链路。在SpringCloud框架里熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败就会启动熔断机制。**熔断机制的注解是@HystrixCommand。** 3.1 熔断原理熔断器，也叫断路器，其英文单词为：Circuit Breaker 熔断状态机3个状态： • Closed：关闭状态，所有请求都正常访问。 • Open：打开状态，所有请求都会被降级。Hystix会对请求情况计数，当一定时间内失败请求百分比达到阈值，则触发熔断，断路器会完全打开。默认失败比例的阈值是50%，请求次数最少不低于20次。 • Half Open：半开状态，open状态不是永久的，打开后会进入休眠时间（默认是5S）。随后断路器会自动进入半开状态。此时会释放部分请求通过，若这些请求都是健康的，则会完全关闭断路器，否则继续保持打开，再次进行休眠计时 当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回&quot;错误&quot;的响应信息。 当检测到该节点微服务调用响应正常后恢复调用链路。在SpringCloud框架里熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败就会启动熔断机制。熔断机制的注解是@HystrixCommand。 3.2 案例演示3.2.1 参考microservicecloud-provider-person-8001 新建microservicecloud-provider-person-hystrix-8001赋值pom文件和代码已经application.yml文件 3.2.2 修改新建hystrix-8001模块的pom文件和启动类（1）pom文件添加hystrix依赖 &lt;!-- hystrix --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改启动类名称并添加@EnableCircuitBreaker注解//对hystrixR熔断机制的支持 @SpringBootApplication//@EnableEurekaClient@EnableDiscoveryClient //服务发现@EnableCircuitBreaker//对hystrixR熔断机制的支持public class ApplicationBootStart8001hystrix &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart8001hystrix.class, args); &#125;&#125; 3.2.3 修改配置文件其实就是修改服务实例名字，避免跟8001模块冲突 3.2.4 修改PersonController，添加熔断处理在某个方法使用@HystrixCommand注解，模拟报异常后如何处理 一旦调用服务方法失败并抛出了错误信息/请求超时后，会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法 下面以get方法为例，代码内容（下面模拟的是出现异常） @RestControllerpublic class PersonController&#123; @Autowired private PersonService service;//全部使用restful风格，返回json字符串 @RequestMapping(value = &quot;/person/add&quot;, method = RequestMethod.POST) public boolean add(@RequestBody Person person) &#123; return service.add(person); &#125; @RequestMapping(value = &quot;/person/get/&#123;id&#125;&quot;, method = RequestMethod.GET) @HystrixCommand(fallbackMethod = &quot;processHystrix_Get&quot;) //关键代码 public Person get(@PathVariable(&quot;id&quot;) Long id) &#123; Person person = service.get(id); if(null == person) &#123; throw new RuntimeException(&quot;该ID：&quot;+id+&quot;没有没有对应的信息&quot;); &#125; return person; &#125; public Person processHystrix_Get(@PathVariable(&quot;id&quot;) Long id)//关键代码 &#123; Person person = new Person(); person.setDeptno(id);person.setDname(&quot;该ID：&quot;+id+&quot;没有没有对应的信息,null--@HystrixCommand&quot;); person.setDb_source(&quot;no this database in MySQL&quot;); return person; &#125; @RequestMapping(value = &quot;/person/list&quot;, method = RequestMethod.GET) public List&lt;Person&gt; list() &#123; return service.list(); &#125;&#125; 注意，配置的fallbackMethod方法必须与被@HystrixCommand注解的方法有相同的入参和返回值 3.2.5 启动EurekaServer集群和8001-hystrix模块、80模块 消费者访问 http://localhost/consumer/person/get/5 输出：{“deptno”:5,”dname”:”开发部”,”db_source”:”test”} 我们假设获取某个不存在的用户 http://localhost/consumer/person/get/55 输出：{“deptno”:55,”dname”:”该ID：55没有没有对应的信息,null–@HystrixCommand”,”db_source”:”no this database in MySQL”} 4.客户端的服务降级既然有了服务熔断，为什么还需要服务降级？ 服务降级处理是在客户端实现完成的,与服务端没有关系，客户端自带一个异常处理机制。上面的服务熔断是在服务端实现 客户端的服务降级，能够快速的响应用户的请求，当服务不可达，那么立即返回定制的错误信息。 整体资源快不够了,忍痛将某些服务先关掉,待渡过难关,再开启回来 优先保证核心服务，而非核心服务不可用或弱可用。 用户的请求故障时，不会被阻塞，更不会无休止的等待或者看到系统崩溃，至少可以看到一个执行结果（例如返回友好的提示信息 。 服务降级虽然会导致请求失败，但是不会导致阻塞，而且最多会影响这个依赖服务对应的线程池中的资源，对其它服务没有响应。 而且上面的服务端熔断案例存在一些缺点：控制层每实现一个方法，就要实现对应的fallBack方法处理相关的异常逻辑，那么代码量会越来越大，而且跟业务逻辑偶合在一起。所以我们需要解耦，我们把熔断的的fallback方法都放在一个类中，去除控制层的@HystrixCommand，这样能够保证业务的纯粹。 4.1 案例演示4.1.1 修改microservicecloud-api的工程, 根据已有的PersonClientService接口新建一个实现类 FallbackFactory接口的类PersonClientServiceFallbackFactory​ @Component//不要忘记添加public class PersonClientServiceFallbackFactory implements FallbackFactory&lt;PersonClientService&gt; &#123; @Override public PersonClientService create(Throwable throwable) &#123; return new PersonClientService() &#123; @Override public Person get(long id) &#123; Person person = new Person(); person.setDeptno(id);person.setDname(&quot;该ID&quot;+id+&quot;没有对应的信息,Consumer客户端提供的降级信息,此服务Provider已经关闭&quot;); person.setDb_source(&quot;no this database in MySQL&quot;); return person; &#125; @Override public List&lt;Person&gt; list() &#123; return null; &#125; @Override public boolean add(Person person) &#123; return false; &#125; &#125;; &#125;&#125; ​ 4.1.2 修改microservicecloud-api工程,PersonClientService接口在注解@FeignCLient中添加fallbackFactory属性值​ @FeignClient(value = &quot;MICROSERVICECLOUD-PERSON&quot;,fallbackFactory = PersonClientServiceFallbackFactory.class)public interface PersonClientService&#123; @RequestMapping(value = &quot;/person/get/&#123;id&#125;&quot;, method = RequestMethod.GET) public Person get(@PathVariable(&quot;id&quot;) long id); @RequestMapping(value = &quot;/person/list&quot;, method = RequestMethod.GET) public List&lt;Person&gt; list(); @RequestMapping(value = &quot;/person/add&quot;, method = RequestMethod.POST) public boolean add(Person person);&#125; ​ 4.1.3 修改80-feign消费者端配置文件，开启服务熔断microservicecloud-consumer-person-80-feign工程修改YML feign: hystrix: enabled: true 4.1.4 测试启动3个eureka先启动和微服务microservicecloud-provider-person-8001启动 microservicecloud-consumer-person-80-feign启动 正常访问测试 http://localhost/consumer/dept/get/5 输出：{“deptno”:5,”dname”:”hr”,”db_source”:”test”} ​ 故意关闭微服务microservicecloud-provider-person-8001 访问测试 http://localhost/consumer/dept/get/5 输出：”deptno”:5,”dname”:”该ID5没有对应的信息,Consumer客户端提供的降级信息,此服务Provider已经关闭”,”db_source”:”no this database in MySQL”} 成功！！！ **此时服务端provider已经down了,但是我们做了服务降级处理,让客户端在服务端不可用时也会获得提示信息而不会挂起耗死服务器** 也就是说，如果服务端能够正常调用那么就返回值，如果不能够调用那么就返回由fallbackFactory定义的值 5. 服务监控HystrixDashboard 除了隔离依赖服务的调用以外,Hystrix还提供了准实时的调用监控(Hystrix Dashboard),Hystrix 会持续地**记录所有通过Hystrix发起的请求的执行信息**,并以统计报表和图形的形式展示给用户,包括每秒执行多少请求多少成功,多少失败等.Netflix通过hystrix-metrics-event-stream项目实现了对以上指标的监控.Spring Cloud也提供了Hystrix Dashboard的整合.对监控内容转化成可视化界面. 也就是说，服务端必须是集成了Hystrix组件，才能够被监控，也即是：启动类添加@EnableCircuitBreaker//对hystrixR熔断机制的支持。例如我们的服务8002和8003是监控不到的 记下来看案例实现 5.1 案例实现5.1.1 新建 microservicecloud-consumer-hystrix-dashboard-9001 模块根据父工程 microservicecloud新建服务监控模块（ 参考microservicecloud-consumer-person-80-feign ） （1）复制80-feign模块pom文件并添加dashboard依赖 &lt;!-- hystrix和 hystrix-dashboard相关--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改application.yml配置文件 server: port: 9001 （3）启动类ApplicationBootStart9001dashboard添加@EnableHystrixDashboard ​ @SpringBootApplication@EnableHystrixDashboardpublic class ApplicationBootStart9001dashboard &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart9001dashboard.class, args); &#125;&#125; ​ （4）所有Provider微服务提供类(8001/8002/8003)都需要监控依赖配置 ​ &lt;!-- actuator监控信息完善 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; ​ （5）完整工程 ![](SpringCloud个人总结/Snipaste_2019-08-19_16-34-32.png) ​ 5.1.2 启动dashboard-9001模块http://localhost:9001/hystrix 部署成功 5.1.3 启动3个eureka集群5.1.4启动microservicecloud-provider-person-hystrix-8001和microservicecloud-consumer-person-80-feign 服务端和消费者5.1.4dashboard填写需要监控的服务地址 1：Delay：该参数用来控制服务器上轮询监控信息的延迟时间，默认为2000毫秒，可以通过配置该属性来降低客户端的网络和CPU消耗。 2：Title：该参数对应了头部标题Hystrix Stream之后的内容，默认会使用具体监控实例的URL，可以通过配置该信息来展示更合适的标题。 点击Monitor Stream 开始监控 怎么查看这张图：关键的部位我已经用红色方框，框住。核心就是：7色（服务的状态），1圈，1线。 1圈 实心圆:共有两种含义.它通过颜色的变化代表了 实例健康程度,它的健康度从绿色&lt;黄色&lt;橙色&lt;红色递减.该实心圆除了颜色的变化之外,它的大小也会根据实例的请求流量发生变化,流量越大该实心圆就越大.所以通过该实心圆的展示,就可以在大量的实例中快速的发现故障实例和高压力实例. 1线 曲线:用来记录2分钟内流量的相对变化,可以通过它来观察到流量的上升和下降趋势. 5.1.5 多次刷新http://localhost/consumer/person/get/1也即是多次请求8001服务，然后查看监控的状态 显示请求的get方法的情况。圆圈变大，曲线上升。（如果请求多个方法会增加图形说明） 十. zuul路由网关通过前面的学习，使用Spring Cloud实现微服务的架构基本成型，大致是这样的： 我们使用Spring Cloud Netflix中的Eureka实现了服务注册中心以及服务注册与发现；而服务间通过Ribbon或Feign实现服务的消费以及均衡负载。为了使得服务集群更为健壮，使用Hystrix的融断机制来避免在微服务架构中个别服务出现异常时引起的故障蔓延。 在该架构中，我们的服务集群包含：内部服务Service A和Service B，他们都会注册与订阅服务至Eureka Server，而Open Service是一个对外的服务，通过均衡负载公开至服务调用方。我们把焦点聚集在对外服务这块，直接暴露我们的服务地址，这样的实现是否合理，或者是否有更好的实现方式呢？ 先来说说这样架构需要做的一些事儿以及存在的不足： • 破坏了服务无状态特点。 为了保证对外服务的安全性，我们需要实现对服务访问的权限控制，而开放服务的权限控制机制将会贯穿并污染整个开放服务的业务逻辑，这会带来的最直接问题是，破坏了服务集群中REST API无状态的特点。 从具体开发和测试的角度来说，在工作中除了要考虑实际的业务逻辑之外，还需要额外考虑对接口访问的控制处理。 • 无法直接复用既有接口。 当我们需要对一个即有的集群内访问接口，实现外部服务访问时，我们不得不通过在原有接口上增加校验逻辑，或增加一个代理调用来实现权限控制，无法直接复用原有的接口。 面对类似上面的问题，我们要如何解决呢？答案是：服务网关！ 为了解决上面这些问题，我们需要将权限控制这样的东西从我们的服务单元中抽离出去，而最适合这些逻辑的地方就是处于对外访问最前端的地方，我们需要一个更强大一些的均衡负载器的 服务网关。 服务网关是微服务架构中一个不可或缺的部分。通过服务网关统一向外系统提供REST API的过程中，除了具备服务路由、均衡负载功能之外，它还具备了权限控制等功能。Spring Cloud Netflix中的Zuul就担任了这样的一个角色，为微服务架构提供了前门保护的作用，同时将权限控制这些较重的非业务逻辑内容迁移到服务路由层面，使得服务集群主体能够具备更高的可复用性和可测试性。 Zuul包含了对请求的路由和过滤两个最主要的功能： 其中路由功能负责将外部请求转发到具体的微服务实例上，是实现外部访问统一入口的基础而过滤器功能则负责对请求的处理过程进行干预，是实现请求校验、服务聚合等功能的基础.Zuul和Eureka进行整合，将Zuul自身注册为Eureka服务治理下的应用，同时从Eureka中获得其他微服务的消息，也即以后的访问微服务都是通过Zuul跳转后获得。 **注意：Zuul服务最终还是会注册进Eureka** 提供=代理+路由+过滤三大功能 1.简介官网：https://github.com/Netflix/zuul Zuul：维基百科 电影《捉鬼敢死队》中的怪兽，Zuul，在纽约引发了巨大骚乱。 事实上，在微服务架构中，Zuul就是守门的大Boss！一夫当关，万夫莫开！ 2.Zuul加入后的架构 不管是来自于客户端（PC或移动端）的请求，还是服务内部调用。一切对服务的请求都会经过Zuul这个网关，然后再由网关来实现 鉴权、动态路由等等操作。Zuul就是我们服务的统一入口。 ​ 3.路由基本配置（例子）3.1新建Module模块microservicecloud-zuul-gateway-99993.2 修改pom文件主要添加： &lt;!-- zuul路由网关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; 完整内容 &lt;dependencies&gt; &lt;!-- zuul路由网关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- actuator监控 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- hystrix容错--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 日常标配 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 热部署插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.3 修改application.ymlserver: port: 9999spring: application: name: microservicecloud-zuul-gatewayeureka: client: service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ instance: instance-id: microservicecloud-zuul-gateway9999 prefer-ip-address: trueinfo: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个zuul 3.4 修改主启动类ApplicationBootStart9999zuul@SpringBootApplication@EnableZuulProxypublic class ApplicationBootStart9999zuul &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart9999zuul.class, args); &#125;&#125; 3.4.1完整工程展示 3.5启动三个eureka集群 、一个服务提供类microservicecloud-provider-person-8001 、9999模块路由 3.6 测试不用路由访问：http://localhost:8001/person/get/2启用路由访问 http://localhost:9999/microservicecloud-person/person/get/2 输出都是：{“deptno”:5,”dname”:”开发部”,”db_source”:”test”} 4.路由访问映射规则上面的案例存在一个问题，那就是使用路由访问服务的时候 http://localhost:9999/microservicecloud-person/person/get/2 我们暴露了 服务名称：microservicecloud-person ，所以我们想隐藏他。 4.1 修改9999模块配置文件添加如下内容 zuul: routes: myperson.serviceId: microservicecloud-person myperson.path: /myperson/** 重要 规则说明： • zuul.routes..path=/xxx/**： 来指定映射路径。是自定义的路由名（在上面是myperson） • zuul.routes..serviceId=service-provider：来指定服务名。 而大多数情况下，我们的路由名称往往和服务名会写成一样的。因此Zuul就提供了一种简化的配置语法：zuul.routes.= 比方说上面我们关于microservicecloud-person的配置可以简化为一条： zuul: routes: microservicecloud-person: /myperson/** # 这里是映射路径# myperson.serviceId: microservicecloud-person# myperson.path: /myperson/** 省去了对服务名称的配置。 4.2 重启9999模块访问如下两个网址 （1）服务名称映射后路由访问OKhttp://localhost:9999/myperson/person/get/5 （2）未映射服务名称原路径访问OK - 默认的路由规则http://localhost:9999/microservicecloud-person/person/get/5 如果在保留第一种方式的情况下，禁止第二种方式的访问。 4.3 原真实服务名忽略-关闭默认的路由规则增你家该属性即可：ignored-services: zuul: ignored-services: microservicecloud-person #单个具体，多个可以用&quot;*&quot; ignored-services: * routes: myperson.serviceId: microservicecloud-person myperson.path: /myperson/** 重启9999zuul模块： 访问：http://localhost:9999/microservicecloud-person/person/get/5 访问失败 4.4 设置统一公共前缀增加prefix属性即可 zuul: ignored-services: microservicecloud-person #单个具体，多个可以用&quot;*&quot; ignored-services: * routes: myperson.serviceId: microservicecloud-person myperson.path: /myperson/** prefix: /sb #必须有反斜杠 http://localhost:9999/sb/myperson/person/get/5 //访问成功 http://localhost:9999/myperson/person/get/5 //不加前缀访问失败 5 .默认的路由规则在使用Zuul的过程中，上面讲述的规则（也就是上面4.1小节的规则）已经大大的简化了配置项。但是当服务较多时，配置也是比较繁琐的。因此Zuul就指定了默认的路由规则： • 默认情况下，一切服务的映射路径就是服务名本身。例如服务名为：service-provider，则默认的映射路径就 是：/service-provider/** 也就是说，刚才的映射规则我们完全不配置也是OK的，不信就试试看。 终极简化版本 zuul:# routes:# microservicecloud-person: /myperson/** # 这里是映射路径# myperson.serviceId: microservicecloud-person# myperson.path: /myperson/** 那么默认microservicecloud-person服务的映射路径是：/microservicecloud-person/** 为了不暴露服务名称，那么我么你需要关闭默认的路由规则：见4.3小节 6.过滤器Zuul作为网关的其中一个重要功能，就是实现请求的鉴权。而这个动作我们往往是通过Zuul提供的过滤器来实现的。 6.1.ZuulFilterZuulFilter是过滤器的顶级父类。在这里我们看一下其中定义的4个最重要的方法： public abstract ZuulFilter implements IZuulFilter&#123; abstract public String filterType(); abstract public int filterOrder(); boolean shouldFilter();// 来自IZuulFilter Object run() throws ZuulException;// IZuulFilter &#125; • shouldFilter：返回一个Boolean值，判断该过滤器是否需要执行。返回true执行，返回false不执行。 • run：过滤器的具体业务逻辑。 • filterType：返回字符串，代表过滤器的类型。包含以下4种： – pre：请求在被路由之前执行 – route：在路由请求时调用 – post：在route和errror过滤器之后调用 – error：处理请求时发生错误调用 • filterOrder：通过返回的int值来定义过滤器的执行顺序，数字越小优先级越高。 6.2.过滤器执行生命周期这张是Zuul官网提供的请求生命周期图，清晰的表现了一个请求在各个过滤器的执行顺序。 正常流程： • 请求到达首先会经过pre类型过滤器，而后到达route类型，进行路由，请求就到达真正的服务提供者，执行请求，返回结果后，会到达post过滤器。而后返回响应。 异常流程： • 整个过程中，pre或者route过滤器出现异常，都会直接进入error过滤器，在error处理完毕后，会将请求交给POST过滤器，最后返回给用户。 • 如果是error过滤器自己出现异常，最终也会进入POST过滤器，将最终结果返回给请求客户端。 • 如果是POST过滤器出现异常，会跳转到error过滤器，但是与pre和route不同的是，请求不会再到达POST过滤器了。 所有内置过滤器列表： 6.3.使用场景场景非常多： • 请求鉴权：一般放在pre类型，如果发现没有访问权限，直接就拦截了 • 异常处理：一般会在error类型和post类型过滤器中结合来处理。 • 服务调用时长统计：pre和post结合使用。 7.自定义过滤器接下来我们来自定义一个过滤器，模拟一个登录的校验。基本逻辑：如果请求中有access-token参数，则认为请求有效，放行。 7.1.定义过滤器类 内容： package com.kingge.filter;import com.netflix.zuul.ZuulFilter;import com.netflix.zuul.context.RequestContext;import com.netflix.zuul.exception.ZuulException;import org.apache.commons.lang.StringUtils;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;import javax.servlet.http.HttpServletRequest;@Componentpublic class LoginFilter extends ZuulFilter &#123; /** * 过滤器类型，前置过滤器 * @return */ @Override public String filterType() &#123; return &quot;pre&quot;; &#125; /** * 过滤器的执行顺序 * @return */ @Override public int filterOrder() &#123; return 1; &#125; /** * 该过滤器是否生效 * @return */ @Override public boolean shouldFilter() &#123; return true; &#125; /** * 登陆校验逻辑 * @return * @throws ZuulException */ @Override public Object run() &#123; // 获取zuul提供的上下文对象 RequestContext context = RequestContext.getCurrentContext(); // 从上下文对象中获取请求对象 HttpServletRequest request = context.getRequest(); // 获取token信息 String token = request.getParameter(&quot;access-token&quot;); // 判断 if (StringUtils.isBlank(token)) &#123; // 过滤该请求，不对其进行路由 context.setSendZuulResponse(false); // 设置响应状态码，401 context.setResponseStatusCode(401); // 设置响应信息 context.setResponseBody(&quot;&#123;\\&quot;status\\&quot;:\\&quot;401\\&quot;, \\&quot;text\\&quot;:\\&quot;request error!\\&quot;&#125;&quot;); &#125; // 校验通过，把登陆信息放入上下文信息，继续向后执行 context.set(&quot;token&quot;, token); return null; &#125;&#125; 7.2.测试没有token参数时，访问失败： 添加token参数后： 8.负载均衡和熔断Zuul中默认就已经集成了Ribbon负载均衡和Hystix熔断机制。但是所有的超时策略都是走的默认值，比如熔断超时时间只有1S，很容易就触发了。因此建议我们手动进行配置： hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 2000 # 设置hystrix的超时时间为6000ms 十一.SpringCloud Config 分布式配置中心**解决第四个问题，统一配置的问题** 1.为什么需要配置中心 举个简单的例子，我们想要修改8001模块的服务名称的值（spring.application.name），那么我们首先得找到8001模块，然后找到配置文件，然后再进去修改。如果修改修改8003模块的配置信息，重复之前的步骤。 所以我们需要一个集中管理所有微服务配置信息的地方。 微服务意味着要将单体应用中的业务拆分成一个个子服务,每个服务的粒度相对较小,因此系统中会出现大量的服务.由于每个服务都需要必要的配置信息才能运行（application.yml）,所以一套集中式的、动态的配置管理设施是必不可少的.SpringCloud提供了ConfigServer来解决这个问题(我们每一个微服务自己带着一个application.yml,上百个配置文件的管理的问题) ​ **总而言之：是为了更加方便的帮助我们集中式的管理微服务架构里面微服务的配置信息。** 2. config配置中心SpringCloud Config 为微服务架构中的微服务提供集中化的外部配置支持,配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部配置，方便我们集中式的修改微服务的配置 SpringCloud Config分为服务端和客户端两部分. 服务端也称为分布式配置中心,它是一个独立的微服务应用,用来连接配置服务器并为客户端提供获取配置信息,加密/解密信息等访问接口 客户端则是通过制定的配置中心来管理应用资源,以及业务相关的配置内容,并在启动的时候从配置中心获取和加载配置信息配置服务器默认采用git来存储配置信息,这样就有助于对环境配置进行版本管理,并且可以通过git客户端工具来方便的管理和访问配置内容.（客户端可以是我们的8001,8003模块，也即是需要获取配置信息的微服务都是客户端） 功能： 1.集中管理配置文件 2.不同环境不同配置,动态化的配置更新,分环境部署比如dev/test/prod/beta/release 3.运行期间动态调整配置,不再需要在每个服务部署的机器上编写配置文件,服务会向配置中心统一拉取配置自己的信息 4.当配置发生变动时,服务不需要重启即可感知到配置的变化并应用新的配置 5.将配置信息以REST接口的形式暴露 配置中心配置文件放置的位置：与GitHub整合配置 由于SpringCloudConfig默认使用Git来存储配置文件(也有其他方式,比如支持SVN和本地文件),但最推荐的还是使用Git,而且使用的是http/https访问形式 3.SpringCloud Config服务端配置3.1 在GitHub上新建一个名为springcloud-config-server的新Repository git@github.com:JeremyKinge/springcloud-config-server.git 3.2 本地磁盘获取上述创建的仓库git命令:git clone git@github.com:JeremyKinge/springcloud-config-server.git 3.3 在上述磁盘新建配置文件并上传到git仓库（1）添加配置文件application.yml spring: profiles: active: - dev---spring: profiles: dev application: #开发环境 name: microservicecloud-config-kingge-dev---spring: profiles: test #测试环境 application: name: microservicecloud-config-kingge-test#请保存为UTF-8格式 （2）git bash执行上传命令： git add .git commit -m “新建配置文件”git push origin master （3）查看github 3.4 新建Module模块microservicecloud-config-10000它即为Cloud的配置中心模块3.5 修改10000模块pom文件和yml配置文件、启动类（1）修改pom文件 &lt;dependencies&gt; &lt;!-- springCloud Config 关键代码--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 避免Config的Git插件报错：org/eclipse/jgit/api/TransportConfigCallback --&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jgit&lt;/groupId&gt; &lt;artifactId&gt;org.eclipse.jgit&lt;/artifactId&gt; &lt;version&gt;4.10.0.201712302008-r&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 图形化监控 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 熔断 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 热部署插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 关键代码 &lt;!-- springCloud Config 关键代码--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt; （2）修改yml文件 server: port: 10000 spring: application: name: microservicecloud-config cloud: config: server: git: uri: git@github.com:JeremyKinge/springcloud-config-server.git #GitHub上面的git仓库名字 #username: xxxxxx #如果访问github需要密码那么填写下面三项 #password: xxxxxxx #force-pull: true （3）启动类 @SpringBootApplication@EnableConfigServerpublic class ApplicationBootStart10000config &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart10000config.class,args); &#125;&#125; 3.6 完整项目结构 3.7 测试通过Config微服务是否可以从GitHub上获取配置内容启动微服务10000http://localhost:10000/application-dev.yml 输出： http://localhost:10000/application-test.yml 输出： http://localhost:10000/application-xxx.yml(不存在的配置) 成功实现了用SpringCloud Config通过GitHub获取配置信息 3.8 配置文件读取规则 上面3.8测试中，我们采用的是第二种方式，通过10000配置中心去github获取配置文件 请求例子： /{application}-{profile}.yml http://localhost:10000/application-dev.yml http://localhost:10000/application-test.yml http://localhost:10000/application-xxx.yml(不存在的配置)/{application}/{profile}[/{label}] http://localhost:10000/application/dev/master http://localhost:10000/application/test/master http://localhost:10000/application/xxx/master/{label}/{application}-{profile}.yml http://localhost:10000/master/application-dev.yml http://localhost:10000/master/application-test.yml 4.SpringCloud Config客户端配置与测试在上面中我们已经搭建好了，配置中心的服务端。那么接下来演示一下客户端怎么取获取服务端的配置信息。 新建8004服务提供者，他的配置信息我们取自配置中心（而不是在application.yml中配置，动态获取） 4.1 新建8004 配置文件并上传到github新建microservicecloud-provider-person-config-client-8004.yml spring: profiles: active: - dev--- server: port: 8004mybatis: config-location: classpath:mybatis/mybatis.cfg.xml # mybatis配置文件所在路径 type-aliases-package: com.kingge.entity # 所有Entity别名类所在包 mapper-locations: - classpath:mybatis/mapper/**/*.xml # mapper映射文件spring: profiles: dev application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://127.0.0.1:3306/test # 数据库名称 username: root password: 123 dbcp2: min-idle: 5 # 数据库连接池的最小维持连接数 initial-size: 5 # 初始化连接数 max-total: 5 # 最大连接数 max-wait-millis: 200 # 等待连接获取的最大超时时间#eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/# http://localhost:7001/eureka #单机版本使用# defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ instance: instance-id: microservicecloud-person8001 #自定义服务实例名 prefer-ip-address: true #访问路径可以显示IP地址 lease-expiration-duration-in-seconds: 10 # 10秒即过期 lease-renewal-interval-in-seconds: 5 # 5秒一次心跳#info: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个提供查询部门人员信息的服务---server: port: 8005mybatis: config-location: classpath:mybatis/mybatis.cfg.xml # mybatis配置文件所在路径 type-aliases-package: com.kingge.entity # 所有Entity别名类所在包 mapper-locations: - classpath:mybatis/mapper/**/*.xml # mapper映射文件spring: profiles: test application: name: microservicecloud-person #很重要，对外暴露的微服务的名称 datasource: type: com.alibaba.druid.pool.DruidDataSource # 当前数据源操作类型 driver-class-name: org.gjt.mm.mysql.Driver # mysql驱动包 url: jdbc:mysql://127.0.0.1:3306/test2 # 数据库名称 username: root password: 123 dbcp2: min-idle: 5 # 数据库连接池的最小维持连接数 initial-size: 5 # 初始化连接数 max-total: 5 # 最大连接数 max-wait-millis: 200 # 等待连接获取的最大超时时间#eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/# http://localhost:7001/eureka #单机版本使用# defaultZone: http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ instance: instance-id: microservicecloud-person8001 #自定义服务实例名 prefer-ip-address: true #访问路径可以显示IP地址 lease-expiration-duration-in-seconds: 10 # 10秒即过期 lease-renewal-interval-in-seconds: 5 # 5秒一次心跳#info: app.name: $&#123;spring.application.name&#125; company.name: kingge.top build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; app.desc: 这是一个提供查询部门人员信息的服务 dev和test环境的不同在于，他们的服务端口不同和访问的数据库不同 4.2 microservicecloud-provider-person-config-client-8004模块（1）修改pom文件 &lt;dependencies&gt; &lt;!-- 引入自己定义的api通用包，可以使用Person用户Entity --&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- SpringCloud Config客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- actuator监控信息完善 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 将微服务provider端注册进eureka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 关键依赖 &lt;!-- SpringCloud Config客户端 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt; 4.3 新建bootstrap.yml 系统级别配置文件applicaiton.yml是用户级的资源配置项bootstrap.yml是系统级的，优先级更加高 Spring Cloud会创建一个Bootstrap Context，作为Spring应用的Application Context的父上下文。初始化的时候，Bootstrap Context负责从外部源加载配置属性并解析配置。这两个上下文共享一个从外部获取的Environment。Bootstrap属性有高优先级，默认情况下，它们不会被本地配置覆盖。 Bootstrap context和Application Context有着不同的约定，所以新增了一个bootstrap.yml文件，保证Bootstrap Context和Application Context配置的分离。 换句话说，如果将下面内容放到application.yml中，那么项目启动会报错，因为是找不到控制层ConfigClientRest导入的属性 @Value(“${spring.application.name}”) （1）增加内容如下： spring: cloud: config: name: microservicecloud-provider-person-config-client-8004 #需要从github上读取的资源名称，注意没有yml后缀名 profile: dev #本次访问的配置项-profile值是什么，决定从github上读取什么 label: master uri: http://localhost:10000 #本微服务启动后先去找config配置中心地址，通过SpringCloudConfig获取GitHub的服务地址 （2）为了配置文件的完整性我们新建一个空的application.yml 4.4 新建一个控制层，获取配置文件的某些属性（测试）实际上如果8004服务端能够正常启动和访问，也能够说明客户端获取config配置中心配置文件成功。不过为了证实一下，所以这里通过控制层输出某些属性 （1）新建ConfigClientRest @RestControllerpublic class ConfigClientRest &#123; @Value(&quot;$&#123;spring.application.name&#125;&quot;) private String applicationName; @Value(&quot;$&#123;eureka.client.service-url.defaultZone&#125;&quot;) private String eurekaServers; @Value(&quot;$&#123;server.port&#125;&quot;) private String port; @Value(&quot;$&#123;spring.datasource.url&#125;&quot;) private String datasourceurl; @RequestMapping(&quot;/config&quot;) public String getConfig() &#123; String str = &quot;applicationName: &quot;+applicationName+&quot;\\t eurekaServers:&quot;+eurekaServers+&quot;\\t port: &quot;+port; System.out.println(&quot;******str: &quot;+ str); return &quot;applicationName: &quot;+applicationName+&quot;\\t eurekaServers:&quot;+eurekaServers+&quot;\\t port: &quot;+port + &quot;\\t datasourceurl：&quot;+datasourceurl; &#125;&#125; 后面可以通过更改 bootstrap.yml的profile属性的值，访问/config,查看服务端口号和数据库url是否改变。 4.5 新建启动类@SpringBootApplicationpublic class ApplicationBootStart8004client &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart8004client.class, args); &#125;&#125; 4.5.1 完整目录机构 4.6 启动10000配置中心和8004服务 （1） 启动Config配置中心10000微服务并自测 （2）启动8004作为Client准备访问 bootstrap.yml里面的profile值是什么，决定从github上读取什么 2.1 启动成功，说明配置文件获取成功 2.2 额外验证 假如目前是 profile: dev dev默认在github上对应的端口就是8004 http://localhost:8004/config 输出：applicationName: microservicecloud-person eurekaServers:http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ port: 8004 datasourceurl：jdbc:mysql://127.0.0.1:3306/test ​​ 假如目前是 profile: test​ test默认在github上对应的端口就是8005​ http://localhost:8005/config​ 输出：applicationName: microservicecloud-person eurekaServers:http://peer1:7001/eureka/,http://peer2:7002/eureka/,http://peer3:7003/eureka/ port: 8005 datasourceurl：jdbc:mysql://127.0.0.1:3306/test2 5.手动刷新配置上面的案例，有个缺点，那就是我们修改了github上面的配置文件（例如修改了连接数据库的地址），对应的8004模块没有刷新，也就是，他获取的连接数据库的地址还是未修改前的。 在真实的案例中，我们不可能手动关闭服务器，然后重启。所以就需要8004它能够自动刷新 需要依赖actuator组件和通过psot方式访问请求配置变动的服务器（也即是@RefreshScope注解所在的bean） （1）8004模块引入actuator依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; （2）8004启动类添加@EnableDiscoveryClient服务发现注解 @SpringBootApplication@EnableDiscoveryClientpublic class ApplicationBootStart8004client &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationBootStart8004client.class, args); &#125;&#125; （3）8004模块修改application.yml文件，暴露所有端点 management: security: enabled: false （4）8004添加controller @RestController@RefreshScopepublic class ConfigClientRest &#123; @Value(&quot;$&#123;spring.application.name&#125;&quot;) private String applicationName; @Value(&quot;$&#123;eureka.client.service-url.defaultZone&#125;&quot;) private String eurekaServers; @Value(&quot;$&#123;server.port&#125;&quot;) private String port; @Value(&quot;$&#123;spring.datasource.url&#125;&quot;) private String datasourceurl; @RequestMapping(&quot;/config&quot;) public String getConfig() &#123; String str = &quot;applicationName: &quot;+applicationName+&quot;\\t eurekaServers:&quot;+eurekaServers+&quot;\\t port: &quot;+port; System.out.println(&quot;******str: &quot;+ str); return &quot;applicationName: &quot;+applicationName+&quot;\\t eurekaServers:&quot;+eurekaServers+&quot;\\t port: &quot;+port + &quot;\\t datasourceurl：&quot;+datasourceurl; &#125;&#125; （5）启动configserver10000和8004模块 打印通过configserver从github上面获取的配置信息 此时数据库的url是：jdbc:mysql://127.0.0.1:3306/test1 （5）这个时候我们修改8004模块在github中引入的配置文件- 也就是修改microservicecloud-provider-person-config-client-8004.yml 文件。 1.修改访问数据库的url为，原先是test1 url: jdbc:mysql://127.0.0.1:3306/test2 2.修改8004的服务端口为8005，原先是8008 - 这个修改我们预测不会成功，因为服务器没有重启 server.port：8005 （6）请求：curl -X POST http://localhost:8008/refresh 手动更新8004模块的配置 我们只需要请求这个地址就可以实现配置文件的更新，而不用重启8004模块，他默认会帮我们刷新配置文件。 我们发现数据库访问地址修改成功，但是服务端口号没有成功（很明显这种做法是错误的，因为服务端口号不会轻易的变动） 注意如果执行curl -X POST http://localhost:8008/refresh 包如下错误 &#123;&quot;timestamp&quot;:1513070580796,&quot;status&quot;:401,&quot;error&quot;:&quot;Unauthorized&quot;,&quot;message&quot;:&quot;Full authentication is required to access this resource.&quot;,&quot;path&quot;:&quot;/refresh&quot;&#125; 那就说明默认开启访问端口验证了。需要关闭 一、在Spring Boot1.5.x版本中通过management.security.enabled=false来暴露所有端点 二、切换SpringBoot版本为2.x 使用IDE的搜索功能，找到类ManagementServerProperties，发现Security内部类已经被删除，通过去官网查看2.0暴露端点的方式得知： 方式1： # 启用端点 envmanagement.endpoint.env.enabled=true # 暴露端点 env 配置多个,隔开management.endpoints.web.exposure.include=env 方式2： 方式1中的暴露方式需要一个一个去开启需要暴露的端点，方式2直接开启和暴露所有端点 management.endpoints.web.exposure.include=* 注意在使用Http访问端点时，需要加上默认/actuator 前缀 三、如果这三种还不行，可以尝试在8004添加security验证依赖，然后给8004模块设置访问账号和密码， 然后通过 curl -X POST http://账号:密码@localhost:8008/refresh 这种方式访问 6.自动刷新配置上面是通过手动刷新方式，缺点就是，如果我们在github上面修改了20个服务器的配置，那么我们需要手动执行20次 curl -X POST —— ，那么我们想能够缩减执行命令的次数或者说自动刷新配置。 额外知识补充1.spring cloud服务发现注解之@EnableDiscoveryClient与@EnableEurekaClient区别在使用服务发现的时候有两种注解， 一种为@EnableDiscoveryClient, 一种为@EnableEurekaClient, 用法上基本一致，下文是从stackoverflow上面找到的对这两者的解释： There are multiple implementations of &quot;Discovery Service&quot; (eureka, consul, zookeeper). @EnableDiscoveryClient lives in spring-cloud-commons and picks the implementation on the classpath. @EnableEurekaClient lives in spring-cloud-netflix and only works for eureka. If eureka is on your classpath, they are effectively the same. 意思也就是spring cloud中discovery service有许多种实现（eureka、consul、zookeeper等等） @EnableDiscoveryClient基于spring-cloud-commons；而 @EnableEurekaClient基于spring-cloud-netflix 对@EnableEurekaClient的源码如下： /** * Convenience annotation for clients to enable Eureka discovery configuration * (specifically). Use this (optionally) in case you want discovery and know for sure that * it is Eureka you want. All it does is turn on discovery and let the autoconfiguration * find the eureka classes if they are available (i.e. you need Eureka on the classpath as * well). * * @author Dave Syer * @author Spencer Gibb */@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@EnableDiscoveryClientpublic @interface EnableEurekaClient &#123;&#125; 注解@EnableEurekaClient上有@EnableDiscoveryClient注解，可以说基本就是EnableEurekaClient有@EnableDiscoveryClient的功能，另外上面的注释中提到，其实@EnableEurekaClient注解就是一种方便使用eureka的注解而已，可以说使用其他的注册中心后，都可以使用@EnableDiscoveryClient注解， 但是使用@EnableEurekaClient的情景，就是在服务采用eureka作为注册中心的时候，使用场景较为单一。 所以还是比较建议使用@EnableDiscoveryClient。 所以上面的还是建议使用@EnableDiscoveryClient替换@EnableEurekaClient 2.Ribbon和Feign这两种都是SPringcloud提供的负载均衡技术，都是客户端的软负载均衡技术。feign优化的Ribbon的服务调用方式，实现了面向接口编程的封装。","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://kingge.top/categories/springcloud/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"springcloud","slug":"springcloud","permalink":"http://kingge.top/tags/springcloud/"},{"name":"微服务架构","slug":"微服务架构","permalink":"http://kingge.top/tags/微服务架构/"}]},{"title":"浅谈dubbo和springcloud","slug":"浅谈dubbo和springcloud","date":"2019-04-27T02:21:59.000Z","updated":"2019-08-25T02:08:25.860Z","comments":true,"path":"2019/04/27/浅谈dubbo和springcloud/","link":"","permalink":"http://kingge.top/2019/04/27/浅谈dubbo和springcloud/","excerpt":"","text":"一、Dubbo负责人的采访刘军，阿里巴巴中间件高级研发工程师，主导了 Dubbo 重启维护以后的几个发版计划，所以让我们来看一下他关于Dubbo和Springcloud是否二选一，他们之间的区别的阐述。 7、目前 Dubbo 被拿来比较最多的就是 Spring Cloud ，您怎么看待二者的关系，业务上是否有所冲突？ 关于 Dubbo 和 Spring Cloud 间的关系，我们在开源中国年终盛典的 Dubbo 分享中也作了简单阐述，首先要明确的一点是 Dubbo 和 Spring Cloud 并不是完全的竞争关系，两者所解决的问题域并不一样：Dubbo 的定位始终是一款 RPC 框架，而 Spring Cloud 的目标是微服务架构下的一站式解决方案。如果非要比较的话，我觉得 Dubbo 可以类比到 Netflix OSS 技术栈，而 Spring Cloud 集成了 Netflix OSS 作为分布式服务治理解决方案，但除此之外 Spring Cloud 还提供了包括 config、stream、security、sleuth 等等分布式问题解决方案。 当前由于 RPC 协议、注册中心元数据不匹配等问题，在面临微服务基础框架选型时 Dubbo 与 Spring Cloud 是只能二选一，这也是为什么大家总是拿 Dubbo 和 Spring Cloud 做对比的原因之一。Dubbo 之后会积极寻求适配到 Spring Cloud 生态，比如作为 Spring Cloud 的二进制通信方案来发挥 Dubbo 的性能优势，或者 Dubbo 通过模块化以及对 http 的支持适配到 Spring Cloud 。 Netflix OSS 技术栈 1.Netflix Eureka 服务注册中心，提供服务的注册和发现 2.Netflix Ribbon 客户端负载均衡 3.Netflix Feign 客户端负载均衡，包装Ribbon，提供了接口式的服务调用 4.Netflix Hystrix 熔断器，负责服务的熔断和降级 5.NetFlix Hystrix dashboard 提供服务监控 6.Netflix zuul 路由网管 提供代理+路由+过滤三大功能 二、Dubbo和Springcloud比较 1.社区活跃度 https://github.com/dubbo dubbo社区https://github.com/springcloud springcloud社区 可以进去看一下他们对于技术的活跃程度曲线 2.解决问题的方向 dubbo：定位始终是一款 RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 springcloud：微服务架构，提供微服务架构的一站式服务。 3.功能对比 根据两者的解决问题的域，得到他们的功能 Dubbo Spring 服务注册中心 Zookeeper Spring Cloud Netfilx Eureka 服务调用方式 RPC REST API 服务监控 Dubbo-monitor Spring Boot Admin 断路器 不完善 Spring Cloud Netflix Hystrix 服务网关 无 Spring Cloud Netflix Zuul 分布式配置 无 Spring Cloud Config 服务跟踪 无 Spring Cloud Sleuth 消息总线 无 Spring Cloud Bus 数据流 无 Spring Cloud Stream 批量任务 无 Spring Cloud Task Dubbo的某些功能都是，通过整合其他组件实现，springcloud是通过实现Netflix oss的技术栈和原有的技术，实现的架构。 Dubbo提供了各种Filter，对于上述中“无”的要素，可以通过扩展Filter来完善。 例如 1．分布式配置：可以使用淘宝的diamond、百度的disconf来实现分布式配置管理 2．服务跟踪：可以使用京东开源的Hydra，或者扩展Filter用Zippin来做服务跟踪 3．批量任务：可以使用当当开源的Elastic-Job、tbschedule 总结：从核心要素来看，Spring Cloud 更胜一筹，在开发过程中只要整合Spring Cloud的子项目就可以顺利的完成各种组件的融合，而Dubbo缺需要通过实现各种Filter来做定制，开发成本以及技术难度略高。Dubbo更像是一个组装机，springcloud是一体机。 4.服务调用方式 Spring Cloud抛弃了RPC通讯，采用基于HTTP的REST方式。Spring Cloud牺牲了服务调用的性能，但是同时也避免了原生RPC带来的问题。REST比RPC更为灵活，不存在代码级别的强依赖，在强调快速演化的微服务环境下，显然更合适。 5.服务获取方式 dubbo通过长连接推送服务提供者地址列表给消费端，即是：Dubbo订阅Zookeeper下相应的节点，当节点的状态发生改变时，Zookeeper会立即反馈订阅的Client，实时性很高。 springcloud的eureka是 消费者端主动去eurekaServer注册中心获取数据，消费者可以配置去EurekaServer拉去服务列表的周期 dubbo支持各种通信协议，而且消费方和服务方使用长链接方式交互，通信速度上略胜Spring Cloud，如果对于系统的响应时间有严格要求，长链接更合适。 6.服务注册中心满足的CAP原则 著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性P在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。 Dubbo推荐使用zookeeper作为服务注册中心，zookeeper满足CP原则，一致性和分区容错性。springcloud的服务注册中心是Eureka，他满足的是AP原则，可用性和分区容错性。 Zookeeper如何保证CP 当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。 Eureka如何保证AP Eureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的 ，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用) 当网络稳定时，当前实例新的注册信息会被同步到其它节点中 因此， Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。 总结：那么既然保证了保证了可用性，那么数据的一致性肯定是不能够保证了，所以这个就是自我保护的机制。所以到底是AP还是CP，又或者是AC（数据库），要看业务场景来定。 而且Eureka部署集群时非常简单的，相比于dubbo部署zookeeper集群。 7.节点性质 Dubbo只有Consumer订阅Provider节点，也就是Consumer发现Provider节点信息 Eureka不区分Consumer或者Provider，两者都统称为Client，一个Client内可能同时含有Provider，Consumer，通过服务发现组件获取的是其他所有的Client节点信息，在调用时根据应用名称来筛选节点 三、总结","categories":[{"name":"dubbo","slug":"dubbo","permalink":"http://kingge.top/categories/dubbo/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://kingge.top/tags/dubbo/"},{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"rpc","slug":"rpc","permalink":"http://kingge.top/tags/rpc/"}]},{"title":"spring注解-辅助学习springboot和springcloud","slug":"spring注解-辅助学习springboot和springcloud","date":"2019-03-14T14:59:59.000Z","updated":"2019-09-03T12:48:30.441Z","comments":true,"path":"2019/03/14/spring注解-辅助学习springboot和springcloud/","link":"","permalink":"http://kingge.top/2019/03/14/spring注解-辅助学习springboot和springcloud/","excerpt":"","text":"1. 前言 因为后面要学习sb（springboot）和sc（springcloud），所以需要学习一些相关的注解和了解他们底层代码实现，例如@import注解，Aware接口，生命周期。 ​ 通过前面几个章节的学习，你会发现无论springboot还是springcloud的很多知识都是来源于spring相关的知识，章节还有一些内容没有补充完整，后续如果有时间会逐步更新完成！！！ 2.组件注册我们知道spring得IOC容器中存储了很多类的实例化对象，那么下面介绍几种往IOC容器中注册实体类的方式 2.1 xml配置文件实例化实体类（方式一） 首先新建spring.xml 文件，通过bean 标签实例化实体类 2.通过ClassPathXmlApplicationContext获取实体类 缺点：大型项目的实体类会非常多，那么配置文件会变得非常的臃肿，而且也不易于维护。 2.2 使用@ Configuration和@bean（方式二）Spring提供了配置实体类的第二种方式，就是不用通过书写xml文件，通过两个注解就可以达到2.1 的功能。 编写配置类，配置bean 通过AnnotationConfigApplicationContext获取实体类 这里是new了一个AnnotationConfigApplicationContext对象，以前new的ClassPathXmlApplicationContext对象，的构造函数里面传的是配置文件的位置，而现在AnnotationConfigApplicationContext对象的构造函数里面传的是配置类的类型 2.3 @ComponentScan-自动扫描组件&amp;指定扫描规则（方式三）**实际上这个注解跟前面两种方式是配合使用的，避免书写ClassPathXmlApplicationContext或者AnnotationConfigApplicationContext** **获取IOC容器。** 我们知道，在实际开发中我们是不会通过ClassPathXmlApplicationContext这样的代码方式获取对象，而是通过包扫描的方式进行实例化对象并注入IOC容器中 他的扫描规则是：以下这几个注解都是继承自@Component @controller(给web层的注解) @service(给serivce层加的注解) @repository(给dao层加的注解) @component(给java类加注解,老版本spring只有这一个注解) 只要书写了上面四个注解的类，那么会自动装配到ioc容器中。Id默认是类名首字母小写 1. @ComponentScan注解的结构 1.这个注解上，也是可以指定要排除哪些包或者是只包含哪些包来进行管理：里面传是一个Filter[]数组。 2.Value ：就相当于spring的xml配置文件- 2.用例–那么我们使用配置类的方式实现component-sacn同样的功能 1.首先在配置文件类，添加扫描的范围 2.添加几个注解类- @controller（UserController） @service（UserService） @repository （UserDao） @component（ComponentTest） 3.书写IOC 输出： 发现-扫描进入IOC容器的bean的id默认是：类名首字母小写 4.使用Filter去除某些注解类 根据注解的方式排除，排除使用@Controller注解注解的类 –注意在使用includeFilters 扫描只包含那些组件的时候，要禁用spring默认全局扫描（跟配置文件一样，也是需要禁用的） 例子4 3. 扩展我们打开Component注解的源码，发现他是： 多了Repeatable注解，也就是说明，这个Component注解是可以多次重复用的 那么你可能会问，如果不是jdk1.8,那么怎么书写多个扫描策略呢？ 也就是说，我们可以在配置类，使用ComponentScans注解，配置多个扫描策略 例子： 跟 { 例子4 } 效果一样 4.1 FilterType 过滤规则 下面我们注重讲解一下，CUSTOM 自定义实现类 需要先实现 TypeFilter 1.首先定义一个扫描规则类 配置类，实现自定义过滤规则 表示 – 扫描的类中如果包含er那么就会被过滤掉（注意：他是会取扫描com.kingge下面的所有类-包括哪些没有被注解，注解的类也会被扫描） 2.4 @Import注解有三种使用方式 第一种写法：直接在import注解中配置需要导入的类 他在IOC容器中的id是：（全类名） 缺点：如果有多个类需要注入IOC，那么代码量就很长 第二种实现方式：自定义导入逻辑，批量导入，只需要返回需要导入的全类名数组 实现ImportSelector 类 配置类上使用 这样 pp就注入到了IOC 容器中 第三种方式：ImportBeanDefinitionRegistrar 实现这个类。 例子： 配置类引用 调用： 2.4.1 总结@Import[快速的给容器中导入一个组件] （1）、 @Import(要导入容器中的组件);容器中就会自动的注册这个组件，id默认是全类名 （2）、 ImportSelector ：返回需要的组件的全类名的数组； （3）、 ImportBeanDefinitionRegistrar : 手动注册bean到容器中 前面学习的springboot中，用到了该注解的次数很多。 2.5 Factorybean 工厂bean 例子： 1.实现这个工厂bean getObject 方法：当调用bean时候，调用这个方法获取bean实例。 getObjectType：返回对象类型 isSIngleton: 是否是单例。False-表示是多例。True-表示是单例。（如果配置类中配置@Scope注解，企图改变UserDao的单实例，无效，以isDingleto方法设置为准） 2.配置类配置 输出： 输出： class com.kingge.dao.UserDao false 、 第二个输出肯定是false 不过为什么第一个输出的是 Userdao的全类名而不是UserDaoFactoryBean的全类名呢？因为在构造的时候spring默认返回的就是getObjectType的值。 那么怎么获取这个工厂bean呢？spring提供了一个方式：加上&amp; 输出：class com.kingge.utils.UserDaoFactoryBean 2.6 总结-组件注册 /** \\* 给容器中注册组件： \\* 1）、扫描+组件标注注解（@Controller/@Service/@Repository/@Component） \\* 【局限于要求是自己写的类，如果导入的第三方没有添加这些注解，那么就注册不上了】 * \\* 2）、@Bean[导入的第三方包里面的组件] \\* 3）、@Import[快速的给容器中导入一个组件] \\* （1）、 @Import(要导入容器中的组件);容器中就会自动的注册这个组件，id默认是全类名 \\* （2）、 ImportSelector ：返回需要的组件的全类名的数组； \\* （3）、 ImportBeanDefinitionRegistrar : 手动注册bean到容器中 * \\* 4）、使用Spring提供的FactoryBean（工厂bean） \\* （1）、默认获取到的是工厂bean调用getObject创建的对象 \\* （2）、要获取工厂bean本身，我们需要给id前面加上一个“&amp;”符号：&amp;userDaoFactoryBean 第一种方式：一般用于自己定义的类，但是如果我们是通过导入第三方jar的方式导入了很多组件（类），但是我想把这些类注册到IOC容器中怎么办呢？这个时候就需要使用bean注解的方式注册组件。 第二种方式：可以实现自定义类或者第三方类的注入到IOC容器。缺点，那就是每实例化一个bean就得写个方法。这样代码量太多。 第三种方式：import标签（作用在配置类） 3.修饰Bean的相关注解3.1 @Scope注解他一般是和@Bean注解配套使用，标识实体类的作用范围。我们知道IOC容器中的实体类，默认是单实例的。 证明： 自定义IOC容器-实现配置类 2.获取 Person实体类 我们不难发现-这里输出的是true，所以spring扫描bean策略默认是单实例。 那么怎么修改这种作用域呢？ 查看Scope注解源码，发现可以指定这四种类型的作用范围 第一个是多实例，第二个是：单实例（默认值） 第三个是：web环境下，用一个请求创建一次实例 第四个是：web环境下，同一个session创建一次实例 那么上诉代码只需要修改为 这样就是多实例。 总结： 单实例：在IOC容器启动的时候就已经实例化好Person（调用getPerson实例化），那么每次获取的时候直接从IOC容器中拿。 这段代码运行时候会去调用getPerson方法完成实例 多实例：IOC容器启动时，不会去实例化Person，而是每次获取的时候才会去调用getPerson获取对象。 3.2 @Lazy注解和@Bean注解配套使用，解决单实例bean在IOC容器启动就马上创建实例的问题。 懒加载bean，这个只对于单实例的情况下才有用 也就是IOC容器初始化的时候，不会去调用getPerson，实例化Person。第一次获取的时候才会去创建，以后再使用该实例化，会使用以前获取的 3.3 @Conditional根据满足某个特定的条件创建一个特定的Bean。 因为我们可能存在一个需求那就是，根据不同的业务场景我们会有选择性的实例化某些bean，那么就可以使用这个注解。 1.例子 需求：根据不同的系统实例化不同的bean。 输出： l 增加需求 – 当使用windows系统时，在IOC容器中创建windows实体类，反之创建linux 实体类 \\1. 实现两个 条件类 2.配置类 \\2. 实例化IOC 容器 你会发现仅仅只是实例化了windows，linux实体类已经不见了，那么说明是条件生效了。 备注： @Conditional注解是可以作用在配置类上面的，那么他的作用就是全局的条件，只有满足了这个条件，配置类里面的bean才能够实例化。（局部方法配置Conditional注解会失效） 4.生命周期我们知道，Bean的生命周期是由IOC容器来管理的，那么我们也是可以自定义初始化方法和销毁方法。 Bean生命周期：bean创建-初始化-销毁，那么下面我们将来介绍，能够控制Bean生命周期的几种方式。 4.1. init-method、destory-method 管理bean生命周期需要注意的是，单实例和多实例的情况下，bean生命周期是不一样的。单实例bean的生命周期全部托管给IOC容器，多实例部分托管。 1.在配置文件XML中： 这两个方法是来控制初始化和销毁的 2.代码控制初始化和销毁单实例情况下其实也就是在Bean注解，上填充init-method和destory-method方法 输出： 因为是单实例的原因-所以容器启动的时候就开始调用了无参构造器创建对象，然后调用init初始化方法，容器关闭时，调用销毁方法. 多实例情况下我们把配置类获取Car对象的方法改为多实例的形式，观察输出。 输出 我们发现我们在关闭容器的时候，他并没有调用destory销毁实例，因为多实例的bean他是不归于容器管辖，需要我们自己手动销毁 3.总结总的来说，bean在IOC容器的生命周期如下： * 我们可以自定义初始化和销毁方法；容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法* * 构造（对象创建）* 单实例：在容器启动的时候创建对象* 多实例：在每次获取的时候创建对象* * BeanPostProcessor.postProcessBeforeInitialization* 初始化：* 对象创建完成，并赋值好，调用初始化方法。。。* BeanPostProcessor.postProcessAfterInitialization* 销毁：* 单实例：容器关闭的时候* 多实例：容器不会管理这个bean；容器不会调用销毁方法； 4.2 InitializingBean和DisposableBean 控制bean生命周期1单实例情况下1.新建Food类实现这两个接口 配置类扫描 测试 输出： 很明显单实例情况下，bean的生命周期是全部托管到IOC容器中。 2.多实例情况下如果Food注入IOC容器时，选择多实例的方式的话，那么上面的案例在启动IOC容器时，不会有任何输出，因为多实例的情况下只有获取对象才会去做相关的初始化工作。 验证1： 没有任何输出。 验证2： 输出： 3.总结 很明显没有调用DisposableBean接口的destory方法和自定义的destory方法。 也就是说在多实例的情况下IOC容器只帮我们做创建和初始化bean的工作，但是销毁bean的工作他没有帮我们做，需要自己去实现。 4.3 @PostConstruct和@PreDestroy注解这两个注解是作用在方法上面的。 可以使用JSR250规范里面定义的两个注解： @PostConstruct :在bean创建完成并且属性赋值完成，来执行初始化方法 @PreDestroy ：在容器销毁bean之前通知我们来进行清理工作 初始化容器 输出： 很明显-这两个注解的作用比4.2章节的两个接口的重载方法的调用更早，注意看官方的 注释说明 注意：这两个注解注解的方法，无返回值（void） 4.4 BeanPostProcessor 后置处理接口（重要）我们发现上面三种管理bean生命周期的方式，他们的方法是没有入参和出参的。，下面这种方式提供了 BeanPostProcessor接口：bean的后置处理器，在bean初始化前后做一些处理工作，这个接口有两个方法： postProcessBeforeInitialization：在初始化之前工作 postProcessAfterInitialization：在初始化之后工作 （1）. 实现Food实体类 （2）.配置类 （3）. 启动容器查看输出 （4）输出 他没有销毁方法。 4.1-4.4总结上面这四种方式调用顺序 对象构造器 –&gt;&gt; PostConstruct -&gt;&gt; afterPropertiesSet -&gt;&gt; init-method -&gt;&gt; -&gt;&gt; -&gt;&gt;PreDestroy注解 自定义实现的destory方法-&gt;&gt; DisposableBean的destroy方法 -&gt;&gt;Food 自定义实现的destoryMethod方法 销毁： @PreDestroy注解的 PreDestroy —》DisposableBean接口的destory —》 destroy-method 4.5BeanPostProcessor 原理1.两个方法打上断点 Dubug方式启动IOC容器 2.查看方法栈调用 创建容器构造器 前置处理器调用的方法：调用getBeanPostProcessors()方法找到容器里面的所有的BeanPostProcessor，挨个遍历，调用BeanPostProcessor的postProcessBeforeInitialization方法，一旦调用postProcessBeforeInitialization方法的返回值为null的时候，就直接跳出遍历 ，后面的BeanPostProcessor 的postProcessBeforeInitialization也就不会执行了： 后置处理器调用的方法：调用getBeanPostProcessors()方法找到容器里面的所有的BeanPostProcessor，挨个遍历，调用BeanPostProcessor的postProcessAfterInitialization方法，一旦调用postProcessAfterInitialization方法的返回值为null的时候，就直接跳出遍历 ，后面的BeanPostProcessor 的postProcessAfterInitialization也就不会执行了： 2.BeanPostProcessor在springboot中的使用查看该接口的实现类 这个接口，其实在spring的IOC容器中使用的频率是很多的，而且spring提供了很多实现类，例如如果我们想在bean中使用IOC容器的话，那么就可以使用 1.ApplicationContextAwareProcessor给实体类，注入IOC容器。 ApplicationContextAware 接口，注入IOC容器 例如：Dog实体类需要使用到IOC容器，那么就可以实现这个接口 然后 他实际上是去 调用这个ApplicationContextAwareProcessor，方法，在创建Dog 对象他会去调用 postProcessBeforeInitialization 方法，判断实例化 然后判断当前Dog实体类是否实现了ApplicationContextAware，如果是，那么调用invokeAwareInterface注入，IOC容器。 最终去调用 Dog的 setApplicationContext 方法，赋值。 1. BeanValidationPostProcessor实体类校验，后置处理器 2.InitDestroyAnnotationBeanPostProcessor这个处理类，就是处理，我们3.3章节的两个注解。 3.AutowiredAnnotationBeanPostProcessor这个类就是处理我们的Autoware注解的 4. BeanFactoryPostProcessorBeanFactory的后置处理器，在BeanFactory的标准初始化之后调用 所有bean的定义已经保存加载到BeanFactory，但是bean的实例还未创建 运行IOC容器 查看输出： 很明显他是在bean实例创建之前执行的。 BeanFactoryPostProcessor原理: 1)、ioc容器创建对象 2)、invokeBeanFactoryPostProcessors(beanFactory); 如何找到所有的BeanFactoryPostProcessor并执行他们的方法； 1）、直接在BeanFactory中找到所有类型是BeanFactoryPostProcessor的组件，并执行他们的方法 2）、在初始化创建其他组件前面执行 5.BeanDefinitionRegistryPostProcessor 启动ioc容器查看输出 6. ApplicationListener 5.属性赋值5.1 @Value注解这个注解一般是作用在类的属性上面，他的作用等同于 那么他可以书写那些值呢？ 第三种是取配置文件的数据 那么怎么使用 第三种方式赋值呢？下面讲解 5.2 @ PropertySource 注解他的作用相当于XML的： 1.配置类添加配置注解 2.Person实体类 输出： 我们也可以通过IOC容器手动的去获取配置的信息 3.3 通过实现 EmbeddedValueResolverAware 获取属性值l 见6.5 章节 6.自动装配6.1 @Autowire、@Qualifier、@Primary（spring规范的注解）在spring的项目中我们是经常这个Autowire来进行实体类之间的依赖注入，他的注入规则是： \\1. 默认按照类型去IOC容器中查找需要的实体类（例如UserDao.class） \\2. 如果找到多个同类型的实体类，那么他会根据属性名作为组件ID去进一步匹配。 例如： 然后IOC容器中有两个UserDao实例，一个是ID为userDao，一个ID为userDao1. 那么上面service注入的是哪一个呢？ 答案：注入的是ID为userDao的实体类。如果想要注入userDao1，那么应该把属性名改为userDao1 @Qualifier，指定需要装配的ID，取消默认根据属性名去匹配。 默认是必须找到需要的依赖实体类，然后注入Service，否则就会报错，我们可以使用required属性来控制 @Primary ： 这个注解是作用在被依赖的实体类（UserDao）上面，明确指定，当某个类（UserService）依赖这个实体类的时候，假设IOC容器中存在多个相同类型的被依赖类（UserDao）那么首选呗Primary注解的被依赖类。（ 如果UserService同时使用了@Qualifier注解 ，那么@Primary的效果将会失效，以Qualifier注解需要的ID为主 @Autowire注解扩展他可以标在构造器上，方法上 6.2 @Resource、@Inject（java规范的注解）![1567417242591](spring注解-辅助学习springboot和springcloud\\1567417242591.png) @Resource注解 他的作用跟@Autowire注解的作用是一样的，默认根据属性名进行装配。Name属性可以更改装配的id @Inject 的使用，需要添加依赖 支持@Primary功能，但是他没有属性 6.3 Aware接口 （重要）自定义组件想要使用Spring容器底层的一些组件（ApplicationContext、BeanFactory…） 自定义组件实现xxxAware接口就可以实现，在创建对象的时候，会调用接口规定的方法注入相关的组件，把Spring底层的一些组件注入到自定义的bean中。 xxxAware等这些都是 利用后置处理器的机制，比如ApplicationContextAware 是通过ApplicationContextAwareProcessor来进行处理的。 如果我们想在自定义实体类中，使用IOC容器的context怎么办呢？ 例子：我有在第四章节中 BeanPostProcessor中讲过。 例子： 1.下面我们就是用一个例子来详细讲解一下Aware接口的工作流程。（1）.实现一个entity，实现ApplicationContextAware 接口 实现该接口的setApplicationContext方法。 （2）. 配置类，配置Blue实体类，实例化到IOC容器中 （3）.获取IOC容器 （4）输出 2.源码分析在setApplicationContext 打个断点。 发现他是去调用 ApplicationContextAwareProcessor 实体类，这个实体类实现了BeanPostProcessor 后置处理器。 2.执行postProcessBeforeInitialization 前置方法，判断当前的实体类是否继承了某些接口。做一些权限判断 3.然后调用 invokeAwareInterfaces ，紧接着调用实体类实现的 方法，注入IOC容器 6.4 @Profile注解和Springboot的profile是一致的。 @profile注解是spring提供的一个用来标明当前运行环境的注解。我们正常开发的过程中经常遇到的问题是，开发环境是一套环境，qa测试是一套环境，线上部署又是一套环境。这样从开发到测试再到部署，会对程序中的配置修改多次，尤其是从qa到上线这个环节，让qa的也不敢保证改了哪个配置之后能不能在线上运行。 为了解决上面的问题，我们一般会使用一种方法，就是配置文件，然后通过不同的环境读取不同的配置文件，从而在不同的场景中跑我们的程序。 那么，spring中的@profile注解的作用就体现在这里。在spring使用DI来依赖注入的时候，能够根据当前制定的运行环境来注入相应的bean。最常见的就是使用不同的DataSource了。 下面-结合 properties配置文件的三种注入方式来讲解一下@Profile注解的用法 —-以作用在方法上，表示只要在当前设置的环境下才会往IOC容器中注册当前bean。 —-作用在类上面 类里面的所有bean，能够被注册到IOC容器中的条件是：只要开发环境满足了当前配置类上面的Prifile注解标识的环境。 例如： 主要开发环境是test 里面的bean才能够被注册。 - 6.5 @profile的使用我们知道，如果在组件上标识了这个注解，那么如果没有激活，那么就不会被注册到IOC容器中。通过这个特性来过滤一些组件的注册。 @Profile(“default”) 是默认注册某个bean 1.配置类 2.测试 输出： 很明显，三个配置都没有被注册在IOC容器中，因为没有指定运行环境。 \\3. 制定运行环境（第一种方式：虚拟机参数位置添加-Dspring.profiles.active=test） 这样 就会输出了。 \\4. 制定运行环境（第二种方式-代码方式） 可以指定多个配置环境 输出： 7.AOP 什么叫AOP和他的作用 在程序运行期间，动态的将某段代码切入到指定方法运行时的指定时机运行，其实就是动态代理。 作用场景 可以在某个业务实现的过程前后，或者出现异常，进行一些额外业务的操作。例如当你调用add()方法进行加法运算的时候，我们可以在调用方法前，得到结果后，或者出现异常时，记录一些日志。以前我们传统的做法是，在方法里面打印日志（System.out.println）,但是这样会造成耦合，而且我们也想把打印日志抽离成一个统一的模块。 1. 例子 Maven依赖：spring提供了对AOP的支持 （1）导入aop依赖&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-aspects --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;4.3.14.RELEASE&lt;/version&gt;&lt;/dependency&gt; （2）MathCalculator.java业务逻辑类：要求在业务方法运行时打印日志 （3）：日志切面类：LogAspects.javapackage com.kingge.aop;import java.util.Arrays;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;/** * 切面类 * @Aspect： 告诉Spring当前类是一个切面类 * */@Aspectpublic class LogAspects &#123; //抽取公共的切入点表达式 //1、本类引用 //2、其他的切面引用 @Pointcut(&quot;execution(public int com.kingge.aop.MathCalculator.*(..))&quot;) public void pointCut()&#123;&#125;; //@Before在目标方法之前切入；切入点表达式（指定在哪个方法切入） @Before(&quot;pointCut()&quot;) public void logStart(JoinPoint joinPoint)&#123; Object[] args = joinPoint.getArgs(); System.out.println(&quot;&quot;+joinPoint.getSignature().getName()+&quot;运行。。。@Before:参数列表是：&#123;&quot;+Arrays.asList(args)+&quot;&#125;&quot;); &#125; @After(&quot;com.kingge.aop.LogAspects.pointCut()&quot;) public void logEnd(JoinPoint joinPoint)&#123; System.out.println(&quot;&quot;+joinPoint.getSignature().getName()+&quot;结束。。。@After&quot;); &#125; //JoinPoint一定要出现在参数表的第一位 @AfterReturning(value=&quot;pointCut()&quot;,returning=&quot;result&quot;) public void logReturn(JoinPoint joinPoint,Object result)&#123; System.out.println(&quot;&quot;+joinPoint.getSignature().getName()+&quot;正常返回。。。@AfterReturning:运行结果：&#123;&quot;+result+&quot;&#125;&quot;); &#125; @AfterThrowing(value=&quot;pointCut()&quot;,throwing=&quot;exception&quot;) public void logException(JoinPoint joinPoint,Exception exception)&#123; System.out.println(&quot;&quot;+joinPoint.getSignature().getName()+&quot;异常。。。异常信息：&#123;&quot;+exception+&quot;&#125;&quot;); &#125;&#125; 这四个方法，都是作用在MathCalculator的add方法，那么他们的切入点表达式都是一样的，为了避免重复书写，我们一般采用抽取公共切入点的方式，抽取出来，复用。---- 使用@PoinCut注解 切面类中的方法也称为通知方法： 前置通知(@Before)：在目标方法运行之前运行 后置通知(@After)：在目标方法运行之后运行，即使出现异常也会运行 返回通知(@AfterReturning)：在目标方法正常返回之后运行 异常通知(@AfterThrowing)：在目标方法运行出现异常之后运行 环绕通知(@Around)：动态代理，手动推进目标方法的运行 （4）开启spring切面自动代理 **使用Spring的切面需要开启Spring的切面自动代理，只需要在配置类中加注解@EnableAspectJAutoProxy，Spring中有很多@EnableXxx（关于这点我们在springcloud中使用的最多，自动配置）注解，用来开启一些功能** 配置bean怎么区分哪个bean是切面类呢，它会看哪个类上有@Aspect注解，另外切面方法的执行仅对Spring容器中的bean起作用，对于我们自己new出来的对象是不起作用的，原因也很简单，我们自己创建的bean并没有被spring管理，也就没有为其设置切面方法等。 通过JoinPoint对象获取调用目标方法时的信息，比如方法名、参数等，使用returning指定用通知方法的哪个入参接收返回值，使用throwing指定用哪个入参接收异常，另外如果使用JoinPoint，则必须将其放在切面方法入参的第一个位置，否则会报错 （5）测试 正常计算 错误计算 给一个出现异常的 1 /0 运算处理，查看日志。 你会发现，无论是否出现异常 logStart 和 logEnd 都会正常输出，如果正常返回那么@AfterReturning标识的方法会被调用，如果运算发生异常那么@AfterReturning标识的方法不会被调用，而@AfterThrowing标识的异常处理方法会被调用 总结 2. AOP原理通过上面的例子我们知道，实现AOP的关键点在于我么能使用@EnableAspectJAutoProxy注解，那么接下来我们查看一下这个注解到底做了什么工作 1.查看@EnableAspectJAutoProxy注解 两个属性的含义： 英文注解已经很详细了,这里简单介绍一下两个参数,一个是控制aop的具体实现方式,为true 的话使用cglib,为false的话使用java的Proxy,默认为false,第二个参数控制代理的暴露方式,解决内部调用不能使用代理的场景，默认为false 2.查看一下 AspectJAutoProxyRegistrar.java 到底导入了哪些类 很明显这个类是采用了ImportBeanDefinitionRegistrar的方式注册了某些类大oIOC容器中，那么我们看一下他到底注入了什么类。 核心是这里： AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry); 一个AOP的工具类,这个工具类的主要作用是把AnnotationAwareAspectJAutoProxyCreator这个类定义为BeanDefinition放到spring容器中,这是通过实现ImportBeanDefinitionRegistrar接口来装载的,具体装载过程不是本篇的重点,这里就不赘述,我们重点看AnnotationAwareAspectJAutoProxyCreator这个类. 从类图是可以大致了解AnnotationAwareAspectJAutoProxyCreator这个类的功能.它实现了一系列Aware的接口,在Bean装载的时候获取BeanFactory(Bean容器),Bean的ClassLoader,还实现了order接口,继承了PorxyConfig,ProxyConfig中主要封装了代理的通用处理逻辑,比如设置目标类,设置使用cglib还是java proxy等一些基础配置. 而能够让这个类参与到bean初始化功能,并为bean添加代理功能的还是因为它实现了BeanPostProcessor这个接口.这个接口的postProcessAfterInitialization方法会在bean初始化结束后(赋值完成)被调用。 3.最顶部的抽象类:AbstractAutoProxyCreator注意看bean初始化的方法 当我们开启了EbableAspectJAutoProxy后,每次Bean的装配时,都会执行这段逻辑.前面主要是校验是否需要对bean进行代理(特殊的类,和已经被代理),核心逻辑在后面几行.getAdvicesAndAdvisorsForBean方法来获取所有符合条件的切面,具体的实现在子类,这里是抽象方法,获取切面后就是创建代理: TargetSource中存放被代理的对象,这段代码主要是为了构建ProxyFactory,将配置信息(是否使用java proxy,是否threadlocal等),目标类,切面,传入ProxyFactory中,而在ProxyFactory中,会通过createAopProxy()方法创建代理工厂DefaultAopProxyFactory,由代理厂生成具体的代理对目标类进行代理: 进入proxyFactory.getProxy(getProxyClassLoader()); 的getProxy()方法 跳到 进入createAopProxy()，跳转到 我们可以查看AopProxy的都有哪些 ，在AOpProxy上按键：ctrl t， 很明显有我们熟悉的cglib和jdk、默认的实现 紧接着进入createAopProxy(this) 是个接口，查看他的默认实现类。 4. DefaultAopProxyFactory aop代理获取类 可以看到,在这里有我们在注解中设置的参数的判断逻辑,是创建java代理,还是cglib代理，有关cglib的讲解请看cglib的使用. 我们主要看一下JdkDynamicAopProxy的实现，因为我们没有设置@EnableAspectJAutoProxy(proxyTargetClass=true) 所以我们默认使用jdk自带实现。cglib其实差不多。 5. JdkDynamicAopProxy 默认切面代理类@Override public Object getProxy() &#123; return getProxy(ClassUtils.getDefaultClassLoader()); &#125; @Override public Object getProxy(@Nullable ClassLoader classLoader) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Creating JDK dynamic proxy: target source is &quot; + this.advised.getTargetSource()); &#125; Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); &#125; findDefinedEqualsAndHashCodeMethods方法是为了查询被代理的接口是否包括equals和hashcode方法，这会影响到下面的调用。 可以看到InvocationHandler的实现就是this。我们看一下invoke方法的实现： @Override @Nullable public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; 关键代码 // Get the interception chain for this method. List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); 构建代理链，因为一个方法可能有多个切点匹配上，这个时候就需要构建一个链式的执行结构。 进入getInterceptorsAndDynamicInterceptionAdvice（）方法 这里做了一个缓存，虽然new了一个对象作为key，但是对象的equals和hashcode方法都被重写了，所以没有问题，我们主要来看一下它是如何组装这个链式处理结构的： 进入getInterceptorsAndDynamicInterceptionAdvice（）方法，紧接着发现是一个接口，那么查看他的实现类 6.DefaultAdvisorChainFactory 处理链式切点 可以看到，它会遍历自己的所有切点，那这些advisor是从哪里来的呢： 还记得最开始,我们说过,AbstractAutoProxyCreator中通过getAdvicesAndAdvisorsForBean方法来装载切面,而这个是一个抽象方法,现在来看它的实现,在AbstractAdvisorAutoProxyCreator中: @Override @Nullable protected Object[] getAdvicesAndAdvisorsForBean(Class&lt;?&gt; beanClass, String beanName, @Nullable TargetSource targetSource) &#123; List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); if (advisors.isEmpty()) &#123; return DO_NOT_PROXY; &#125; return advisors.toArray(); &#125;protected List&lt;Advisor&gt; findEligibleAdvisors(Class&lt;?&gt; beanClass, String beanName) &#123; List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors; &#125; findCandidateAdvisors又是一个抽象方法,主要功能就是找到候选的切面,为什么是候选的,因为它是加载了所有的切面,有些切面并不需要,在最底层AnnotationAwareAspectJAutoProxyCreator的实现类中也有:protected List&lt;Advisor&gt; findCandidateAdvisors() &#123; // Add all the Spring advisors found according to superclass rules. List&lt;Advisor&gt; advisors = super.findCandidateAdvisors(); // Build Advisors for all AspectJ aspects in the bean factory. if (this.aspectJAdvisorsBuilder != null) &#123; advisors.addAll(this.aspectJAdvisorsBuilder.buildAspectJAdvisors()); &#125; return advisors; &#125; 可以看到,通过aspectJAdvisorsBuilder来将该类关心的所有的切面装载进来,并添加到父类的集合里面.aspectJAdvisorsBuilder里缓存了advisor的信息,拿到切面后,通过findAdvisorsThatCanApply方法来筛选合适的切面,之后对切面进行排序(如果实现了Order接口),然后返回切面的链表. 8.声明式事务@Transactional注解1.前言我们知道spring的事务管理分为两大部分：声明式和编程式，两种方式均为我们提供便捷的事务管理方法，各自优劣。 声明式事务 声明式的事务管理对业务代码基本0入侵，能够很好的把事务管理和业务代码剥离开来，提高代码扩展性和可读性但是控制的粒度只能是方法级别而且必须是public，同时还不能在一个类中调用等。 编程式事务 编程式事务则需要通过编写具体的事务代码来获得事务的管理能力，TransactionTemplate，或者直接使用PlatformTransactionManager，好处是控制粒度小，没有太多限制，坏处就是对业务代码有入侵，如果事务需要嵌套或者事务本身很繁琐，使用编程式则会十分麻烦。 这里讲述的是声明式事务，因为他比较常用。而且两种方式的源码其实是一样的。 2. 环境搭建1.1导入相关依赖：数据源、数据库驱动、Spring-jdbc模块 1.2配置数据源、JdbcTemplate（Spring提供简化数据库操作的工具）操作数据 1.3新建PersonDao、PersonService 1.4 测试 插入成功 1.5 测试事务修改Service方法，故意暴露一个异常 我们运行测试，发现还是插入成功，那么怎么阻止这种行为呢？添加事务 1.6 添加事务 给insertUser方法添加注解- @Transactional @EnableTransactionManagement 开启基于注解的事务管理功能 配置事务管理器来控制事务 事务添加成功 1.8 再次运行测试发现插入失败，满足事务的原子性。 1.7 源码分析声明式事务： 环境搭建：1、导入相关依赖 数据源、数据库驱动、Spring-jdbc模块2、配置数据源、JdbcTemplate（Spring提供的简化数据库操作的工具）操作数据3、给方法上标注 @Transactional 表示当前方法是一个事务方法；4、 @EnableTransactionManagement 开启基于注解的事务管理功能； @EnableXXX5、配置事务管理器来控制事务; @Bean public PlatformTransactionManager transactionManager() 原理：1）、@EnableTransactionManagement 利用TransactionManagementConfigurationSelector给容器中会导入组件 导入两个组件 AutoProxyRegistrar ProxyTransactionManagementConfiguration2）、AutoProxyRegistrar： 给容器中注册一个 InfrastructureAdvisorAutoProxyCreator 组件； InfrastructureAdvisorAutoProxyCreator：？ 利用后置处理器机制在对象创建以后，包装对象，返回一个代理对象（增强器），代理对象执行方法利用拦截器链进行调用； 3）、ProxyTransactionManagementConfiguration 做了什么？ 1、给容器中注册事务增强器； 1）、事务增强器要用事务注解的信息，AnnotationTransactionAttributeSource解析事务注解 2）、事务拦截器： TransactionInterceptor；保存了事务属性信息，事务管理器； 他是一个 MethodInterceptor； 在目标方法执行的时候； 执行拦截器链； 事务拦截器： 1）、先获取事务相关的属性 2）、再获取PlatformTransactionManager，如果事先没有添加指定任何transactionmanger 最终会从容器中按照类型获取一个PlatformTransactionManager； 3）、执行目标方法 如果异常，获取到事务管理器，利用事务管理回滚操作； 如果正常，利用事务管理器，提交事务 3.源码深入分析未完待续 9 总结通过使用AOP和声明式事务，我们知道了一个套路，如果我们想使用某项功能，例如上面的aop和声明式事务、或者以后的springcloud的eureka、zull、feign等等功能，都遵循一下三点： 1.导入功能组件相关的依赖 2.在配置类开启组件（@EnableXXXX） 3.在关键位置标示使用的地方（例如@Aspect、@Transactional） 所以以后需要在spring中使用某个组件，一般都是遵循这样的思路 #","categories":[{"name":"spring注解和生命周期","slug":"spring注解和生命周期","permalink":"http://kingge.top/categories/spring注解和生命周期/"}],"tags":[{"name":"spring注解","slug":"spring注解","permalink":"http://kingge.top/tags/spring注解/"},{"name":"aop","slug":"aop","permalink":"http://kingge.top/tags/aop/"},{"name":"spring生命周期","slug":"spring生命周期","permalink":"http://kingge.top/tags/spring生命周期/"}]},{"title":"持续集成技术总结","slug":"持续集成技术总结","date":"2019-03-08T15:59:59.000Z","updated":"2019-08-25T04:37:53.613Z","comments":true,"path":"2019/03/08/持续集成技术总结/","link":"","permalink":"http://kingge.top/2019/03/08/持续集成技术总结/","excerpt":"","text":"一、DockerMaven插件的使用这个插件的目的就是，能够把本地项目远程打包到docker，并生成相应的镜像。 微服务部署有两种方法： （1）手动部署：首先基于源码打包生成jar包（或war包）,将jar包（或war包）上传至虚 拟机并拷贝至JDK容器。 （2）通过Maven插件自动部署。 对于数量众多的微服务，手动部署无疑是非常麻烦的做法，并且容易出错。所以我们这 里学习如何自动部署，这也是企业实际开发中经常使用的方法 而且使用dockermaven插件自动部署项目，有两种方式，一种是编写Dockerfile方式，一种是纯xml方式。第一种方式在本人的总结的《Docker总结》第十二章有写到。下面的案例主要是讲解通过xml的方式部署项目。 其实他们本质上是一样的 Maven插件自动部署步骤：（1）修改宿主机的docker配置，让其可以远程访问 Vi /lib/systemd/system/docker.service 其中ExecStart=后添加配置 ‐H tcp://0.0.0.0:2375 ‐H unix:///var/run/docker.sock 修改后如下： （2）刷新配置，重启服务 systemctl daemon‐reload systemctl restart docker docker start registry 注意：这里的registry是本人新建的本地仓库（怎么新建本地仓库以及怎么使用，参见docker的文章）-当然你也可以把镜像提交到阿里云镜像仓库中。 使用SpringBoot2.0+DockerFile+Maven插件构建镜像并推送到阿里云仓库 https://blog.csdn.net/haogexiang9700/article/details/88318867 （3）在工程pom.xml 增加配置 &lt;build&gt; &lt;finalName&gt;app&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring‐boot‐maven‐plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker‐maven‐plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt;&lt;imageName&gt;192.168.1.105:5000/$&#123;project.artifactId&#125;:$&#123;project.version&#125; &lt;/imageName&gt;&lt;baseImage&gt;jdk1.8&lt;/baseImage&gt; &lt;entryPoint&gt;[&quot;java&quot;, &quot;‐jar&quot;, &quot;/$&#123;project.build.finalName&#125;.jar&quot;]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125; &lt;/directory&gt;&lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;dockerHost&gt;http://192.168.1.105:2375&lt;/dockerHost&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; app org.springframework.boot spring‐boot‐maven‐plugin com.spotify docker‐maven‐plugin 0.4.13 192.168.1.105:5000/${project.artifactId}:${project.version} jdk1.8 [“java”, “‐jar”, “/${project.build.finalName}.jar”] / ${project.build.directory} ${project.build.finalName}.jar http://192.168.1.105:2375 以上配置会自动生成Dockerfile FROM jdk1.8 ADD app.jar / ENTRYPOINT [“java”,”‐jar”,”/app.jar”] 创建一个镜像，继承自jdk1.8，同时拷贝本地的app.jar到镜像的根目录。在容器启动时，执行java –jar /app.jar(也就是启动项目) （4）在windows的命令提示符下，进入工程app所在的目录，进行打包和上传镜像 mvn install mvn docker:build ‐DpushImage 执行后，会有如下输出，代码正在上传 （6）进入宿主机 , 查看镜像 Docker images （7）启动容器 访问成功！！！ 二、持续集成工具Jenkins官网：https://jenkins.io/zh/doc/tutorials/build-a-java-app-with-maven/ 主要是解决自动化构建项目，发布项目等等问题。 微服务架构下，带来了运维上的额外复杂性，尤其是在服务部署和服务监控上。单体应用是集中式的，就一个单体跑在一起，部署和管理的时候非常简单。而微服务是一个网状分布的，有很多服务需要维护和管理，对它进行部署和维护的时候则比较复杂，所以就需要一个工具能够自动化的完成自动编译、发布和测试，从而尽快地发现集成错误，让团队能够更快的开发内聚的软件。 **持续集成具有的特点：** 它是一个自动化的周期性的集成测试过程，从检出代码、编译构建、运行测试、结果 记录、测试统计等都是自动完成的，无需人工干预； 需要有专门的集成服务器来执行集成构建； 需要有代码托管工具支持，我们下一小节将介绍Git以及可视化界面Gogs的使用 持续集成的作用： 保证团队开发人员提交代码的质量，减轻了软件发布时的压力； 持续集成中的任何一个环节都是自动完成的，无需太多的人工干预，有利于减少重复 过程以节省时间、费用和工作量； 基本思路： 1.Jenkins简介 Jenkins，原名Hudson，2011年改为现在的名字，它 是一个开源的实现持续集成的 软件工具。官方网站：http://jenkins-ci.org/。 Jenkins 能实施监控集成中存在的错误，提供详细的日志文件和提醒功能，还能用图 表的形式形象地展示项目构建的趋势和稳定性。 特点： 易安装：仅仅一个 java -jar jenkins.war，从官网下载该文件后，直接运行，无需额 外的安装，更无需安装数据库； 易配置：提供友好的GUI配置界面； 变更支持：Jenkins能从代码仓库（Subversion/CVS）中获取并产生代码更新列表并 输出到编译输出信息中； 支持永久链接：用户是通过web来访问Jenkins的，而这些web页面的链接地址都是 永久链接地址，因此，你可以在各种文档中直接使用该链接； 集成E-Mail/RSS/IM：当完成一次集成时，可通过这些工具实时告诉你集成结果（据 我所知，构建一次集成需要花费一定时间，有了这个功能，你就可以在等待结果过程 中，干别的事情）； JUnit/TestNG**测试报告：**也就是用以图表等形式提供详细的测试报表功能； 支持分布式构建：Jenkins可以把集成构建等工作分发到多台计算机中完成； 文件指纹信息：Jenkins会保存哪次集成构建产生了哪些jars文件，哪一次集成构建使 用了哪个版本的jars文件等构建记录； 支持第三方插件：使得 Jenkins 变得越来越强大 2. Jenkins安装资源准备 当然你可以通过wget命令进行下载相应的rpm包 2.1 JDK安装（1）将jdk-8u171-linux-x64.rpm上传至服务器（虚拟机） （2）执行安装命令 rpm ‐ivh jdk‐8u171‐linux‐x64.rpm （3）查看是否安装成功 因为rpm包已经自动为我们做了相关的环境变量配置，所以不需要额外配置（相关的命令在 /usr/bin） 2.2 maven安装（1）将 apache‐maven‐3.5.4‐bin.tar.gz上传至服务器（虚拟机） （2）解压 tar zxvf apache‐maven‐3.5.4‐bin.tar.gz （3））编辑setting.xml配置文件 vi /opt/software/apache-maven-3.5.4/conf/settings.xml 增加如下配置 /opt/software/mavenRepostory （4）将开发环境（win10）的本地仓库上传至服务器（虚拟机）并移动到/opt/software/mavenRepostory 执行此步是为了以后在打包的时候不必重新下载，缩短打包的时间 2.3 Jenkins安装与启动在安装jenkins之前必须要安装jdk。 （1）下载jenkins wget https://pkg.jenkins.io/redhat/jenkins‐2.83‐1.1.noarch.rpm 或将应下载好的jenkins-2.83-1.1.noarch.rpm上传至服务器 https://pkg.jenkins.io/redhat-stable/ 官网下载（建议下载最新版本，避免安装插件时报错 jenkins-2.187-1.1.noarch.rpm） （2）安装jenkins rpm ‐ivh jenkins‐2.83‐1.1.noarch.rpm 自动安装完成之后： /usr/lib/jenkins/jenkins.war WAR**包** /etc/sysconfig/jenkins 配置文件 /var/lib/jenkins/ 默认的JENKINS_HOME目录 /var/log/jenkins/jenkins.log Jenkins**日志文件** （3）配置jenkins vi /etc/sysconfig/jenkins 修改用户和端口 JENKINS_USER=”root” JENKINS_PORT=”8888” （4）启动服务 systemctl start Jenkins （5）访问链接 http://49.234.188.74:8888 从/var/lib/jenkins/secrets/initialAdminPassword中获取初始密码串 （6）安装插件 选择推荐安装，避免漏掉一些jenkins必须依赖的插件，后面有额外需要的插件我们还可以手动安装 安装插件中 需要注意的是，在安装插件的可能会失败，一般是因为当前jenkins版本太低的原因，可以先忽略错误，进去后更新jenkins为新的版本，插件自己会自动安装成功。 （7）新建用户 完成安装进入主界面 3. Jenkins插件安装我们以安装maven插件为例，演示插件的安装 （1）点击左侧的“系统管理”菜单 ,然后点点击 4.全局工具配置选择系统管理，全局工具配置 （1）JDK配置 设置javahome为 /usr/java/jdk1.8.0_171-amd64 （2）Git配置 （本地已经安装了Git软件） 如果linux 没有安装过git。那么可以参考 《额外补充》第三章节进行安装。 如果不知道本地git的地址可以使用命令：whereis git 获取地址。 默认在启动jenkins时，应安装好了git插件 （3）Maven配置 然后保存即可，接下来上传代码到git，然后创建任务。 5 代码上传至Git服务器5.1 Gogs搭建与配置Gogs 是一款极易搭建的自助 Git 服务。 Gogs 的目标是打造一个最简单、最快速和最轻松的方式搭建自助 Git 服务。使用 Go 语 言开发使得 Gogs 能够通过独立的二进制分发，并且支持 Go 语言支持的 所有平台，包 括 Linux、Mac OS X、Windows 以及 ARM 平台 地址：https://gitee.com/Unknown/gogs （1）下载镜像 docker pull gogs/gogs （2）创建容器 docker run -d -p 10022:22 -p 10080:3000 \\ –name=gogs \\ -v /opt/docker/gogs/:/data \\ gogs/gogs （3）安装gogs 在地址栏输入 http://49.234.188.74: 10080/，会进入首次运行安装程序页面，我们可以选择一种数据 库作为gogs数据的存储，最简单的是选择SQLite3。如果对于规模较大的公司，可以选择 MySQL 点击“立即安装” 这里的域名要设置为centos的IP地址,安装后显示主界面 （4）注册 （5）登录 登陆后 （6）创建仓库 5.2 提交代码到git步骤： （1）在本地安装git(Windows版本) （2）在IDEA中选择菜单 : File – settings , 在窗口中选择Version Control – Gi （3）选择菜单VCS –&gt; Enable Version Control Integration （4）设置远程地址: 右键点击工程选择菜单 Git –&gt; Repository –&gt;Remotes… url填写我们之前用gogs创建的git仓库：http://49.234.188.74:10080/root/kingge.git （5）右键点击工程选择菜单 Git –&gt; Add （6）右键点击工程选择菜单 Git –&gt; Commit Directory… （7）右键点击工程选择菜单 Git –&gt; Repository –&gt; Push … 6 jenkins任务的创建与执行（1）回到首页，点击新建按钮 .如下图，输入名称，选择创建一个Maven项目，点击OK （2）源码管理，选择Git （3）填写构建语句 clean package docker:build ‐DpushImage （4）查看项目 我们点击执行后，可以看到左下角出现这个任务进度 这样就能够上传成功了 7.总结通过jenkins，我们可以很好的集成、部署了我们开发的项目，自动构建项目的镜像，这样为我们节省了很多的时间。 三、容器管理工具Rancher管理docker和Kubernetes容器，通过可视化界面的方式，管理，部署，启动docker容器，同时支持分布式集群部署docekr容器。 1.安装RancherRancher是一个开源的企业级全栈化容器部署及管理平台。Rancher为容器提供一揽 子基础架构服务：CNI兼容的网络服务、存储服务、主机管理、负载均衡、防护墙…… Rancher让上述服务跨越公有云、私有云、虚拟机、物理机环境运行，真正实现一键式应 用部署和管理 https://www.cnrancher.com/ （1）下载Rancher 镜像 docker pull rancher/server （2）创建Rancher容器 docker run ‐di ‐‐name=rancher ‐p 9090:8080 rancher/server （3）在浏览器输入地址： http://192.168.184.136:9090 2.初始化Rancher2.1 添加环境Rancher 支持将资源分组归属到多个环境。 每个环境具有自己独立的基础架构资源及服 务，并由一个或多个用户、团队或组织所管理。 例如，您可以创建独立的“开发”、“测试”及“生产”环境以确保环境之间的安全隔离，将“开 发”环境的访问权限赋予全部人员，但限制“生产”环境的访问权限给一个小的团队。 （1） 选择“Default –&gt;环境管理” 菜单 （2）填写名称，点击“创建”按钮 可以添加多个环境，分别给区分不同用户使用场景 （4））你可以通过点击logo右侧的菜单在各种环境下切换 2.2 添加主机（1）选择基础架构–&gt;主机 菜单，点击添加主机 直接保存，当然你也可以选择其他主机（管理非当前服务器） （2）拷贝脚本，主机和rancher服务建立连接 （3）在主机上运行脚本 （4）点击关闭按钮后，会看到界面中显示此主机。我们可以很方便地管理主机的每个容 器的开启和关闭 在这里可以看到当前主机下，已经创建好的容器 2.3 添加应用（1）点击应用–&gt;全部(或用户) ，点击“添加应用”按钮 应用是多个服务的载体。例如一个mysql容器就是一个服务，它提供数据库服务。例如redis也是一个服务等等。 2.4 添加服务相对于docker而言，添加一个服务就等同于创建一个容器 2.4.1 添加mysql服务镜像：mysql:latest 增加mysql数据库服务 打开我们在2.3节创建的应用，然后点击创建服务 点击创建按钮，完成创建。上述操作相当于以下docker命令 docker run ‐di ‐‐name mysql ‐p 3306:3306 ‐e MYSQL_ROOT_PASSWORD=123456 mysql:latest 创建中 创建完毕 2.4.2 添加RabbitMQ服务镜像：rabbitmq:management 端口映射5671 5672 4369 15671 15672 25672 2.4.3 Redis服务 2.5 部署我们在第一章节dockermaven构建的app容器 开始部署服务 点击创建部署中 创建成功 访问 3. rancher的扩容与缩容就是对某个服务（容器）进行数量的增加或者减少，因为我们知道大型的项目一个服务是部署到多台服务器的，构建成一个服务集群，是为了应变访问量的高低。所以rancher也是提供了这种功能。 一下例子，以我们在上面构建的app服务为例子。 （1）部署app服务，不指定端口 这里为什么不指定端口，因为假设我们把服务扩容到两台，那么端口号就会冲突，所以这里让docker为我们默认分配端口。 （2）在选择菜单API –&gt;WebHooks ，点击“添加接收器”按钮 （3）填写名称等信息，选择要扩容的服务，点击创建按钮 创建成功 提供了一个触发扩容操作的url http://192.168.1.105:9090/v1-webhooks/endpoint?key=ctm3rFpH6oAsbOA5mXMHvZyW7BoCkz1QQgpw00ay&amp;projectId=1a5 （4）测试扩容服务 必须使用post的方式，触发上面提供的url 在触发之前，我们先查看一下app服务的数量 是一个 开始触发 扩容成功，数量变成了3 那么这个时候，我么你会有个疑问，我们怎么访问这些服务呢？这些服务并没有提供对外暴露的端口？ 答案是：采用创建负载均衡服务的方式访问 4.负载均衡（1）创建负载均衡服务 创建负载均衡服务中 创建成功 尝试访问 访问成功，他会通过轮询的方式访问这三台容器。 四、时间序列数据库influxDB他主要是用来存储一些实时的数据，例如下面我们将会学习的cAdvisor，监控容器的内存相关的信息，就是存储在influDB中。记住他并不是用来存储大量数据的，不是mysql这样类型的数据库。 1. 什么是influxDB influxDB是一个分布式时间序列数据库。cAdvisor仅仅显示实时信息，但是不存储 监视数据。因此，我们需要提供时序数据库用于存储cAdvisor组件所提供的监控信息， 以便显示除实时信息之外的时序数据。 2 influxDB安装（1）下载镜像 docker pull tutum/influxdb （2）创建容器 docker run ‐di \\ ‐p 8083:8083 \\ ‐p 8086:8086 \\ ‐‐expose 8090 \\ ‐‐expose 8099 \\ ‐‐name influxsrv \\ tutum/influxdb 端口概述： 8083端口:web访问端口 8086:数据写入端口 （后面启动adviosr的时候会用到） docker run -di -p 8083:8083 -p 8086:8086 –expose 8090 –expose 8099 –name influxsrv tutum/influxdb 打开浏览器 http://192.168.1.105:8083/ 3 influxDB常用操作3.1 创建数据库 CREATE DATABASE “cadvisor” 回车创建数据库 查看数据库 SHOW DATABASES 3.2 创建用户并授权 创建用户 CREATE USER “cadvisor” WITH PASSWORD ‘cadvisor’ WITH ALL PRIVILEGES 查看用户 SHOW USRES 用户授权 grant all privileges on cadvisor to cadvisor （on后面跟的是数据库名，to后面跟的是 用户名） grant WRITE on cadvisor to cadvisor grant READ on cadvisor to cadvisor 3.3 查看采集的数据 切换到cadvisor数据库，使用以下命令查看采集的数据 SHOW MEASUREMENTS 现在我们还没有数据，如果想采集系统的数据，我们需要使用Cadvisor软件来实现 五、容器监控工具cAdvisor1 什么是cAdvisor Google开源的用于监控基础设施应用的工具，它是一个强大的监控工具，不需要任何配置就可以通过运行在Docker主机上的容器来监控Docker容器，而且可以监控Docker 主机。更多详细操作和配置选项可以查看Github上的cAdvisor项目文档。 2 cAdvisor安装（1）下载镜像docker pull google/cadvisor （2）创建容器 docker run –volume=/:/rootfs:ro –volume=/var/run:/var/run:rw –volume=/sys:/sys:ro –volume=/var/lib/docker/:/var/lib/docker:ro –publish=8080:8080 –detach=true –link influxsrv:influxsrv –name=cadvisor google/cadvisor -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 我们可以看到启动cadvisor容器时，可以看到storage_driver 指定了存储的用的数据库，storage_driver_db指定了用的数据库名称，storage_driver_host指定写入数据库地址，–link 表示连接到我们在上面创建的influxsrv容器 （3）查看容器WEB前端访问地址 http://192.168.1.105:8080/containers/ 性能指标含义参照如下地址 https://blog.csdn.net/ZHANG_H_A/article/details/53097084 （4）查看influxsrv容器再次查看influxDB，发现已经有很多数据被采集进去了 可以使用select语句查询某张表的数据 3.总结经过上面的学习，cadvisor和influxsrv 的使用我们可以检测采集，以及采集数据保存到influxdb中，但是有个缺点，那就是采集到infludb的数据，通过select语句查询，不是那么直观，那么下面我们将使用grafana图表工具展示influx的数据。 六、图表工具Grafana1 什么是Grafana Grafana是一个可视化面板（Dashboard），有着非常漂亮的图表和布局展示，功能齐全的度量仪表盘和图形编辑器。支持Graphite、zabbix、InfluxDB、Prometheus和 OpenTSDB作为数据源。 Grafana主要特性：灵活丰富的图形化选项；可以混合多种风格；支持白天和夜间模式，多个数据源。 2 Grafana安装（1）下载镜像docker pull grafana/grafana （2）创建容器​ docker run -d -p 3001:3000 -e INFLUXDB_HOST=influxsrv -e INFLUXDB_PORT=8086 -e INFLUXDB_NAME=cadvisor -e INFLUXDB_USER=root -e INFLUXDB_PASS=itcast –link influxsrv:influxsrv –name=grafana grafana/grafana （3）访问http://192.168.1.105:3001 用户名密码均为admin 登录后需要修改密码，密码修改为：123456 （4）之后进入主页面 （5）添加数据源 选择InfluxDB （5）填写数据源信息 点击save 连接成功 （5）添加仪表盘选择Dashboards –Manager 然后选择展示数据的图表样式 这里我们选择第一个，柱状图 （6）设置图表数据源 进入下个界面 点击edit 然后点击右上角保存即可 （6）预警通知设置 （1）选择菜单 alerting–&gt; Notification channels （2）点击Add channel 按钮 （3）填写名称，选择类型为webhook ,填写钩子地址 这个钩子地址是之前对base微服务扩容的地址 （4）点击SendTest 测试 观察基础微服务是否增加容器 （5）点击save保存 （6）按照同样的方法添加缩容地址 6.4.4 仪表盘预警设置 （7）再次打开刚刚编辑的仪表盘 （8）点击 Create Alert 设置预警线 （9）选择通知 保存更改 总结Influxdb和cadvisor、grafana，这三者一般都是一起使用，cadvisor负责往influxdb里面写入监控的数据，grafana负责从infuxdb中读取数据，并做可视化显示。 额外补充1 使用docker安装jenkins在第二章节中我们是使用rpm的方式安装jenkins，其实我们也是可以使用docker镜像的方式安装jenkins。但是这种方式创建数据卷会存在权限问题，所以一般也不常用 https://hub.docker.com/_/jenkins/ 官网。注意查看jenkins容器启动后配置文件在容器中的位置，方便我们设置容器卷链接到本地虚拟机 1.1 下载jenkins镜像docker pull jenkins 1.2 启动jenkins容器在镜像文档里，我们知道Jenkins访问的端口号是8080，另外还需要暴露一个tcp的端口号50000。我们使用如下命令启动Jenkins镜像 1.首先创建本地挂载目录并修改权限 创建目录赋权限 mkdir /home/jenkins ls -nd jenkins chown -R 1000:1000 jenkins/ （设置权限） 为什么需要修改本地数据卷文件的权限呢？ 我们首先查看一下jenkins容器数据卷的权限信息 如果不把全权限修改一致，那么当本地数据卷信息同步到容器数据卷目录的时候，就会出现没有权限的问题。 https://www.cnblogs.com/jackluo/p/5783116.html 可以查看这篇文章。 \\2. docker run -itd -p 6666:8080 -p 6660:50000 –name jenkins –privileged=true -v /home/jenkins:/var/jenkins_home jenkins 1.3 开始安装访问 http://192.168.1.105:6666 查看初始化密码 位置在我们关联的本地数据卷目录中 如果安装过程中出现下面这个错误 说明需要更换jenkin安装插件地址 2. Docker Volume 之权限管理https://www.cnblogs.com/jackluo/p/5783116.html 1.启动Jenkins官方镜像，并检查日志 docker run -d -p 8080:8080 -p 50000:50000 –name jenkins jenkin docker logs jenkins 我们可以发现”jenkins”容器日志显示结果一切正常 2.然而为了持久化Jenkins配置数据，当我们把宿主机当前目录下的data文件夹挂载到容器中的目录”/var/jenkins_home”的时候，问题出现了： docker rm -f jenkins docker run -d -p 8080:8080 -p 50000:50000 -v $(pwd)/data:/var/jenkins_home –name jenkins jenkins docker logs jenkins 错误日志如下 touch: cannot touch ‘/var/jenkins_home/copy_reference_file.log’: Permission denied Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions? \\3. 我们检查一下之前启动方式的”/var/jenkins_home”目录权限，查看Jenkins容器的当前用户: 当前用户是”jenkins”而且”/var/jenkins_home”目录是属于jenkins用户拥有的 docker@default:~$ docker run -ti –rm –entrypoint=”/bin/bash” jenkins -c “whoami &amp;&amp; id” jenkins uid=1000(jenkins) gid=1000(jenkins) groups=1000(jenkins) docker@default:~$ docker run -ti –rm –entrypoint=”/bin/bash” jenkins -c “ls -la /var/jenkins_home” total 20 drwxr-xr-x 2 jenkins jenkins 4096 Jun 5 08:39 . drwxr-xr-x 28 root root 4096 May 24 16:43 .. -rw-r–r– 1 jenkins jenkins 220 Nov 12 2014 .bash_logout -rw-r–r– 1 jenkins jenkins 3515 Nov 12 2014 .bashrc -rw-r–r– 1 jenkins jenkins 675 Nov 12 2014 .profile 而当映射本地数据卷时，/var/jenkins_home目录的拥有者变成了root用户 docker run -ti –rm -v $(pwd)/data:/var/jenkins_home –entrypoint=”/bin/bash” jenkins -c “ls -la /var/jenkins_home” total 4 drwxr-sr-x 2 root staff 40 Jun 5 08:32 . drwxr-xr-x 28 root root 4096 May 24 16:43 .. 这就解释了为什么当”jenkins”用户的进程访问”/var/jenkins_home”目录时，会出现 Permission denied 的问题 我们再检查一下宿主机上的数据卷目录，当前路径下”data”目录的拥有者是”root”，这是因为这个目录是Docker进程缺省创建出来的。 docker@default:~$ ls -la data total 0 drwxr-sr-x 2 root staff 40 Jun 5 08:32 ./ drwxr-sr-x 5 docker staff 160 Jun 5 08:32 ../ 发现问题之后，相应的解决方法也很简单：把当前目录的拥有者赋值给uid 1000，再启动”jenkins”容器就一切正常了。 sudo chown -R 1000 data docker start jenkins 这时利用浏览器访问 “http://192.168.99.100:8080/“ 就可以看到Jenkins的Web界面了。注：如无法访问，可能需要通过docker-machine ip命令获得当前Docker宿主机的IP地址。 当我们再进入容器内部查看”/var/jenkins_home”目录的权限，其拥有者已经变成 “jenkins” 复制代码 docker@default:~$ docker exec jenkins ls -la /var/jenkins_home total 24 drwxr-sr-x 11 jenkins staff 340 Jun 5 09:00 . drwxr-xr-x 28 root root 4096 May 24 16:43 .. drwxr-sr-x 3 jenkins staff 60 Jun 5 08:59 .java -rw-r–r– 1 jenkins staff 289 Jun 5 08:59 copy_reference_file.log … 复制代码 而有趣的是在宿主机上我们看到的 “data”目录的拥有者是”docker”，这是因为”docker”用户在”boot2docker”宿主机上的uid也是”1000”。 docker@default:~$ ls -la data total 20 drwxr-sr-x 2 docker staff 40 Jun 5 11:55 ./ drwxr-sr-x 6 docker staff 180 Jun 5 11:55 ../ … 这时我们已经可以知道：容器的本地数据卷中文件/目录的权限是和宿主机上一致的，只是uid/gid在Docker容器和宿主机中可能映射为不同的用户/组名称。 在上文，我们使用了一个常见的技巧，即在宿主机上执行chown命令时采用了uid而不是具体的用户名，这样就可以保证设置正确的拥有者。 问题虽然解决了，但思考并没有结束。因为当使用本地数据卷时，Jenkins容器会依赖宿主机目录权限的正确性，这会给自动化部署带来额外的工作。有没有方法让Jenkins容器为数据卷自动地设置正确的权限呢？这个问题对很多以non-root方式运行的应用也都有借鉴意义。 为non-root应用正确地挂载本地数据卷 我们可以从万能的stackoverflow.com找到很多相关的讨论，其中一个非常有借鉴意义问答如下 http://stackoverflow.com/questions/23544282/what-is-the-best-way-to-manage-permissions-for-docker-shared-volumes 其中的基本思路有两个： 一个是利用Data Container的方法在容器间共享数据卷。这样就规避了解决宿主机上数据卷的权限问题。由于在1.9版本之后，Docker提供了named volume来取代纯数据容器，我们还需要真正地解决这个问题。 另外一个思路就是让容器中以root用户启动，在容器启动脚本中利用”chown”命令来修正数据卷文件权限，之后切换到non-root用户来执行程序 我们来参照第二个思路来解决这个问题 下面是一个基于Jenkins镜像的Dockerfile：它会切换到”root”用户并在镜像中添加”gosu”命令，和新的入口点”/entrypoint.sh” FROM jenkins:latest USER root RUN GOSU_SHA=5ec5d23079e94aea5f7ed92ee8a1a34bbf64c2d4053dadf383992908a2f9dc8a \\ &amp;&amp; curl -sSL -o /usr/local/bin/gosu “https://github.com/tianon/gosu/releases/download/1.9/gosu-$(dpkg –print-architecture)” \\ &amp;&amp; chmod +x /usr/local/bin/gosu \\ &amp;&amp; echo “$GOSU_SHA /usr/local/bin/gosu” | sha256sum -c - COPY entrypoint.sh /entrypoint.sh ENTRYPOINT [“/entrypoint.sh”] 注释：gosu 是经常出现在官方Docker镜像中的一个小工具。它是”su”和”sudo”命令的轻量级替代品，并解决了它们在tty和信号传递中的一些问题。 新入口点的”entrypoint.sh”的内容如下：它会为”JENKINS_HOME”目录设置”jenkins”的拥有权限，并且再利用”gosu”命令切换到”jenkins”用户来执行”jenkins”应用。 #! /bin/bash set -e chown -R 1000 “$JENKINS_HOME” exec gosu jenkins /bin/tini – /usr/local/bin/jenkins.sh 您可以直接从 https://github.com/denverdino/docker-jenkins 获得相关代码，并构建自己的Jenkins镜像。执行命令如下： git clone https://github.com/AliyunContainerService/docker-jenkins cd docker-jenkins/jenkins docker build -t denverdino/jenkins . 然后基于新镜像启动Jenkins容器 docker rm -f jenkins docker run -d -p 8080:8080 -p 50000:50000 -v $(pwd)/data:/var/jenkins_home –name jenkins denverdino/jenkins 3.linux安装git3.1 使用yum命令进行安装yum install git 安装简单，但是安装的git版本太老，而且版本不好控制，推荐使用下面的方式进行安装 3.2 源码编译安装Git①、获取github最新的Git安装包下载链接，进入Linux服务器，执行下载，命令为： wget https://github.com/git/git/archive/v2.22.0.tar.gz ； ②、压缩包解压，命令为： tar -zxvf v2.22.0.tar.gz ； ③、安装编译源码所需依赖，命令为： yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker 耐心等待安装，出现提示输入y即可； ④、安装依赖时，yum自动安装了Git，需要卸载旧版本Git，命令为： yum remove git 出现提示输入y即可； 很明显自己安装了一个1.8.3.1版本的git，卸载。 ⑤、进入解压后的文件夹，命令 cd git-2.22.0 ，然后执行编译，命令为 make prefix=/usr/local/git all 耐心等待编译即可； ⑥、安装Git至/usr/local/git路径，命令为 make prefix=/usr/local/git install ； ⑦、打开环境变量配置文件，命令 vim /etc/profile ，在底部加上Git相关配置信息： PATH=$PATH:/usr/local/git/bin export PATH 然后保存，退出！ ⑧、输入命令 git –version ，查看安装的git版本，校验通过，安装成功。 source /etc/profile","categories":[{"name":"持续集成技术","slug":"持续集成技术","permalink":"http://kingge.top/categories/持续集成技术/"}],"tags":[{"name":"持续集成技术","slug":"持续集成技术","permalink":"http://kingge.top/tags/持续集成技术/"},{"name":"容器管理","slug":"容器管理","permalink":"http://kingge.top/tags/容器管理/"},{"name":"自动部署","slug":"自动部署","permalink":"http://kingge.top/tags/自动部署/"}]},{"title":"docker个人总结","slug":"docker个人总结","date":"2019-02-28T13:59:59.000Z","updated":"2019-08-25T04:35:01.597Z","comments":true,"path":"2019/02/28/docker个人总结/","link":"","permalink":"http://kingge.top/2019/02/28/docker个人总结/","excerpt":"","text":"之前学习过docker，但是很浅显的使用，概念和流程各个方面总结的不够到位，下面根据旧版本的文档，重新的梳理。 一、docker出现的契机作为开发人员，我们经常会遇到一个问题，那就是环境不统一的问题。什么意思呢？自己在本地测试的项目是运行正常的，但是打包给测试或者运维人员部署使用时，经常会出现，部署报错，运行不起来，等等问题。就算是再详细的部署文档也还是会出错。 这个时候就产生了大量沟通的成本，通常产生这些问题的原因是部署的环境并不是开发人员的那一份环境，可能是jdk版本或者tomcat版本，数据库等等环境产生的问题。所以就需要我们开发人员打包一份连同环境和配置以及项目，交付给测试或者运维。这样就能够保证项目运行环境的一致性，也容易排查问题。这个就是docker的雏形 就好比我们在迁移一棵树的时候，尾部，总是会保留着一些原先的土，就是为了解决生长环境的不同额度适配问题。 Docker之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。 环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。开发人员利用 Docker 可以消除协作编码时“在我的机器上可正常工作”的问题。 *总的来说，以前我们是通过提交**war**包的方式，那么现在是连同**war***运行的环境***一起打包给测试或者运维。* 传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等(java为例)。而为了让这些程序可以顺利执行，开发团队也得准备完整的部署文件，让维运团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件+所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker镜像的设计，使得Docker得以打破过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间（类似于jvm的理念）的无缝接轨运作。 1.1 理念Docker是基于Go语言实现的云开源项目。 Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，也就是通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP（可以是一个WEB应用或数据库应用等等）及其运行环境能够做到“一次封装，到处运行”。 Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作 也就是说，我们可以把项目运行成功所需要的环境（redis，nginx，mysql）等等组件，通过编译打包的形式，打包成一个个的货仓。 项目部署到其他环境时（windowsàlinux）只需要运行这些货仓就可以安装这些环境，做到一次封装到处运行，解决了因为环境不同导致app部署或运行失败的问题 docker的logo就阐述了这一理念，部署项目的时候直接搬运已经测试成功的app运行环境。 **特别是在多集群的环境下，docker的作用更显而易见（避免多次安装环境）** 总的来说：解决了运行环境和配置问题软件容器（每个容器对应着一个集装箱，每个集装箱对应着项目运行所需的软件或者配置），方便做持续集成并有助于整体发布的容器虚拟化技术。 二、docker的演化2.1 虚拟机技术一个虚拟机的结构图 虚拟机（virtual machine）就是带环境安装的一种解决方案。 带环境安装的意思是：它里面模拟了一个正常的操作系统所具备的各种环境和配置（内存、处理器、硬盘。。。。） 它可以在一种操作系统里面运行另一种操作系统，比如在Windows 系统里面运行Linux 系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。 ​ 缺点： \\1. 启动很慢 \\2. 资源占用多 \\3. 冗余步骤多 所以docker在这之上就演化出了 容器虚拟化技术 2.2 容器虚拟化技术由于前面虚拟机存在这些缺点，Linux 发展出了另一种虚拟化技术：Linux 容器（Linux Containers，缩写为 LXC）。 Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。 相比虚拟机技术的系统结构图，很明显发现，公用库api模块被移除。各个app维护自己的所依赖的api模块。好处就是，节省了资源的占用。 2.3 总结不同比较了 Docker 和传统虚拟化方式的不同之处： \\1. 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程 \\2. 而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 \\3. 每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。 4. Linux虚拟机安装包可能需要4G，但是docker只需要170M。很明显这是一个很大的提升，换句话说，docker就是一个精细版的linux虚拟机 三、docker的好处一次构建、随处运行 更快速的应用交付和部署 更便捷的升级和扩缩容 更简单的系统运维 更高效的计算资源利用 四、安装和下载docker官网：http://www.docker.com docker中文网站： https://www.docker-cn.com/ Docker Hub官网: https://hub.docker.com/ Docker 分为 CE 和 EE 两大版本。 CE 即社区版（免费，支持周期 7 个月）， EE 即企业版，强调安全，付费使用，支持周期 24 个月。下面安装的是CE版本。 4.1 docker安装前提条件Docker支持以下的CentOS版本：CentOS 7 (64-bit) CentOS 6.5 (64-bit) 或更高的版本 目前，CentOS 仅发行版本中的内核支持 Docker。 Docker 运行在 CentOS 7 上，要求系统为64位、系统内核版本为 3.10 以上。 Docker 运行在 CentOS-6.5 或更高的版本的 CentOS 上，要求系统为64位、系统内核版本为 2.6.32-431 或者更高版本。 为了避免我们后面启动tomcat 容器做测试的时候，外部浏览器访问tomcat容器时，端口被拦截。这里先关闭虚拟机的防火墙 service firewalld status ；查看防火墙状态 service firewalld stop：关闭防火墙 查看自己linux 内核uname命令用于打印当前系统相关信息（内核版本号、硬件架构、主机名称和操作系统类型等）。 查看已安装的CentOS版本信息（CentOS6.8有，CentOS7无该命令） 另一种方式查询 4.2 docker安装一下安装是使用yum命令进行安装，所以linux虚拟机需要能够连接互联网 官方手册： https://docs.docker-cn.com/engine/installation/linux/docker-ce/centos/#prerequisites CentOS6.8安装Docker\\1. yum install -y epel-release 使用root用户执行改命令。 Docker使用EPEL发布，RHEL系的OS首先要确保已经持有EPEL仓库，否则先检查OS的版本，然后安装相应的EPEL包 \\2. yum install -y docker-io 发现这个命令在centos6.10 版本时，提示docker包找不到，于是花了另一命令：yum –y install docker 安装成功 \\3. 安装后的配置文件：/etc/sysconfig/docker \\4. 启动Docker后台服务：service docker start 5.docker version验证 CentOS7以上安装Docker（本人使用的版本）-推荐 下面使用仓库的方式进行安装docker-ce \\1. cat /etc/redhat-release 命令查看centos版本 \\2. yum安装gcc相关 执行以下两条命令 yum -y install gcc yum -y install gcc-c++ 3.卸载老版本（如果之前没有装过，可以忽略这一步） 注意看官网的操作手册，里面有着一段命令。 $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine \\4. 安装需要的软件包 yum install -y yum-utils device-mapper-persistent-data lvm2 \\5. 设置stable镜像仓库 执行命令： yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 注意这里使用的是阿里的镜像仓库，不要使用官网推荐的仓库 ​ yum-config-manager –add-repo https://download.docker.com/linux/centos/docker-ce.repo 不能使用这条命令 \\6. 更新yum软件包索引 yum makecache fast \\7. 安装DOCKER CE yum -y install docker-ce \\8. 启动docker systemctl start docker \\9. 测试 docker version :查看docker版本 docker pull hello-world（从阿里云仓库中获取hello-world镜像） docker run hello-world （要先下载hello-world镜像后才能够运行） 10.卸载 执行以下三条命令： systemctl stop docker yum -y remove docker-ce rm -rf /var/lib/docker 4.3阿里云镜像加速因为docker官网提供的获取镜像地址（hub.docker），访问速度太过缓慢，这里改换成阿里云的镜像服务。 \\1. 登录阿里云 https://www.aliyun.com/ 进入管理中心 \\2. 搜索容器镜像服务 可以看到镜像加速器 获得加速器地址连接 \\3. 配置本机Docker运行镜像加速器 Centos6.8版本设置： vim /etc/sysconfig/docker 将获得的自己账户下的阿里云加速地址配置进 other_args=”–registry-mirror=https://你自己的账号加速信息.mirror.aliyuncs.com“ Centos7.6版本设置： sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json &lt;&lt;-‘EOF’ { “registry-mirrors”: [“https://sk6o0yc78m.mirror.aliyuncs.com“] } EOF sudo systemctl daemon-reload sudo systemctl restart docker Centos7以上的配置文件时：/etc/docker/daemon.json 4．检查配置是否生效 Centos6.8 检查命令： 启动docker，执行命令 ps –ef| grep docker Centos7.6检查命令： 4.4 设置docker开机启动systemctl enable docker 五、docker组成和分析5.1 Docker组成镜像（image）Docker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 一个类可以new多个对象。 镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。 UnionFS（联合文件系统）UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 跟花卷一样，一层一层的 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 Docker镜像加载原理 Docker镜像加载原理： docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。 bootfs(boot file system)主要包含bootloader和kernel, bootloader主要是引导加载kernel, Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs (root file system) ，在bootfs之上。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。 平时我们安装进虚拟机的CentOS都是好几个G，为什么docker这里才200M？？ 对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host（宿主机）的kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别, 因此不同的发行版可以公用bootfs。 分层的镜像以我们的pull为例，在下载的过程中我们可以看到docker的镜像好像是在一层一层的在下载 有多个complete，说明有多个层次 为什么 Docker 镜像要采用这种分层结构呢最大的一个好处就是 - 共享资源 比如：有多个镜像都从相同的 base 镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像， 同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。 特点Docker镜像都是只读的 当容器启动时，一个新的可写层被加载到镜像的顶部。 这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 容器（container）鲸鱼背上的集装箱Docker 利用容器（Container）独立运行的一个或一组应用。容器是用镜像创建的运行实例。 它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 即是：容器=镜像+可读写层 仓库（repository）仓库（Repository）是集中存放镜像文件的场所。 仓库(Repository)和仓库注册服务器（Registry）是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。 最大的公开仓库是 Docker Hub(https://hub.docker.com/)， 存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云 等 总结需要正确的理解仓储/镜像/容器这几个概念: Docker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就似乎 image镜像文件。只有通过这个镜像文件才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 \\1. image 文件生成的容器实例，本身也是一个文件，称为镜像文件。 \\2. 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器 \\3. 至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。 总结：仓库存放着很多镜像，通过使用镜像可以生成多个容器 5.2 Docker运行流程 Docker 使用 C/S 结构，即客户端/服务器体系结构。 Docker 客户端与 Docker 服务器进行交互，Docker服务端负责构建、运行和分发 Docker 镜像。 Docker 客户端和服务端可以运行在一台机器上，也可以通过 RESTful 、 stock 或网络接口与远程 Docker 服务端进行通信。 执行docker run hello-world这个例子在上面我们已经使用过了，对照docker结构图我们来分析 Client：客户端就是我们的linux的命令窗口，也就是执行docker run hello-world的地方 Docker-host： docker主机，也就是执行客户端发出请求的地方，也就是我们启动的docker进程。收到一个执行hello-world容器的命令，现在本地种查找是否存在这个容器（镜像），存在则直接运行。不存在，则在Repository中取（上面我们配置了阿里云镜像仓库），pull镜像后放到本地，然后新建一个容器执行这个hello-world镜像。 Repository:：仓库，存放镜像的地方 下次执行docker run hello-world，则会从本地中拿hello-world镜像，然后新建容器执行。 完整流程如图： 六、docker常用操作命令6.1 帮助命令docker versiondocker info能够查看更详细的docker信息，比docker version命令更加详细 docker –help6.2 镜像命令6.2.1 docker images列出本地主机上的镜像 各个选项说明: REPOSITORY：表示镜像的仓库源 TAG：镜像 标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 redis，docker 将默认使用 redis:latest镜像 OPTIONS说明： -a :列出本地所有的镜像（含中间映像层） -q :只显示镜像ID。 –digests :显示镜像的摘要信息 –no-trunc :显示完整的镜像信息 6.2.2 docker search 某个XXX镜像名字需要注意，查询是从网站 https://hub.docker.com上进行查询，拉取镜像的时候是从阿里云上拉取 命令： docker search [OPTIONS] 镜像名字 OPTIONS说明： –no-trunc : 显示完整的镜像描述 -s : 列出收藏数不小于指定值的镜像。（就是下面的STARS数） –automated : 只列出 automated build类型的镜像； 6.2.3 docker pull 某个XXX镜像名字下载镜像 命令：ker pull 镜像名字[:TAG] （如果不指明标签默认下载最新版本） 6.2.4 docker rmi 某个XXX镜像名字ID删除镜像 删除单个： docker rmi -f 镜像ID （不指明标签默认删除latest） docker rmi -f 镜像名称 （不指明标签默认删除latest） 删除多个：docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除全部：ocker rmi -f $(docker images -qa) （docker images –qa 查询当前docker中所有镜像id） 6.3容器命令6.3.1有镜像才能创建容器，这是根本前提(下载一个CentOS镜像演示)docker pull centos 所以说可以把容器看做是一个简易版的 Linux 环境 6.3.2新建并启动容器docker run [OPTIONS] IMAGE [COMMAND] [ARG…] OPTIONS说明 OPTIONS说明（常用）：有些是一个减号，有些是两个减号 –name=”容器新名字”: 为容器指定一个名称； -d: 后台运行容器，并返回容器ID，也即启动守护式容器； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； -P: 随机端口映射； -p: 指定端口映射，有以下四种格式 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 下面运行6.3.1中下载的centos 启动交互式容器（跟下面我们所说的启动守护式容器有区别） #使用镜像centos:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。 docker run -it centos /bin/bash 等同于 docker run -it centos 案例演示1.从Hub上下载tomcat镜像到本地并成功运行 （1）docker pull tomcat （2）docker run -it -p 8088:8080 tomcat 运行tomcat，第一个端口8088表示docker对外暴露访问内部tomcat的端口，映射内部tomcat 的8080端口，什么意思呢？ 查看运行的容器 访问tomcat成功，直接访问http://49.234.188.74:8080/ 失败，因为我们知道，tomcat 是docker运行的一个容器，所以需要docker对外暴露后才能够访问。 （3）使用-P 不指名端口的方式运行tomcat 很明显使用大P的方式启动tomcat，docker会随机分配一个对外暴露的端口 6.3.3列出当前所有正在运行的容器docker ps [OPTIONS] OPTIONS说明（常用）： -a :列出当前所有正在运行的容器+历史上运行过的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 –no-trunc :不截断输出。 查看6.3.2 运行的centos 6.3.4退出容器exit容器停止退出，销毁容器。注意，容器内的数据也会一并消失，类似java的对象，close销毁后就不会存在了。 ctrl+P+Q容器不停止退出,回到宿主机。（那么怎么回到容器呢？-请看下面补充章节的第五小点） 6.3.5启动容器docker start 容器ID或者容器名 （可以启动已经关闭的容器）（docker ps –l 查看最近运行过得容器） 6.3.6重启容器docker restart 容器ID或者容器名 6.3.7停止容器docker stop 容器ID或者容器名 6.3.8强制停止容器docker kill 容器ID或者容器名 6.3.9删除已停止的容器docker rm 容器ID 一次性删除多个容器（下面两种方式） docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm 6.3.10 迁移与备份 补充启动守护式容器 命令：docker run -d 容器名 docker run -d centos 问题：然后docker ps -a 进行查看, 会发现容器已经退出，也就是说没有刚才我们启动的容器 很重要的要说明的一点: Docker**容器后台运行,就必须有一个前台进程**. 容器运行的命令如果不是那些一直挂起的命令（比如运行**top，tail**），就是会自动退出的。 这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如 service nginx start 但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用, 这样的容器后台启动后,会立即自杀因为他觉得他没事可做了. 所以，最佳的解决方案是,将你要运行的程序以前台进程的形式运行 查看容器日志命令：docker logs -f -t –tail 容器ID 参数解析： * -t 是加入时间戳 * -f 跟随最新的日志打印 * –tail 数字 显示最后多少条 例子： 启动守护式容器（因为存在前台程序一直循环输出,那么他就不会退出） docker run -d centos /bin/sh -c “while true;do echo hello kingge;sleep 2;done” 查看容器内运行的进程docker top 容器ID 返回的信息时，上个例子中启动的守护式容器 查看容器内部细节docker inspect 容器ID 返回一个json串的描述格式 进入正在运行的容器并以命令行交互回到以ctrl+p+q的方式退出的容器中 第一种方式： 使用命令：docker exec -it 容器ID bashShell （后面必须携带bash指令） 登录操作 效果等同 例子2： 打印后台启动的centos容器的根目录的消息，我们发现，他并没有进入后台的容器，只是把容器执行的指令的结果输出到宿主。所以他的功能是比docker attach指令还要强大的 第二种方式： docker attach 容器ID 两种方式的区别： attach 直接进入容器启动命令的终端，不会启动新的进程 exec 是在容器中打开新的终端，并且可以启动新的进程 从容器内拷贝文件到主机上宿主机上执行 docker cp 容器ID:容器内路径 目的主机路径 Docker镜像commit\\1. commit提交容器副本使之成为一个新的镜像 docker commit -m=“提交的描述信息” -a=“作者” 容器ID 要创建的目标镜像名:[标签名] 案例演示1.删除运行的tomcat 的文档模块，然后提交为新的镜像 这个时候再来访问tomcat容器的docs文档模块，肯定是404. \\2. 也即当前的tomcat运行实例是一个没有文档内容的容器，以它为模板commit一个没有doc的tomcat新镜像kingge/tomcatnodoc 注意这个标红框的镜像id必须是某一个正在运行的容器id 3.启动重新上传的tomcat 查看是否存在doc目录 不存在，说明这个版本就是我们亲自提交的删除文档的tomcat版本。 这个时候可以同时启动原先的tomcat版本，查看区别。 命令图例 七、Docker容器数据卷生产环境中使用Docker的过程中，往往需要对数据进行持久化，或者需要在多个容器之间进行数据共享，这必然涉及容器的数据管理操作。 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性. 卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷.( 数据卷是一个可供容器使用的特殊目录，它将主机操作系统目录直接映射进容器，类似于Linux中的mount操作) ​ 特点： 1：数据卷可在容器之间共享或重用数据 2：卷中的更改可以直接生效 3：数据卷中的更改不会包含在镜像的更新中 4：数据卷的生命周期一直持续到没有容器使用它为止 容器中管理数据主要有两种方式： \\1. 数据卷（Data Volumes）：容器内数据直接映射到本地主机环境，如何在容器内创建数据卷，并且把本地的目录或文件挂载到容器内的数据卷中。 \\2. 数据卷容器（Data Volume Containers）：使用特定容器维护数据卷。如何使用数据卷容器在容器和主机、容器和容器之间共享数据，并实现数据的备份和恢复。 7.1 创建数据卷7.1.1 第一种方式：使用命令直接添加命令： docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 就是把宿主机的某个目录关联到容器中，两者数据共通（文件夹不存在时，自动创建） 1.使用命令创建数据卷 宿主机也创建了myHostVal目录 2.校验是否数据共通 宿主机创建一个文本 查看容器是否存在hello.txt 容器存在。 反之，容器创建一个文件，宿主机也会出现同样的文件。 尖叫提示： Docker挂载数据卷的默认权限是读写（rw），用户也可以通过ro指定为只读 \\3. 极端测试 容器停止退出后（exit），宿主机创建或者修改文件，再重启容器（start），查看宿主机创建或者修改的文件是否有相应的变化。 经测试，答案是会有相应的变化。 4.查看容器的内部细节 docker inspect 容器ID 返回的json串中可以找到这样的一行描述 5.容器挂载文件夹的读写方式 docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 经过操作我们发现，宿主机创建或者修改的文件都能够同步到容器中，但是在容器中只能够查看对应的宿主机同步过来的文件，容器中不能够新增删除修改文件，只能够查看。 再次使用inspect 命令查看挂载的状态 发现可读写方式变为false，只读。 7.1.2 第二种方式：DockerFile方式添加DockerFile就是对于一个镜像的描述文件，类似于java代码编译后形成的.class文件，他是关于一个java类的描述。 1初探dockerfile结构打开docker hub，随便查看一个tomcat版本的dockerfile 打开后得到下面的代码，下面就是 这个dockerfile文件很好的阐述了tomcat镜像文件为什么这么大，而且为什么我们能够访问8080端口。 查看centos的dockerfile镜像描述文件 2.创建数据卷可在Dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷 VOLUME[“/dataVolumeContainer”,”/dataVolumeContainer2”,”/dataVolumeContainer3”] 说明： 出于可移植和分享的考虑，用-v 主机目录:容器目录这种方法不能够直接在Dockerfile中实现。 由于宿主机目录是依赖于特定宿主机的，并不能够保证在所有的宿主机上都存在这样的特定目录。（意思就是说容器中数据卷在linux01宿主机上关联的目录是myvolume1，但是如果该镜像在linux02宿主机上运行，那么容器容器启动后，可能找不到关联的myvolume1，因为你不能够保证linux02宿主机的相关目录结构是跟linux01一样的，所以使用dockerfile的方式创建数据卷的时候，单方面的指定容器中数据卷目录位置，容器启动后，会帮我们自动创建相关联的宿主机的目录） 也就是说，VOLUME命令只能够单方面的在容器中创建数据卷，不能够指明对应宿主机关联的目录（但是他会自动在宿主机创建相关联的目录） \\1. 宿主机创建dockerfile文件，依赖已经存在的centos镜像。 也就是说我们以现有的centos为某一层创建一个新的镜像（符合UnionFS） 在宿主机创建一个文件夹，存放创建的dockerfile文件 ![img](docker个人总结\\clip_image129.png) 新建的dockerfile文件dc。内容 \\2. 根据dockerfile文件dc，构建新镜像 Docker images 查看现存在的镜像 启动我们创建的centos镜像 确实主动给我们创建了两个数据卷，那么他们关联的宿主机的目录是什么呢？ 使用docekr inspect命令查看 7.2数据卷容器使用特定容器维护数据卷。7.1中使用的数据卷的方式是宿主机直接映射到容器进行数据传输，但是如果我们想两个容器之间共享传递数据怎么办呢？ 就需要创建数据卷容器 7.2.1先启动一个父容器dc01以上一步新建的镜像kingge/centos为模板并运行容器dc01 在创建容器卷dataVolumeContainer2新增内容，touch hello1.txt 7.2.2 创建dc02、03继承自dc01使用–volumes-from关键命令 在dc02、03的dataVolumeContainer2目录下查看是否存在dc01创建的hello1txt，很明显是可以看到的。 7.2.3测试数据共通性1.dc02/dc03分别在dataVolumeContainer2各自新增内容，touch hello2.txt和touch hello3.txt 分别查看dc01 dc02 dc03的dataVolumeContainer2目录下是否存在hello1.txt hello2.txt hello3.txt 这三个文件，答案是：都存在这三个文件 \\2. 删除dc01，dc02修改后dc03可否访问 答案很明显是存在的，也就是说删除dc01并不会影响dc02和dc03的数据互通 结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止 八、DockerFile解析8.1 dockerfile概念Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。通过：编写Dockerfile文件-&gt; docker build -&gt; docker run，生成一个镜像文件 查看centos的dockerfile镜像描述文件 1：每条保留字指令都必须为大写字母且后面要跟随至少一个参数 2：指令按照从上到下，顺序执行 3：#表示注释 4：每条指令都会创建一个新的镜像层，并对镜像进行提交 （1）docker从基础镜像运行一个容器（from scratch） （2）执行一条指令并对容器作出修改 （3）执行类似docker commit的操作提交一个新的镜像层 （4）docker再基于刚提交的镜像运行一个新容器 （5）执行dockerfile中的下一条指令直到所有指令都执行完成 从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段， * Dockerfile是软件的原材料 * Docker镜像是软件的交付品 * Docker容器则可以认为是软件的运行态。 Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。 1 Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等; 2 Docker镜像，在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行 Docker镜像时，会真正开始提供服务; 3 Docker容器，容器是直接提供服务的。 尖叫提示：Docker Hub 中 99% 的镜像都是通过在 base 镜像中安装和配置需要的软件构建出来的 8.2 dockerfile指令（保留字指令）FROM基础镜像，当前新镜像是基于哪个镜像的 MAINTAINER镜像维护者的姓名和邮箱地址 RUN容器构建时需要运行的命令 EXPOSE当前容器对外暴露出的端口 WORKDIR指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点 ENV用来在构建镜像过程中设置环境变量 ADD将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和**解压tar压缩包** COPY类似ADD，拷贝文件和目录到镜像中。 将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置，没有解压功能 COPY src dest COPY [“src”, “dest”] VOLUME容器数据卷，用于数据保存和持久化工作 CMD指定一个容器启动时要运行的命令 Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换（跟ENTRYPOINT指令的区别） 举个例子，查看tomcat 的dockerfile文件，我们可以发现最后是通过cmd命令启动了tomcat。那么为了证明CMD会不会被docker run后面的参数替换，请看下面例子。 ENTRYPOINT指定一个容器启动时要运行的命令 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数，但是他是以追加的形式而不是覆盖 docker run 之后的参数会被当做参数传递给 ENTRYPOINT，之后形成新的命令组合 ONBUILD当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发 小总结 8.3 案例案例一需求：修改默认的centos，修改他的落脚点（默认运行centos后容器进入的根目录）和添加vim指令、ifconfig（默认centos镜像没有安装这两个组件） \\1. 创建dockefile镜像描述文件 内容是 FROM centos MAINTAINER kingge&#51;&#57;&#x33;&#50;&#49;&#53;&#x36;&#x36;&#49;&#64;&#x71;&#113;&#46;&#x63;&#111;&#x6d; ENV MYPATH /usr/local WORKDIR $MYPATH RUN yum -y install vim RUN yum -y install net-tools EXPOSE 80 CMD echo $MYPATH CMD echo “success————–ok” CMD /bin/bash \\2. 根据创建的Dockerfile构建镜像 docker build -t 新镜像名字:TAG . （注意这里还有一个点，表示当前文件夹） 默认去找当前目录下名字为Dockerfile的文件构建镜像。 也可以用这个命令指定dockerfile： docker build -f /mydockerfile/Dockerfile -t kingge/mycentos:1.1 . 很明显跟原先从docker hub上拉取下来的centos多个两百多M，因为我们安装了vim和net-tools指令。 \\3. 列出镜像的变更历史 docker history 镜像名（imagesid） 可以看到构建这个镜像的每一层相关的操作。 4.运行构建的镜像 可以看到容器登陆后落脚点变更为了我们设定的/usr/local，同时也支持了vim 和ifconfig命令。 案例2通过自定义一个tomcat的方式我们来使用一些dockerfile常用的指令 2.1 新建一个工作目录 存放待传输到容器中的压缩包（测试ADD命令专用）和一个文本文件（测试COPY指令专用） 2.2 根据原版centos新建Dockerfile文件 内容是： FROM centos MAINTAINER kingge&#51;&#57;&#51;&#50;&#x31;&#53;&#x36;&#x36;&#x31;&#x40;&#x71;&#x71;&#x2e;&#x63;&#x6f;&#x6d; #把宿主机当前上下文的hello.txt拷贝到容器/usr/local/路径下 #并重命名为helloNewName.txt COPY hello.txt /usr/local/helloNewName.txt #把java与tomcat添加到容器中 ADD jdk-8u144-linux-x64.tar.gz /usr/local/ ADD apache-tomcat-9.0.21.tar.gz /usr/local/ #安装vim编辑器 RUN yum -y install vim #设置工作访问时候的WORKDIR路径，登录落脚点 ENV MYPATH /usr/local WORKDIR $MYPATH #配置java与tomcat环境变量 ENV JAVA_HOME /usr/local/jdk1.8.0_144 ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar ENV CATALINA_HOME /usr/local/apache-tomcat-9.0.21 ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.21 ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin #容器运行时监听的端口 EXPOSE 8080 #启动时运行tomcat # ENTRYPOINT [“/usr/local/apache-tomcat-9.0.21/bin/startup.sh” ] # CMD [“/usr/local/apache-tomcat-9.0.21/bin/catalina.sh”,”run”] CMD /usr/local/apache-tomcat-9.0.21/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.21/bin/logs/catalina.out 2.3 构建 构建完成 2.4 执行（RUN） docker run -d -p 9080:8080 –name myt9 -v /mydockerfile/mytomcat/tomcat9/project:/usr/local/apache-tomcat-9.0.21/webapps/project -v /mydockerfile/mytomcat/tomcat9/logs/:/usr/local/apache-tomcat-9.0.21/logs –privileged=true mytomcat921 命令的意思是：后台执行tomcat镜像，docker对外暴露8080端口，外部可以通过9080端口访问docker容器的8080端口。 –name：启动的容器重命名为myt9 -v：新建两个数据卷 –privileged=true: Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied解决办法：在挂载目录后多加一个–privileged=true参数即可 启动成功 2.5 验证1.首先验证两个tar包是否已经上传到容器并解压成功，hello.txt文件是否已经copy到容器、容器登录后落脚点是否是在我们设置的/usr/local 2.查看数据卷是否创建成功 宿主机对应数据卷创建成功 容器数据卷创建成功，project出现 3.校验数据卷 2.6 部署项目因为我们在创建数据卷的时候： /mydockerfile/mytomcat/tomcat9/project:/usr/local/apache-tomcat-9.0.21/webapps/project 宿主机的/tomcat9/project目录映射到了容器的webapps/project目录，那么可以利用数据卷的数据共通原理。在宿主机的project目录上传一个项目，然后重启容器，那么就可以实现项目的发布。 上传解压后的项目文件 很明显会自动同步到容器的webapps/project目录下 重启容器 访问项目 总结 九、常用插件安装https://www.runoob.com/docker/docker-install-mysql.html 9.1总体步骤搜索镜像、拉取镜像、查看镜像、启动镜像、停止容器、移除容器 9.2 tomcat安装9.2.1 docker hub上面查找tomcat镜像 docker search tomcat 或者直接使用浏览器登录docker hub查找也可以 9.2.2 从docker hub上拉取tomcat镜像到本地 docker pull tomcat 9.2.3 docker images查看是否有拉取到的tomcat 为什么拉取下来的tomcat有五百多M?上面文章已经做了解释（因为里面包含了jdk等等，这个也就是为什么我们可以直接运行tomcat镜像而不用配置jdk环境的原因） 9.2.4 使用tomcat镜像创建容器(也叫运行镜像) docker run -it -p 8080:8080 tomcat -p 主机端口:docker容器端口 -P 随机分配端口 i:交互 t:终端 9.3安装mysqlhttps://hub.docker.com/_/mysql 官网文档 9.3.1 docker hub上面查找mysql镜像9.3.2 从docker hub上(阿里云加速器)拉取mysql镜像到本地标签为5.6 9.3.3 使用mysql5.6镜像创建容器(也叫运行镜像) docker run -p 12345:3306 –name mysql -v /kingge/mysql/conf:/etc/mysql/conf.d -v / kingge /mysql/logs:/logs -v / kingge /mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 命令说明： -p 12345:3306：将主机的12345端口映射到docker容器的3306端口。 –name mysql：运行服务名字 -v / kingge /mysql/conf:/etc/mysql/conf.d ：将主机/ kingge /mysql录下的conf/my.cnf 挂载到容器的 /etc/mysql/conf.d -v / kingge /mysql/logs:/logs：将主机/ kingge /mysql目录下的 logs 目录挂载到容器的 /logs。 -v / kingge /mysql/data:/var/lib/mysql ：将主机/ kingge /mysql目录下的data目录挂载到容器的 /var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456：初始化 root 用户的密码。 -d mysql:5.6 : 后台程序运行mysql5.6 登录测试 docker exec -it 954efcffa04d /bin/bash 成功 外部软件连接成功 9.4 安装redis9.4.1 从docker hub上(阿里云加速器)拉取redis镜像到本地标签为3.2 9.4.2 使用redis3.2镜像创建容器(也叫运行镜像) docker run -p 6379:6379 -v /kingge/myredis/data:/data -v /kingge/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis:3.2 redis-server /usr/local/etc/redis/redis.conf –appendonly yes 这个时候可以直接连接redis了： 命令：docker exec -it 运行着Rediis服务的容器ID redis-cli 设置redis配置文件： 在主机/kingge/myredis/conf/redis.conf目录下新建redis.conf文件 vim / kingge /myredis/conf/redis.conf/redis.conf Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 。。。。省略 测试持久化文件生成 十、本地镜像发布到阿里云 1.镜像生成方式（1）使用DockerFile的方式创建镜像 （2）根据运行的容器创建一个新的镜像 docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] （参见6.3章节的补充模块的Docker镜像commit） 2. 将本地镜像推送到阿里云2.1 登录阿里云，创建镜像仓库https://cr.console.aliyun.com/cn-hangzhou/instances/repositories 2.2 创建命名空间 2.3 点击镜像仓库的管理 可以获取推送镜像到阿里云仓库的地址 $ sudo docker login –username=393215661@qq.com registry.cn-hangzhou.aliyuncs.com $ sudo docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/kingge/myrepo:[镜像版本号] $ sudo docker push registry.cn-hangzhou.aliyuncs.com/kingge/myrepo:[镜像版本号] 2.4 推送镜像到阿里云首先进行登录 标记我们需要上传的镜像 开始推送 推送成功 2.5 查看是否推送成功 2.6 从阿里云下载我们推送的镜像 十一、新建本地仓库 本质就是通过一个名字为registry的镜像，构建仓库 （1）拉取私有仓库镜像 docker pull registry （2）启动私有仓库容器 docker run -di --name=registry -p 5000:5000 registry （3）打开浏览器 输入地址http://49.234.188.74:5000/v2/_catalog看到{&quot;repositories&quot;:[]} 表示私有仓库搭建成功并且内容为空 或者使用crul 命令查看也可以 这里有个hello-world镜像，是本人之前上传的。如果没有上传过，那么这个应该返回的是{“repositories”:[]} （4）修改daemon.json vi /etc/docker/daemon.json 添加以下内容，保存退出。 {“insecure-registries”:[“49.234.188.74:5000”]} 例如 此步用于让 docker信任私有仓库地址 （5）重启docker 服务 systemctl restart docker 上传镜像到本地仓库（1）标记此镜像为私有仓库的镜像 docker tag hello-world 49.234.188.74:5000/ hello-world （本质就是创建一个关于hello-world的引用，镜像名字更改为hello-world 49.234.188.74:5000/hello-world ） （2）再次启动私服容器 docker start registry （3）上传标记的镜像 docker push 49.234.188.74:5000/ hello-world (4)查看是否上传成功 其他服务器获取上传的容器需求：192.168.1.105 服务器需要从 49.234.188.74 服务器创建的本地仓库获取上床的hello-world镜像 \\1. 192.168.1.105设置可信任仓库站点 vi /etc/docker/daemon.json 添加以下内容，保存退出。 {“insecure-registries”:[“49.234.188.74:5000”]} 如果不设置这一步，那么在从49.234.188.74服务器pull镜像的时候会报以下错误 默认不支持http请求的方式获取镜像 \\2. 拉取镜像成功 十二、使用DockerMaven插件构建项目微服务部署有两种方法： （1）手动部署：首先基于源码打包生成jar包（或war包）,将jar包（或war包）上传至虚 拟机并拷贝至JDK容器。 （2）通过Maven插件自动部署。 对于数量众多的微服务，手动部署无疑是非常麻烦的做法，并且容易出错。 （1）修改宿主机的docker配置，让其可以远程访问Vi /lib/systemd/system/docker.service 其中ExecStart=后添加配置 ‐H tcp://0.0.0.0:2375 ‐H unix:///var/run/docker.sock （2）刷新配置，重启服务systemctl daemon‐reload systemctl restart docker docker start registry （这里使用的是本地仓库） （3） springboot的pom文件添加插件 最后执行：mvn clean package docker:build 即可把镜像上传到本地仓库中 上面的方式是构建 项目到本地仓库的方式。如果我们自己申请了阿里云仓库，那么可以使用下面的方式将项目推送到阿里云仓库中。 使用SpringBoot2.0+DockerFile+Maven插件构建镜像并推送到阿里云仓库 https://blog.csdn.net/haogexiang9700/article/details/88318867 问题总结1 启动mysql后使用外部数据库连接工具访问时，报错错误提示 2059 - authentication plugin ‘caching_sha2_password’” 通过查看本人启动mysql容器，mysql的版本是： 经过查询得知：出现这个问题的原因是mysql8 之前的版本中加密规则是mysql_native_password,而在mysql8之后,加密规则是caching_sha2_password, 解决问题方法是把mysql用户登录密码加密规则还原成mysql_native_password 也就是数据库访问工具还是使用mysql_native_password这样的价码规则访问数据库。 关键的位置是在：mysql数据库中的user表 解决方法： 通过命令行的方式登陆数据库 mysql -uroot -p密码 然后分别执行以下代码 use mysql; ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘123456’; ALTER USER ‘root’@’%’ IDENTIFIED WITH mysql_native_password BY ‘123456’; FLUSH PRIVILEGES; 修改完毕 修改host为localhost和%(任意客户端)的密码认证方式 官方文档对应mysql8的更新说明 https://dev.mysql.com/doc/refman/8.0/en/upgrading-from-previous-series.html 2.docker数据卷权限问题参见《持续集成和容器管理》-《额外补充》章节，启动jenkins dokcer容器时，添加数据卷权限问题。 https://www.cnblogs.com/jackluo/p/5783116.html","categories":[{"name":"docker","slug":"docker","permalink":"http://kingge.top/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://kingge.top/tags/docker/"},{"name":"容器","slug":"容器","permalink":"http://kingge.top/tags/容器/"}]},{"title":"springboot个人总结","slug":"springboot个人总结","date":"2019-01-30T14:21:59.000Z","updated":"2019-08-25T04:05:13.223Z","comments":true,"path":"2019/01/30/springboot个人总结/","link":"","permalink":"http://kingge.top/2019/01/30/springboot个人总结/","excerpt":"","text":"一．前言1.1 什么是微服务？单服务场景 开发简单，测试简单，部署简单 下面举个例子，一种单一的服务场景：所有的模块都是打包成一个war包的形式，然后部署到tomcat中。 缺点： 1) 只能采用同一种技术，很难用不同的语言或者语言不同版本开发不同模块； 2) 系统耦合性强，一旦其中一个模块有问题，整个系统就瘫痪了；一旦升级其中一个模块，整个系统就停机了； 3) 集群只能是复制整个系统，即使只是其中一个模块压力大。（可能整个订单处理，仅仅是支付模块压力过大， 按道理只需要升级支付模块，但是在单一场景里面是不能的） 1.2 微服务概念 下面是微服务（Micro-Service）架构，不同模块放到不同的进程**/**服务器上，模块之间通过网络通讯进行协作。 各个模块都部署在不同的服务器上面，模块之间可以通过http请求进行协作。例如web服务器收到请求，那么可以根据请求的业务吧请求分配到响应的处理服务器，例如短信消息服务器。 优点： 1) 可以用不同的语言或者语言不同版本开发不同模块； 2) 系统耦合性弱，其中一个模块有问题，可以通过“降级熔断”等手段来保证不停机； 3) 可以对不同模块用不同的集群策略，哪里慢集群哪里。 缺点： 1) 开发难度大，系统结构更复杂； 2) 运行效率低； （模块之间相互请求时间长等等） 详细参照微服务文档 https://martinfowler.com/articles/microservices.html#MicroservicesAndSoa 所以就衍生出了 springboot和springcloud两个框架，当然springboot并不能代表微服务的概念，springcloud才是。 因为springboot只是简化了单一web服务开发的流程，摒弃了大量的xml配置，提供了大量的自动化配置。 那么springboot跟springcloud是什么关系呢？ SpringBoot专注于快速方便的开发单个个体微服务。SpringCloud是关注全局的微服务协调整理治理框架，它将SpringBoot开发的一个个单体微服务整合并管理起来，为各个微服务之间提供，配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务。SpringBoot可以离开SpringCloud独立使用开发项目，但是SpringCloud离不开SpringBoot，属于依赖的关系. SpringBoot专注于快速、方便的开发单个微服务个体，SpringCloud关注全局的服务治理框架。 1.3 spring的演化Spring1.x 时代在Spring1.x时代，都是通过xml文件配置bean，随着项目的不断扩大，需要将xml配置分放到不同的配置文件中，需要频繁的在java类和xml配置文件中切换。 Spring2.x时代随着JDK 1.5带来的注解支持，Spring2.x可以使用注解对Bean进行申明和注入，大大的减少了xml配置文件，同时也大大简化了项目的开发。 那么，问题来了，究竟是应该使用xml还是注解呢？ 最佳实践： 1、 应用的基本配置用xml，比如：数据源、资源文件等； 2、 业务开发用注解，比如：Service中注入bean等； Spring3.x到Spring4.x从Spring3.x开始提供了Java配置方式，使用Java配置方式可以更好的理解你配置的Bean，现在我们就处于这个时代，并且Spring4.x和Spring boot都推荐使用java配置的方式。 1.4为什么要学习SpringBootjava一直被人诟病的一点就是臃肿、麻烦。当我们还在辛苦的搭建项目时，可能Python程序员已经把功能写好了，究其原因主要是两点： · 复杂的配置 项目各种配置其实是开发时的损耗， 因为在思考 Spring 特性配置和解决业务问题之间需要进行思维切换，所以写配置挤占了写应用程序逻辑的时间。 · 混乱的依赖管理 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了，你还要知道这些库的哪个版本和其他库不会有冲突，这也是件棘手的问题。并且，依赖管理也是一种损耗，添加依赖不是写应用程序代码。一旦选错了依赖的版本，随之而来的不兼容问题毫无疑问会是生产力杀手。 而SpringBoot让这一切成为过去！ 二． 什么是springboot 三．Springboot使用这里开发工具使用的是sts和idea，他们两者开发springboot 的方式没有什么区别，所以下面两种方式我都会嵌套使用。下面的工程采用springboot版本是：1.5.9。 3.1 使用maven方式手动搭建sb项目Idea maven环境配置 编码设置： 需求：浏览器发送hello请求，服务器接受请求并处理，响应Hello World字符串； 1、创建一个maven工程2、导入spring boot相关的依赖&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 3、 编写一个主程序；启动Spring Boot应用 /* @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */ @SpringBootApplication public class HelloWorldMainApplication { public static void main(String[] args) { // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); } } /** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */@SpringBootApplicationpublic class HelloWorldMainApplication &#123; public static void main(String[] args) &#123; // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); &#125;&#125; 4、编写相关的Controller、Service@Controllerpublic class HelloController &#123; @ResponseBody @RequestMapping(&quot;/hello&quot;) public String hello()&#123; return &quot;Hello World!&quot;; &#125;&#125; @Controller public class HelloController { @ResponseBody @RequestMapping(“/hello”) public String hello(){ return “Hello World!”; } } 5、运行主程序测试直接运行HelloWorldMainApplication的main方法，启动sb程序。 在浏览器输入 127.0.0.1:8080/hello 可看到输出 Hello World! 6、打包成嵌入式web的可执行jar包&lt;!-- 这个插件，可以将应用打包成一个可执行的jar包；--&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 运行maven的 mvn package 打包命令，可以看到生成的jar包。 直接使用java -jar的命令进行执行，效果同 5 。 3.2 使用Spring Initializer快速创建Spring Boot项目-推荐3.2.1 sts编译器方式STS工具下创建CRUD的步骤（IDEA相同的步骤） 1.主菜单：File→New→Spring Starter Project。在Type中选Maven，Package选War 下一步中搜索勾选Web。 点击【Finish】会创建项目，第一次创建完成后会进行maven包的下载等，需要几分钟。项目创建成功后，他会生成一个springboot的启动类。后面就是靠他来启动springboot 2.新建一个Controller 3.新建一个index.html 需要注意的是，界面要放置在 resouces – templates 目录下（不是放在我们以前的webroot下面了） 4.启动程序，访问。 可以看到，他非常的简单。不需要配置一大堆文件 3.2.2 IDEA编译器方式方式同上 IDE都支持使用Spring的项目创建向导快速创建一个Spring Boot项目； 选择我们需要的模块；向导会联网创建Spring Boot项目； 默认生成的Spring Boot项目； - 主程序已经生成好了，我们只需要我们自己的逻辑 - resources文件夹中目录结构 - static：保存所有的静态资源； js css images； - templates：保存所有的模板页面；（Spring Boot默认jar包使用嵌入式的Tomcat，默认不支持JSP页面）；可以使用模板引擎（freemarker、thymeleaf）； - application.properties：Spring Boot应用的配置文件；可以修改一些默认设置；例如修改服务器端口号 四. Springboot核心下面分析根据helloworld 源码。 4.1、POM文件&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; 单击进去查看，发现他的父项目是： &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt; &lt;/parent&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;&lt;/parent&gt; 再单击进去发现 它里面配置了很多properties,定义了各种依赖的版本。也就是说他是用来真正管理Spring Boot应用里面的所有依赖版本，Spring Boot的版本仲裁中心；以后我们导入依赖默认是不需要写版本；（没有在dependencies里面管理的依赖自然需要声明版本号-） 4.2、web启动器&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; spring-boot-starter-web： spring-boot-starter：spring-boot场景启动器；帮我们导入了web模块正常运行所依赖的组件； Spring Boot将所有的功能场景都抽取出来，做成一个个的starters（启动器），只需要在项目里面引入这些starter相关场景的所有依赖都会导入进来。要用什么功能就导入什么场景的启动器 4.2、主程序类，主入口类 /* @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */ @SpringBootApplication public class HelloWorldMainApplication { public static void main(String[] args) { // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); } } /** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */@SpringBootApplicationpublic class HelloWorldMainApplication &#123; public static void main(String[] args) &#123; // Spring应用启动起来 SpringApplication.run(HelloWorldMainApplication.class,args); &#125;&#125; 1 注解分析-@SpringBootApplication@SpringBootApplication: Spring Boot应用标注在某个类上说明这个类是SpringBoot的主配置类，SpringBoot就应该运行这个类的main方法来启动SpringBoot应用； 1．@SpringBootConfiguration @SpringBootConfiguration:Spring Boot的配置类，sb自定义的一个注解，标注在某个类上，表示这是一个Spring Boot的配置类。 点进去发现实际上他是使用spring的@Configuration来注解的，而@Configuration的上一层是@Component。 在Spring Boot项目中推荐使用@ SpringBootConfiguration替代@Configuration **总的来说：这个注解的作用是把当前启动类加入到spring容器中，同时当做一个配置类来使用。** 2. @EnableAutoConfiguration @EnableAutoConfiguration：开启自动配置功能，以前我们需要配置的东西，Spring Boot帮我们自动配置；例如：我们添加了spring-boot-starter-web的依赖，项目中也就会引入SpringMVC的依赖，Spring Boot就会自动配置tomcat和SpringMVC，下面我们查看这个注解具体的作用。 @AutoConfigurationPackage：自动配置包 利用Spring的底层注解@Import，给IOC容器中导入组件，导入的组件由AutoConfigurationPackages.Registrar.class来控制。 额外提示： 可以参见在网站上发布的《spring注解》文章，关于@import注解导入bean的三种方式 （1）源码分析 查看AutoConfigurationPackages的内部类Registrar，他实现了ImportBeanDefinitionRegistrar接口。通过这个接口的重载方法registerBeanDefinitions（），控制导入IOC容器的bean。 static class Registrar implements ImportBeanDefinitionRegistrar,DeterminableImports &#123; @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) &#123; 1. register(registry, new PackageImport(metadata).getPackageName()); &#125; @Override public Set&lt;Object&gt; determineImports(AnnotationMetadata metadata) &#123; return Collections.singleton(new PackageImport(metadata)); &#125;&#125; static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { 1. register(registry, new PackageImport(metadata).getPackageName()); } @Override public Set&lt;Object&gt; determineImports(AnnotationMetadata metadata) { return Collections.singleton(new PackageImport(metadata)); } } 会去调用registerBeanDefinitions方法注册bean，紧接着registerBeanDefinitions方法调用register方法。 该方法会首先判断，IOC容器中是否存在 org.springframework.boot.autoconfigure.AutoConfigurationPackages 类，不存在则把该类注册到容器中，不存在则创建 在1处打个断点 发现获取的metadata元数据信息是HelloWorldMainApplication的。计算new PackageImport(metadata).getPackageName()的值得出的是HelloWorldMainApplication所在包的包名。Registry是IOC容器实例 总结： @AutoConfigurationPackage注解的作用是将主配置类（@SpringBootApplication标注的类）的所在包及下面所有子包里面的所有组件扫描到Spring容器，也就是说，如果controller，service等等类不在这个包下的话，是不会注册到spring容器中的，所以需要注意。 尖叫提示： 我们知道spring启动的时候会默认扫描启动类所在的包下面的所有类，注入springioc容器中，但是如果我们需要注入ioc容器的类不在启动类包下，那么我们可以通过这个@ImportResource(locations = {“classpath:beans.xml”})注解进行注入额外的类（注解加在 启动类上） 注解进行注入额外的类（注解加在 启动类上） @Import(AutoConfigurationImportSelector.class)（1）整体结构分析 还是利用了@import注解导入了AutoConfigurationImportSelector.class 类， 查看他的继承结构。 题外话： 可以看到很多组件都实现了Aware接口，关于实现Aware的作用是，我们可以在自定义组件中使用Spring底层的一些组件，例如创建一个bean时候，我们想查看IOC容器的信息或者查看当前系统的信息，那么这个时候就需要用到Aware接口，注入这些底层组件，供自定义组件调用。 可以参见在网站上发布的《spring注解》文章，关于Aware接口 关键代码 selectImports方法，返回的是需要导入到IOC容器的bean的全类名。类似 (2)详细分析 查看selectImport()，可以看到最终返回的是自动配置实体类autoConfigurationEntry. getConfigurations()方法的返回值，就是一个List 那么很明显获取自动配置实体类的代码是接下来我们所要关注的重点。 2.进入AutoConfigurationImportSelector.getAutoConfigurationEntry()方法 同理可得，关键代码是：List configurations 3.接着看getCandidateConfigurations方法里面的关键代码段。 关键代码：loadFactoryNames()方法，他的第一个参数通过debug我们可以得知，也就是getSpringFactoriesLoaderFactoryClass()方法的值是： EnableAutoConfiguration.class –&gt; org.springframework.boot.autoconfigure.EnableAutoConfiguration SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class,classLoader)**；** 深入查看loadFactoryNames方法可知。 他是通过 最终loadFactoryNames()方法的返回值是： （3）总结： @Import(AutoConfigurationImportSelector.class)注解的作用是，通过以key为 org.springframework.boot.autoconfigure.EnableAutoConfiguration，然后在META-INF/spring.factories文件中，获取key对应的value值，然后打包成一个值是全类名的List，最终返回到selectImport()方法，然后调用IOC容器把这些List里面bean，注册到IOC容器中，完成自动配置。 META-INF/spring.factories文件的位置。 J2EE**的整体整合解决方案和自动配置都在**spring-boot-autoconfigure-2.0.5.RELEASE.jar 关于自动配置：下面的5.12章节也会阐述到。 3. @ComponentScan 4.3 关闭自动配置通过上述，我们得知，Spring Boot会根据项目中的jar包依赖，自动做出配置，Spring Boot支持的自动配置如下（非常多）： 如果我们不需要Spring Boot自动配置，想关闭某一项的自动配置，该如何设置呢？ 比如：我们不想自动配置Redis，想手动配置。 当然了，其他的配置就类似了。 4.4 自定义Banner-了解即可-没什么用启动Spring Boot项目后会看到这样的图案： 这个图片其实是可以自定义的： \\1. 打开网站： http://patorjk.com/software/taag/#p=display&amp;h=3&amp;v=3&amp;f=4Max&amp;t=itcast%20Spring%20Boot 拷贝生成的字符到一个文本文件中，并且将该文件命名为banner.txt \\2. 将banner.txt拷贝到项目的resources目录中： \\3. 重新启动程序，查看效果： 如果不想看到任何的banner，也是可以将其关闭的： 4.5全局配置文件Spring Boot项目使用一个全局的配置文件application.properties或者是application.yml，在resources目录下或者类路径下的/config下，一般我们放到resources下。 1、 修改tomcat的端口为8088 重新启动应用，查看效果： 2、 修改进入DispatcherServlet的规则为：*.html 测试： 五. 配置文件SpringBoot使用一个全局的配置文件，配置文件名是固定的（名字不能更改否则无效） •application.properties （默认生成） •application.yml 而且通过观察发现 application.properties 的优先级别比 application.yml 高，同样的属性配置，**properties会覆盖yml**的配置 配置文件的作用：修改SpringBoot自动配置的默认值，SpringBoot在底层都给我们自动配置好。 5.1 YAML他是一种标记语言 5.2YAML语法 5.3配置文件值注入 5.4 @Value获取值和@ConfigurationProperties获取值比较 5.5、配置文件注入值数据校验 5.6、@PropertySource&amp;@ImportResource&amp;@Bean1. @PropertySource：加载指定的配置文件； 应用场景，我们知道application.properties是项目的整体配置文件，但是如果存在大量跟项目关系不是那么密切的配置信息，我们是可以配置到其他的配置的文件中去，避免造成application.peoperties文件的大小过大。这个时候就需要PropertySource注解进行指定。 2. @ImportResource**：** 导入Spring的配置文件，让配置文件里面的内容生效，Spring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别，想让Spring的配置文件生效，加载进来，@ImportResource标注在一个配置类上。 3.@bean Bean注解一般是配合着@Configure注解使用 5.7比较application.properties和application.yml和自定义properti的优先级别同样的属性配置 ： application.properties &gt; application.yml &gt; 自定义properties 前者会覆盖后者 5.8、配置文件占位符 5.9、Profile使用场景，针对于配置文件。例如我们在开发过程中使用的是开发环境的properties，那么生产使用的是生产的properties。那么我们怎么指定呢？ spring提供了profile功能 5.10、配置文件加载位置(Important)springboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件 –file:./config/ （在项目的的根目录下新建config） –file:./ （在项目的的根目录下） –classpath:/config/ （项目的resource目录下新建config） –classpath:/ （默认生成的application.properties是在类路径下面的） 优先级由高到底，高优先级的配置会覆盖低优先级的配置； SpringBoot会从这四个位置全部加载主配置文件；互补配置； 5.11、外部配置加载顺序 参考官方文档 5.11 bookstrap.yml其实还有一个系统级别的配置文件，这个是springcloud的在使用configserver组件的时候使用的，详情可查看后续的springcloud文章-springcloud的配置中心。 5.12、自动配置原理配置文件到底能写什么？怎么写？自动配置原理； 配置文件能配置的属性参照 1、自动配置原理1）、SpringBoot启动的时候加载主配置类，开启了自动配置功能通过这个注解@EnableAutoConfiguration 2）、@EnableAutoConfiguration作用： · 利用EnableAutoConfigurationImportSelector给容器中导入一些组件？ · 可以查看selectImports()方法的内容； · List configurations = getCandidateConfigurations(annotationMetadata, attributes);获取候选的配置 SpringFactoriesLoader.loadFactoryNames()扫描所有jar包类路径下 META-INF/spring.factories把扫描到的这些文件的内容包装成properties对象从properties中获取到EnableAutoConfiguration.class类（类名）对应的值，然后把他们添加在容器中 将 类路径下 META-INF/spring.factories 里面配置的所有**EnableAutoConfiguration**的值加入到了容器中 # Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.context.PropertyPlaceholderAu。。。。。。。。。。。。。。。。等等 每一个这样的 xxxAutoConfiguration类都是容器中的一个组件，都加入到容器中；用他们来做自动配置； 3）、每一个自动配置类进行自动配置功能； 4）、以HttpEncodingAutoConfiguration（Http编码自动配置）为例解释自动配置原理； @Configuration //表示这是一个配置类，以前编写的配置文件一样，也可以给容器中添加组件@EnableConfigurationProperties(HttpEncodingProperties.class) //启动指定类的ConfigurationProperties功能；将配置文件中对应的值和HttpEncodingProperties绑定起来；并把HttpEncodingProperties加入到ioc容器中@ConditionalOnWebApplication //Spring底层@Conditional注解（Spring注解版），根据不同的条件，如果满足指定的条件，整个配置类里面的配置就会生效； 判断当前应用是否是web应用，如果是，当前配置类生效@ConditionalOnClass(CharacterEncodingFilter.class) //判断当前项目有没有这个类CharacterEncodingFilter；SpringMVC中进行乱码解决的过滤器；@ConditionalOnProperty(prefix = &quot;spring.http.encoding&quot;, value = &quot;enabled&quot;, matchIfMissing = true) //判断配置文件中是否存在某个配置 spring.http.encoding.enabled；如果不存在，判断也是成立的//即使我们配置文件中不配置pring.http.encoding.enabled=true，也是默认生效的；public class HttpEncodingAutoConfiguration &#123; //他已经和SpringBoot的配置文件映射了 private final HttpEncodingProperties properties; //只有一个有参构造器的情况下，参数的值就会从容器中拿 public HttpEncodingAutoConfiguration(HttpEncodingProperties properties) &#123; this.properties = properties; &#125; @Bean //给容器中添加一个组件，这个组件的某些值需要从properties中获取 @ConditionalOnMissingBean(CharacterEncodingFilter.class) //判断容器没有这个组件？ public CharacterEncodingFilter characterEncodingFilter() &#123; CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter(); filter.setEncoding(this.properties.getCharset().name()); filter.setForceRequestEncoding(this.properties.shouldForce(Type.REQUEST)); filter.setForceResponseEncoding(this.properties.shouldForce(Type.RESPONSE)); return filter; &#125; 根据当前不同的条件判断，决定这个配置类是否生效？ 一但这个配置类生效；这个配置类就会给容器中添加各种组件；这些组件的属性是从对应的properties类中获取的，这些类里面的每一个属性又是和配置文件绑定的； 5）、所有在配置文件中能配置的属性都是在xxxxProperties类中封装者。配置文件能配置什么就可以参照某个功能对应的这个属性类 @ConfigurationProperties(prefix = &quot;spring.http.encoding&quot;) //从配置文件中获取指定的值和bean的属性进行绑定public class HttpEncodingProperties &#123; public static final Charset DEFAULT_CHARSET = Charset.forName(&quot;UTF-8&quot;); 精髓： 1**）、SpringBoot启动会加载大量的自动配置类** 2**）、我们看我们需要的功能有没有SpringBoot默认写好的自动配置类；** 3**）、我们再来看这个自动配置类中到底配置了哪些组件；（只要我们要用的组件有，我们就不需要再来配置了）** 4**）、给容器中自动配置类添加组件的时候，会从properties类中获取某些属性。我们就可以在配置文件中指定这些属性的值；** xxxxAutoConfigurartion：自动配置类； 给容器中添加组件 xxxxProperties:封装配置文件中相关属性； 2、细节1、@Conditional派生注解（Spring注解版原生的@Conditional作用）作用：必须是@Conditional指定的条件成立，才给容器中添加组件，配置配里面的所有内容才生效； @Conditional**扩展注解** 作用（判断是否满足当前指定条件） @ConditionalOnJava 系统的java版本是否符合要求 @ConditionalOnBean 容器中存在指定Bean； @ConditionalOnMissingBean 容器中不存在指定Bean； @ConditionalOnExpression 满足SpEL表达式指定 @ConditionalOnClass 系统中有指定的类 @ConditionalOnMissingClass 系统中没有指定的类 @ConditionalOnSingleCandidate 容器中只有一个指定的Bean，或者这个Bean是首选Bean @ConditionalOnProperty 系统中指定的属性是否有指定的值 @ConditionalOnResource 类路径下是否存在指定资源文件 @ConditionalOnWebApplication 当前是web环境 @ConditionalOnNotWebApplication 当前不是web环境 @ConditionalOnJndi JNDI存在指定项 自动配置类必须在一定的条件下才能生效； 我们怎么知道哪些自动配置类生效； ==**我们可以通过在application.properties中启用 debug=true属性；来让控制台打印自动配置报告==**，这样我们就可以很方便的知道哪些自动配置类生效； =========================AUTO-CONFIGURATION REPORT=========================Positive matches:（自动配置类启用的）----------------- DispatcherServletAutoConfiguration matched: - @ConditionalOnClass found required class &apos;org.springframework.web.servlet.DispatcherServlet&apos;; @ConditionalOnMissingClass did not find unwanted class (OnClassCondition) - @ConditionalOnWebApplication (required) found StandardServletEnvironment (OnWebApplicationCondition) Negative matches:（没有启动，没有匹配成功的自动配置类）----------------- ActiveMQAutoConfiguration: Did not match: - @ConditionalOnClass did not find required classes &apos;javax.jms.ConnectionFactory&apos;, &apos;org.apache.activemq.ActiveMQConnectionFactory&apos; (OnClassCondition) AopAutoConfiguration: Did not match: - @ConditionalOnClass did not find required classes &apos;org.aspectj.lang.annotation.Aspect&apos;, &apos;org.aspectj.lang.reflect.Advice&apos; (OnClassCondition) 六．日志七 springboot的web开发7.1 引言使用SpringBoot； 1**）、创建SpringBoot应用，选中我们需要的模块；** 2**）、SpringBoot已经默认将这些场景配置好了，只需要在配置文件中指定少量配置就可以运行起来** 3**）、自己编写业务代码；** 我们知道sb框架已经给我们自动配置了很多相关的配置，我们只需要着重于业务代码的编写，但是有些细则还是需要了解的，例如我们打包的sb程序成jar包的形式，那么我们网站的css js等等资源是放置在那个目录呢？换言说，静态资源的访问是个怎么流程呢？ 7.2、SpringBoot对静态资源的映射规则1.Web**开发的自动配置类** org.springframework.boot.autoconfigure.web.WebMvcAutoConfiguration 查看关键代码 7.2.1 查看第一层映射关系这段代码的意思是：所有/webjars/**的请求 ，都去classpath:/META-INF/resources/webjars/ 找资源。 例如请求jauery的请求 localhost:8080/webjars/jquery/3.3.1/jquery.js 1.**什么是webjars？** 以jar包的方式引入静态资源，可登陆这个网址了解http://www.webjars.org/ 也就是说，网站需要的jq等等资源你可以通过maven的方式导入到项目中。 在访问的时候只需要写webjars下面资源的名称即可 org.webjars jquery 3.3.1 &lt;!--引入jquery-webjar--&gt;在访问的时候只需要写webjars下面资源的名称即可 &lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt; &lt;/dependency&gt; 导入到sb项目后查看依赖 启动项目请求：localhost:8080/webjars/jquery/3.3.1/jquery.js。 输出jquery内容。 总结： 也就是说，如果我们需要引用网上相关的资源可以使用webjars的方式导入相关的组件，但是有个问题，那就是如果我们项目中已经存在某些**css和js，那么怎么引用到sb**项目中呢（也就是请求不满足第一层映射关系）？ 例如： 7.2.2 第二层映射关系 查看staticPathPattern的值：staticPathPattern = “/“。也就是说改代码块匹配访问当前项目任何资源的请求** 那么他是去哪里寻找资源回应请求呢？ 查看getStaticLocations()方法，通过查看发现： private static final String[] CLASSPATH_RESOURCE_LOCATIONS = &#123; &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &#125;; private static final String[] CLASSPATH_RESOURCE_LOCATIONS = { &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; }; 也就是说，请求会去类路径的根路径下的这个几个目录寻找相应的资源响应。 类路径是指： 例如sbp项目中，java和resources都是属于类路径的根路径。静态资源，我们可以在resources下创建 resources目录存放，或者创建public目录存放，static**目录默认已经创建** 2.案例 我们把一些样式文件放置到static目录下 启动sb项目，访问静态资源。http://127.0.0.1:8080/asserts/img/4D74191A.jpg 访问成功。很明显不满足第一层映射关系，走的是第二层映射关系的逻辑 7.2.3 欢迎页配置 通过查看他也是从7.2.2章节中静态资源存放目录下寻找index.html。 localhost:8080/ 找index页面 7.2.4 网站图标所有的 **/favicon.ico 都是在静态资源文件下找；==（也是从7.2.2章节中静态资源存放目录下） 7.2.5 修改静态资源文件夹路径 7.3、模板引擎常用的模板引擎有JSP、Velocity、Freemarker、Thymeleaf。核心理念是，通过传入模板代码和需要替换的数据到模板引擎中，模板引擎自动转化成静态的界面显示 SpringBoot推荐的Thymeleaf，语法更简单，功能更强大； 1、引入thymeleaf&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 我这里默认导入的是3.011版本。 如果你们想要修改导入版本，那么也是可以的： Pom.xml文件中添加版本覆盖即可 2、Thymeleaf使用我们通过查看他的自动配置源码来了解一下他的加载结构： org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration 的ThymeleafProperties 属性 发下它默认加载类路径下的templates目录下的后缀为html的文件。 只要我们把HTML页面放在classpath:/templates/，thymeleaf就能自动渲染； 2.1 案例1（常规使用）（1） 在templates目录下新建ok.html界面 （2） 在controller中添加请求处理，最终是跳转到ok.html界面 （3）启动sb项目，并访问http://127.0.0.1:8080/ok 2.2 动态赋值1.修改ok.html界面 2.修改请求处理方法 ok() 3.启动sb项目，并访问http://127.0.0.1:8080/ok 总结： 这里建议给html界面加上命名空间，这样在使用thyeleaf语法时会有提示 4. Thymeleaf语法https://www.thymeleaf.org/ 官方地址 1）、th:text；改变当前元素里面的文本内容； th：任意html属性；来替换原生属性的值 2）、表达式？ 7.4、SpringMVC自动配置https://docs.spring.io/spring-boot/docs/2.1.5.RELEASE/reference/html/boot-features-developing-web-applications.html#boot-features-spring-mvc-auto-configuration 官方文档 1. Spring MVC auto-configuration通过查看官方文档查看springboot为springmvc自动配置了那些工作 逐句分析： Spring Boot 自动配置好了SpringMVC 以下是SpringBoot对SpringMVC的默认配置:关键类WebMvcAutoConfiguration 1 Inclusion of ContentNegotiatingViewResolver and BeanNameViewResolver beans. - 自动配置了ViewResolver（视图解析器：根据方法的返回值得到视图对象（View），视图对象决定如何渲染（转发或者重定向）） - ContentNegotiatingViewResolver：组合所有的视图解析器的； - 如何定制：我们可以自己给容器中添加一个视图解析器；自动的将其组合进来 2.Support for serving static resources, including support for WebJars (see below).静态资源文件夹路径,webjars 3.Static index.html support. 静态首页访问 4.Custom Favicon support (see below). favicon.ico 5.自动注册了 of Converter, GenericConverter, Formatter beans. - Converter：转换器； public String hello(User user)：类型转换使用Converter - Formatter 格式化器； 2017.12.17===Date； 关键代码： @Bean@ConditionalOnProperty(prefix = &quot;spring.mvc&quot;, name = &quot;date-format&quot;)//在文件中配置日期格式化的规则public Formatter&lt;Date&gt; dateFormatter() &#123; return new DateFormatter(this.mvcProperties.getDateFormat());//日期格式化组件&#125; @Bean @ConditionalOnProperty(prefix = &quot;spring.mvc&quot;, name = &quot;date-format&quot;)//在文件中配置日期格式化的规则 public Formatter&lt;Date&gt; dateFormatter() { return new DateFormatter(this.mvcProperties.getDateFormat());//日期格式化组件 } 自己添加的格式化器转换器，我们只需要放在容器中即可 6**.Support for HttpMessageConverters (see below).** 7**. Automatic registration of MessageCodesResolver (see below).**定义错误代码生成规则 8**. Automatic use of a ConfigurableWebBindingInitializer bean (see below).** 2、扩展SpringMVC虽然springboot框架给我们自动配置了很多组件，但是在真实的应用场景中，肯定还需要自己实现一些组件，来扩展我们的springboot程序，例如我们需要定义一个拦截器和一个特定功能的视图解析器。那么就需要扩展了。 官方文档有段话，给我们阐述了如果扩展： 总的来说：编写一个配置类（@Configuration），是WebMvcConfigurer类型，不能标注@EnableWebMvc，即可实现扩展功能。 **尖叫提示：有些文档还是使用WebMvcConfigurerAdapter。** 需要注意的是：在springboot2.0版本以上，WebMvcConfigurerAdapter类已经过时，需要使用WebMvcConfigurer 接口或者WebMvcConfigurationSupport 过时原因：原因是springboot2.0以后，引用的是spring5.0，而spring5.0取消了WebMvcConfigurerAdapter 2.1 案例实现一个视图解析器，将请求是/no时，直接重定向到success界面(也就是说不用编写controller**方法**)。 访问请求：http://127.0.0.1:8080/no 重要总结总结：建议以后所有关于**mvc扩展的自定义的功能组件（视图解析器，国际化，拦截器等等），都放在某一个自实现的mvcConfig中（例如上面的MyViewConfig），方便管理，也可以实现多个扩展的webMvcConfig,**按照功能放置自定义的组件。 2.2 扩展mvc原理查看WebMvcAutoConfiguration**代码的适配器代码，可以知道关键代码是**@Import(EnableWebMvcConfiguration.class) 点进去 EnableWebMvcConfiguration.class**，发现他还是WebMvcAutoConfiguration的一个内部类。** 查看 DelegatingWebMvcConfiguration 类。 可以看到他是加载了容器中所有**WebMvcConfigurer类型的配置类，然后逐个调用。这里也可以证明了，我们自定义的扩展类仅仅只是扩展了mvc**的功能而已，并没有让其他自动配置功能失效。 3、全面接管SpringMVC全面接管也就意味着：SpringBoot**对SpringMVC的自动配置不需要了，所有都是我们自己配置，所有的SpringMVC的自动配置都会失效。** 我们需要在配置类中添加**@EnableWebMvc**即可 访问：发现提示界面找不到了。 加上注解，启动springboot项目，我们通过控制台输出的日志中我们发现少了一些filter。然后我们对于静态资源的默认访问路径等等都失效了。也就是说，访问html界面等等都行不通了。 这些代码都会失效，所以请求访问不到静态界面 3.1 原理1**）@EnableWebMvc的核心** 可以看到他的核心是导入一个类 DelegatingWebMvcConfiguration 2**）DelegatingWebMvcConfiguration类** 这个类就是我们上面2.2节查看注册各种适配器的关键代码，那么这里并没有什么代码控制 mvc**自动配置失效** 发现它继承 WebMvcConfigurationSupport ，这个类我们好像在那里看见过 2**）查看**WebMvcAutoConfiguration 让我们回过头查看mvc自动配置类 注意到一段代码：@ConditionalOnMissingBean(WebMvcConfigurationSupport.class) 意思是：当WebMvcConfigurationSupport类在容器中找不到时，执行自动配置类。 那么我们加上了@EnableWebMvc**注解，就相当于把WebMvcConfigurationSupport类注入到了容器中，所以WebMvcAutoConfiguration自动配置类失效。** 总结3**）、@EnableWebMvc将WebMvcConfigurationSupport组件导入进来；** 4**）、导入的WebMvcConfigurationSupport只是SpringMVC最基本的功能；** 7.5 默认访问首页通过查看WebMVCAutoConfiguration源代码可以知道，默认的”/”请求是会定向到类路径 下的index.html界面，那么我们可不可以手动控制他呢？ 很明显是可以的。 1.实现方式1实现一个controller，映射”/”请求 ​ 2.实现方式2 7.6 国际化1**）、编写国际化配置文件；** 2）、使用ResourceBundleMessageSource管理国际化资源文件 3）、在页面使用fmt:message取出国际化内容 步骤： 1）、编写国际化配置文件，抽取页面需要显示的国际化消息 第一个properties表示是在没有指定语言的情况下，默认的显示值 2）、SpringBoot自动配置好了管理国际化资源文件的组件； 3）、配置国际化基础名 在application.properties中设置 3）、去页面获取国际化的值； 效果：根据浏览器语言设置的信息切换了国际化； 原理国际化的核心是： 国际化Locale（区域信息对象）；LocaleResolver（获取区域信息对象）； 查看默认的区域信息解析器： 在WebMvcAutoConfiguration中 初始化了一个区域信息解析器，查看获取区域信息相关带代码 AcceptHeaderLocaleResolver ，看到这个类，我们可以猜测他是通过请求头中获取区域编码 看到 Accept-Language 说明上面我们的猜测是正确的。 也就是说，国际化的区域信息，默认是通过请求头中获取。而且需要注意的是当**spring容器中如果缺失LocaleResolver这个bean**实例，那么才会去加载默认的，区域化信息解析器。（下面的点击链接切换国际化就是很好的利用了这个注解） 点击链接切换国际化知道了上的原理，我们就可以手动的控制，切换语言环境。 1.实现一个区域信息解析器，通过获取请求中l的值，设定响应的语言环境。 因为我们自定义实现了LocalResolver，那么根据@ConditionalOnMissingBean这个注解 那么springboot默认的区域化解析器就会失效。 \\2. 实例化到spring容器中 上面的两个步骤可以整合在一起，最终实现： 在自定义的LocaleResolver中直接注入到spring容器中。注意需要指明**bean的id是localeResolver** 3.通过链接实现 http://127.0.0.1:8080/login.html?i=en_US http://127.0.0.1:8080/login.html?i=en_US 7.7 themleaf关闭缓存Thymeleaf会在第一次对模板解析之后进行缓存，极大的提高了并发处理能力，但是在开发期间模板引擎页面修改以后，要实时生效，所以我们开发阶段可以关掉缓存使用 1）、禁用模板引擎的缓存 \\# 禁用缓存 spring.thymeleaf.cache=false 2）、页面修改完成以后ctrl+f9：重新编译； 7.8 自定义拦截器 1. 第一种实现方式继承WebMvcConfigurerAdapter 1.定义一个拦截器实现HandlerInterceptor接口 2.注册到容器中 我们把扩展mvc的组件都放在自定义mvc扩展类中：MyMvcConfig 2. 第二种方式（常见）1. \\2. 然后定义配置类，注册拦截器 3.运行输出 接下来运行并查看日志： 你会发现日志中只有这些打印信息，springMVC的日志信息都没有，因为springMVC记录的log级别是debug，springboot默认是显示info以上，我们需要进行配置。 SpringBoot通过logging.level.=debug来配置日志级别，填写包名 # 设置org.springframework包的日志级别为debug logging.level.org.springframework=debug 设置完后，访问请求，查看输出 相比springmvc，springboot已经帮我们做好了静态资源的映射访问，所以不需要额外处理。 7.9 springmvc默认日期类型转化例如从前台传回一个时间字符串，后台通过date日期类型进行映射获取。那么如果前台日期格式是：2017/12/12 ，那么mvc会自动转化为date类型，因为他的默认时间转换器是以/ 进行分割的。 如果是2017-12-12、2017.12.12 这样的日期格式，那么就需要我们自己实现日期转换器 通过查看WebMvcAutoConfiguration的源码得知： 默认的日期格式是使用反斜杠来分割。 –修改默认的日期格式化： 只需要在applicatio.proeprties文件中，覆盖默认的日期格式即可 7.10 post/get请求转化为put或其他我们知道html的form表单只支持get/post两种请求，那么我们使用restful url进行请求数据的时候，那么需要使用到put请求，那么怎么转化呢？ 7.11错误处理机制1）、SpringBoot默认的错误处理机制当我们访问一个不存在的资源时，那么springboot默认帮我们跳转到一个错误界面 1.使用pc浏览器访问 2.使用其他工具访问（postman） 我们不难发现，他返回错误的形式都是不一样的，前者返回一个html界面，后者返回的是一个json字符串。为什么会有两种错误请求返回方式呢？接下来看源码就知道了。 原理： 可以参照ErrorMvcAutoConfiguration；错误处理的自动配置； 给容器中添加了以下四个组件 1、DefaultErrorAttributes： 2、BasicErrorController：他是一个基本的错误控制类。处理默认/error请求 是一个controller，处理/error请求 这里就解释了为什么会出现，两种错误请求响应方式（**html和josn**） 那么会产生另一个问题，他是怎么知道客户端是**pc还是其他访问工具呢？究竟怎么选择返回哪种格式的错误消息??????????** 很明显是通过请求头进行区分的。 1.通过postman发出的请求，请求头是： 2.通过浏览器发出的请求，请求头是： 3、ErrorPageCustomizer： 查看getpath()， 总的来说：系统出现错误以后来到error请求进行处理；（类似我们在web.xml注册的错误页面规则-根据不同的错误码，响应不同的错误界面：使用标签） 4、DefaultErrorViewResolver： 主要解析代码： 总结步骤： 一但系统出现4xx或者5xx之类的错误；ErrorPageCustomizer就会生效（定制错误的响应规则）；就会来到/error请求；就会被BasicErrorController处理； 1）响应页面；去哪个页面是由DefaultErrorViewResolver解析得到的； 2）、如果定制错误响应：1）、如何定制错误的页面；1**）、有模板引擎的情况下**；error/状态码; 【将错误页面命名为 错误状态码.html 放在模板引擎文件夹里面的 error文件夹下】，发生此状态码的错误就会来到对应的页面； 那么就会存在一个缺点，那就是如果我想把所有4开头的错误码都定向到一个错误界面怎么办呢？ 我们回过头查看DefaultErrorViewResolver 源代码发现，他默认注册了两个规则4xx和5xx。 我们可以使用**4xx和5xx作为错误页面的文件名来匹配这种类型的所有错误，精确优先（优先寻找精确的状态码.html**）； 页面能获取的信息（我们可以在自定义的错误界面获取到这些信息，参见DefaultErrorAttributes源码） timestamp：时间戳 status：状态码 error：错误提示 exception：异常对象 message：异常消息 errors：JSR303数据校验的错误都在这里 2）、没有模板引擎（模板引擎找不到这个错误页面），静态资源文件夹（static）下找 3）、以上都没有错误页面，就是默认来到SpringBoot默认的错误提示页面 2）、如何定制错误的json数据；1）、自定义异常处理&amp;返回定制json数据； 实现一个异常处理controller 只要是UserNotExistEception的异常都交由此方法处理，并返回自己自己定制json信息。 缺点: 永远返回json格式信息，没有html界面（7.11.1.1）。 2）、转发到/error进行自适应响应效果处理 缺点：自定义的map错误数据，没有携带过去。 3）、将我们的定制数据携带出去；出现错误以后，会来到/error请求，会被BasicErrorController处理，响应出去可以获取的数据是由getErrorAttributes得到的（是AbstractErrorController（ErrorController）规定的方法）； 1、完全来编写一个ErrorController的实现类【或者是编写AbstractErrorController的子类】，放在容器中（这种方式太过麻烦不推荐） 2、页面上能用的数据，或者是json返回能用的数据都是通过errorAttributes.getErrorAttributes得到（推荐这种方法） 容器中DefaultErrorAttributes.getErrorAttributes()；默认进行数据处理的； 自定义ErrorAttributes 八. 默认嵌套的Servlet服务器 打开springboot项目的pom文件，查看依赖。 8.1如何定制和修改Servlet容器的相关配置1、修改和server有关的配置（ServerProperties也是EmbeddedServletContainerCustomizer的实现）-第一种方式 就是我们直接在application.properties文件中的修改操作 server.port=8081 server.context-path=/crud server.tomcat.uri-encoding=UTF-8 2、编写一个EmbeddedServletContainerCustomizer：嵌入式的Servlet容器的定制器；来修改Servlet容器的配置 – 第二种方式 8.2 替换为其他嵌入式Servlet容器Springboot 完美的集成了下面这三款servlet容器，只需要做一些小改动即可切换使用。 具体实现的容器工厂类。 1. 默认使用tomcat 2. Jetty 做法其实很简单，那就是一出web模块自动装配的tomcat 组件，然后引入jetty组件即可。Jetty适合在开发长连接的项目中使用（例如聊天类的项目） 3. Undertow Undertow不支持jsp界面 8.3 嵌入式Servlet容器自动配置原理1. 源码分析EmbeddedServletContainerAutoConfiguration：嵌入式的Servlet容器自动配置 可以看到容器的切换，是通过注解来进行控制。通过容器工厂进行创建相应的容器 1）、EmbeddedServletContainerFactory（嵌入式Servlet容器工厂） 通过 getEmbeddedServletContainer()获得相应的servlet容器。 2）、EmbeddedServletContainer：（嵌入式的Servlet容器） 总的来说：通过servlet容器工厂类来创建相应的servlet容器 2.以tomcat容器工厂为例，分析流程 1.查看TomcatEmbeddedServletContainerFactory **重点是：****getEmbeddedServletContainer****方法** 3. ServerProperties、EmbeddedServletContainerCustomizer我们知道我们修改application.properties都是映射成ServerProperties 他实现了EmbeddedServletContainerCustomizer接口，所以衍生出了，第二种修改servlet容器的配置的方法： 自定义实现EmbeddedServletContainerCustomizer： 那么这些改动是怎么样生效的呢? 容器中导入了EmbeddedServletContainerCustomizerBeanPostProcessor 步骤： 1）、SpringBoot根据导入的依赖情况，给容器中添加相应的EmbeddedServletContainerFactory【TomcatEmbeddedServletContainerFactory】 2）、容器中某个组件要创建对象就会惊动后置处理器；EmbeddedServletContainerCustomizerBeanPostProcessor； 只要是嵌入式的Servlet容器工厂，后置处理器就工作； 3）、后置处理器，从容器中获取所有的EmbeddedServletContainerCustomizer，调用定制器的定制方法 8.4 嵌入式Servlet容器启动原理什么时候创建嵌入式的Servlet容器工厂？什么时候获取嵌入式的Servlet容器并启动Tomcat； 获取嵌入式的Servlet容器工厂： 1）、SpringBoot应用启动运行run方法 2）、refreshContext(context);SpringBoot刷新IOC容器【创建IOC容器对象，并初始化容器，创建容器中的每一个组件】；如果是web应用创建AnnotationConfigEmbeddedWebApplicationContext，否则：AnnotationConfigApplicationContext 3）、refresh(context);刷新刚才创建好的**ioc**容器； 4）、 onRefresh(); web的ioc容器重写了onRefresh方法 5）、webioc容器会创建嵌入式的Servlet容器；createEmbeddedServletContainer(); 6**）、获取嵌入式的Servlet容器工厂：** EmbeddedServletContainerFactory containerFactory = getEmbeddedServletContainerFactory(); 从ioc容器中获取EmbeddedServletContainerFactory 组件；TomcatEmbeddedServletContainerFactory创建对象，后置处理器一看是这个对象，就获取所有的定制器来先定制Servlet容器的相关配置； 7）、使用容器工厂获取嵌入式的**Servlet**容器：this.embeddedServletContainer = containerFactory .getEmbeddedServletContainer(getSelfInitializer()); 8）、嵌入式的Servlet容器创建对象并启动Servlet容器； 先启动嵌入式的**Servlet容器，再将ioc**容器中剩下没有创建出的对象获取出来； ==IOC**容器启动创建嵌入式的Servlet容器**== 九． springboot注册三大组件三大组件：servlet、filter、listenner 传统的web项目，我们可以在webroot/WEB_INFO/web.xml 中配置我们三大组件，但是springboot打包方式采用jar的方式，内嵌servlet容器，那么我们应该在哪里注册这三大组件呢？ 9.1 注册servle1.首先自定义一个servler 2.通过ServletRegistrationBean注册自定义servle 注意：如果存在一个**controller同时映射了/myServlet 请求，那么就会失效（被自定义servlet所覆盖）。** 9.2 注册filter1.首先自定义一个filter 2.FilterRegistrationBean 9.3 注册listenner1.首先自定义一个listenner 2.ServletListenerRegistrationBean 总结SpringBoot帮我们自动SpringMVC的时候，自动的注册SpringMVC的前端控制器；DIspatcherServlet； DispatcherServletAutoConfiguration中： 10 使用外置的Servlet容器嵌入式Servlet容器：应用打成可执行的jar 优点：简单、便携 缺点：默认不支持JSP、优化定制比较复杂（虽然可以使用定制器【ServerProperties、自定义EmbeddedServletContainerCustomizer】，自己编写嵌入式Servlet容器的创建工厂【EmbeddedServletContainerFactory】等方式修改配置，但是还是在需要通读代码的情况下才能够修改） 外置的Servlet容器：外面安装Tomcat—应用war包的方式打包 步骤1）、必须创建一个项目打包类型为war的项目；（利用idea创建好目录结构，因为默认生成的springboot项目是没有webapp和web.xml等文件，需要手动创建） 下一步 完成 很明显没有生成相应的**webapp目录，同时tomcat的作用域修改为了运行时使用，打包不使用，这个也就是为我们使用外置servlet**容器打下基础。 项目最终目录结构 2）、将嵌入式的Tomcat指定为provided； Springboot**默认已经帮我们完成** 3）、必须编写一个SpringBootServletInitializer的子类，并调用configure方法 **Springboot****默认已经帮我们完成** 4）、启动服务器就可以使用 可以编写一个jsp界面，访问测试 原理jar包：执行SpringBoot主类的main方法，启动ioc容器，创建嵌入式的Servlet容器 war包：启动服务器，服务器启动SpringBoot应用【SpringBootServletInitializer】，启动ioc容器 实现方式：Servlet3.0是一次Java EE规范 https://blog.csdn.net/f641385712/article/details/87474907 相关连接 规则：1）、服务器启动（web应用启动）会创建当前web应用里面每一个jar包里面ServletContainerInitializer实例 2）、ServletContainerInitializer的实现放在jar包的META-INF/services文件夹下，有一个名为javax.servlet.ServletContainerInitializer的文件，内容就是ServletContainerInitializer的实现类的全类名 3）、还可以使用@HandlesTypes，在应用启动的时候加载我们感兴趣的类 流程：1）、启动Tomcat 2）、org\\springframework\\spring-web\\4.3.14.RELEASE\\spring-web-4.3.14.RELEASE.jar!\\META-INF\\services\\javax.servlet.ServletContainerInitializer： Spring的web模块里面有这个文件：org.springframework.web.SpringServletContainerInitializer 3）、SpringServletContainerInitializer将@HandlesTypes(WebApplicationInitializer.class)标注的所有这个类型的类都传入到onStartup方法的Set","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://kingge.top/categories/SpringBoot/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"springboot","slug":"springboot","permalink":"http://kingge.top/tags/springboot/"},{"name":"微服务","slug":"微服务","permalink":"http://kingge.top/tags/微服务/"}]},{"title":"dubbo分布式服务框架","slug":"dubbo分布式服务框架","date":"2019-01-05T02:21:59.000Z","updated":"2019-08-25T02:24:55.615Z","comments":true,"path":"2019/01/05/dubbo分布式服务框架/","link":"","permalink":"http://kingge.top/2019/01/05/dubbo分布式服务框架/","excerpt":"","text":"Dubbo分布式服务框架** 一、分布式概念分布式的概念：某个业务逻辑的完成，拆分成几个功能/服务来实现，这些服务部署在不同的机器上。 官方解释： 分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是利用更多的机器，处理更多的数据。 首先需要明确的是，只有当单个节点的处理能力无法满足日益增长的计算、存储任务的时候，且硬件的提升（加内存、加磁盘、使用更好的CPU）高昂到得不偿失的时候，应用程序也不能进一步优化的时候，我们才需要考虑分布式系统。因为，分布式系统要解决的问题本身就是和单机系统一样的，而由于分布式系统多节点、通过网络通信的拓扑结构，会引入很多单机系统没有的问题，为了解决这些问题又会引入更多的机制、协议，带来更多的问题。 ​ 随着互联网的发展，网站应用的规模不断扩大，常规的垂直应用架构已无法应对，分布式服务架构以及流动计算架构势在必行，需一个治理系统确保架构有条不紊的演进。 1.1分布式的演化 1.1.1单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起(例如)，以减少部署节点和成本。 总的来说，一个项目就打包成一个war包，然后订单业务处理、商品等等所有业务都在这个war包里。 适用于小型网站，小型管理系统，将所有功能都部署到一个功能里，简单易用。 缺点： 1、性能扩展比较难 2、存在重复性开发的代码 3、不利于升级维护 4、 只能采用同一种技术，很难用不同的语言或者语言不同版本开发不同模块； 5、系统耦合性强，一旦其中一个模块有问题，整个系统就瘫痪了；一旦升级其中一个模块，整个系统就停机了； 6、 集群只能是复制整个系统，即使只是其中一个模块压力大。（可能整个订单处理，仅仅是支付模块压力过大， 按道理只需要升级支付模块，但是在单一场景里面是不能的） 最直观问题就是， 1.代码不够整洁，多个业务的功能都堆在一个包中，例如UserService、OrderService等等功能都放在com.kingge.service包中。当功能增多时，这个包会变得更加臃肿，代码阅读性很差。 2.服务之间依赖错综复杂不够清晰。 3.只能够通过部署集群来提高系统性能，资源浪费 1.1.2垂直应用架构​ 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率，这样就可以单独修改某个模块而不用重启或者影响其他模块，同时也可以给某个访问量剧增的模块，单独添加服务器。 ​ 通过切分业务来实现各个模块独立部署，降低了维护和部署的难度，团队各司其职更易管理，性能扩展也更方便，更有针对性。 缺点： 公用模块无法重复利用，开发性的浪费 面对突变的应用场景，可能某个模块对于web界面会频繁修改，但是模块业务功能没有变化，这样会造成单个应用频繁修改。所以需要界面+业务逻辑的实现分离。 没有处理好应用之间的交互问题，例如订单模块可能会需要查询商品模块的信息。 1.1.3分布式服务架构​ 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键（例如Dubbo）。（springcloud比他更全面，Dubbo只是相当于Spring Cloud中的Eureka模块） 分布式服务框架很好的解决了垂直应用架构的缺点，实现界面和服务的分离，实现界面和服务，以及服务与服务之间的调度。 但是存在一个问题，那就是没有一个基于访问压力的调度中心和服务注册中心，容易造成资源浪费，什么意思呢？假设用户服务部署了200台服务器，但是在某个时间段，他的访问压力很小，订单服务的访问压力剧增，服务器不够用。那么就会造成资源浪费和倾斜，存在服务器闲置或者请求量少的情况。 1.1.4流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)[ Service Oriented Architecture]是关键。 很好的解决了分布式架构的缺点。 SOA解决了服务的管理和注册，但是还是缺少服务容灾处理，网关处理以及全局配置模块等等（分别对应springcloud的hystrix、zuul、config） 1.1.5 微服务架构其实SOA架构跟微服务架构是很接近的，其实我也不是那么清晰的能够分别这种两种架构，所以这一章节暂缺。 但是他提供了比SOA架构更加细致化的服务拆分和管理，一般常用restful的方式进行数据的传输，SOA架构的传输技术是比较复杂多样的。 他的实现，见springcloud架构 1.2 RPC1.2.1什么叫RPCRPC【Remote Procedure Call】是指远程过程调用，是一种进程间通信方式，他是一种技术的思想，而不是规范。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。 1.2.2RPC基本原理 ​ 模型中多了一个stub的组件，这个是约定的接口，也就是server提供的服务。注意这里的”接口”，不是指JAVA中的interface，因为RPC是跨平台跨语言的，用JAVA写的客户端，应该能够调用用C语言提供的过程 一次完整的RPC调用流程（同步调用，异步另说）如下：1）服务消费方（client）调用以本地调用方式调用服务；2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；3）client stub找到服务地址，并将消息发送到服务端；4）server stub收到消息后进行解码；5）server stub根据解码结果调用本地的服务；6）本地服务执行并将结果返回给server stub；7）server stub将返回结果打包成消息并发送至消费方；8）client stub接收到消息，并进行解码；9）服务消费方得到最终结果。 RPC两个核心模块：通讯，序列化。 也就是说决定RPC连接效率的核心因素了，服务之间通讯的速度，以及消息序列化和反序列化的速度（这个就是为什么hadoop会采用自己的反序列化机制而不是java的反序列化，Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效） 常用的RPC框架：dubbo，gRPC，thrift。HSF 二、Dubbo概念和理解2.1 概念官网： [http://dubbo.apache.org/]{} Apache Dubbo 是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。即是：提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案 Dubbo是阿里巴巴公司开源的一个高性能优秀的服务框架，后来Dubbo 进入 Apache 孵化器。 Dubbo 采用全 Spring 配置方式，透明化接入应用，对应用没有任何 API 侵入，只需用 Spring 加载 Dubbo 的配置即可，Dubbo 基于 Spring 的 Schema 扩展 进行加载。 如果不想使用 Spring 配置，可以通过 API 的方式 进行调用。 官方文档 http://dubbo.apache.org/zh-cn/docs/user/quick-start.html 2.2 dubbo结构 服务提供者（Provider）：暴露服务的服务提供方，服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者（Consumer）: 调用远程服务的服务消费方，服务消费者在启动时，向注册中心订阅自己所需的服务，服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 注册中心（Registry）：注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者 监控中心（Monitor）：服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心 调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法（不是硬件级别的负载均衡），选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 2.3 dubbo架构Dubbo 架构具有以下几个特点，分别是连通性、健壮性、伸缩性、以及向未来架构的升级性。 连通性 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 服务提供者向注册中心注册其提供的服务，并汇报调用时间到监控中心，此时间不包含网络开销 服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心，此时间包含网络开销 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 伸缩性 注册中心为对等集群，可动态增加机器部署实例，所有客户端将自动发现新的注册中心 服务提供者无状态，可动态增加机器部署实例，注册中心将推送新的服务提供者信息给消费者 升级性 当服务集群规模进一步扩大，带动IT治理结构进一步升级，需要实现动态部署，进行流动计算，现有分布式服务架构不会带来阻力。下图是未来可能的一种架构： 节点角色说明 节点 角色说明 Deployer 自动部署服务的本地代理 Repository 仓库用于存储服务应用发布包 Scheduler 调度中心基于访问压力自动增减服务提供者 Admin 统一管理控制台 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 三、Dubbo环境搭建3.1 windows下搭建dubbo环境3.1.1 安装zookeeperDubbo推荐使用zookeeper作为注册中心 1.登录zookeeper官网下载zookeeper[https://archive.apache.org/dist/zookeeper/zookeeper-3.4.13/]{.underline} 这里以zookeeper-3.4.13为例 2.下载后解压修改zoo.cfg配置文件将conf下的zoo_sample.cfg复制一份改名为zoo.cfg即可。 注意几个重要位置： dataDir=./ 临时数据存储的目录（可写相对路径） clientPort=2181 zookeeper的端口号 修改完成后启动zookeeper 在zookeeeper目录下新建zooData目录保存数据 3. 运行zkServer.cmd启动成功后，使用zkCli.cmd测试 ls /：列出zookeeper根目录下保存的所有节点 create -e /kingge 123：创建一个临时的kingge节点，值为123 get / kingge：获取/ kingge节点的值 3.1.2安装dubbo-admin管理控制台dubbo本身并不是一个服务软件。它其实就是一个jar包能够帮你的java程序连接到zookeeper，并利用zookeeper消费、提供服务。所以你不用在Linux上启动什么dubbo服务。 但是为了让用户更好的管理监控众多的dubbo服务，官方提供了一个可视化的监控程序，不过这个监控即使不装也不影响使用。 1、下载dubbo-admin[https://github.com/apache/incubator-dubbo-ops]{.underline} 下载下来后解压 2、进入目录，修改dubbo-admin配置修改 src\\main\\resources\\application.properties 指定zookeeper地址 3、打包dubbo-admin（maven环境已经配置好）mvn clean package -Dmaven.test.skip=true 4、运行dubbo-adminjava -jar dubbo-admin-0.0.1-SNAPSHOT.jar （springboot方式启动项目） 注意：【有可能控制台看着启动了，但是网页打不开，需要在控制台按下ctrl+c即可】 默认使用root/root 登陆 3.2 linux下搭建dubbo环境3.2.1安装zookeeper1、安装jdk1、下载jdk[http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html]{.underline} 不要使用wget命令获取jdk链接，这是默认不同意，导致下载来的jdk压缩内容错误 2、上传到服务器并解压 3、设置环境变量/usr/local/java/jdk1.8.0_171 文件末尾加入下面配置 export JAVA_HOME=/usr/local/java/jdk1.8.0_171export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 4、使环境变量生效&amp;测试JDK 2、安装zookeeper1、下载zookeeper网址 [https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/]{.underline} wget [https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz]{.underline} 2、解压 3、移动到指定位置并改名为zookeeper 3、开机启动zookeeper（可选）1）-复制如下脚本 #!/bin/bash#chkconfig:2345 20 90#description:zookeeper#processname:zookeeperZK_PATH=/usr/local/zookeeperexport JAVA_HOME=/usr/local/java/jdk1.8.0_171case $1 in start) sh $ZK_PATH/bin/zkServer.sh start;; stop) sh $ZK_PATH/bin/zkServer.sh stop;; status) sh $ZK_PATH/bin/zkServer.sh status;; restart) sh $ZK_PATH/bin/zkServer.sh restart;; *) echo &quot;require start|stop|status|restart&quot; ;;esac 2）-把脚本注册为Service 3）-增加权限 4、配置zookeeper1、初始化zookeeper配置文件拷贝/usr/local/zookeeper/conf/zoo_sample.cfg 到同一个目录下改个名字叫zoo.cfg 2、启动zookeeper 3.2.2安装dubbo-admin管理控制台1、安装Tomcat8（旧版dubbo-admin是war，新版是jar不需要安装Tomcat）1、下载Tomcat8并解压[https://tomcat.apache.org/download-80.cgi]{.underline} wget[http://mirrors.shu.edu.cn/apache/tomcat/tomcat-8/v8.5.32/bin/apache-tomcat-8.5.32.tar.gz]{.underline} 提供两种访问方式 2、解压移动到指定位置 3、开机启动tomcat8 复制如下脚本#!/bin/bash#chkconfig:2345 21 90#description:apache-tomcat-8#processname:apache-tomcat-8CATALANA_HOME=/opt/apache-tomcat-8.5.32export JAVA_HOME=/opt/java/jdk1.8.0_171case $1 instart) echo &quot;Starting Tomcat...&quot; $CATALANA_HOME/bin/startup.sh ;;stop) echo &quot;Stopping Tomcat...&quot; $CATALANA_HOME/bin/shutdown.sh ;;restart) echo &quot;Stopping Tomcat...&quot; $CATALANA_HOME/bin/shutdown.sh sleep 2 echo echo &quot;Starting Tomcat...&quot; $CATALANA_HOME/bin/startup.sh ;;*) echo &quot;Usage: tomcat &#123;start|stop|restart&#125;&quot; ;; esac 4、注册服务&amp;添加权限 5、启动服务&amp;访问tomcat测试 2、安装dubbo-admindubbo本身并不是一个服务软件。它其实就是一个jar包能够帮你的java程序连接到zookeeper，并利用zookeeper消费、提供服务。所以你不用在Linux上启动什么dubbo服务。 但是为了让用户更好的管理监控众多的dubbo服务，官方提供了一个可视化的监控程序，不过这个监控即使不装也不影响使用。 1、下载dubbo-admin[https://github.com/apache/incubator-dubbo-ops]{.underline} 2、进入目录，修改dubbo-admin配置修改 src\\main\\resources\\application.properties 指定zookeeper地址 3、打包dubbo-adminmvn clean package -Dmaven.test.skip=true 4、运行dubbo-adminjava -jar dubbo-admin-0.0.1-SNAPSHOT.jar 默认使用root/root 登陆 3.3 测试dubbo（dubbo-hello）[http://dubbo.apache.org/zh-cn/docs/user/quick-start.html]{.underline} 官方例子 4.1）、提出需求某个电商系统，订单服务需要调用用户服务获取某个用户的所有地址； 我们现在 需要创建两个服务模块进行测试 模块 功能 订单服务web模块 创建订单等 用户服务service模块 查询用户地址等 测试预期结果： 订单服务web模块在A服务器，用户服务模块在B服务器，A可以远程调用B的功能。 4.2）、工程架构根据 dubbo《服务化最佳实践》 http://dubbo.apache.org/zh-cn/docs/user/best-practice.html 1、分包（就是抽取公共的接口或者实体类到一个工程）​ 建议将服务接口，服务模型，服务异常等均放在 API 包中，因为服务模型及异常也是 API 的一部分，同时，这样做也符合分包原则：重用发布等价原则(REP)，共同重用原则(CRP)。 ​ 如果需要，也可以考虑在 API 包中放置一份 spring 的引用配置，这样使用方，只需在 spring 加载过程中引用此配置即可，配置建议放在模块的包目录下，以免冲突，如：com/alibaba/china/xxx/dubbo-reference.xml。 ​ 就是为了避免重复书写某些代码，抽取公共部分代码形成一个模块（例如下面的common-interface），其他模块需要用到时，添加依赖即可（pom文件） 2、粒度​ 服务接口尽可能大粒度，每个服务方法应代表一个功能，而不是某功能的一个步骤，否则将面临分布式事务问题，Dubbo 暂未提供分布式事务支持。服务接口建议以业务场景为单位划分，并对相近业务做抽象，防止接口数量爆炸。不建议使用过于抽象的通用接口，如：Map query(Map)，这样的接口没有明确语义，会给后期维护带来不便。 4.3）、创建模块1、common-interface：公共接口层（model，service，exception…）（符合分包理念）作用：定义公共接口，也可以导入公共依赖，保存服务提供者和服务消费者的接口 好处就是，可以让服务生产者和消费者同时依赖这个公共接口层maven项目，避免书写重复代码。 2、user-module：用户模块（对用户接口的实现，服务提供者） pom.xml (引用公共接口层) &lt;dependency&gt; &lt;groupId&gt;com.kingge.common&lt;/groupId&gt; &lt;artifactId&gt;common-interface&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 用户接口实现 这里为了快速演示，就做了一些虚拟数据，不会去连接数据库。 到时候订单模块只需要调用即可 4、order-module：订单模块（调用用户模块）1. pom.xml (引用公共接口层) &lt;dependency&gt; &lt;groupId&gt;com.kingge.common&lt;/groupId&gt; &lt;artifactId&gt;common-interface&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 2. 订单接口实现 现在这样是无法进行调用的。我们order-module引入了common-interface，但是common-interface公用模块里的用户接口的具体实现在user-module，我们并没有引入user-module模块，所以上诉代码调用肯定是失败的。而且user-module模块可能还在别的服务器中（不在同一个进程中）。 ​ 所以我们需要把user-module模块的用户接口的实现类暴露出去，供order-module模块使用。 接下来使用dubbo来实现这样的功能。 4.4）、使用dubbo改造1、改造user-module作为服务提供者（1）引入dubbo （pom.xml引入依赖） &lt;!-- 引入dubbo --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 由于我们使用zookeeper作为注册中心，所以需要操作zookeeperdubbo 2.6以前的版本引入zkclient操作zookeeper dubbo 2.6及以后的版本引入curator操作zookeeper下面两个zk客户端根据dubbo版本2选1即可--&gt; &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt; &lt;/dependency&gt; &lt;!-- curator-framework --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt; &lt;/dependency&gt; （2）配置提供者（新建provider.xml文件） 内容如下： &lt;!-- 1、指定当前服务/应用的名字（同样的服务名字相同，不要和别的服务同名） --&gt; &lt;dubbo:application name=&quot;user-module&quot;&gt;&lt;/dubbo:application&gt;&lt;!-- 2、指定注册中心的位置 --&gt;&lt;!-- &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot;&gt;&lt;/dubbo:registry&gt; 这种方式也可以 --&gt; &lt;dubbo:registry protocol=&quot;zookeeper&quot; address=&quot;127.0.0.1:2181&quot;&gt;&lt;/dubbo:registry&gt; &lt;!--使用dubbo协议，将服务暴露在20880端口 consumer和provider连接的协议，协议由提供方指定，消费方被动接受 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt;&lt;!-- 指定需要暴露的服务 --&gt;&lt;dubbo:service interface=&quot;com.kingge.common.service.UserService&quot; ref=&quot;userServiceImpl&quot; &gt;&lt;/dubbo:service&gt; &lt;!-- 服务的实现 --&gt;&lt;bean id=&quot;userServiceImpl&quot; class=&quot;com.kingge.user.service.impl.UserServiceImpl&quot;&gt;&lt;/bean&gt; dubbo支持多种传输协议 （3）启动服务 public class MainApplication &#123; public static void main(String[] args) throws IOException &#123; ClassPathXmlApplicationContext ioc = new ClassPathXmlApplicationContext(&quot;provider.xml&quot;); ioc.start(); System.in.read(); &#125;&#125; 运行后，打开我们之前配置的dubbo-admin 2、改造order-module作为服务消费者（1）引入dubbo （pom.xml引入依赖） &lt;!-- 引入dubbo --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 由于我们使用zookeeper作为注册中心，所以需要操作zookeeperdubbo 2.6以前的版本引入zkclient操作zookeeper dubbo 2.6及以后的版本引入curator操作zookeeper下面两个zk客户端根据dubbo版本2选1即可--&gt; &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt; &lt;/dependency&gt; &lt;!-- curator-framework --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt; &lt;/dependency&gt; （2）配置消费者信息 新建consumer.xml文件 &lt;!—配置包扫描 --&gt;&lt;context:component-scan base-package=&quot;com.kingge.order.service.impl&quot;&gt;&lt;/context:component-scan&gt;&lt;!-- 应用名 --&gt; &lt;dubbo:application name=&quot;order-module&quot;&gt;&lt;/dubbo:application&gt;&lt;!-- 指定注册中心地址 --&gt; &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot; /&gt;&lt;!-- 生成远程服务代理，可以和本地bean一样使用userService --&gt;&lt;dubbo:reference interface=&quot;com.kingge.common.service.UserService&quot; id=&quot;userService&quot; &gt;&lt;/dubbo:reference&gt; （3）修改OrderServiceImpl （4）创建启动类，启动consumer 3、测试调用调用成功 查看dubbo admin 3.4 配置dubbo监控中心3.4.1 dubbo-admin图形化的服务管理页面；安装时需要指定注册中心地址，即可从注册中心中获取到所有的提供者/消费者进行配置管理 安装和使用方式在上面我们已经说过了，这里就不在阐述。 3.4.2 dubbo-monitor-simple简单的监控中心； 1、下载 dubbo-opshttps://github.com/apache/incubator-dubbo-ops 2、修改配置指定注册中心地址进入 dubbo-monitor-simple\\src\\main\\resources\\conf 修改 dubbo.properties文件 3、打包dubbo-monitor-simplemvn clean package -Dmaven.test.skip=true 打包完成后，打开target目录，解压下面的tar.gz包 他跟3.4.1 的dubbo-admin不一样，不是一个springboot项目，不能够直接使用java –jar命令执行。 打开解压后的目录 Conf目录存放就是我们的dubbo.properties assembly.bin目录存放运行dubbo-monitor-simple 的可执行文件 双击 start.bat 执行 4.在项目中使用simple监控中心 以3.3的例子为例： 分别在provider.xml和consumer.xml中添加以下代码 &lt;dubbo:monitor protocol=&quot;registry&quot;&gt;&lt;/dubbo:monitor&gt; &lt;!-- &lt;dubbo:monitor address=&quot;127.0.0.1:7070&quot;&gt;&lt;/dubbo:monitor&gt;--&gt; 然后重新运行项目，再查看 四、dubbo整合springboot改造4.3章节普通的maven项目为springboot项目，关键代码没有变化，主要是一些配置和依赖发生了变化 关于idea怎么创建多模块项目这里就不阐述了，详情可参见一下网址 [https://blog.csdn.net/tiantangdizhibuxiang/article/details/81130297]{.underline} https://www.cnblogs.com/zjfjava/p/9696086.html 4.1 新建boot-common-interface：公共接口层（model，service，exception…）（符合分包理念） 一路next完成。 复制4.3.1 章节common-interface的相关代码到此工程 4.2 新建boot-user-module：用户模块4.2.1 新建模块 一路next创建即可。 4.2.2 代码实现4.2.2同理复制章节4.3.2 user-module项目代码到此项目并修改pom.xml文件 添加一下依赖 &lt;!--引用公用模块--&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge&lt;/groupId&gt; &lt;artifactId&gt;boot-common-interface&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!--引入dubbo starter--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; 需要注意的是： 注意springboot的starter版本跟dubbo版本适配： 因为本人使用的是springboot2.1.6版本，故使用0.2.0版本dubbo starter 4.2.3修改application.proeprties文件dubbo.application.name=boot-user-moduledubbo.registry.protocol=zookeeperdubbo.registry.address=127.0.0.1:2181#开启包扫描，可替代 @EnableDubbo 注解#dubbo.scan.base-packages=com.kingge.orderdubbo.protocol.name=dubbodubbo.protocol.port=20880dubbo.monitor.protocol=registry application.name就是服务名，不能跟别的dubbo提供端重复 registry.protocol 是指定注册中心协议 registry.address 是注册中心的地址加端口号 protocol.name 是分布式固定是dubbo,不要改。 dubbo.scan.base-packages 需要暴露的接口的实现类所在的包（跟启动类在同一个包或者子包下都不需要配置这个属性的值，因为springboot启动类会扫描注入。） dubbo.monitor.protocol 开启simple检测中心 4.2.4 配置需要暴露的服务（通过注解方式） 4.2.5 boot启动类开启dubbo功能 4.2.6 完整项目结构和启动服务提供者 4.3 新建boot-order-module：订单模块4.3.1 因为我们打算通过web的方式访问controller，再去调用暴露的接口，所以需要引入web依赖 一路next即可 4.3.2 代码实现4.3.2 同理复制章节4.3.3 order-module项目代码到此项目并修改pom.xml文件 添加一下依赖 &lt;!--引用公用模块--&gt; &lt;dependency&gt; &lt;groupId&gt;com.kingge&lt;/groupId&gt; &lt;artifactId&gt;boot-common-interface&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!--引入dubbo starter--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; 4.3.3 修改application.proeprties文件dubbo.application.name=boot-order-module dubbo.registry.address= zookeeper://127.0.0.1:2181 dubbo.monitor.protocol=registry 4.3.4 service使用@reference调用暴露接口 4.3.5 实现一个controller，接受请求 4.3.6 boot启动类开启dubbo功能 4.3.7 启动工程，并访问请求 服务调用成功 4.4 补充整合dubbo的三种方式：官网 [http://dubbo.apache.org/zh-cn/docs/user/configuration/api.html]{.underline} * SpringBoot与dubbo整合的三种方式： * 1）、导入dubbo-starter，在application.properties配置属性，使用\\@Service【暴露服务】使用@Reference【引用服务】使用\\@EnableDubbo，开启dubbo注解功能（上面例子我们使用的就是这种方式） * 2）、保留dubbo xml配置文件 * 导入dubbo-starter，使用\\@ImportResource导入dubbo的配置文件即可 * 3）、使用注解API的方式： * 将每一个组件手动创建到容器中,让dubbo来扫描其他的组件 第二种整合方式解决： 如果我们需要在boot-user-module服务提供者配置方法级别的超时时间，那么有没有响应的注解配置呢？ 答案是没有的，那么就需要我们使用xml的方式进行配置。 1. 首先去掉@Service、@Reference、@EnableDubbo等注解 2. @ImportResource(locations=\\”classpath:provider.xml\\”) 使用 ImportResource注解导入外部xml文件 五、Dubbo配置[http://dubbo.apache.org/zh-cn/docs/user/configuration/annotation.html]{.underline} JVM 启动 -D 参数优先，这样可以使用户在部署和启动时进行参数重写，比如在启动时需改变协议的端口。 XML（springboot项目中对应的是application.properties文件） 次之，如果在 XML 中有配置，则 dubbo.properties 和代码中的相应配置项无效。 使用代码设置的方式优先级排在第三。 Properties 最后，相当于缺省值，只有 XML 没有配置时，dubbo.properties 的相应配置项才会生效，通常用于共享公共配置，比如应用名。 六、Dubbo官方提供的例子（功能）[http://dubbo.apache.org/zh-cn/docs/user/demos/preflight-check.html]{.underline} 6.1启动时检查 6.2 服务超时调用和重试次数1. timeout属性 2. retries 属性 默认重试2次，也就是说一共会调用提供者服务三次（第一次调用不计次数） 如果在注册中心，提供者有三个（a1、a2、a3），那么消费者(b1)在重试获取服务的时候，这三个都会可能去调用。 b1请求a1服务超时，发现注册中心存在相同的服务a2、a3那么b1会去请求a2或者a3，当然也有可能再次请求a1 那么什么时候使用这个字段。建议在“幂等”的业务场景下使用，不要在非幂等的场景下使用。 幂等：就是提供者提供的服务，调用多次跟调用一次的起到的作用是一致的（例如对数据库的delete某条数据的操作） 非幂等：单次调用和多次调用的结果是不一样的（数据库的insert操作） 设置为0，表示不进行重试，直接报异常 6.3标签属性配置优先级 6.4多版本当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。 可以按照以下的步骤进行版本迁移： 1.在低压力时间段，先升级一半提供者为新版本 2.再将所有消费者升级为新版本 3.然后将剩下的一半提供者升级为新版本 使用场景：新版接口需要替代旧版接口时。 Version=”*“表示任何调用新旧版本 举个例子： 使用第四章节的例子： 1.boot-user-module模块，添加一个新的UserService接口实现类，作为新接口的实现 给两个接口添加版本标识（标识新旧接口） 2.修改boot-order-module模块消费者消费接口版本 通过version属性实现新旧接口的调用 6.5本地存根 6.6 更多用法参见官网的事例七、Dubbo高可用7.1、zookeeper宕机与dubbo直连现象：zookeeper注册中心宕机，还可以消费dubbo暴露的服务。 原因： 健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 高可用：通过设计，减少系统不能提供服务的时间； 直连dubbo：因为我们知道zookeeper注册中心保存的信息主要是消息提供者的位置，那么我们消费者可以通过url的方式直接访问消息提供者提供的服务地址也是可以的。 7.2、集群下dubbo负载均衡配置在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。 负载均衡策略 http://dubbo.apache.org/zh-cn/docs/user/demos/loadbalance.html 1.Random LoadBalance 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 可以在Dubbo-admin设置某个服务的权重 2.RoundRobin LoadBalance 轮循，按公约后的权重设置轮循比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 3.LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 在消费服务的时候，总是查询上一个服务处理请求处理最快的那一台服务器 很明显请求会发到一号服务器，因为他处理最快。每次都记录请求处理的时间。 4.ConsistentHash LoadBalance 一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。算法参见：http://en.wikipedia.org/wiki/Consistent\\_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 \\ 缺省用 160 份虚拟节点，如果要修改，请配置 \\ 7.3、整合hystrix，服务熔断与降级处理1、服务降级什么是服务降级？ 当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作。 可以通过服务降级功能临时屏蔽某个出错的非关键服务，并定义降级后的返回策略。 向注册中心写入动态配置覆盖规则： RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf(&quot;zookeeper://10.20.153.10:2181&quot;));registry.register(URL.valueOf(&quot;override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;mock=force:return+null&quot;)); 其中： mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 相当于在dubbo-admin中设置如下： 还可以改为 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 2、集群容错[http://dubbo.apache.org/zh-cn/docs/user/demos/fault-tolerent-strategy.html]{.underline} 在集群调用失败时，Dubbo 提供了多种容错方案，缺省为 failover 重试。 集群容错模式 Failover Cluster失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=&quot;2&quot; 来设置重试次数(不含第一次)。重试次数配置如下：&lt;dubbo:service retries=&quot;2&quot; /&gt;或&lt;dubbo:reference retries=&quot;2&quot; /&gt;或&lt;dubbo:reference&gt; &lt;dubbo:method name=&quot;findFoo&quot; retries=&quot;2&quot; /&gt;&lt;/dubbo:reference&gt;Failfast Cluster快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。Failsafe Cluster失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。Failback Cluster失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。Forking Cluster并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=&quot;2&quot; 来设置最大并行数。Broadcast Cluster广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。集群模式配置按照以下示例在服务提供方和消费方配置集群模式&lt;dubbo:service cluster=&quot;failsafe&quot; /&gt;或&lt;dubbo:reference cluster=&quot;failsafe&quot; /&gt; 3、整合hystrixHystrix 旨在通过控制那些访问远程系统、服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。Hystrix具备拥有回退机制和断路器功能的线程和信号隔离，请求缓存和请求打包，以及监控和配置等功能 1、配置spring-cloud-starter-netflix-hystrixspring boot官方提供了对hystrix的集成，直接在pom.xml里加入依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;version&gt;1.4.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; 然后在Application类上增加\\@EnableHystrix来启用hystrix starter： @SpringBootApplication@EnableHystrixpublic class ProviderApplication &#123; 2、配置Provider端在Dubbo的Provider上增加\\@HystrixCommand配置，这样子调用就会经过Hystrix代理。 @com.alibaba.dubbo.config.annotation.Service (version = &quot;1.0.0&quot;)public class HelloServiceImpl implements HelloService &#123; @HystrixCommand(commandProperties = &#123; @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;), @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value = &quot;2000&quot;) &#125;) @Override public String sayHello(String name) &#123; // System.out.println(&quot;async provider received: &quot; + name); // return &quot;annotation: hello, &quot; + name; throw new RuntimeException(&quot;Exception to show hystrix enabled.&quot;); &#125;&#125; 3、配置Consumer端对于Consumer端，则可以增加一层method调用，并在method上配置@HystrixCommand。当调用出错时，会走到fallbackMethod = \\”reliable\\”的调用里。 @Reference(version = &quot;1.0.0&quot;)private HelloService demoService;@HystrixCommand(fallbackMethod = &quot;reliable&quot;)public String doSayHello(String name) &#123; return demoService.sayHello(name);&#125;public String reliable(String name) &#123; return &quot;hystrix fallback value&quot;;&#125; 八、Dubbo原理1、RPC原理 一次完整的RPC调用流程（同步调用，异步另说）如下： 1）服务消费方（client）调用以本地调用方式调用服务； 2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； 3）client stub找到服务地址，并将消息发送到服务端； 4）server stub收到消息后进行解码； 5）server stub根据解码结果调用本地的服务； 6）本地服务执行并将结果返回给server stub； 7）server stub将返回结果打包成消息并发送至消费方； 8）client stub接收到消息，并进行解码； 9）服务消费方得到最终结果。RPC框架的目标就是要2~8这些步骤都封装起来，这些细节对用户来说是透明的，不可见的。 2、netty通信原理Netty是一个异步事件驱动的网络应用程序框架， 用于快速开发可维护的高性能协议服务器和客户端。它极大地简化并简化了TCP和UDP套接字服务器等网络编程。 BIO：(Blocking IO) NIO (Non-Blocking IO) Selector 一般称 为选择器 ，也可以翻译为 多路复用器， Connect（连接就绪）、Accept（接受就绪）、Read（读就绪）、Write（写就绪） Netty基本原理： 3、dubbo原理1、dubbo原理 -框架设计[http://dubbo.apache.org/zh-cn/docs/dev/design.html]{.underline} config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 2、dubbo原理 -启动解析、加载配置信息我们知道spring加载配置文件，是通过BeanDefinitionparser这个接口的实现类进行绑定的 调用parse方法，解析标签 根据xml文件的每一行进行处理解析 不同的标签处理逻辑是不一样的 那么每一个标签对应的类是哪一个。这个是哪里定义的呢？ 通过DubboNamespaceHandler来定义标签对应的解析类 3、dubbo原理 -服务暴露通过上面的配置文件解析，我们知道服务相关的信息是通过解析后存放在ServiceBean中 public class ServiceBean&lt;T&gt; extends ServiceConfig&lt;T&gt; implements InitializingBean, DisposableBean, ApplicationContextAware, ApplicationListener&lt;ContextRefreshedEvent&gt;, BeanNameAware &#123; 关键的两个接口 InitializingBean Spring的接口,当组件创建完对象之后, 组件属性设置完成，会调用InitializingBean中的afterPropertiesSet方法 ApplicationListener应用监听器 监听IOC容器的刷新事件.当IOC容器中，所有对象都创建完成会回调onApplicationEvent方法 设置了延迟暴露，dubbo在Spring实例化bean（initializeBean）的时候会对实现了InitializingBean的类进行回调，回调方法是afterPropertySet() 没有设置延迟或者延迟为-1，dubbo会在Spring实例化完bean之后，在刷新容器最后一步发布ContextRefreshEvent事件的时候，通知实现了ApplicationListener的类进行回调onApplicationEvent 总的调用接口如下： 4、dubbo原理 -服务引用 5、dubbo原理 -服务调用","categories":[{"name":"dubbo","slug":"dubbo","permalink":"http://kingge.top/categories/dubbo/"}],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"http://kingge.top/tags/dubbo/"},{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"rpc","slug":"rpc","permalink":"http://kingge.top/tags/rpc/"}]},{"title":"RabbitMQ消息队列","slug":"RabbitMQ消息队列","date":"2018-09-03T12:53:09.000Z","updated":"2019-09-03T13:00:57.208Z","comments":true,"path":"2018/09/03/RabbitMQ消息队列/","link":"","permalink":"http://kingge.top/2018/09/03/RabbitMQ消息队列/","excerpt":"","text":"RabbitMQ 简介​ MQ全称为Message Queue，即消息队列， RabbitMQ是由erlang语言开发，基于AMQP（Advanced MessageQueue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开 ​ 发中应用非常广泛。RabbitMQ官方地址：http://www.rabbitmq.com/ 开发中消息队列通常有如下应用场景： 1、任务异步处理。将不需要同步处理的并且耗时长的操作由消息队列通知消息接收方进行异步处理。提高了应用程序的响应时间。 2、应用程序解耦合MQ相当于一个中介，生产方通过MQ与消费方交互，它将应用程序进行解耦合。 市场上还有哪些消息队列？ ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ、Redis。 为什么使用RabbitMQ呢？1、使得简单，功能强大。2、基于AMQP协议。3、社区活跃，文档完善。4、高并发性能好，这主要得益于Erlang语言。5、Spring Boot默认已集成RabbitMQ 其它相关知识 AMQP是什么 ？ ​ AMQP是一套公开的消息队列协议，最早在2003年被提出，它旨在从协议层定义消息通信数据的标准格式，为的就是解决MQ市场上协议不统一的问题。RabbitMQ就是遵循AMQP标准协议开发的MQ服务。 官方：http://www.amqp.org/ JMS是什么？ ​ JMS是java提供的一套消息服务API标准，其目的是为所有的java应用程序提供统一的消息通信的标准，类似java的jdbc，只要遵循jms标准的应用程序之间都可以进行消息通信。它和AMQP有什么 不同，jms是java语言专属的消息服务标准，它是在api层定义标准，并且只能用于java应用；而AMQP是在协议层定义的标准，是跨语言的 。 RabbitMQ的工作原理 组成部分说明如下： Broker：消息队列服务进程，此进程包括两个部分：Exchange和Queue。 ​ Exchange：消息队列交换机，按一定的规则将消息路由转发到某个队列，对消息进行过虑。 ​ Queue：消息队列，存储消息的队列，消息到达队列并转发给指定的消费方。 Producer：消息生产者，即生产方客户端，生产方客户端将消息发送到MQ。 Consumer：消息消费者，即消费方客户端，接收MQ转发的消息。 消息发布接收流程： —–发送消息—–1、生产者和Broker建立TCP连接。2、生产者和Broker建立通道。3、生产者通过通道消息发送给Broker，由Exchange将消息进行转发。4、Exchange将消息转发到指定的Queue（队列） —-接收消息—–1、消费者和Broker建立TCP连接2、消费者和Broker建立通道3、消费者监听指定的Queue（队列）4、当有消息到达Queue时Broker默认将消息推送给消费者。5、消费者接收到消息。 安装使用下载安装​ RabbitMQ由Erlang语言开发，Erlang语言用于并发及分布式系统的开发，在电信领域应用广泛，OTP（OpenTelecom Platform）作为Erlang语言的一部分，包含了很多基于Erlang开发的中间件及工具库，安装RabbitMQ需要安装Erlang/OTP，并保持版本匹配，如下图： RabbitMQ的下载地址：http://www.rabbitmq.com/download.html erlang的下载地址：http://erlang.org/download/otp_win64_20.3.exe 这里使用：Erlang/OTP 20.3版本和RabbitMQ3.7.3版本。 特别提示 安装erlang和rabbitMQ以管理员身份运行。 当卸载重新安装时会出现RabbitMQ服务注册失败，此时需要进入注册表清理erlang搜索RabbitMQ、ErlSrv，将对应的项全部删除。 安装erlang下载完成后，右键管理员运行安装，都是下一步安装完成即可。 配置erlang环境变量： ERLANG_HOME=D:\\erl9.3 在path中添加%ERLANG_HOME%\\bin; 安装RabbitMQ以管理员方式运行此文件，安装。 启动RabbitMQ安装成功后会自动创建RabbitMQ服务并且启动。 1）从开始菜单启动RabbitMQ RabbitMQ Service-install :安装服务RabbitMQ Service-remove 删除服务RabbitMQ Service-start 启动RabbitMQ Service-stop 启动 2）通过命令启动RabbitMQ 1.安装并运行服务 rabbitmq-service.bat install 安装服务 ​ rabbitmq-service.bat stop 停止服务 ​ rabbitmq-service.bat start 启动服务 2.安装管理插件 安装rabbitMQ的管理插件，方便在浏览器端管理RabbitMQ 管理员身份运行命令： rabbitmq-plugins.bat enable rabbitmq_management 3.启动成功 登录RabbitMQ ​ 进入浏览器，输入：http://localhost:15672 ​ 初始账号和密码：guest/guest 官方 Hello World 入门例子官方各个语言集成RabbitMQ说明：https://www.rabbitmq.com/devtools.html 下面的例子我们暂时使用，javaClient的方式，通过导入amqp-client-5.7.2.jar完成例子的使用 我们先用 rabbitMQ官方提供的java client测试，目的是对RabbitMQ的交互过程有个清晰的认识。参考 ：https://github.com/rabbitmq/rabbitmq-java-client/ 官网例子地址：https://www.rabbitmq.com/getstarted.html 创建maven工程-这里为了方面后面的集成，直接创建springboot项目创建生产者工程和消费者工程，分别加入RabbitMQ java client的依赖。 test-rabbitmq-producer：生产者工程test-rabbitmq-consumer：消费者工程 &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp‐client&lt;/artifactId&gt; &lt;version&gt;4.0.3&lt;/version&gt;&lt;!‐‐此版本与spring boot 1.5.9版本匹配‐‐&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring‐boot‐starter‐logging&lt;/artifactId&gt;&lt;/dependency&gt; 完整项目目录： 生产者在生产者工程下的test中创建测试类如下： import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Producer01 &#123; //队列 private static final String QUEUE = &quot;helloworld&quot;; public static void main(String[] args) &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); Connection connection = null; Channel channel = null; try &#123; //建立新连接 connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 channel = connection.createChannel(); //声明队列，如果队列在mq 中没有则要创建 //参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE,true,false,false,null); //发送消息 //参数：String exchange, String routingKey, BasicProperties props, byte[] body /** * 参数明细： * 1、exchange，交换机，如果不指定将使用mq的默认交换机（设置为&quot;&quot;） * 2、routingKey，路由key，交换机根据路由key来将消息转发到指定的队列，如果使用默认交换机，routingKey设置为队列的名称 * 3、props，消息的属性 * 4、body，消息内容 */ //消息内容 String message = &quot;hello world kingge发送消息2&quot;; channel.basicPublish(&quot;&quot;,QUEUE,null,message.getBytes()); System.out.println(&quot;send to mq &quot;+message); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭连接 //先关闭通道 try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 消费者import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer01 &#123; //队列 private static final String QUEUE = &quot;helloworld&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); //建立新连接 Connection connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); //监听队列 //声明队列，如果队列在mq 中没有则要创建 //参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE,true,false,false,null); //实现消费方法 DefaultConsumer defaultConsumer = new DefaultConsumer(channel)&#123; /** * 当接收到消息后此方法将被调用 * @param consumerTag 消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume * @param envelope 信封，通过envelope * @param properties 消息属性 * @param body 消息内容 * @throws IOException */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //交换机 String exchange = envelope.getExchange(); //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收（因为如果下面的autoAck设置为false就需要自行回复mq消息已经接受） long deliveryTag = envelope.getDeliveryTag(); //消息内容 String message= new String(body,&quot;utf-8&quot;); System.out.println(&quot;receive message:&quot;+message); &#125; &#125;; //监听队列 //参数：String queue, boolean autoAck, Consumer callback /** * 参数明细： * 1、queue 队列名称 * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复 * 3、callback，消费方法，当消费者接收到消息要执行的方法 */ channel.basicConsume(QUEUE,true,defaultConsumer); &#125;&#125; 测试（1）启动消费者：发送消息 （2）查询RabbitMQ后台管理浏览器 （3）启动消费者 消息消费成功 已经变为了0 总结 1、发送端操作流程 1）创建连接 2）创建通道 3）声明队列 4）发送消息 2、接收端 1）创建连接 2）创建通道 3）声明队列 4）监听队列 5）接收消息 6）ack回复 通过代码我们不难发现，生产者和消费者的代码，其实前半段都是一模一样的。（创建连接工厂，创建连接，创建通道，声明队列） RabbitMQ工作模式RabbitMQ有以下几种工作模式 ：https://www.rabbitmq.com/getstarted.html 每后一个模式都能够实现前一个模式的功能 Work queues​ work queues与入门程序相比，多了一个消费端，两个消费端共同消费同一个队列中的消息，而且它支持负载均衡的方式消费消息。 应用场景：对于任务过重或任务较多情况使用工作队列可以提高任务处理的速度（开启多个消费者消费同一个队列，处理相应的业务）。 案例1、使用入门程序，启动多个消费者（启动多次即可）。 这里启动了两个消费者，都是监听helloworld队列。 2、生产者发送多个消息。 修改入门例子： for(int i=0;i&lt;5;i++)&#123; //消息内容 String message = &quot;this is a msg&quot;; channel.basicPublish(&quot;&quot;,QUEUE,null,message.getBytes()); System.out.println(&quot;send to mq &quot;+message); &#125; 运行生产者 3.查看消费者输出 第一个消费者： 第二个消费者： 结果： 1、一条消息只会被一个消费者接收。2、rabbit采用轮询的方式将消息是平均发送给消费者的。3、消费者在处理完某条消息后，才会收到下一条消息。 Publish/subscribe 发布订阅模式：1、每个消费者监听自己的队列。2、生产者将消息发给broker，由交换机将消息转发到绑定此交换机的每个队列，每个绑定交换机的队列都将接收到消息（也就是说，生产者发出的消息，每个队列都会收到。例如发出五个消息，那么每个队列都会收到五个） 案例用户通知消费通知。当用户充值成功或转账完成系统通知用户，通知方式有短信、邮件多种方法 。 1.生产者代码声明Exchange_fanout_inform交换机，声明两个队列并且绑定到此交换机，绑定时不需要指定routingkey，发送消息时不需要指定routingkey import com.rabbitmq.client.BuiltinExchangeType;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Producer02_publish &#123; //队列名称 private static final String QUEUE_INFORM_EMAIL = &quot;queue_inform_email&quot;; private static final String QUEUE_INFORM_SMS = &quot;queue_inform_sms&quot;; private static final String EXCHANGE_FANOUT_INFORM=&quot;exchange_fanout_inform&quot;; public static void main(String[] args) &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); Connection connection = null; Channel channel = null; try &#123; //建立新连接 connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 channel = connection.createChannel(); //声明队列，如果队列在mq 中没有则要创建 //参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_EMAIL,true,false,false,null); channel.queueDeclare(QUEUE_INFORM_SMS,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_FANOUT_INFORM, BuiltinExchangeType.FANOUT); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_EMAIL,EXCHANGE_FANOUT_INFORM,&quot;&quot;); channel.queueBind(QUEUE_INFORM_SMS,EXCHANGE_FANOUT_INFORM,&quot;&quot;); //发送消息 //参数：String exchange, String routingKey, BasicProperties props, byte[] body /** * 参数明细： * 1、exchange，交换机，如果不指定将使用mq的默认交换机（设置为&quot;&quot;） * 2、routingKey，路由key，交换机根据路由key来将消息转发到指定的队列，如果使用默认交换机，routingKey设置为队列的名称 * 3、props，消息的属性 * 4、body，消息内容 */ for(int i=0;i&lt;5;i++)&#123; //消息内容 String message = &quot;send inform message to user&quot;; channel.basicPublish(EXCHANGE_FANOUT_INFORM,&quot;&quot;,null,message.getBytes()); System.out.println(&quot;send to mq &quot;+message); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭连接 //先关闭通道 try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 2.邮件发送消费者import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer02_subscribe_email &#123; //队列名称 private static final String QUEUE_INFORM_EMAIL = &quot;queue_inform_email&quot;; private static final String EXCHANGE_FANOUT_INFORM=&quot;exchange_fanout_inform&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); //建立新连接 Connection connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_EMAIL,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_FANOUT_INFORM, BuiltinExchangeType.FANOUT); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_EMAIL, EXCHANGE_FANOUT_INFORM, &quot;&quot;); //实现消费方法 DefaultConsumer defaultConsumer = new DefaultConsumer(channel)&#123; /** * 当接收到消息后此方法将被调用 * @param consumerTag 消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume * @param envelope 信封，通过envelope * @param properties 消息属性 * @param body 消息内容 * @throws IOException */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //交换机 String exchange = envelope.getExchange(); //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收 long deliveryTag = envelope.getDeliveryTag(); //消息内容 String message= new String(body,&quot;utf-8&quot;); System.out.println(&quot;receive message:&quot;+message); &#125; &#125;; //监听队列 //参数：String queue, boolean autoAck, Consumer callback /** * 参数明细： * 1、queue 队列名称 * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复 * 3、callback，消费方法，当消费者接收到消息要执行的方法 */ channel.basicConsume(QUEUE_INFORM_EMAIL,true,defaultConsumer); &#125;&#125; 3.短信发送消费者import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer02_subscribe_sms &#123; //队列名称 private static final String QUEUE_INFORM_SMS = &quot;queue_inform_sms&quot;; private static final String EXCHANGE_FANOUT_INFORM=&quot;exchange_fanout_inform&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); //建立新连接 Connection connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_SMS,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_FANOUT_INFORM, BuiltinExchangeType.FANOUT); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_SMS, EXCHANGE_FANOUT_INFORM, &quot;&quot;); //实现消费方法 DefaultConsumer defaultConsumer = new DefaultConsumer(channel)&#123; /** * 当接收到消息后此方法将被调用 * @param consumerTag 消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume * @param envelope 信封，通过envelope * @param properties 消息属性 * @param body 消息内容 * @throws IOException */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //交换机 String exchange = envelope.getExchange(); //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收 long deliveryTag = envelope.getDeliveryTag(); //消息内容 String message= new String(body,&quot;utf-8&quot;); System.out.println(&quot;receive message:&quot;+message); &#125; &#125;; //监听队列 //参数：String queue, boolean autoAck, Consumer callback /** * 参数明细： * 1、queue 队列名称 * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复 * 3、callback，消费方法，当消费者接收到消息要执行的方法 */ channel.basicConsume(QUEUE_INFORM_SMS,true,defaultConsumer); &#125;&#125; 4.测试1.启动消息生产者，发送消息 2.RabbitMQ的管理界面查看交换机 单击进去 可以看到他绑定了两个队列 查看队列信息： 两个队列各有五条消息。 3.启动两个消费者 总结1.使用生产者发送若干条消息，每条消息都转发到各各队列，每消费者都接收到了消息。 思考 publish/subscribe与work queues有什么区别和相似点。 区别：1）work queues不用定义交换机（使用默认的交换机），而publish/subscribe需要定义交换机。2）publish/subscribe的生产方是面向交换机发送消息，work queues的生产方是面向队列发送消息(底层使用默认交换机)。3）publish/subscribe需要设置队列和交换机的绑定，work queues不需要设置，实质上work queues会将队列绑定到默认的交换机 。 相同点： 两者实现的发布/订阅的效果是一样的，多个消费端监听同一个队列不会重复消费消息。 建议使用 publish/subscribe，发布订阅模式比工作队列模式更强大，并且发布订阅模式可以指定自己专用的交换机。同时发布订阅模式，可以实现工作队列模式的功能（我们可以启动两个Consumer02_subscribe_sms消费者，然后他们都监听queue_inform_sms队列，当queue_inform_sms收到消息时他们会采用工作队列模式的轮循方式消费消息） Routing路由模式： 1、每个消费者监听自己的队列，并且设置routingkey。2、生产者将消息发给交换机，由交换机根据routingkey来转发消息到指定的队列。 也就是说相比于发布订阅模式，他多了路由这个功能。 案例通过rootingkey分别发送五条消息到sms队列和email队列 1.生产者代码声明exchange_routing_inform交换机，声明两个队列并且绑定到此交换机，绑定时需要指定routingkey发送消息时需要指定routingkey import com.rabbitmq.client.BuiltinExchangeType;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Producer03_routing &#123; //队列名称 private static final String QUEUE_INFORM_EMAIL = &quot;queue_inform_email&quot;; private static final String QUEUE_INFORM_SMS = &quot;queue_inform_sms&quot;; //路由交换机名称 private static final String EXCHANGE_ROUTING_INFORM=&quot;exchange_routing_inform&quot;; //两个队列对应的rootingkey private static final String ROUTINGKEY_EMAIL=&quot;inform_email&quot;; private static final String ROUTINGKEY_SMS=&quot;inform_sms&quot;; public static void main(String[] args) &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); Connection connection = null; Channel channel = null; try &#123; //建立新连接 connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 channel = connection.createChannel(); //声明队列，如果队列在mq 中没有则要创建 //参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_EMAIL,true,false,false,null); channel.queueDeclare(QUEUE_INFORM_SMS,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_ROUTING_INFORM, BuiltinExchangeType.DIRECT); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_EMAIL,EXCHANGE_ROUTING_INFORM,ROUTINGKEY_EMAIL); channel.queueBind(QUEUE_INFORM_SMS,EXCHANGE_ROUTING_INFORM,ROUTINGKEY_SMS); //发送消息 //参数：String exchange, String routingKey, BasicProperties props, byte[] body /** * 参数明细： * 1、exchange，交换机，如果不指定将使用mq的默认交换机（设置为&quot;&quot;） * 2、routingKey，路由key，交换机根据路由key来将消息转发到指定的队列，如果使用默认交换机，routingKey设置为队列的名称 * 3、props，消息的属性 * 4、body，消息内容 */ for(int i=0;i&lt;5;i++)&#123; //发送消息的时候指定routingKey String message = &quot;send email inform message to user&quot;; channel.basicPublish(EXCHANGE_ROUTING_INFORM,ROUTINGKEY_EMAIL,null,message.getBytes()); System.out.println(&quot;send to mq &quot;+message); &#125; for(int i=0;i&lt;5;i++)&#123; //发送消息的时候指定routingKey String message = &quot;send sms inform message to user&quot;; channel.basicPublish(EXCHANGE_ROUTING_INFORM,ROUTINGKEY_SMS,null,message.getBytes()); System.out.println(&quot;send to mq &quot;+message); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭连接 //先关闭通道 try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 跟发布订阅模式的生产者相比代码的区别： 1.声明一个交换机时候，类型修改为BuiltinExchangeType.DIRECT 路由工作模式 2.在进行交换机和队列绑定的时候（channel.queueBind）多出了指明队列指定的rootingkey这一参数 3.发消息时，需要指明消息发送的rootingkey（也即是要发送到哪一个队列） 2.邮件消费者import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer03_routing_email &#123; //队列名称 private static final String QUEUE_INFORM_EMAIL = &quot;queue_inform_email&quot;; private static final String EXCHANGE_ROUTING_INFORM=&quot;exchange_routing_inform&quot;; private static final String ROUTINGKEY_EMAIL=&quot;inform_email&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); //建立新连接 Connection connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_EMAIL,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_ROUTING_INFORM, BuiltinExchangeType.DIRECT); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_EMAIL, EXCHANGE_ROUTING_INFORM,ROUTINGKEY_EMAIL); //实现消费方法 DefaultConsumer defaultConsumer = new DefaultConsumer(channel)&#123; /** * 当接收到消息后此方法将被调用 * @param consumerTag 消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume * @param envelope 信封，通过envelope * @param properties 消息属性 * @param body 消息内容 * @throws IOException */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //交换机 String exchange = envelope.getExchange(); //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收 long deliveryTag = envelope.getDeliveryTag(); //消息内容 String message= new String(body,&quot;utf-8&quot;); System.out.println(&quot;receive message:&quot;+message); &#125; &#125;; //监听队列 //参数：String queue, boolean autoAck, Consumer callback /** * 参数明细： * 1、queue 队列名称 * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复 * 3、callback，消费方法，当消费者接收到消息要执行的方法 */ channel.basicConsume(QUEUE_INFORM_EMAIL,true,defaultConsumer); &#125;&#125; 3.短信消费者import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer03_routing_sms &#123; //队列名称 private static final String QUEUE_INFORM_SMS = &quot;queue_inform_sms&quot;; private static final String EXCHANGE_ROUTING_INFORM=&quot;exchange_routing_inform&quot;; private static final String ROUTINGKEY_SMS=&quot;inform_sms&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); //建立新连接 Connection connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_SMS,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_ROUTING_INFORM, BuiltinExchangeType.DIRECT); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_SMS, EXCHANGE_ROUTING_INFORM,ROUTINGKEY_SMS); //实现消费方法 DefaultConsumer defaultConsumer = new DefaultConsumer(channel)&#123; /** * 当接收到消息后此方法将被调用 * @param consumerTag 消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume * @param envelope 信封，通过envelope * @param properties 消息属性 * @param body 消息内容 * @throws IOException */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //交换机 String exchange = envelope.getExchange(); //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收 long deliveryTag = envelope.getDeliveryTag(); //消息内容 String message= new String(body,&quot;utf-8&quot;); System.out.println(&quot;receive message:&quot;+message); &#125; &#125;; //监听队列 //参数：String queue, boolean autoAck, Consumer callback /** * 参数明细： * 1、queue 队列名称 * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复 * 3、callback，消费方法，当消费者接收到消息要执行的方法 */ channel.basicConsume(QUEUE_INFORM_SMS,true,defaultConsumer); &#125;&#125; 4.测试1.启动生产者，打开RabbitMQ的管理界面，观察交换机绑定情况： ​ 使用生产者发送若干条消息，交换机根据routingkey转发消息到指定的队列。 总结 Routing模式和Publish/subscibe的区别 Routing模式要求队列在绑定交换机时要指定routingkey，消息会转发到符合routingkey的队列。 Topics 我们可以称它为加强版的Routing模式： 1、每个消费者监听自己的队列，并且设置带统配符的routingkey。2、生产者将消息发给broker，由交换机根据routingkey来转发消息到指定的队列。 (星号) 只能替换 1 个条件。 (井号) 可替换 0 或多个条件。 案例​ 根据用户的通知设置去通知用户，设置接收Email的用户只接收Email，设置接收sms的用户只接收sms，设置两种通知类型都接收的则两种通知都有效。 1.生产者代码声明交换机，指定topic类型： 关键代码 /** * 声明交换机 * param1：交换机名称 * param2:交换机类型 四种交换机类型：direct、fanout、topic、headers */ channel.exchangeDeclare(EXCHANGE_TOPICS_INFORM, BuiltinExchangeType.TOPIC);//Email通知channel.basicPublish(EXCHANGE_TOPICS_INFORM, &quot;inform.email&quot;, null, message.getBytes());//sms通知channel.basicPublish(EXCHANGE_TOPICS_INFORM, &quot;inform.sms&quot;, null, message.getBytes());//两种都通知channel.basicPublish(EXCHANGE_TOPICS_INFORM, &quot;inform.sms.email&quot;, null, message.getBytes()); 完整代码 import com.rabbitmq.client.BuiltinExchangeType;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Producer04_topics &#123; //队列名称 private static final String QUEUE_INFORM_EMAIL = &quot;queue_inform_email&quot;; private static final String QUEUE_INFORM_SMS = &quot;queue_inform_sms&quot;; //交换机名称 private static final String EXCHANGE_TOPICS_INFORM=&quot;exchange_topics_inform&quot;; //路由通配符 private static final String ROUTINGKEY_EMAIL=&quot;inform.#.email.#&quot;; private static final String ROUTINGKEY_SMS=&quot;inform.#.sms.#&quot;; public static void main(String[] args) &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); Connection connection = null; Channel channel = null; try &#123; //建立新连接 connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 channel = connection.createChannel(); //声明队列，如果队列在mq 中没有则要创建 //参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_EMAIL,true,false,false,null); channel.queueDeclare(QUEUE_INFORM_SMS,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_TOPICS_INFORM, BuiltinExchangeType.TOPIC); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_EMAIL,EXCHANGE_TOPICS_INFORM,ROUTINGKEY_EMAIL); channel.queueBind(QUEUE_INFORM_SMS,EXCHANGE_TOPICS_INFORM,ROUTINGKEY_SMS); //发送消息 //参数：String exchange, String routingKey, BasicProperties props, byte[] body /** * 参数明细： * 1、exchange，交换机，如果不指定将使用mq的默认交换机（设置为&quot;&quot;） * 2、routingKey，路由key，交换机根据路由key来将消息转发到指定的队列，如果使用默认交换机，routingKey设置为队列的名称 * 3、props，消息的属性 * 4、body，消息内容 */ for(int i=0;i&lt;5;i++)&#123; //发送消息的时候指定routingKey String message = &quot;send email inform message to user&quot;; channel.basicPublish(EXCHANGE_TOPICS_INFORM,&quot;inform.email&quot;,null,message.getBytes()); System.out.println(&quot;send to mq &quot;+message); &#125; for(int i=0;i&lt;5;i++)&#123; //发送消息的时候指定routingKey String message = &quot;send sms inform message to user&quot;; channel.basicPublish(EXCHANGE_TOPICS_INFORM,&quot;inform.sms&quot;,null,message.getBytes()); System.out.println(&quot;send to mq &quot;+message); &#125; for(int i=0;i&lt;5;i++)&#123; //发送消息的时候指定routingKey String message = &quot;send sms and email inform message to user&quot;; channel.basicPublish(EXCHANGE_TOPICS_INFORM,&quot;inform.sms.email&quot;,null,message.getBytes()); System.out.println(&quot;send to mq &quot;+message); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭连接 //先关闭通道 try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (TimeoutException e) &#123; e.printStackTrace(); &#125; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 分析一下规则： 在上面的代码中我们定义了下面两个通配符规则 //路由通配符private static final String ROUTINGKEY_EMAIL=&quot;inform.#.email.#&quot;;private static final String ROUTINGKEY_SMS=&quot;inform.#.sms.#&quot;; # 可以匹配多个词，符号*可以匹配一个词语。 举例子： inform.email 和 inform.sms 分别满足上面的ROUTINGKEY_EMAIL和ROUTINGKEY_SMS，所以消息各自发送到sms队列和email队列，因为#在这里是0个。 那么当inform.sms.email时，同时满足ROUTINGKEY_EMAIL和ROUTINGKEY_SMS，也就是说他把消息同时发送到sms队列和email队列。 2.邮件消费者import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer04_topics_email &#123; //队列名称 private static final String QUEUE_INFORM_EMAIL = &quot;queue_inform_email&quot;; private static final String EXCHANGE_TOPICS_INFORM=&quot;exchange_topics_inform&quot;; private static final String ROUTINGKEY_EMAIL=&quot;inform.#.email.#&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); //建立新连接 Connection connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_EMAIL,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_TOPICS_INFORM, BuiltinExchangeType.TOPIC); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_EMAIL, EXCHANGE_TOPICS_INFORM,ROUTINGKEY_EMAIL); //实现消费方法 DefaultConsumer defaultConsumer = new DefaultConsumer(channel)&#123; /** * 当接收到消息后此方法将被调用 * @param consumerTag 消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume * @param envelope 信封，通过envelope * @param properties 消息属性 * @param body 消息内容 * @throws IOException */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //交换机 String exchange = envelope.getExchange(); //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收 long deliveryTag = envelope.getDeliveryTag(); //消息内容 String message= new String(body,&quot;utf-8&quot;); System.out.println(&quot;receive message:&quot;+message); &#125; &#125;; //监听队列 //参数：String queue, boolean autoAck, Consumer callback /** * 参数明细： * 1、queue 队列名称 * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复 * 3、callback，消费方法，当消费者接收到消息要执行的方法 */ channel.basicConsume(QUEUE_INFORM_EMAIL,true,defaultConsumer); &#125;&#125; 3.短信消费者import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer04_topics_sms &#123; //队列名称 private static final String QUEUE_INFORM_SMS = &quot;queue_inform_sms&quot;; private static final String EXCHANGE_TOPICS_INFORM=&quot;exchange_topics_inform&quot;; private static final String ROUTINGKEY_SMS=&quot;inform.#.sms.#&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; //通过连接工厂创建新的连接和mq建立连接 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;127.0.0.1&quot;); connectionFactory.setPort(5672);//端口 connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); //设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq connectionFactory.setVirtualHost(&quot;/&quot;); //建立新连接 Connection connection = connectionFactory.newConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_INFORM_SMS,true,false,false,null); //声明一个交换机 //参数：String exchange, String type /** * 参数明细： * 1、交换机的名称 * 2、交换机的类型 * fanout：对应的rabbitmq的工作模式是 publish/subscribe * direct：对应的Routing 工作模式 * topic：对应的Topics工作模式 * headers： 对应的headers工作模式 */ channel.exchangeDeclare(EXCHANGE_TOPICS_INFORM, BuiltinExchangeType.TOPIC); //进行交换机和队列绑定 //参数：String queue, String exchange, String routingKey /** * 参数明细： * 1、queue 队列名称 * 2、exchange 交换机名称 * 3、routingKey 路由key，作用是交换机根据路由key的值将消息转发到指定的队列中，在发布订阅模式中调协为空字符串 */ channel.queueBind(QUEUE_INFORM_SMS, EXCHANGE_TOPICS_INFORM,ROUTINGKEY_SMS); //实现消费方法 DefaultConsumer defaultConsumer = new DefaultConsumer(channel)&#123; /** * 当接收到消息后此方法将被调用 * @param consumerTag 消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume * @param envelope 信封，通过envelope * @param properties 消息属性 * @param body 消息内容 * @throws IOException */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //交换机 String exchange = envelope.getExchange(); //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收 long deliveryTag = envelope.getDeliveryTag(); //消息内容 String message= new String(body,&quot;utf-8&quot;); System.out.println(&quot;receive message:&quot;+message); &#125; &#125;; //监听队列 //参数：String queue, boolean autoAck, Consumer callback /** * 参数明细： * 1、queue 队列名称 * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复 * 3、callback，消费方法，当消费者接收到消息要执行的方法 */ channel.basicConsume(QUEUE_INFORM_SMS,true,defaultConsumer); &#125;&#125; 4.测试1.启动生产者 查看浏览器控制台 查看生产者代码解释为什么各自有10条消息 使用生产者发送若干条消息，交换机根据routingkey统配符匹配并转发消息到指定的队列。所以这里两个队列各拥有10条消息。 总结 本案例的需求使用Routing工作模式能否实现 ​ 上面的案例的本质就是通过通配符的方式。我们可以使用Routing精确匹配也可实现。 使用Routing模式也可以实现本案例，共设置三个 routingkey，分别是email、sms、all，email队列绑定email和all，sms队列绑定sms和all，这样就可以实现上边案例的功能，实现过程比topics复杂。 Topic模式更多加强大，它可以实现Routing、publish/subscirbe模式的功能。 Header模式 header模式与routing不同的地方在于，header模式取消routingkey，使用header中的 key/value（键值对）匹配队列。 案例根据用户的通知设置去通知用户，设置接收Email的用户只接收Email，设置接收sms的用户只接收sms，设置两种通知类型都接收的则两种通知都有效。 1.生产者代码队列与交换机绑定的代码与之前不同，如下： Map&lt;String, Object&gt; headers_email = new Hashtable&lt;String, Object&gt;();headers_email.put(&quot;inform_type&quot;, &quot;email&quot;);Map&lt;String, Object&gt; headers_sms = new Hashtable&lt;String, Object&gt;();headers_sms.put(&quot;inform_type&quot;, &quot;sms&quot;);channel.queueBind(QUEUE_INFORM_EMAIL,EXCHANGE_HEADERS_INFORM,&quot;&quot;,headers_email);channel.queueBind(QUEUE_INFORM_SMS,EXCHANGE_HEADERS_INFORM,&quot;&quot;,headers_sms); String message = &quot;email inform to user&quot;+i;Map&lt;String,Object&gt; headers = new Hashtable&lt;String, Object&gt;();headers.put(&quot;inform_type&quot;, &quot;email&quot;);//匹配email通知消费者绑定的header//headers.put(&quot;inform_type&quot;, &quot;sms&quot;);//匹配sms通知消费者绑定的headerAMQP.BasicProperties.Builder properties = new AMQP.BasicProperties.Builder();properties.headers(headers);//Email通知channel.basicPublish(EXCHANGE_HEADERS_INFORM, &quot;&quot;, properties.build(), message.getBytes()); 2.邮件消费者channel.exchangeDeclare(EXCHANGE_HEADERS_INFORM, BuiltinExchangeType.HEADERS);Map&lt;String, Object&gt; headers_email = new Hashtable&lt;String, Object&gt;();headers_email.put(&quot;inform_email&quot;, &quot;email&quot;);//交换机和队列绑定channel.queueBind(QUEUE_INFORM_EMAIL,EXCHANGE_HEADERS_INFORM,&quot;&quot;,headers_email);//指定消费队列channel.basicConsume(QUEUE_INFORM_EMAIL, true, consumer); 测试 RPC RPC即客户端远程调用服务端的方法 ，使用MQ可以实现RPC的异步调用，基于Direct交换机实现，流程如下：1、客户端即是生产者就是消费者，向RPC请求队列发送RPC调用消息，同时监听RPC响应队列。2、服务端监听RPC请求队列的消息，收到消息后执行服务端的方法，得到方法返回的结果3、服务端将RPC方法 的结果发送到RPC响应队列4、客户端（RPC调用方）监听RPC响应队列，接收到RPC调用结果。 Spring整合RibbitMQ我们选择基于Spring-Rabbit去操作RabbitMQ https://github.com/spring-projects/spring-amqp 使用spring-boot-starter-amqp会自动添加spring-rabbit依赖，所以注释掉我们上面使用的java client方式引入的amqp-client依赖。 生产者修改pom文件修改为 &lt;dependencies&gt;&lt;!-- &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;4.0.3&lt;/version&gt;&amp;lt;!&amp;ndash;此版本与spring boot 1.5.9版本匹配&amp;ndash;&amp;gt; &lt;/dependency&gt;--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.yml配置文件server: port: 44004spring: application: name: test-rabbitmq-consumer rabbitmq: host: 127.0.0.1 port: 5672 username: guest password: guest virtualHost: / 定义RabbitConﬁg类，配置Exchange、Queue、及绑定交换机这里以配置Topics模式为例子。 import org.springframework.amqp.core.*;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class RabbitmqConfig &#123; public static final String QUEUE_INFORM_EMAIL = &quot;queue_inform_email&quot;; public static final String QUEUE_INFORM_SMS = &quot;queue_inform_sms&quot;; public static final String EXCHANGE_TOPICS_INFORM=&quot;exchange_topics_inform&quot;; public static final String ROUTINGKEY_EMAIL=&quot;inform.#.email.#&quot;; public static final String ROUTINGKEY_SMS=&quot;inform.#.sms.#&quot;; //声明交换机 @Bean(EXCHANGE_TOPICS_INFORM) public Exchange EXCHANGE_TOPICS_INFORM()&#123; //durable(true) 持久化，mq重启之后交换机还在 return ExchangeBuilder.topicExchange(EXCHANGE_TOPICS_INFORM).durable(true).build(); &#125; //声明QUEUE_INFORM_EMAIL队列 @Bean(QUEUE_INFORM_EMAIL) public Queue QUEUE_INFORM_EMAIL()&#123; return new Queue(QUEUE_INFORM_EMAIL); &#125; //声明QUEUE_INFORM_SMS队列 @Bean(QUEUE_INFORM_SMS) public Queue QUEUE_INFORM_SMS()&#123; return new Queue(QUEUE_INFORM_SMS); &#125; //ROUTINGKEY_EMAIL队列绑定交换机，指定routingKey @Bean public Binding BINDING_QUEUE_INFORM_EMAIL(@Qualifier(QUEUE_INFORM_EMAIL) Queue queue, @Qualifier(EXCHANGE_TOPICS_INFORM) Exchange exchange)&#123; return BindingBuilder.bind(queue).to(exchange).with(ROUTINGKEY_EMAIL).noargs(); &#125; //ROUTINGKEY_SMS队列绑定交换机，指定routingKey @Bean public Binding BINDING_ROUTINGKEY_SMS(@Qualifier(QUEUE_INFORM_SMS) Queue queue, @Qualifier(EXCHANGE_TOPICS_INFORM) Exchange exchange)&#123; return BindingBuilder.bind(queue).to(exchange).with(ROUTINGKEY_SMS).noargs(); &#125;&#125; 生产端使用RarbbitTemplate发送消息 import com.alibaba.fastjson.JSON;import com.xuecheng.test.rabbitmq.config.RabbitmqConfig;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import java.util.HashMap;import java.util.Map;@SpringBootTest@RunWith(SpringRunner.class)public class Producer05_topics_springboot &#123; @Autowired RabbitTemplate rabbitTemplate; //使用rabbitTemplate发送消息 @Test public void testSendEmail()&#123; String message = &quot;send email message to user&quot;; /** * 参数： * 1、交换机名称 * 2、routingKey * 3、消息内容 */ rabbitTemplate.convertAndSend(RabbitmqConfig.EXCHANGE_TOPICS_INFORM,&quot;inform.email&quot;,message); &#125;&#125; 消费端者修改修改pom文件 &lt;dependencies&gt;&lt;!-- &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;4.0.3&lt;/version&gt;&amp;lt;!&amp;ndash;此版本与spring boot 1.5.9版本匹配&amp;ndash;&amp;gt; &lt;/dependency&gt;--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 使用@RabbitListener注解监听队列@Componentpublic class ReceiveHandler &#123; @RabbitListener(queues = &#123;RabbitmqConfig.QUEUE_INFORM_EMAIL&#125;) public void receive_email(String msg,Message message,Channel channel)&#123; System.out.println(&quot;receive message is:&quot;+msg); &#125;&#125;","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://kingge.top/categories/消息队列/"}],"tags":[{"name":"RabbitMq","slug":"RabbitMq","permalink":"http://kingge.top/tags/RabbitMq/"},{"name":"消息队列","slug":"消息队列","permalink":"http://kingge.top/tags/消息队列/"},{"name":"分布式通信","slug":"分布式通信","permalink":"http://kingge.top/tags/分布式通信/"}]},{"title":"flume总结","slug":"flume总结","date":"2018-06-27T16:00:00.000Z","updated":"2020-05-10T02:19:54.235Z","comments":true,"path":"2018/06/28/flume总结/","link":"","permalink":"http://kingge.top/2018/06/28/flume总结/","excerpt":"","text":"一、 Flume 简介案例场景1： 公司使用python爬虫，从网上爬取了3T的数据存储到本地磁盘中，而且每天都会增加。那么这样会产生一个问题，磁盘会面临容量瓶颈，存不下了！！那么怎么办呢？ 解决方案一：使用hadoop 的put 命令或者书写mapreduce，来把数据剪切到HDFS中。 这个方案的好处就是利用了hadoop的hdfs可以存储大量数据的好处，弊端就是，假设爬虫实时爬取的数据很大，那么在转移数据到hdfs之前，马上就达到了瓶颈，那么就会出现问题。而且使用这种方案，一次性转移大量的数据，可能会使得hadoop崩溃等等。 Flume\\ 就是能够解决这个问题，实时的进行数据采集保存到hdfs***。*** 1) Flume 提供一个分布式的，可靠的，对大数据量的日志进行高效收集、聚集、移动的服务， Flume 只能在 Unix 环境下运行。 2) Flume 基于流式架构，容错性强，也很灵活简单。 3) Flume、Kafka 用来实时进行数据收集，Spark、Storm 用来实时处理数据，使用MR或者Hive处理离线数据，impala 用来实时查询。 Flume***和* *Kafka**都是可以实时进行数据采集，那么他们两个的区别是什么呢？flume适合操作存储在**磁盘上的实时数据**，kafka适合操作保存在**内存中的实时数据*** 二、Flume 角色 2.1、Source用于采集数据，Source 是产生数据流的地方，同时 Source 会将产生的数据流传输到 Channel， 这个有点类似于 Java IO 部分的 Channel。 2.2、Channel用于桥接 Sources 和 Sinks，类似于一个队列。 2.3、Sink从 Channel 收集数据，将数据写到目标源(可以是下一个 Source，也可以是 HDFS 或者 HBase)。 2.4、Event传输单元，Flume 数据传输的基本单元，以事件的形式将数据从源头送至目的地。 三、Flume 传输过程source 监控某个文件或数据流，数据源产生新的数据，拿到该数据后，将数据封装在一个 Event 中，并 put 到 channel 后 commit 提交，channel 队列先进先出，sink 去 channel 队列中 拉取数据，然后写入到 HDFS 中。 四、Flume 部署及使用4.1、文件配置flume-env.sh 涉及修改项： ​ ​ export JAVA_HOME=/home/admin/modules/jdk1.8.0_121 4.1.1 关于文件夹中的cdh版本的flume 这个版本和apache官方发行的flume有什么区别呢？ Clodera发布的cdh版本，里面包含了关于大数据一整套常用的资源。例如上面的cdh就包含了 他这么做的目的就是为了适配各个***hadoop**常用框架之间的兼容性，能够保证这里面的版本使用起来没有问题*** 但是如果单独使用apache发行的flume，那么可能会和其他hadoop框架发生不兼容问题，需要自己去解决。例如flume1.7 跟 hadoop3.0 就会不兼容 4.2、案例4.2.1、案例一：监控端口数据目标：Flume 监控一端 Console，另一端 Console 发送消息，使被监控端实时显示。 分步实现： 1) 安装 telnet 工具 （rpm 需要事先下载，在有网的环境下可以通过yum下载） ​ $ sudo rpm -ivh xinetd-2.3.14-40.el6.x86_64.rpm $ sudo rpm -ivh telnet-0.17-48.el6.x86_64.rpm $ sudo rpm -ivh telnet-server-0.17-48.el6.x86_64.rpm 2) 创建 Flume Agent 配置文件 flume-telnet.conf 具体配置意义推荐查看官方文档：第一部门，定义agent相关信息，source和sink以及channels的名称（不能够重复） 第二部分：定义source 第三部门：定义sink的输出位置 第四部门：定义channel 第五部分：source绑定channel和sink绑定channel。 a1.sources.r1.channels 可以看到这里是复数channel，表示source可以绑定多个channel。然后一个channel只能够绑定一个sink，只能够有一个输出 al 表示的是 整个agent的别名 – 必须是唯一的\\一因为你可能同时启动了多个flume任务，那么如果agent的id不是唯一，就会出现数据紊乱 r1 和 k1 分别是 source和sink的id，只要在当前agent内保证不相同即可，没有类似***agent**的**id**全局唯一的要求*** \\# Name the components on thisagent a1.sources = r1a1.sinks = k1a1.channels = c1\\# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444\\# Describe the sinka1.sinks.k1.type = logger\\# Use a channel which buffers events in memory a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100\\# Bind the source and sink to the channela1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 3) 判断 44444 端口是否被占用 ​ ​ $ netstat -tunlp | grep 44444 4) 先开启 flume 先听端口（启动后会挂在哪里，需要打开另一个窗口发送数据） $ bin/flume-ng agent –conf conf/ –name a1 –conf-file job/flume-telnet.conf -Dflume.root.logger==INFO,console 5) 使用 telnet 工具向本机的 44444 端口发送内容 ​ $ telnet localhost 44444 4.2.2、案例二：实时读取本地文件到 HDFS目标：实时监控 hive 日志，并上传到 HDFS 中 分步实现： 1) 拷贝 Hadoop 相关 jar 到 Flume 的 lib 目录下（要学会根据自己的目录和版本查找 jar 包**）** $ cp share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.6.jar ./lib/ $ cp share/hadoop/common/lib/commons-configuration-1.6.jar ./lib/ $ cp share/hadoop/mapreduce1/lib/hadoop-hdfs-2.5.0-cdh5.3.6.jar ./lib/ $ cp share/hadoop/common/hadoop-common-2.5.0-cdh5.3.6.jar ./lib/ $ cp ./share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar ./lib/ $ cp ./share/hadoop/hdfs/lib/commons-io-2.4.jar ./lib/ 尖叫提示：标红的 jar 为 1.99 版本 flume 必须引用的 jar 2) 创建 flume-hdfs.conf 文件 # Name the components on this agent a2.sources = r2 a2.sinks = k2 a2.channels = c2 # Describe/configure the source a2.sources.r2.type = exec a2.sources.r2.command = tail -F /home/admin/modules/apache-hive-1.2.2-bin/hive.log a2.sources.r2.shell = /bin/bash –c //执行上述command的命令前缀是/bin/bash –c # Describe the sink a2.sinks.k2.type = hdfs a2.sinks.k2.hdfs.path = hdfs://linux01:8020/flume/%Y%m%d/%H #上传文件的前缀 a2.sinks.k2.hdfs.filePrefix = logs- #是否按照时间滚动文件夹 a2.sinks.k2.hdfs.round = true #多少时间单位创建一个新的文件夹 a2.sinks.k2.hdfs.roundValue = 1 #重新定义时间单位 a2.sinks.k2.hdfs.roundUnit = hour #是否使用本地时间戳 a2.sinks.k2.hdfs.useLocalTimeStamp = true #积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k2.hdfs.batchSize = 1000 #设置文件类型，可支持压缩 a2.sinks.k2.hdfs.fileType = DataStream #多久生成一个新的文件 a2.sinks.k2.hdfs.rollInterval = 600 #设置每个文件的滚动大小 a2.sinks.k2.hdfs.rollSize = 134217700 #文件的滚动与 Event 数量无关 a2.sinks.k2.hdfs.rollCount = 0 #最小冗余数 a2.sinks.k2.hdfs.minBlockReplicas = 1 # Use a channel which buffers events in memory a2.channels.c2.type = memory a2.channels.c2.capacity = 1000 a2.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2 为什么要指定 a2.sources.r2.shell 因为我们知道我们在执行linux命令的时候，例如cat xxx.txt 实际上完整的命令应该是 /bin/bash –c cat xxx.txt. 因为linux 系统已经帮我们配置好了所以不需要显式输入。但是我们知道可能每台linux 的环境变量配置都不一样，那么所以需要显式指定bash地址 3) 执行监控配置 ​ $ bin/flume-ng agent –conf conf/ –name a2 –conf-file job/flume-hdfs.conf 4.2.3、案例三：实时读取目录文件到 HDFS目标：使用 flume 监听整个目录的文件 分步实现： 1) 创建配置文件 flume-dir.conf a3.sources = r3 a3.sinks = k3 a3.channels = c3 # Describe/configure the source a3.sources.r3.type = spooldir a3.sources.r3.spoolDir = /home/admin/modules/apache-flume-1.7.0-bin/upload a3.sources.r3.fileSuffix = .COMPLETED //上传完后 upload目录下的文件后缀加上.COMPLETED a3.sources.r3.fileHeader = true #忽略所有以.tmp 结尾的文件，不上传 a3.sources.r3.ignorePattern = ([^ ]*.tmp) # Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://linux01:8020/flume/upload/%Y%m%d/%H #上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- #是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true #多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 #重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour #是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true #积攒多少个 Event 才 flush 到 HDFS 一次 a3.sinks.k3.hdfs.batchSize = 100 #设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream #多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 600 #设置每个文件的滚动大小大概是 128M a3.sinks.k3.hdfs.rollSize = 134217700 #文件的滚动与 Event 数量无关 a3.sinks.k3.hdfs.rollCount = 0 #最小冗余数 a3.sinks.k3.hdfs.minBlockReplicas = 1 # Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3 2) 执行测试：执行如下脚本后，请向 upload 文件夹中添加文件试试 ​ $ bin/flume-ng agent –conf conf/ –name a3 –conf-file job/flume-dir.conf 尖叫提示： 在使用 Spooling Directory Source 时 1) 不要在监控目录中创建并持续修改文件 2) 上传完成的文件会以.COMPLETED 结尾 3) 被监控文件夹每 500 毫秒扫描一次文件变动 4.2.4、案例四：Flume 与 Flume 之间数据传递：单 Flume 多 Channel、Sink， 目标：使用 flume-1 监控文件变动，flume-1 将变动内容传递给 flume-2，flume-2 负责存储到HDFS。同时 flume-1 将变动内容传递给 flume-3，flume-3 负责输出到。local filesystem。 分步实现： 1) 创建 flume-1.conf，用于监控 hive.log 文件的变动，同时产生两个 channel 和两个 sink 分别输送给 flume-2 和 flume3： \\# Name the components on this agent a1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2\\# 将数据流复制给多个 channela1.sources.r1.selector.type = replicating\\# Describe/configure the source a1.sources.r1.type = execa1.sources.r1.command = tail -F /home/admin/modules/apache-hive-1.2.2-bin/hive.loga1.sources.r1.shell = /bin/bash -c\\# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = linux01a1.sinks.k1.port = 4141a1.sinks.k2.type = avroa1.sinks.k2.hostname = linux01a1.sinks.k2.port = 4142\\# Describe the channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.chanels.c1.transactionCapacity = 100a1.channels.c2.type = memorya1.channel.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100\\# Bind the source and sink to the channel a1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2 2) 创建 flume-2.conf，用于接收 flume-1 的 event，同时产生 1 个 channel 和 1 个 sink，将数据输送给 hdfs： # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source a2.sources.r1.type = avro a2.sources.r1.bind = linux01 a2.sources.r1.port = 4141 # Describe the sink a2.sinks.k1.type = hdfs a2.sinks.k1.hdfs.path = hdfs://linux01:8020/flume2/%Y%m%d/%H #上传文件的前缀 a2.sinks.k1.hdfs.filePrefix = flume2- #是否按照时间滚动文件夹 a2.sinks.k1.hdfs.round = true #多少时间单位创建一个新的文件夹 a2.sinks.k1.hdfs.roundValue = 1 #重新定义时间单位 a2.sinks.k1.hdfs.roundUnit = hour #是否使用本地时间戳 a2.sinks.k1.hdfs.useLocalTimeStamp = true #积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k1.hdfs.batchSize = 100 #设置文件类型，可支持压缩 a2.sinks.k1.hdfs.fileType = DataStream #多久生成一个新的文件 a2.sinks.k1.hdfs.rollInterval = 600 #设置每个文件的滚动大小大概是 128M a2.sinks.k1.hdfs.rollSize = 134217700 #文件的滚动与 Event 数量无关 a2.sinks.k1.hdfs.rollCount = 0 #最小冗余数 a2.sinks.k1.hdfs.minBlockReplicas = 1 # Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1 3) 创建 flume-3.conf，用于接收 flume-1 的 event，同时产生 1 个 channel 和 1 个 sink，将数据输送给本地目录： # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c1 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = linux01 a3.sources.r1.port = 4142 # Describe the sink a3.sinks.k1.type = file_roll a3.sinks.k1.sink.directory = /home/admin/Desktop/flume3 # Describe the channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000 a3.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c1 a3.sinks.k1.channel = c1 尖叫提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录 4) 执行测试：分别开启对应 flume-job（依次启动 flume-1，flume-2，flume-3），同时产生文件变动并观察结果： $ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group-job1/flume-1.conf $ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group-job1/flume-2.conf $ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group-job1/flume-3.conf 4.2.5、案例五：Flume 与 Flume 之间数据传递，多 Flume 汇总数据到单 Flume 目标：flume-1 监控文件 hive.log，flume-2 监控某一个端口的数据流，flume-1 与 flume-2 将数据发送给 flume-3，flume3 将最终数据写入到 HDFS。 分步实现： 1) 创建 flume-1.conf，用于监控 hive.log 文件，同时 sink 数据到 flume-3： # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /home/admin/modules/apache-hive-1.2.2-bin/hive.log a1.sources.r1.shell = /bin/bash -c # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = linux01 a1.sinks.k1.port = 4141 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 2) 创建 flume-2.conf，用于监控端口 44444 数据流，同时 sink 数据到 flume-3： ​ # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source a2.sources.r1.type = netcat a2.sources.r1.bind = linux01 a2.sources.r1.port = 44444 # Describe the sink a2.sinks.k1.type = avro a2.sinks.k1.hostname = linux01 a2.sinks.k1.port = 4141 # Use a channel which buffers events in memory a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1 3) 创建 flume-3.conf，用于接收 flume-1 与 flume-2 发送过来的数据流，最终合并后 sink 到 HDFS： # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c1 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = linux01 a3.sources.r1.port = 4141 # Describe the sink a3.sinks.k1.type = hdfs a3.sinks.k1.hdfs.path = hdfs://linux01:8020/flume3/%Y%m%d/%H #上传文件的前缀 a3.sinks.k1.hdfs.filePrefix = flume3- #是否按照时间滚动文件夹 a3.sinks.k1.hdfs.round = true #多少时间单位创建一个新的文件夹 a3.sinks.k1.hdfs.roundValue = 1 #重新定义时间单位 a3.sinks.k1.hdfs.roundUnit = hour #是否使用本地时间戳 a3.sinks.k1.hdfs.useLocalTimeStamp = true #积攒多少个 Event 才 flush 到 HDFS 一次 a3.sinks.k1.hdfs.batchSize = 100 #设置文件类型，可支持压缩 a3.sinks.k1.hdfs.fileType = DataStream #多久生成一个新的文件 a3.sinks.k1.hdfs.rollInterval = 600 #设置每个文件的滚动大小大概是 128M a3.sinks.k1.hdfs.rollSize = 134217700 #文件的滚动与 Event 数量无关 a3.sinks.k1.hdfs.rollCount = 0 #最小冗余数 a3.sinks.k1.hdfs.minBlockReplicas = 1 # Describe the channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000 a3.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c1 a3.sinks.k1.channel = c1 4) 执行测试：分别开启对应 flume-job（依次启动 flume-1，flume-2，flume-3），同时产生文件变动并观察结果： ​ $ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group-job2/flume-1.conf $ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group-job2/flume-2.conf $ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group-job2/flume-3.conf ​ 尖叫提示：测试时记得启动 hive 产生一些日志，同时使用 telnet 向 44444 端口发送内容，如： ​ $ bin/hive$ telnet linux01 44444 l 数据追加到一个文件中***(**注意：数据的存入是无序的，也就是说在某个时间点**flume1**存入**flume3**的数据可能早于**flume2**，反之亦然**—**数据交叉存放**)**，如果设置了滚动时间等等其他参数，那么条件一到就会在**hdfs**生成新的文件来进行存储*** 解决***flume**之间数据输入不按照顺序的问题，利用**kafka**的队列即可。**Flume**先把数据输入到**kafka**中再输入到**flume**然后进行保存*** 五、Flume 监控之 Ganglia5.1 Ganglia 的安装与部署1) 安装 httpd 服务与 php ​ ​ # yum -y install httpd php 2) 安装其他依赖 # yum -y install rrdtool perl-rrdtool rrdtool-devel # yum -y install apr-devel 3) 安装 ganglia # rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm # yum -y install ganglia-gmetad //这个相当于ganglia的服务端 # yum -y install ganglia-web //服务端的信息展示web界面 # yum install -y ganglia-gmond //相当于客户端 – 可以启动多个客户端，数据会发布到服务端，可以通过这个特性实现分布式 4) 修改配置文件文件 ganglia.conf**：** # vi /etc/httpd/conf.d/ganglia.conf 修改为： # # Ganglia monitoring system php web frontend – 配置文件是配置，允许那些主机访问ganglia服务器 # Alias /ganglia /usr/share/ganglia Order deny,allowDeny from allAllow from all # 表示允许所有主机访问 – 默认是只允许本机# Allow from 127.0.0.1# Allow from ::1# Allow from .example.com 文件 gmetad.conf**： 主节点 - 需要注意的是 gmetad.conf不一定在/etc/ganglia/这个目录下，因为上面我们是使用yum自动安装，默认保存在这个位置，**如果手动安装则以你设置路径为主 # vi /etc/ganglia/gmetad.conf 修改为：第二个参数表示，当前ganglia集群名称，以及主句所在服务器ip data_source “linux” 192.168.216.20 文件 gmond.conf**：从节点 从节点向主节点发送信息的相关配置** # vi /etc/ganglia/gmond.conf 修改为： cluster { name = “linux” owner = “unspecified” latlong = “unspecified” url = “unspecified” } udp_send_channel { #bind_hostname = yes # Highly recommended, soon to be default. # This option tells gmond to use a source address # that resolves to the machine’s hostname. Without # this, the metrics may appear to come from any # interface and the DNS names associated with # those IPs will be used to create the RRDs. # mcast_join = 239.2.11.71 #这个意思是使用广播的形式发送数据到ganglia集群-我们下面使用单播形式 host = 192.168.216.20 port = 8649 ttl = 1 } udp_recv_channel { # mcast_join = 239.2.11.71 port = 8649 bind = 192.168.216.20 retry_bind = true # Size of the UDP buffer. If you are handling lots of metrics you really # should bump it up to e.g. 10MB or even higher. # buffer = 10485760 } 文件 config**：（如果不配置这个selinux linux的安全子系统，那么我们在第六步访问web时会提示权限不足）** # vi /etc/selinux/config 修改为： # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of these two values: # targeted - Targeted processes are protected, # mls - Multi Level Security protection. SELINUXTYPE=targeted 尖叫提示：selinux 本次生效关闭必须重启，如果此时不想重启，可以临时生效之： ​ ​ $ sudo setenforce 0 不配置此项时***，**访问**gangliaweb**报错*** 5) 启动 ganglia ​ $ sudo service httpd start $ sudo service gmetad start $ sudo service gmond start 6) 打开网页浏览 ganglia 页面 ​ http://192.168.216.20/ganglia 尖叫提示：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia 目录的权限： ​ ​ $ sudo chmod -R 777 /var/lib/ganglia 5.2 操作 Flume 测试监控监控类型 ganglia – 默认值 汇报信息的ip和端口 – 就是刚才在主节点配置的 1) 修改 flume-env.sh 配置： export JAVA_OPTS=”-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.216.20:8649 -Xms100m -Xmx200m” 2) 启动 flume 任务 ​ ​ $ bin/flume-ng agent \\ –conf conf/ \\ –name a1 \\ –conf-file job/group-job0/flume-telnet.conf \\ -Dflume.root.logger==INFO,console \\ -Dflume.monitoring.type=ganglia \\ -Dflume.monitoring.hosts=192.168.216.20:8649 后两个参数其实可以去掉，因为刚才我们在flume-env.sh 已经配置**，这里的配置原因是想覆盖配置文件里面配置的ip和端口号，**以当前为准 3) 发送数据观察 ganglia 监测图 ​ ​ $ telnet localhost 44444 样式如图： 图例说明： 字段（图表名称） 字段含义 EventPutAttemptCount source 尝试写入 channel 的事件总数量 EventPutSuccessCount 成功写入 channel 且提交的事件总数量 EventTakeAttemptCount sink 尝试从 channel 拉取事件的总数量。这不 意味着每次事件都被返回，因为 sink 拉取的 时候 channel 可能没有任何数据。 EventTakeSuccessCount sink 成功读取的事件的总数量 StartTime channel 启动的时间（毫秒） StopTime channel 停止的时间（毫秒） ChannelSize 目前 channel 中事件的总数量 ChannelFillPercentage channel 占用百分比 ChannelCapacity channel 的容量 六、练习 目标： 1) flume-1 监控 hive.log 日志，flume-1 的数据传送给 flume-2，flume-2 将数据追加到本地文件，同时将数据传输到 flume-3。 2) flume-4 监控本地另一个自己创建的文件 any.txt，并将数据传送给 flume-3。 3) flume-3 将汇总数据写入到 HDFS。 请先画出结构图，再开始编写任务脚本。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"flume","slug":"flume","permalink":"http://kingge.top/tags/flume/"},{"name":"数据采集","slug":"数据采集","permalink":"http://kingge.top/tags/数据采集/"}]},{"title":"sqoop总结","slug":"sqoop总结","date":"2018-05-17T16:00:00.000Z","updated":"2020-05-09T09:55:32.317Z","comments":true,"path":"2018/05/18/sqoop总结/","link":"","permalink":"http://kingge.top/2018/05/18/sqoop总结/","excerpt":"","text":"一、Sqoop简介Apache Sqoop(TM)是一种旨在有效地在Apache Hadoop和诸如关系数据库等结构化数据存储之间传输大量数据的工具。 Sqoop于2012年3月孵化出来，现在是一个顶级的Apache项目。 最新的稳定版本是1.4.6。Sqoop2的最新版本是1.99.7。请注意，1.99.7与1.4.6不兼容，且没有特征不完整，它并不打算用于生产部署。 二、Sqoop原理将导入或导出命令翻译成mapreduce程序来实现。 在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。 三、Sqoop安装安装Sqoop的前提是已经具备Java和Hadoop的环境。 3.1、下载并解压1) 最新版下载地址：http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/ 2) 上传安装包sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz到虚拟机中，如我的上传目录是：/home/admin/softwares/installation 3) 解压sqoop安装包到指定目录，如： $ tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C ~/modules/ 3.2、修改配置文件Sqoop的配置文件与大多数大数据框架类似，在sqoop根目录下的conf目录中。 1) 重命名配置文件 $ mv sqoop-env-template.sh sqoop-env.sh $ mv sqoop-site-template.xml sqoop-site.xml 2) 修改配置文件 sqoop-env.sh export HADOOP_COMMON_HOME=/home/admin/modules/hadoop-2.7.2export HADOOP_MAPRED_HOME=/home/admin/modules/hadoop-2.7.2export HIVE_HOME=/home/admin/modules/apache-hive-1.2.2-binexport ZOOKEEPER_HOME=/home/admin/modules/zookeeper-3.4.5export ZOOCFGDIR=/home/admin/modules/zookeeper-3.4.5/conf 这里的zookeeper是配合HBASE使用的，所以这里可以不配置他，因为没有用到HBASE 3.3、拷贝JDBC驱动拷贝jdbc驱动到sqoop的lib目录下，如： $ cp -a mysql-connector-java-5.1.27-bin.jar ~/modules/sqoop-1.4.6.bin__hadoop-2.0.4-alpha/lib/ 3.4、验证Sqoop我们可以通过某一个command来验证sqoop配置是否正确： $ bin/sqoop help出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version information 3.5、测试Sqoop是否能够成功连接数据库$ bin/sqoop list-databases --connect jdbc:mysql://linux01:3306/ --username root --password 123456出现如下输出：information_schemametastoremysqlperformance_schema 四、Sqoop的简单使用案例4.1、导入数据在Sqoop中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做：导入，即使用import关键字。 4.1.1、RDBMS到HDFS1) 确定Mysql服务开启正常 2) 在Mysql中新建一张表并插入一些数据 $ mysql -uroot -p123456mysql&gt; create database company;mysql&gt; create table company.staff(id int(4) primary key not null auto_increment, name varchar(255), sex varchar(255));mysql&gt; insert into company.staff(name, sex) values(&apos;Thomas&apos;, &apos;Male&apos;);mysql&gt; insert into company.staff(name, sex) values(&apos;Catalina&apos;, &apos;FeMale&apos;); 3) 导入数据 ​ （1）全部导入 $ bin/sqoop import \\ --这里反斜杠的意思是，命令没有书写完，此时敲回车的继续书写下一行的意思。如果没有这个反斜杠敲回车是直接执行当前命令了--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--table staff \\--target-dir /user/company \\--delete-target-dir \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; 前三行-连接mysql需要信息，第四行：源mysql数据表 第五六行：输出到hdfs那个目录、如果目录存在则删除 第七行：导数据需要一个map 第八行：每一列分隔符（） （2）查询导入 $ bin/sqoop import \\--connect jdbc:mysql://hadoop101:3306/company \\--username root \\--password 123456 \\--target-dir /user/company \\--delete-target-dir \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; \\--query &apos;select name,sex from staff where id &lt;=1 and $CONDITIONS;&apos; 尖叫提示：must contain ‘$CONDITIONS’ in WHERE clause.( 详情参见文档 Sqoop User Guide (v1.4.6).mhtml ) 尖叫提示：如果query后使用的是双引号，则$CONDITIONS前必须加转移符，防止shell识别为自己的变量。（linux系统以为使用linux变量） 尖叫提示：–query选项，不能同时与–table选项使用 当我们启用三个map***去处理mysql**过来的数据的时候，那么每个map**读取的数据的索引是不同的，为了避免重复读取，那么我们就需要记住索引，那么CONDITIONS**的作用就是如此。记住当前map**读取数据的索引。例如map1 –* *获取第一行到第十行数据、map2**获取11-20**、map3**获取21-31*** 我们要知道使用$CONDITIONS\\$的时候，必须同时使用***split by*，标注，我们需要根据表中的那个字段分割数据。*注意当**map num=1**的时候不需要指明**split by**（上面的代码就没有指明**split by**）*** ​ （3**）导入指定列** $ bin/sqoop import \\--connect jdbc:mysql://hadoop101:3306/company \\--username root \\--password 123456 \\--target-dir /user/company \\--delete-target-dir \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; \\--columns id,sex \\--table staff 尖叫提示：columns中如果涉及到多列，用逗号分隔，分隔时不要添加空格 ​ （4**）使用sqoop**关键字筛选查询导入数据 $ bin/sqoop import \\--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--target-dir /user/company \\--delete-target-dir \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; \\--table staff \\--where &quot;id=1&quot; 尖叫提示：在Sqoop中可以使用sqoop import -D property.name=property.value这样的方式加入执行任务的参数，多个参数用空格隔开。 4.1.2、RDBMS到Hive$ bin/sqoop import \\--connect jdbc:mysql://hadoop101:3306/company \\--username root \\--password 123456 \\ --table staff \\--num-mappers 1 \\--hive-import \\--fields-terminated-by &quot;\\t&quot; \\--hive-overwrite \\--hive-table staff_hive 尖叫提示：该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库 尖叫提示：第一步默认的临时目录是/user/admin/表名 如果不指明导入hive那张表，那么会自动仔hive中创建跟mysql表名相同的表，存储数据 4.2、导出数据在Sqoop中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群（RDBMS）中传输数据，叫做：导出，即使用export关键字。 4.2.1、HIVE/HDFS到RDBMS$ bin/sqoop export \\--connect jdbc:mysql://hadoop101:3306/company \\--username root \\--password 123456 \\--table staff \\--num-mappers 1 \\--export-dir /user/hive/warehouse/staff \\--input-fields-terminated-by &quot;\\t&quot; 尖叫提示：Mysql中如果表不存在，不会自动创建 思考：数据是覆盖还是追加 -追加 4.3、脚本打包使用opt格式的文件打包sqoop命令，然后执行 1) 创建一个.opt**文件** $ mkdir opt $ touch opt/job_HDFS2RDBMS.opt 2) 编写sqoop**脚本（每个参数一行的风格：参数一行，值一行）** $ vi opt/job_HDFS2RDBMS.optexport--connectjdbc:mysql://linux01:3306/company--usernameroot--password123456--tablestaff--num-mappers1--export-dir/user/hive/warehouse/staff_hive--input-fields-terminated-by&quot;\\t&quot; 3) 执行该脚本 $ bin/sqoop –options-file opt/job_HDFS2RDBMS.opt 五、Sqoop一些常用命令及参数5.1、常用命令列举这里给大家列出来了一部分Sqoop操作时的常用参数，以供参考，需要深入学习的可以参看对应类的源代码。 序号 命令 类 说明 1 import ImportTool 将数据导入到集群 2 export ExportTool 将集群数据导出 3 codegen CodeGenTool 获取数据库中某张表数据生成Java并打包Jar 4 create-hive-table CreateHiveTableTool 创建Hive表 5 eval EvalSqlTool 查看SQL执行结果 6 import-all-tables ImportAllTablesTool 导入某个数据库下所有表到HDFS中 7 job JobTool 用来生成一个sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务。 8 list-databases ListDatabasesTool 列出所有数据库名 9 list-tables ListTablesTool 列出某个数据库下所有表 10 merge MergeTool 将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中 11 metastore MetastoreTool 记录sqoop job的元数据信息，如果不启动metastore实例，则默认的元数据存储目录为：~/.sqoop，如果要更改存储目录，可以在配置文件sqoop-site.xml中进行更改。 12 help HelpTool 打印sqoop帮助信息 13 version VersionTool 打印sqoop版本信息 5.2、命令&amp;参数详解刚才列举了一些Sqoop的常用命令，对于不同的命令，有不同的参数，让我们来一一列举说明。 首先来我们来介绍一下公用的参数，所谓公用参数，就是大多数命令都支持的参数。 5.2.1、公用参数：数据库连接 序号 参数 说明 1 –connect 连接关系型数据库的URL 2 –connection-manager 指定要使用的连接管理类（不常用） 3 –driver JDBC的driver class（不常用） 4 –help 打印帮助信息（不常用） 5 –password 连接数据库的密码 6 –username 连接数据库的用户名 7 –verbose 在控制台打印出详细信息（不常用） 5.2.2、公用参数：import 序号 参数 说明 1 –enclosed-by 给字段值前后加上指定的字符 2 –escaped-by 对字段中的双引号加转义符 3 –fields-terminated-by 设定每个字段是以什么符号作为结束，默认为逗号 4 –lines-terminated-by 设定每行记录之间的分隔符，默认是\\n 5 –mysql-delimiters Mysql默认的分隔符设置，字段之间以逗号分隔，行之间以\\n分隔，默认转义符是\\，字段值以单引号包裹。 6 –optionally-enclosed-by 给带有双引号或单引号的字段值前后加上指定字符。 5.2.3、公用参数：export 序号 参数 说明 1 –input-enclosed-by 对字段值前后加上指定字符 2 –input-escaped-by 对含有转移符的字段做转义处理 3 –input-fields-terminated-by 字段之间的分隔符 4 –input-lines-terminated-by 行之间的分隔符 5 –input-optionally-enclosed-by 给带有双引号或单引号的字段前后加上指定字符 5.2.4、公用参数：hive 序号 参数 说明 1 –hive-delims-replacement 用自定义的字符串替换掉数据中的\\r\\n和\\013 \\010等字符 2 –hive-drop-import-delims 在导入数据到hive时，去掉数据中的\\r\\n\\013\\010这样的字符 3 –map-column-hive 生成hive表时，可以更改生成字段的数据类型（更改mysql数据类型再导入hive） 4 –hive-partition-key 创建分区，后面直接跟分区名，分区字段的默认类型为string 5 –hive-partition-value 导入数据时，指定某个分区的值 6 –hive-home hive的安装目录，可以通过该参数覆盖之前默认配置的目录 7 –hive-import 将数据从关系数据库中导入到hive表中 8 –hive-overwrite 覆盖掉在hive表中已经存在的数据 9 –create-hive-table 默认是false，即，如果目标表已经存在了，那么创建任务失败。 10 –hive-table 后面接要创建的hive表,默认使用MySQL的表名 11 –table 指定关系数据库的表名 公用参数介绍完之后，我们来按照命令介绍命令对应的特有参数。 5.2.5、命令&amp;参数：import将关系型数据库中的数据导入到HDFS（包括Hive，HBase）中，如果导入的是Hive，那么当Hive中没有对应表时，则自动创建。 1) 命令： 如：导入数据到hive中 $ bin/sqoop import \\--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--table staff \\--hive-import //如果删除这行，表示是导入到hdfs中。路径默认是/user/admin/表名 如：增量导入数据到hive中，mode=append append导入：$ bin/sqoop import \\--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--table staff \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; \\--target-dir /user/hive/warehouse/staff_hive \\ //如果不指定那么会放到临时目录中，然后再删除--check-column id \\--incremental append \\ //追加的方式添加到hive中--last-value 3 //从mysql中取第三行后面的数据追加（不包括第三行） 尖叫提示：append不能与–hive-等参数同时使用（Append mode for hive imports is not yet supported. Please remove the parameter –append-mode） 如：增量导入数据到hdfs中，mode=lastmodified 先在mysql中建表并插入几条数据：mysql&gt; create table company.staff_timestamp(id int(4), name varchar(255), sex varchar(255), last_modified timestamp DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP);mysql&gt; insert into company.staff_timestamp (id, name, sex) values(1, &apos;AAA&apos;, &apos;female&apos;);mysql&gt; insert into company.staff_timestamp (id, name, sex) values(2, &apos;BBB&apos;, &apos;female&apos;);先导入一部分数据：$ bin/sqoop import \\--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--table staff_timestamp \\--delete-target-dir \\--m 1再增量导入一部分数据：mysql&gt; insert into company.staff_timestamp (id, name, sex) values(3, &apos;CCC&apos;, &apos;female&apos;);$ bin/sqoop import \\--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--table staff_timestamp \\--check-column last_modified \\--incremental lastmodified \\--last-value &quot;2017-09-28 22:20:38&quot; \\--m 1 \\--append 尖叫提示：使用lastmodified方式导入数据要指定增量数据是要–append（追加）还是要–merge-key（合并） 尖叫提示：last-value指定的值是会包含于增量导入的数据中 2) 参数： 序号 参数 说明 1 –append 将数据追加到HDFS中已经存在的DataSet中，如果使用该参数，sqoop会把数据先导入到临时文件目录，再合并。 2 –as-avrodatafile 将数据导入到一个Avro数据文件中 3 –as-sequencefile 将数据导入到一个sequence文件中 4 –as-textfile 将数据导入到一个普通文本文件中 5 –boundary-query 边界查询，导入的数据为该参数的值（一条sql语句）所执行的结果区间内的数据。 6 –columns 指定要导入的字段 7 –direct 直接导入模式，使用的是关系数据库自带的导入导出工具，以便加快导入导出过程。 8 –direct-split-size 在使用上面direct直接导入的基础上，对导入的流按字节分块，即达到该阈值就产生一个新的文件 9 –inline-lob-limit 设定大对象数据类型的最大值 10 –m或–num-mappers 启动N个map来并行导入数据，默认4个。 11 –query或–e 将查询结果的数据导入，使用时必须伴随参–target-dir，–hive-table，如果查询中有where条件，则条件后必须加上$CONDITIONS关键字 12 –split-by 按照某一列来切分表的工作单元，不能与–autoreset-to-one-mapper连用（请参考官方文档） 13 –table 关系数据库的表名 14 –target-dir 指定HDFS路径 15 –warehouse-dir 与14参数不能同时使用，导入数据到HDFS时指定的目录 16 –where 从关系数据库导入数据时的查询条件 17 –z或–compress 允许压缩 18 –compression-codec 指定hadoop压缩编码类，默认为gzip(Use Hadoop codec default gzip) 19 –null-string string类型的列如果null，替换为指定字符串 20 –null-non-string 非string类型的列如果null，替换为指定字符串 21 –check-column 作为增量导入判断的列名 22 –incremental mode：append或lastmodified 23 –last-value 指定某一个值，用于标记增量导入的位置 5.2.6、命令&amp;参数：export从HDFS（包括Hive和HBase）中奖数据导出到关系型数据库中。 1) 命令： 如： $ bin/sqoop export \\--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--table staff \\--export-dir /user/company \\--input-fields-terminated-by &quot;\\t&quot; \\--num-mappers 1 2) 参数： 序号 参数 说明 1 –direct 利用数据库自带的导入导出工具，以便于提高效率 2 –export-dir 存放数据的HDFS的源目录 3 -m或–num-mappers 启动N个map来并行导入数据，默认4个 4 –table 指定导出到哪个RDBMS中的表 5 –update-key 对某一列的字段进行更新操作 6 –update-mode updateonly allowinsert(默认) 7 –input-null-string 请参考import该类似参数说明 8 –input-null-non-string 请参考import该类似参数说明 9 –staging-table 创建一张临时表，用于存放所有事务的结果，然后将所有事务结果一次性导入到目标表中，防止错误。 10 –clear-staging-table 如果第9个参数非空，则可以在导出操作执行前，清空临时事务结果表 5.2.7、命令&amp;参数：codegen将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段。 如： $ bin/sqoop codegen \\--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--table staff \\--bindir /home/admin/Desktop/staff \\--class-name Staff \\--fields-terminated-by &quot;\\t&quot; 序号 参数 说明 1 –bindir 指定生成的Java文件、编译成的class文件及将生成文件打包为jar的文件输出路径 2 –class-name 设定生成的Java文件指定的名称 3 –outdir 生成Java文件存放的路径 4 –package-name 包名，如com.z，就会生成com和z两级目录 5 –input-null-non-string 在生成的Java文件中，可以将null字符串或者不存在的字符串设置为想要设定的值（例如空字符串） 6 –input-null-string 将null字符串替换成想要替换的值（一般与5同时使用） 7 –map-column-java 数据库字段在生成的Java文件中会映射成各种属性，且默认的数据类型与数据库类型保持对应关系。该参数可以改变默认类型，例如：–map-column-java id=long, name=String 8 –null-non-string 在生成Java文件时，可以将不存在或者null的字符串设置为其他值 9 –null-string 在生成Java文件时，将null字符串设置为其他值（一般与8同时使用） 10 –table 对应关系数据库中的表名，生成的Java文件中的各个属性与该表的各个字段一一对应 5.2.8、命令&amp;参数：create-hive-table生成与关系数据库表结构对应的hive表结构。 命令： 如： $ bin/sqoop create-hive-table \\ –connect jdbc:mysql://hadoop101:3306/company \\ –username root \\ –password 123456 \\ –table staff \\ –hive-table hive_staff 参数： 序号 参数 说明 1 –hive-home Hive的安装目录，可以通过该参数覆盖掉默认的Hive目录 2 –hive-overwrite 覆盖掉在Hive表中已经存在的数据 3 –create-hive-table 默认是false，如果目标表已经存在了，那么创建任务会失败 4 –hive-table 后面接要创建的hive表 5 –table 指定关系数据库的表名 5.2.9、命令&amp;参数：eval可以快速的使用SQL语句对关系型数据库进行操作，经常用于在import数据之前，了解一下SQL语句是否正确，数据是否正常，并可以将结果显示在控制台。 命令： 如： $ bin/sqoop eval \\ –connect jdbc:mysql://hadoop101:3306/company \\ –username root \\ –password 123456 \\ –query “SELECT * FROM staff” 参数： 序号 参数 说明 1 –query或–e 后跟查询的SQL语句 5.2.10、命令&amp;参数：import-all-tables可以将RDBMS中的所有表导入到HDFS中，每一个表都对应一个HDFS目录 命令： 如： $ bin/sqoop import-all-tables \\ –connect jdbc:mysql://linux01:3306/company \\ –username root \\ –password 123456 \\ –warehouse-dir /all_tables 参数： 序号 参数 说明 1 –as-avrodatafile 这些参数的含义均和import对应的含义一致 2 –as-sequencefile 3 –as-textfile 4 –direct 5 –direct-split-size 6 –inline-lob-limit 7 –m或—num-mappers 8 –warehouse-dir 9 -z或–compress 10 –compression-codec 5.2.11、命令&amp;参数：job用来生成一个sqoop任务，生成后不会立即执行，需要手动执行。 命令： 如： $ bin/sqoop job \\ --create myjob -- import-all-tables \\ --connect jdbc:mysql://linux01:3306/company \\ --username root \\ --password 123456$ bin/sqoop job \\--list$ bin/sqoop job \\--exec myjob 尖叫提示：注意import-all-tables和它左边的–之间有一个空格 尖叫提示：如果需要连接metastore，则–meta-connect jdbc:hsqldb:hsql://linux01:16000/sqoop 参数： 序号 参数 说明 1 –create 创建job参数 2 –delete 删除一个job 3 –exec 执行一个job 4 –help 显示job帮助 5 –list 显示job列表 6 –meta-connect 用来连接metastore服务 7 –show 显示一个job的信息 8 –verbose 打印命令运行时的详细信息 尖叫提示：在执行一个job时，如果需要手动输入数据库密码，可以做如下优化 &lt;property&gt; &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, allow saved passwords in the metastore.&lt;/description&gt;&lt;/property&gt; 5.2.12、命令&amp;参数：list-databases命令： 如： $ bin/sqoop list-databases \\ –connect jdbc:mysql://linux01:3306/ \\ –username root \\ –password 123456 参数：与公用参数一样 5.2.13、命令&amp;参数：list-tables命令： 如： $ bin/sqoop list-tables \\ –connect jdbc:mysql://linux01:3306/company \\ –username root \\ –password 123456 参数：与公用参数一样 5.2.14、命令&amp;参数：merge将HDFS中不同目录下面的数据合并在一起并放入指定目录中 数据环境： new_staff1 AAA male2 BBB male3 CCC male4 DDD maleold_staff1 AAA female2 CCC female3 BBB female6 DDD female 尖叫提示：上边数据的列之间的分隔符应该为\\t，行与行之间的分割符为\\n，如果直接复制，请检查之。 命令： 如： 创建JavaBean：$ bin/sqoop codegen \\--connect jdbc:mysql://linux01:3306/company \\--username root \\--password 123456 \\--table staff \\--bindir /home/admin/Desktop/staff \\--class-name Staff \\--fields-terminated-by &quot;\\t&quot;开始合并：$ bin/sqoop merge \\--new-data /test/new/ \\--onto /test/old/ \\--target-dir /test/merged \\--jar-file /home/admin/Desktop/staff/Staff.jar \\--class-name Staff \\--merge-key id结果：1 AAA MALE2 BBB MALE3 CCC MALE4 DDD MALE6 DDD FEMALE 参数： 序号 参数 说明 1 –new-data HDFS 待合并的数据目录，合并后在新的数据集中保留 2 –onto HDFS合并后，重复的部分在新的数据集中被覆盖 3 –merge-key 合并键，一般是主键ID 4 –jar-file 合并时引入的jar包，该jar包是通过Codegen工具生成的jar包 5 –class-name 对应的表名或对象名，该class类是包含在jar包中的 6 –target-dir 合并后的数据在HDFS里存放的目录 5.2.15、命令&amp;参数：metastore记录了Sqoop job的元数据信息，如果不启动该服务，那么默认job元数据的存储目录为~/.sqoop，可在sqoop-site.xml中修改。 命令： 如：启动sqoop的metastore服务 $ bin/sqoop metastore 参数： 序号 参数 说明 1 –shutdown 关闭metastore 如果不启动这个服务，那么外部的机器是无法访问到这个***sqoop**的，只能够进行本机操作而已。**Metastore**就是暴露本机的**sqoop**，向外提供服务，能够使远程的机器调用本机的**sqoop**进行操作（类似**hive* *的* *bin/hive –service metastore* *向外暴露**hive**服务）***","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"http://kingge.top/tags/sqoop/"},{"name":"数据传输工具","slug":"数据传输工具","permalink":"http://kingge.top/tags/数据传输工具/"}]},{"title":"hive总结","slug":"hive总结","date":"2018-05-15T16:00:00.000Z","updated":"2020-05-09T09:39:59.207Z","comments":true,"path":"2018/05/16/hive总结/","link":"","permalink":"http://kingge.top/2018/05/16/hive总结/","excerpt":"","text":"一 Hive基本概念1.1 什么是HiveHive：由Facebook开源用于解决海量结构化日志的数据统计。 Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。 本质是：将HQL转化成MapReduce程序（所说他是基于Hadoop） 1）Hive处理的数据存储在HDFS（hive元数据存储在用户配置的数据库里面） 2）Hive分析数据底层的实现是MapReduce 3）执行程序运行在Yarn上 1.2 Hive的优缺点1.2.1 优点1）操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手） 2）避免了去写MapReduce，减少开发人员的学习成本。 3）Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合；（mysql实时性就比较快，但是如果数据量很大-上亿，那么mysql就会崩溃-所以一般业务场景是hive和mysql 配合使用） 4）Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。 5）Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 1.2.2 缺点1）Hive的HQL表达能力有限（也就是说，部分复杂的业务场景，需要用户自己实现***MapReduce***） （1）迭代式算法无法表达 （2）数据挖掘方面不擅长 2）Hive的效率比较低 （1）Hive自动生成的MapReduce作业，通常情况下不够智能化 （2）Hive调优比较困难，粒度较粗 1.3 Hive架构原理（图要记住）​ ​ 如图中所示，Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。 1）用户接口：Client CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive） 2）元数据：Metastore 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore 3）Hadoop 使用HDFS进行存储，使用MapReduce进行计算。 4）驱动器：Driver （1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。 （2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。 （3）优化器（Query Optimizer）：对逻辑执行计划进行优化。 （4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。 1.4 Hive和数据库比较由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。 1.4.1 查询语言由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。 1.4.2 数据存储位置Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。 1.4.3 数据更新由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不支持对数据的改写和添加，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。 1.4.4 索引Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive***要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高*。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。*由于数据的访问延迟较高，决定了* *Hive* *不适合在线数据查询。*** 1.4.5 执行Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。 1.4.6 执行延迟Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。 1.4.7 可扩展性由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有100台左右。 1.4.8 数据规模由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。 1.4.9 hive元数据https://blog.csdn.net/xjp8587/article/details/81411879 https://www.cnblogs.com/qingyunzong/p/8710356.html Hive 的元数据信息通常存储在关系型数据库中，常用MySQL数据库作为元数据库管理（因为使用默认的derby数据库，只能提供给一个client使用-参见2.4） 里面存储了，hive数据库仓库相关的信息，例如，hive版本信息，创建了那些表，数据库或者表数据在hdfs存储的位置等等。 二 Hive安装环境准备2.1 Hive安装地址1）Hive官网地址： http://hive.apache.org/ 2）文档查看地址： https://cwiki.apache.org/confluence/display/Hive/GettingStarted 3）下载地址： http://archive.apache.org/dist/hive/ 4）github地址： https://github.com/apache/hive 2.2 Hive安装部署1）Hive安装及配置 （1）把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下 （2）解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面 [kingge@hadoop102 software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/ （3）修改apache-hive-1.2.1-bin.tar.gz的名称为hive [kingge@hadoop102 module]$ mv apache-hive-1.2.1-bin/ hive （4）修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh [kingge@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh ​ （5）配置hive-env.sh文件 ​ （a）配置HADOOP_HOME路径 export HADOOP_HOME=/opt/module/hadoop-2.7.2 ​ （b）配置HIVE_CONF_DIR路径 export HIVE_CONF_DIR=/opt/module/hive/conf 2）Hadoop集群配置 （1）必须启动hdfs和yarn [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （2）在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /user/hive/warehouse 3）Hive基本操作（默认操作***hive**创建的default**数据库***） （1）启动hive ​ [kingge@hadoop102 hive]$ bin/hive ​ （2）查看数据库 ​ hive&gt;show databases; （3）打开默认数据库 ​ hive&gt;use default; （4）显示default数据库中的表 ​ hive&gt;show tables; （5）创建一张表 ​ hive&gt; create table student(id int, name string) ; （6）显示数据库中有几张表 ​ hive&gt;show tables; （7）查看表的结构 ​ hive&gt;desc student; （8）向表中插入数据 hive&gt; insert into student values(1000,”ss”); （9）查询表中数据 ​ hive&gt; select * from student; （10）退出hive ​ hive&gt; quit; 你会发现，执行inset操作的时候实际上他是启动了一个mr程序-任何sql***操作都会转化成MapReduce**来进行。*** 2.3 将本地文件导入Hive案例需求：将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。 1）数据准备：在/opt/module/datas/student.txt这个目录下准备数据 （1）在/opt/module/目录下创建datas ​ [kingge@hadoop102 module]$ mkdir datas （2）在/opt/module/datas/目录下创建student.txt文件并添加数据 ​ [kingge@hadoop102 datas]$ touch student.txt [kingge@hadoop102 datas]$ vi student.txt 1001 zhangshan 1002 lishi 1003 zhaoliu 注意以tab键间隔。 2）Hive实际操作 （1）启动hive ​ [kingge@hadoop102 hive]$ bin/hive （2）显示数据库 hive&gt;show databases; （3）使用default数据库 ​ hive&gt;use default; （4）显示default数据库中的表 ​ hive&gt;show tables; （5）删除已创建的student表 hive&gt; drop table student; （6）创建student表, 并声明文件分隔符’\\t’ ​ hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\\t’; （7）加载/opt/module/datas/student.txt 文件到student数据库表中。 ​ hive&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table student; （8）Hive查询结果 hive&gt; select * from student; OK 1001 zhangshan 1002 lishi 1003 zhaoliu Time taken: 0.266 seconds, Fetched: 3 row(s) 2.4 遇到的问题（默认Derby数据库）再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常。 Exception in thread &quot;main&quot; java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) ... 8 more 原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore; 官方文档已经说明，这个默认数据库仅限于测试使用，而且只能够允许一个客户端访问 https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+3.0+Administration#AdminManualMetastore3.0Administration-Option2:ExternalRDBMS 2.4 MySql安装2.4.1 安装包准备1）查看mysql是否安装，如果安装了，卸载mysql ​ （1）查看 ​ [root@hadoop102 桌面]# rpm -qa|grep mysql mysql-libs-5.1.73-7.el6.x86_64 ​ （2）卸载 ​ [root@hadoop102 桌面]# rpm -e –nodeps mysql-libs-5.1.73-7.el6.x86_64 2）解压mysql-libs.zip文件到当前目录 [root@hadoop102 software]# unzip mysql-libs.zip ​ [root@hadoop102 software]# ls mysql-libs.zip mysql-libs 3）进入到mysql-libs文件夹下，并设置当前用户执行权限 [root@hadoop102 mysql-libs]# ll 总用量 76048 -rw-r–r–. 1 root root 18509960 3月 26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm -rw-r–r–. 1 root root 3575135 12月 1 2013 mysql-connector-java-5.1.27.tar.gz -rw-r–r–. 1 root root 55782196 3月 26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm [root@hadoop102 mysql-libs]# chmod u+x ./* [root@hadoop102 mysql-libs]# ll 总用量 76048 -rwxr–r–. 1 root root 18509960 3月 26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm -rwxr–r–. 1 root root 3575135 12月 1 2013 mysql-connector-java-5.1.27.tar.gz -rwxr–r–. 1 root root 55782196 3月 26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm 2.4.2 安装MySql服务器（建议使用root用户进行操作-涉及到某些目录）1）安装mysql服务端 [root\\@hadoop102 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm 2）查看产生的随机密码 [root\\@hadoop102 mysql-libs]# cat /root/.mysql_secret ​ OEXaQuS8IWkG19Xs 3）查看mysql状态 ​ [root@hadoop102 mysql-libs]# service mysql status 4）启动mysql ​ [root@hadoop102 mysql-libs]# service mysql start 2.4.3 安装MySql客户端1）安装mysql客户端 ​ [root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm 2）链接mysql ​ [root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs 3）修改密码 ​ mysql&gt;SET PASSWORD=PASSWORD(‘000000’); 4）退出mysql mysql&gt;exit 2.4.4 MySql中user表中主机配置配置只要是root***用户+**密码，在任何主机上都能登录MySQL**数据库。*** 详情参见这个网址： https://blog.csdn.net/lthirdonel/article/details/79011033\\ 1）进入mysql [root@hadoop102 mysql-libs]# mysql -uroot -p000000 2）显示数据库 mysql&gt;show databases; 3）使用mysql数据库 mysql&gt;use mysql; 4）展示mysql数据库中的所有表 mysql&gt;show tables; 5）展示user表的结构 ​ mysql&gt;desc user; 6）查询user表 mysql&gt;select User, Host, Password from user; 7）修改user表，把Host表内容修改为% ​ mysql&gt;update user set host=’%’ where host=’localhost’; 8）删除root用户的其他host mysql&gt;delete from user where Host=’hadoop102 ‘; mysql&gt;delete from user where Host=’127.0.0.1’; mysql&gt;delete from user where Host=’::1’; 9）刷新 ​ mysql&gt;flush privileges; 10）退出 mysql&gt; quit; 2.5 Hive元数据配置到MySql2.5.1 驱动拷贝1）在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包 ​ [root@hadoop102 mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz 2）拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的 mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/ [root@hadoop102 mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/ 2.5.2 配置Metastore到MySql1）在/opt/module/hive/conf目录下创建一个hive-site.xml [kingge@hadoop102 conf]$ touch hive-site.xml [kingge@hadoop102 conf]$ vi hive-site.xml 2）根据官方文档配置参数，拷贝数据到hive-site.xml文件中。 https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin &lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;000000&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 3）配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群） 2.5.3 多窗口启动Hive测试1）先启动MySQL [kingge@hadoop102 mysql-libs]$ mysql -uroot -p000000 ​ 查看有几个数据库 mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || test |+--------------------+ 2）再次打开多个窗口，分别启动hive [kingge@hadoop102 hive]$ bin/hive 3）启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库 ​ mysql&gt; show databases; +——————–+ | Database | +——————–+ | information_schema | | metastore | | mysql | | performance_schema | | test | +——————–+ 2.6 Hive常用交互命令[kingge@hadoop102 hive]$ bin/hive -helpusage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property=value&gt; Use value for given property --hivevar &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) 1）“-e”不进入hive的交互窗口执行sql语句 [kingge@hadoop102 hive]$ bin/hive -e “select id from student;” 2）“-f”执行脚本中sql语句 ​ （1）在/opt/module/datas目录下创建hivef.sql文件 ​ [kingge@hadoop102 datas]$ touch hivef.sql ​ 文件中写入正确的sql语句 ​ select *from student; ​ （2）执行文件中的sql语句 [kingge@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql （3）执行文件中的sql语句并将结果写入文件中 [kingge@hadoop102 hive]$ bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt 2.7 Hive其他命令操作1）退出hive窗口： hive(default)&gt;exit; hive(default)&gt;quit; 在新版的oracle中没区别了，在以前的版本是有的： exit:先隐性提交数据，再退出； quit:不提交数据，退出； 2）在hive cli命令窗口中如何查看hdfs文件系统 ​ hive(default)&gt;dfs -ls /; 3）在hive cli命令窗口中如何查看hdfs本地系统 hive(default)&gt;! ls /opt/module/datas; 4）查看在hive中输入的所有历史命令 ​ （1）进入到当前用户的根目录/root或/home/kingge ​ （2）查看. hivehistory文件 [kingge@hadoop102 ~]$ cat .hivehistory 2.8 Hive常见属性配置（重要）2.8.1 Hive数据仓库位置配置​ 1）Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下 ​ Hive***默认创建了一个名字为“default**”的数据库，如果没有指明具体操作那个数据库，那么所有的操作针对的都是Default**。所以我们在操作书写sql**时，最好使用 use* *命令指明操作那个数据库。*** ​ 2）在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。 这句话的意思是： \\1. 我们如果在default数据库创建了一张表，那么实际上就是在/user/hive/warehouse路径下，创建了一个以表名为名称的文件\\。 Insert***如数据库表的数据会在该文件夹下面生成一个文件存储。每insert**一次就会生成一个（名字一般是000000_0**）,**如果通过load**的方式导入数据，那么存储数据文件就是load**数据的那个文件名。*** 例如 load data local inpath ‘/opt/module/datas/studentnto table default.student \\2. 如果我们创建了一个数据库，那么就会在 /user/hive/warehouse路径下创建 一个名字为数据库在HDFS上的默认存储路径是/user/hive/warehouse/数据库名.db。 的文件夹。 ​ 3）修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中） &lt;property&gt;&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;&lt;value&gt;/user/hive/warehouse&lt;/value&gt;&lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 配置同组用户有执行权限 bin/hdfs dfs -chmod g+w /user/hive/warehouse 2.8.2 查询后信息显示配置1）在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。（通过这项配置我们一颗额外的指导，启动***hive**后默认**use default* *数据库***） &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; ​ 2）重新启动hive，对比配置前后差异 （1）配置前 （2）配置后 2.8.3 Hive运行日志信息配置1）Hive的log默认存放在/tmp/kingge/hive.log目录下（当前用户名下）。 2）修改hive的log存放日志到/opt/module/hive/logs ​ （1）修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为 hive-log4j.properties [kingge@hadoop102 conf]$ pwd /opt/module/hive/conf ​ [kingge@hadoop102 conf]$ mv hive-log4j.properties.template hive-log4j.properties ​ （2）在hive-log4j.properties文件中修改log存放位置 hive.log.dir=/opt/module/hive/logs 2.8.4 参数配置方式1）查看当前所有的配置信息 ​ hive&gt;set; 2）参数的配置三种方式 ​ （1）配置文件方式 默认配置文件：hive-default.xml 用户自定义配置文件：hive-site.xml ​ 注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动\\的所有Hive进程都有效。 （2）命令行参数方式 启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。 例如： [kingge@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10; 注意：仅对本次hive启动有效（退出当前hive client客户端，失效并恢复默认值） 查看参数设置： hive (default)&gt; set mapred.reduce.tasks; （3）参数声明方式 可以在HQL中使用SET关键字设定参数 例如： hive (default)&gt; set mapred.reduce.tasks=100; 注意：仅对本次hive启动有效。（退出当前hive client客户端，失效并恢复默认值） 查看参数设置 hive (default)&gt; set mapred.reduce.tasks; 上述三种设定方式的优先级依次递增。即配置文件***&lt;**命令行参数**&lt;**参数声明*。*注意某些系统级的参数，例如**log4j**相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。*** 三 Hive数据类型3.1 基本数据类型 Hive数据类型 Java数据类型 长度 例子 TINYINT byte 1byte有符号整数 20 SMALINT short 2byte有符号整数 20 INT int 4byte有符号整数 20 BIGINT long 8byte有符号整数 20 BOOLEAN boolean 布尔类型，true或者false TRUE FALSE FLOAT float 单精度浮点数 3.14159 DOUBLE double 双精度浮点数 3.14159 STRING string 字符系列。可以指定字符集。可以使用单引号或者双引号。 ‘now is the time’ “for all good men” TIMESTAMP 时间类型 BINARY 字节数组 ​ 对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。 3.2 集合数据类型 数据类型 描述 语法示例 STRUCT 和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。 struct() MAP MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素 map() ARRAY 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。 Array() Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。 案例实操 1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为 &#123; &quot;name&quot;: &quot;songsong&quot;, &quot;friends&quot;: [&quot;bingbing&quot; , &quot;lili&quot;] , //列表Array, &quot;children&quot;: &#123; //键值Map, &quot;xiao song&quot;: 18 , &quot;xiaoxiao song&quot;: 19 &#125; &quot;address&quot;: &#123; //结构Struct, &quot;street&quot;: &quot;hui long guan&quot; , &quot;city&quot;: &quot;beijing&quot; &#125;&#125; 2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据。 创建本地测试文件test.txt songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 注意，MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。 3）Hive上创建测试表test create table test(name string,friends array&lt;string&gt;,children map&lt;string, int&gt;,address struct&lt;street:string, city:string&gt;)row format delimited fields terminated by &apos;,&apos;collection items terminated by &apos;_&apos;map keys terminated by &apos;:&apos;lines terminated by &apos;\\n&apos;; 字段解释： row format delimited fields terminated by ‘,’ – 列分隔符 collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号) map keys terminated by ‘:’ – MAP中的key与value的分隔符 lines terminated by ‘\\n’; – 行分隔符 4）导入文本数据到测试表 hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’ into table test; 5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式 hive (default)&gt; select friends[1],children[&apos;xiao song&apos;],address.city from test where name=&quot;songsong&quot;;OK_c0 _c1 citylili 18 beijingTime taken: 0.076 seconds, Fetched: 1 row(s) 3.3 类型转化Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。 1）隐式类型转换规则如下。 （1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。 （2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。 （3）TINYINT、SMALLINT、INT都可以转换为FLOAT。 （4）BOOLEAN类型不可以转换为任何其它的类型。 2）可以使用CAST操作显示进行数据类型转换，例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。 四 DDL数据定义4.1 创建数据库1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。 hive (default)&gt; create database db_hive; //最终在HDFS中是/user/hive/warehouse/ db_hive.db。 2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法） hive&gt; create database db_hive;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exiStshive (default)&gt; create database if not exists db_hive; 3）创建一个数据库，指定数据库在HDFS上存放的位置 hive (default)&gt; create database db_hive2 location ‘/db_hive2.db’; create database db_hive2 location ‘/ ‘; 不指明生成的文件夹名称，那么也会创建成功，只不过他没有像上面那样生成了一个文件夹（也就是访问HDFS根目录时，发现是空的），但是数据库已经创建成功。当你使用该数据库创建一张表的时候。会在根目录下直接生成以表名为名称的文件夹。保存表的数据（跟之前的一样）–所以推荐还是指明创建的数据库名称\\ 4.2 修改数据库（只能修改数据库描述信息，其他数据不能修改）用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。（跟***mysql**的差别***） hive (default)&gt; alter database db_hive set dbproperties(‘createtime’=’20170830’); 在mysql中查看修改结果 hive&gt; desc database extended db_hive; db_name comment location owner_name owner_type parameters db_hive hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db kingge USER {createtime=20170830} 4.3 查询数据库4.3.1 显示数据库1）显示数据库 hive&gt; show databases; 2）过滤显示查询的数据库 ​ hive&gt; show databases like ‘db_hive*’; OK db_hive db_hive_1 4.3.2 查看数据库详情1）显示数据库信息 hive&gt; desc database db_hive; OK db_hive hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db kinggeUSER 2）显示数据库详细信息，extended hive&gt; desc database extended db_hive; OK db_hive hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db kinggeUSER 4.3.3 切换当前数据库hive (default)&gt; use db_hive; 4.4 删除数据库1）删除空数据库 hive&gt;drop database db_hive2; 2）如果删除的数据库不存在，最好采用 if exists判断数据库是否存在 hive&gt; drop database db_hive2; FAILED: SemanticException [Error 10072]: Database does not exist: db_hive hive&gt; drop database if exists db_hive2; 3）如果数据库不为空，可以采用cascade命令，强制删除 hive&gt; drop database db_hive;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.) hive&gt; drop database db_hive cascade; 4.5 创建表1）建表语法 CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] 2）字段解释说明： （1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 （2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 （3）COMMENT：为表和列添加注释。 （4）PARTITIONED BY创建分区表 （5）CLUSTERED BY创建分桶表 （6）SORTED BY不常用 （7）ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] ​ [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)] 用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。 （8）STORED AS***指定存储文件类型（默认是txt**）*** 常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件） 如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 （9）LOCATION ：指定表在HDFS上的存储位置。 （10）LIKE允许用户复制现有的表结构，但是不复制数据。 4.5.1 管理表1）理论 默认创建的表都是所谓的管理表，有时也被称为内部表\\。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 当我们删除一个管理表时，Hive也会删除这个表中数据（删除HDFS存储的目录和数据）。管理表不适合和其他工具共享数据。 2**）案例实操** （1）普通创建表 create table if not exists student2( id int, name string ) row format delimited fields terminated by ‘\\t’ stored as textfile location ‘/user/hive/warehouse/student2’; （2）根据查询结果创建表（查询的结果会添加到新创建的表中） create table if not exists student3 as select id, name from student; （3）根据已经存在的表结构创建表（只拷贝student结构，没有获取student数据） create table if not exists student4 like student; （4）查询表的类型 hive (default)&gt; desc formatted student2; Table Type: MANAGED_TABLE 4.5.2 外部表1）理论 因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据（即在hdfs中还保存着数据库的数据），不过描述表的元数据信息会被删除掉。（mysql中关于该表的元数据信息就会被删除） 2）管理表和外部表的使用场景： 每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。 3）案例实操 分别创建部门和员工外部表，并向表中导入数据。 （1）原始数据 ​ dept.txt 10 ACCOUNTING 170020 RESEARCH 180030 SALES 190040 OPERATIONS 1700 emp.txt7369 SMITH CLERK 7902 1980-12-17 800.00 207499 ALLEN SALESMAN 7698 1981-2-20 1600.00 300.00 307521 WARD SALESMAN 7698 1981-2-22 1250.00 500.00 307566 JONES MANAGER 7839 1981-4-2 2975.00 207654 MARTIN SALESMAN 7698 1981-9-28 1250.00 1400.00 307698 BLAKE MANAGER 7839 1981-5-1 2850.00 307782 CLARK MANAGER 7839 1981-6-9 2450.00 107788 SCOTT ANALYST 7566 1987-4-19 3000.00 207839 KING PRESIDENT 1981-11-17 5000.00 107844 TURNER SALESMAN 7698 1981-9-8 1500.00 0.00 307876 ADAMS CLERK 7788 1987-5-23 1100.00 207900 JAMES CLERK 7698 1981-12-3 950.00 307902 FORD ANALYST 7566 1981-12-3 3000.00 207934 MILLER CLERK 7782 1982-1-23 1300.00 10 ​ （2）建表语句 ​ 创建部门表 create external table if not exists default.dept(deptno int,dname string,loc int)row format delimited fields terminated by &apos;\\t&apos;; ​ 创建员工表 create external table if not exists default.emp(empno int,ename string,job string,mgr int,hiredate string, sal double, comm double,deptno int)row format delimited fields terminated by &apos;\\t&apos;; （3）查看创建的表 hive (default)&gt; show tables; OK tab_name dept emp ​ （4）向外部表中导入数据 ​ 导入数据 hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept; hive (default)&gt; load data local inpath ‘/opt/module/datas/emp.txt’ into table default.emp; 查询结果 hive (default)&gt; select * from emp; hive (default)&gt; select * from dept; ​ （5）查看表格式化数据 hive (default)&gt; desc formatted dept; Table Type: EXTERNAL_TABLE 4.6 分区表分区表实际上就是对应一个***HDFS**文件系统上的独立的文件夹***，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 举个例子：我们知道项目日志框架会根据日期每天生成一个日志文件，这样方便我们查看，也方便存储的打开。因为如果把所有的日志信息都输入到一个文件中，那么查阅的时候就很不方便。所以分区表的含义也是如此，根据某些关键字把数据库表的数据拆分成多个文件夹存储。 需要注意，假设创建的是分区表，那么插入数据的时候（insert***或者load**的方式），都需要指明partition**字段的值，否则会报错。*** 如果某张表是分区表。那么每个分区的定义，其实就表现为了这张表的数据存储目录下的一个子目录，如果是分区表。那么数据文件一定要存储在某个分区中，而不能直接存储在表中。\\ 4.6.1 分区表基本操作1）引入分区表（需要根据日期对日志进行管理） ​ /user/hive/warehouse/log_partition/20170702/20170702.log ​ /user/hive/warehouse/log_partition/20170703/20170703.log /user/hive/warehouse/log_partition/20170704/20170704.log 2）创建分区表语法（注意如果在创建表时不指定分区，那么后面就不能够使用分区相关特性，例如\\ load***数据时不能够指定load**到哪个分区）*** hive (default)&gt; create table dept_partition( deptno int, dname string, loc string ) partitioned by (month string) row format delimited fields terminated by &apos;\\t&apos;; 3）加载数据到分区表中 测试直接导入dep数据：发现报错 必须指明partition 字段 hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201709’); hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201708’); hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition partition(month=’201707’); 4）查询分区表中数据 查询所有分区 select * from dept_partition ​ 单分区查询 hive (default)&gt; select * from dept_partition where month=’201709’; 多分区联合查询 hive (default)&gt; select * from dept_partition where month=’201709’ ​ union ​ select * from dept_partition where month=’201708’ ​ union ​ select * from dept_partition where month=’201707’; _u3.deptno _u3.dname _u3.loc _u3.month 10 ACCOUNTING NEW YORK 201707 10 ACCOUNTING NEW YORK 201708 10 ACCOUNTING NEW YORK 201709 20 RESEARCH DALLAS 201707 20 RESEARCH DALLAS 201708 20 RESEARCH DALLAS 201709 30 SALES CHICAGO 201707 30 SALES CHICAGO 201708 30 SALES CHICAGO 201709 40 OPERATIONS BOSTON 201707 40 OPERATIONS BOSTON 201708 40 OPERATIONS BOSTON 201709 5）增加分区 ​ 创建单个分区（也就是在hdfs中创建文件夹month=201706） hive (default)&gt; alter table dept_partition add partition(month=’201706’) ; ​ 同时创建多个分区 hive (default)&gt; alter table dept_partition add partition(month=’201705’) partition(month=’201704’); 6）删除分区 ​ 删除单个分区 hive (default)&gt; alter table dept_partition drop partition (month=’201704’); 同时删除多个分区 hive (default)&gt; alter table dept_partition drop partition (month=’201705’), partition (month=’201706’); 7）查看分区表有多少分区 hive&gt;show partitions dept_partition; 8）查看分区表结构 ​ hive&gt;desc formatted dept_partition; # Partition Information # col_name data_type comment month string 4.6.2 分区表注意事项1）创建二级分区表 hive (default)&gt; create table dept_partition2( deptno int, dname string, loc string ) partitioned by (month string, day string) row format delimited fields terminated by &apos;\\t&apos;; 2）正常的加载数据 （1）加载数据到二级分区表中 hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table default.dept_partition2 partition(month=’201709’, day=’13’); （2）查询分区数据 hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’13’; 3）把数据直接上传到分区目录上，让分区表和数据产生关联的两种方式 ​ （1）方式一：上传数据后修复 ​ 上传数据 hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12; hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; ​ 查询数据（查询不到刚上传的数据） hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’12’; 执行修复命令 ​ hive&gt;msck repair table dept_partition2; 添加数据的元数据信息到mysql中，关联表 再次查询数据 hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’12’; ​ （2）方式二：上传数据后添加分区 ​ 上传数据 hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11; hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=11; ​ 执行添加分区 ​ hive (default)&gt; alter table dept_partition2 add partition(month=’201709’, day=’11’); ​ 查询数据 hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’11’; ​ （3）方式三：上传数据后load数据到分区 ​ 创建目录 hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10; 上传数据 hive (default)&gt; load data local inpath ‘/opt/module/datas/dept.txt’ into table dept_partition2 partition(month=’201709’,day=’10’); 查询数据 hive (default)&gt; select * from dept_partition2 where month=’201709’ and day=’10’; 4.7 修改表（不支持删除某个字段）4.7.1 重命名表​ （1）语法 ALTER TABLE table_name RENAME TO new_table_name ​ （2）实操案例 hive (default)&gt; alter table dept_partition2 rename to dept_partition3; 4.7.2 增加、修改和删除表分区详见4.6.1分区表基本操作。 4.7.3 增加/修改/替换列信息1）语法 ​ 更新列 ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 增加和替换列 ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …) 注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。 2）实操案例 （1）查询表结构 hive&gt;desc dept_partition; （2）添加列 hive (default)&gt; alter table dept_partition add columns(deptdesc string); （3）查询表结构 hive&gt;desc dept_partition; （4）更新列 hive (default)&gt; alter table dept_partition change column deptdesc desc int; （5）查询表结构 hive&gt;desc dept_partition; （6）替换列 hive (default)&gt; alter table dept_partition replace columns(deptno string, dname string, loc string); （全局改变表的列，只保留指定的列） （7）查询表结构 hive&gt;desc dept_partition; 4.8 删除表hive (default)&gt; drop table dept_partition; 五 DML数据操作5.1 数据导入5.1.1 向表中装载数据（Load）1）语法 hive&gt;load data [local] inpath ‘/opt/module/datas/student.txt’ [overwrite] into table student [partition (partcol1=val1,…)]; （1）load data:表示加载数据 （2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表 （3）inpath:表示加载数据的路径 （4）overwrite:表示覆盖表中已有数据，否则表示追加 （5）into table:表示加载到哪张表 （6）student:表示具体的表 （7）partition:表示上传到指定分区 2）实操案例 ​ （0）创建一张表 hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by ‘\\t’; （1）加载本地文件到hive hive (default)&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table default.student; ​ （2）加载HDFS文件到hive中 ​ 上传文件到HDFS hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/kingge/hive; 加载HDFS上数据 hive (default)&gt;load data inpath ‘/user/kingge/hive/student.txt’ into table default.student; 需要注意的是：/user/kingge/hive/student.txt\\ 这个文件就不存在的，因为他是通过剪切的方式把数据 load***到了 /user/kingge/hive/student* *目录下*** ​ （3）加载数据覆盖表中已有的数据 ​ 上传文件到HDFS hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/kingge/hive; 加载数据覆盖表中已有的数据 hive (default)&gt;load data inpath ‘/user/kingge/hive/student.txt’ overwrite into table default.student; 5.1.2 通过查询语句向表中插入数据（Insert）1）创建一张分区表 hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by ‘\\t’; 2）基本插入数据 hive (default)&gt; insert into table student partition(month=’201709’) values(1,’wangwu’); 3）基本模式插入（根据单张表查询结果） hive (default)&gt; insert overwrite table student partition(month=’201708’) ​ select id, name from student where month=’201709’; 4）多插入模式（根据多张表查询结果） hive (default)&gt; from student ​ insert overwrite table student partition(month=’201707’) ​ select id, name where month=’201709’ ​ insert overwrite table student partition(month=’201706’) ​ select id, name where month=’201709’; 5.1.3 查询语句中创建表并加载数据（As Select）详见4.5.1章创建表。 根据查询结果创建表（查询的结果会添加到新创建的表中） create table if not exists student3 as select id, name from student; 5.1.4 创建表时通过Location指定加载数据路径1）创建表，并指定在hdfs\\上的位置 hive (default)&gt; create table if not exists student5( ​ id int, name string ​ ) ​ row format delimited fields terminated by ‘\\t’ ​ location ‘/user/hive/warehouse/student5’; 2）上传数据到hdfs上 hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5; 3）查询数据 hive (default)&gt; select * from student5; 5.1.5 Import数据到指定Hive表中注意：先用export导出后，再将数据导入。（数据来源hdfs） hive (default)&gt; import table student2 partition(month=’201709’) from ‘/user/hive/warehouse/export/student’; 5.2 数据导出5.2.1 Insert导出1）将查询的结果导出到本地 hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/export/student’ select * from student; 2）将查询的结果格式化导出到本地 hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/export/student1’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\\t’ select * from student; 3）将查询的结果导出到HDFS上(没有local) hive (default)&gt; insert overwrite directory ‘/user/kingge/student2’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\\t’ select * from student; 5.2.2 Hadoop命令导出到本地 hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/datas/export/student3.txt; 5.2.3 Hive Shell 命令导出基本语法：（hive -f/-e 执行语句或者脚本 &gt; file） [kingge@hadoop102 hive]$ bin/hive -e ‘select * from default.student;’ &gt; /opt/module/datas/export/student4.txt; 5.2.4 Export导出到HDFS上 hive (default)&gt; export table default.student to ‘/user/hive/warehouse/export/student’; 5.2.5 Sqoop导出后续课程专门讲。 5.3 清除表中数据（Truncate）注意：Truncate只能删除管理表，不能删除外部表中数据 hive (default)&gt; truncate table student; 六 查询https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select [WITH CommonTableExpression (, CommonTableExpression)*] (Note: Only available starting with Hive 0.13.0)SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [ORDER BY col_list] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list] ] [LIMIT number] 6.1 基本查询（Select…From）6.1.1 全表和特定列查询1）全表查询 hive (default)&gt; select * from emp; 2）选择特定列查询 hive (default)&gt; select empno, ename from emp; 注意： （1）SQL 语言大小写不敏感。 （2）SQL 可以写在一行或者多行 （3）关键字不能被缩写也不能分行 （4）各子句一般要分行写。 （5）使用缩进提高语句的可读性。 6.1.2 列别名1）重命名一个列。 2）便于计算。 3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’ 4）案例实操 ​ （1）查询名称和部门 ​ hive (default)&gt; select ename AS name, deptno dn from emp; 6.1.3 算术运算符 运算符 描述 A+B A和B 相加 A-B A减去B A*B A和B 相乘 A/B A除以B A%B A对B取余 A&amp;B A和B按位取与 A\\ B A和B按位取或 A^B A和B按位取异或 ~A A按位取反 案例实操 ​ 查询出所有员工的薪水后加1显示。 hive (default)&gt; select sal +1 from emp; 6.1.4 常用函数1）求总行数（count） hive (default)&gt; select count(*) cnt from emp; 2）求工资的最大值（max） hive (default)&gt; select max(sal) max_sal from emp; 3）求工资的最小值（min） hive (default)&gt; select min(sal) min_sal from emp; 4）求工资的总和（sum） hive (default)&gt; select sum(sal) sum_sal from emp; 5）求工资的平均值（avg） ​ hive (default)&gt; select avg(sal) avg_sal from emp; 6.1.5 Limit语句典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。 hive (default)&gt; select * from emp limit 5; 6.2 Where语句1）使用WHERE子句，将不满足条件的行过滤掉。 2）WHERE子句紧随FROM子句。 3）案例实操 查询出薪水大于1000的所有员工 hive (default)&gt; select * from emp where sal &gt;1000; 6.2.1 比较运算符（Between/In/ Is Null）1）下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中。 操作符 支持的数据类型 描述 A=B 基本数据类型 如果A等于B则返回TRUE，反之返回FALSE A&lt;=&gt;B 基本数据类型 如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL A&lt;&gt;B, A!=B 基本数据类型 A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE A&lt;B 基本数据类型 A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE A&lt;=B 基本数据类型 A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE A&gt;B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE A&gt;=B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE A [NOT] BETWEEN B AND C 基本数据类型 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。 A IS NULL 所有数据类型 如果A等于NULL，则返回TRUE，反之返回FALSE A IS NOT NULL 所有数据类型 如果A不等于NULL，则返回TRUE，反之返回FALSE IN(数值1, 数值2) 所有数据类型 使用 IN运算显示列表中的值 A [NOT] LIKE B STRING 类型 B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。 A RLIKE B, A REGEXP B STRING 类型 B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。 2）案例实操 （1）查询出薪水等于5000的所有员工 hive (default)&gt; select * from emp where sal =5000; ​ （2）查询工资在500到1000的员工信息 hive (default)&gt; select * from emp where sal between 500 and 1000; ​ （3）查询comm为空的所有员工信息 hive (default)&gt; select * from emp where comm is null; ​ （4）查询工资是1500和5000的员工信息 hive (default)&gt; select * from emp where sal IN (1500, 5000); 6.2.2 Like和RLike1）使用LIKE运算选择类似的值 2）选择条件可以包含字符或数字: % 代表零个或多个字符(任意个字符)。 _ 代表一个字符。 3）RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。 4）案例实操 ​ （1）查找以2开头薪水的员工信息 ​ hive (default)&gt; select * from emp where sal LIKE ‘2%’; ​ （2）查找第二个数值为2的薪水的员工信息 hive (default)&gt; select * from emp where sal LIKE ‘_2%’; ​ （3）查找薪水中含有2的员工信息 hive (default)&gt; select * from emp where sal RLIKE ‘[2]’; 6.2.3 逻辑运算符（And/Or/Not） 操作符 含义 AND 逻辑并 OR 逻辑或 NOT 逻辑否 案例实操 ​ （1）查询薪水大于1000，部门是30 hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30; ​ （2）查询薪水大于1000，或者部门是30 hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30; ​ （3）查询除了20部门和30部门以外的员工信息 hive (default)&gt; select * from emp where deptno not IN(30, 20); 6.3 分组6.3.1 Group By语句GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。 案例实操： ​ （1）计算emp表每个部门的平均工资 hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno; ​ （2）计算emp每个部门中每个岗位的最高薪水 hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job; 6.3.2 Having语句1）having与where不同点 （1）where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。（也就是说在***select* *查询结果完后，having**里面的语句才会执行。*** Where***是在select**结果时立即生效，参与过滤。）*** （2）where后面不能写分组函数，而having后面可以使用分组函数。 （3）having只用于group by分组统计语句。 2）案例实操： （1）求每个部门的平均薪水大于2000的部门 ​ 求每个部门的平均工资 ​ hive (default)&gt; select deptno, avg(sal) from emp group by deptno; ​ 求每个部门的平均薪水大于2000的部门 hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000; 6.4 Join语句6.4.1 等值JoinHive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。 何为非等值连接 案例实操 （1）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门编号； hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept d on e.deptno = d.deptno; 6.4.2 表的别名1）好处 （1）使用别名可以简化查询。 （2）使用表名前缀可以提高执行效率。 2）案例实操 合并员工表和部门表 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno; 6.4.3 内连接内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno; 6.4.4 左外连接​ 左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno; 6.4.5 右外连接右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno; 6.4.6 满外连接（oracle支持fulljoin语法，mysql没有该语法）​ 满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。 hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno; 6.4.7 多表连接注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。 0）数据准备 1700 Beijing1800 London1900 Tokyo 1）创建位置表 create table if not exists default.location( loc int, loc_name string ) row format delimited fields terminated by ‘\\t’; 2）导入数据 hive (default)&gt; load data local inpath ‘/opt/module/datas/location.txt’ into table default.location; 3）多表连接查询 hive (default)&gt;SELECT e.ename, d.deptno, l. loc_name FROM emp e JOIN dept d ON d.deptno = e.deptno JOIN location l ON d.loc = l.loc; 大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。 注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。 6.4.8 笛卡尔积1）笛卡尔集会在下面条件下产生: （1）省略连接条件 （2）连接条件无效 （3）所有表中的所有行互相连接 2）案例实操 hive (default)&gt; select empno, deptno from emp, dept; FAILED: SemanticException Column deptno Found in more than One Tables/Subqueries 6.4.9 连接谓词中不支持orhive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno or e.ename=d.ename; 错误的 6.5 排序6.5.1 全局排序（Order By）Order By：全局排序，一个MapReduce 1）使用 ORDER BY 子句排序 ASC（ascend）: 升序（默认） DESC（descend）: 降序 2）ORDER BY 子句在SELECT语句的结尾。 3）案例实操 ​ （1）查询员工信息按工资升序排列 hive (default)&gt; select * from emp order by sal; ​ （2）查询员工信息按工资降序排列 hive (default)&gt; select * from emp order by sal desc; 6.5.2 按照别名排序按照员工薪水的2倍排序 hive (default)&gt; select ename, sal*2 twosal from emp order by twosal; 6.5.3 多个列排序按照部门和工资升序排序 hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ; 6.5.4 每个MapReduce内部排序（Sort By）Sort By：每个MapReduce内部进行排序，对全局结果集来说不是排序。 ​ 1）设置reduce个数 hive (default)&gt; set mapreduce.job.reduces=3; 2）查看设置reduce个数 hive (default)&gt; set mapreduce.job.reduces; 3）根据部门编号降序查看员工信息 hive (default)&gt; select * from emp sort by empno desc; ​ 4）将查询结果导入到文件中（按照部门编号降序排序） hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/sortby-result’ select * from emp sort by deptno desc; 6.5.5 分区排序（Distribute By）Distribute By：类似MR中partition，进行分区，结合sort by使用。 ​ 注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。 对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。 案例实操： ​ （1）先按照部门编号分区，再按照员工编号降序排序。 hive (default)&gt; set mapreduce.job.reduces=3; ​ hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/distribute-result’ select * from emp distribute by deptno sort by empno desc; 6.5.6 Cluster By当distribute by和sorts by字段相同时，可以使用cluster by方式。 cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。 1）以下两种写法等价 hive (default)&gt; select * from emp cluster by deptno; hive (default)&gt; select * from emp distribute by deptno sort by deptno; 注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。 6.6 分桶及抽样查询6.6.1 分桶表数据存储​ 分区针对的是数据的存储路径（通过分文件夹的方式存储数据）；分桶针对的是数据文件。（通过把一份数据分为几分去存储） 分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。 ​ 分桶是将数据集分解成更容易管理的若干部分的另一个技术。 1）先创建分桶表，通过直接导入数据文件的方式 （0）数据准备 1001 ss11002 ss21003 ss31004 ss41005 ss51006 ss61007 ss71008 ss81009 ss91010 ss101011 ss111012 ss121013 ss131014 ss141015 ss151016 ss16 （1）创建分桶表 create table stu_buck(id int, name string) clustered by(id) into 4 buckets row format delimited fields terminated by ‘\\t’; （2）查看表结构 hive (default)&gt; desc formatted stu_buck; Num Buckets: 4 （3）导入数据到分桶表中 hive (default)&gt; load data local inpath ‘/opt/module/datas/student.txt’ into table stu_buck; （4）查看创建的分桶表中是否分成4个桶 发现并没有分成4个桶。是什么原因呢？ 2）创建分桶表时，数据通过子查询的方式导入 ​ （1）先建一个普通的stu表 create table stu(id int, name string) row format delimited fields terminated by ‘\\t’; ​ （2）向普通的stu表中导入数据 load data local inpath ‘/opt/module/datas/student.txt’ into table stu; ​ （3）清空stu_buck表中数据 truncate table stu_buck; select * from stu_buck; ​ （4）导入数据到分桶表，通过子查询的方式 insert into table stu_buck select id, name from stu; （5）发现还是只有一个分桶 （6）需要设置一个属性 hive (default)&gt; set hive.enforce.bucketing=true; hive (default)&gt; set mapreduce.job.reduces=-1; hive (default)&gt; insert into table stu_buck select id, name from stu; l 可以看到，把student***的数据分为了四份存储。*** （7）查询分桶的数据 hive (default)&gt; select * from stu_buck; OK stu_buck.id stu_buck.name 1001 ss1 1005 ss5 1009 ss9 1012 ss12 1016 ss16 1002 ss2 1006 ss6 1013 ss13 1003 ss3 1007 ss7 1010 ss10 1014 ss14 1004 ss4 1008 ss8 1011 ss11 1015 ss15 6.6.2 分桶抽样查询对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。 查询表stu_buck中的数据。 hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id); 注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。 y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。 x表示从哪个bucket开始抽取。例如，table总bucket数为4，tablesample(bucket 4 out of 4)，表示总共抽取（4/4=）1个bucket的数据，抽取第4个bucket的数据。 注意：x的值必须小于等于y的值，否则 FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck 6.6.3 数据块抽样Hive提供了另外一种按照百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行的抽样。 hive (default)&gt; select * from stu tablesample(0.1 percent) ; 提示：这种抽样方式不一定适用于所有的文件格式。另外，这种抽样的最小抽样单元是一个HDFS数据块。因此，如果表的数据大小小于普通的块大小128M的话，那么将会返回所有行。 七 函数7.1 系统自带的函数1）查看系统自带的函数 hive&gt; show functions; 2）显示自带的函数的用法 hive&gt; desc function upper; 3）详细显示自带的函数的用法 hive&gt; desc function extended upper; 7.2 自定义函数1）Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。 2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。 3）根据用户自定义函数类别分为以下三种： ​ （1）UDF（User-Defined-Function） ​ 一进一出 ​ （2）UDAF（User-Defined Aggregation Function） ​ 聚集函数，多进一出 ​ 类似于：count/max/min ​ （3）UDTF（User-Defined Table-Generating Functions） ​ 一进多出 ​ 如lateral view explore() 4）官方文档地址 https://cwiki.apache.org/confluence/display/Hive/HivePlugins 5）编程步骤： ​ （1）继承org.apache.hadoop.hive.ql.UDF ​ （2）需要实现evaluate函数；evaluate函数支持重载； ​ （3）在hive的命令行窗口创建函数 ​ a）添加jar ​ add jar linux_jar_path ​ b）创建function， ​ create [temporary] function [dbname.]function_name AS class_name; ​ （4）在hive的命令行窗口删除函数 ​ Drop [temporary] function [if exists] [dbname.]function_name; 6）注意事项 ​ （1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void； 7.3 自定义UDF函数开发案例1）创建一个java工程，并创建一个lib文件夹 2）将hive的jar包解压后，将apache-hive-1.2.1-bin\\lib文件下的jar包都拷贝到java工程中。 3）创建一个类 package com.kingge.hive; import org.apache.hadoop.hive.ql.exec.UDF; public class Lower extends UDF { public String evaluate (final String s) { if (s == null) { return null; } return s.toString().toLowerCase(); } } 4）打成jar包上传到服务器/opt/module/jars/udf.jar 5）将jar包添加到hive的classpath hive (default)&gt; add jar /opt/module/datas/udf.jar; 6）创建临时函数\\与开发好的java class关联（临时函数***hive**命令窗口一关闭马上销毁，本质是没有注册到元数据中***） hive (default)&gt; create temporary function udf_lower as “com.kingge.hive.Lower”; 7）即可在hql中使用自定义的函数strip hive (default)&gt; select ename, udf_lower(ename) lowername from emp; 八 压缩和存储8.1 Hadoop源码编译支持Snappy压缩8.1.1 资源准备1）CentOS联网 配置CentOS能连接外网。Linux虚拟机ping www.baidu.com 是畅通的 注意：采用root角色编译，减少文件夹权限出现问题 2）jar包准备(hadoop源码、JDK8 、maven、protobuf) （1）hadoop-2.7.2-src.tar.gz （2）jdk-8u144-linux-x64.tar.gz （3）snappy-1.1.3.tar.gz （4）apache-maven-3.0.5-bin.tar.gz （5）protobuf-2.5.0.tar.gz 8.1.2 jar包安装0）注意：所有操作必须在root用户下完成 1）JDK解压、配置环境变量JAVA_HOME和PATH，验证java-version(如下都需要验证是否配置成功) [root@hadoop101 software] # tar -zxf jdk-8u144-linux-x64.tar.gz -C /opt/module/ [root@hadoop101 software]# vi /etc/profile #JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：java -version 2）Maven解压、配置 MAVEN_HOME和PATH。 [root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile #MAVEN_HOME export MAVEN_HOME=/opt/module/apache-maven-3.0.5 export PATH=$PATH:$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn -version 8.1.3 编译源码1）准备编译环境 [root@hadoop101 software]# yum install svn [root@hadoop101 software]# yum install autoconf automake libtool cmake [root@hadoop101 software]# yum install ncurses-devel [root@hadoop101 software]# yum install openssl-devel [root@hadoop101 software]# yum install gcc* 2）编译安装snappy [root@hadoop101 software]# tar -zxvf snappy-1.1.3.tar.gz -C /opt/module/ [root@hadoop101 module]# cd snappy-1.1.3/ [root@hadoop101 snappy-1.1.3]# ./configure [root@hadoop101 snappy-1.1.3]# make [root@hadoop101 snappy-1.1.3]# make install # 查看snappy库文件 [root@hadoop101 snappy-1.1.3]# ls -lh /usr/local/lib |grep snappy 3）编译安装protobuf [root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/ [root@hadoop101 module]# cd protobuf-2.5.0/ [root@hadoop101 protobuf-2.5.0]# ./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make install # 查看protobuf版本以测试是否安装成功 [root@hadoop101 protobuf-2.5.0]# protoc –version 4）编译hadoop native [root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz [root@hadoop101 software]# cd hadoop-2.7.2-src/ [root@hadoop101 software]# mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy 执行成功后，/opt/software/hadoop-2.7.2-src/hadoop-dist/target/hadoop-2.7.2.tar.gz即为新生成的支持snappy压缩的二进制安装包。 8.2 Hadoop压缩配置8.2.1 MR支持的压缩编码 压缩格式 工具 算法 文件扩展名 是否可切分 DEFAULT 无 DEFAULT .deflate 否 Gzip gzip DEFAULT .gz 否 bzip2 bzip2 bzip2 .bz2 是 LZO lzop LZO .lzo 是 Snappy 无 Snappy .snappy 否 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 8.2.2 压缩参数配置要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）： 参数 默认值 阶段 建议 io.compression.codecs （在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.Lz4Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.DefaultCodec mapper输出 使用LZO、LZ4或snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec org.apache.hadoop.io.compress. DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 8.3 开启Map输出阶段压缩开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下： 案例实操： ​ 1）开启hive中间传输数据压缩功能 hive (default)&gt;set hive.exec.compress.intermediate=true; 2）开启mapreduce中map输出压缩功能 hive (default)&gt;set mapreduce.map.output.compress=true; 3）设置mapreduce中map输出数据的压缩方式 hive (default)&gt;set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec; 4）执行查询语句 ​ hive (default)&gt; select count(ename) name from emp; 8.4 开启Reduce输出阶段压缩当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。 案例实操： 1）开启hive最终输出数据压缩功能 hive (default)&gt;set hive.exec.compress.output=true; 2）开启mapreduce最终输出数据压缩 hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true; 3）设置mapreduce最终数据输出压缩方式 hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; 4）设置mapreduce最终数据输出压缩为块压缩 hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK; 5）测试一下输出结果是否是压缩文件 hive (default)&gt; insert overwrite local directory ‘/opt/module/datas/distribute-result’ select * from emp distribute by deptno sort by empno desc; 8.5 文件存储格式Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。 8.5.1 列式存储和行式存储上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。 行存储的特点： 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。 列存储的特点： 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。 TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的； ORC和PARQUET是基于列式存储的。 8.5.2 TextFile格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。 8.5.3 Orc格式Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。 可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer： 1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。 2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。 3）Stripe Footer：存的是各个Stream的类型，长度等信息。 每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。 8.5.4 Parquet格式Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。 Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。 通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。 上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。 8.5.5 主流文件存储格式对比实验从存储文件的压缩比和查询速度两个角度对比。 存储文件的压缩比测试： 0）测试数据 1）TextFile （1）创建表，存储数据格式为TEXTFILE create table log_text ( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) row format delimited fields terminated by ‘\\t’ stored as textfile ; （2）向表中加载数据 hive (default)&gt; load data local inpath ‘/opt/module/datas/log.data’ into table log_text ; （3）查看表中数据大小 hive (default)&gt; dfs -du -h /user/hive/warehouse/log_text; 18.1 M /user/hive/warehouse/log_text/log.data 2）ORC ​ （1）创建表，存储数据格式为ORC create table log_orc( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) row format delimited fields terminated by ‘\\t’ stored as orc ; （2）向表中加载数据 hive (default)&gt; insert into table log_orc select * from log_text ; （3）查看表中数据大小 hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ; 2.8 M /user/hive/warehouse/log_orc/000000_0 3）Parquet ​ （1）创建表，存储数据格式为parquet create table log_parquet( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) row format delimited fields terminated by ‘\\t’ stored as parquet ; （2）向表中加载数据 hive (default)&gt; insert into table log_parquet select * from log_text ; （3）查看表中数据大小 hive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ; 13.1 M /user/hive/warehouse/log_parquet/000000_0 存储文件的压缩比总结： ORC &gt; Parquet &gt; textFile 存储文件的查询速度测试： 1）TextFile hive (default)&gt; select count(*) from log_text; _c0 100000 Time taken: 21.54 seconds, Fetched: 1 row(s) Time taken: 21.08 seconds, Fetched: 1 row(s) 2）ORC hive (default)&gt; select count(*) from log_orc; _c0 100000 Time taken: 20.867 seconds, Fetched: 1 row(s) Time taken: 22.667 seconds, Fetched: 1 row(s) 3）Parquet hive (default)&gt; select count(*) from log_parquet; _c0 100000 Time taken: 22.922 seconds, Fetched: 1 row(s) Time taken: 21.074 seconds, Fetched: 1 row(s) 存储文件的查询速度总结：查询速度相近。 8.6 存储和压缩结合8.6.1 修改Hadoop集群具有Snappy压缩方式1）查看hadoop checknative命令使用 [kingge@hadoop104 hadoop-2.7.2]$ hadoop ​ checknative [-a|-h] check native hadoop and compression libraries availability 2）查看hadoop支持的压缩方式 ​ [kingge@hadoop104 hadoop-2.7.2]$ hadoop checknative 17/12/24 20:32:52 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version 17/12/24 20:32:52 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library Native library checking: hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so zlib: true /lib64/libz.so.1 snappy: false lz4: true revision:99 bzip2: false 3）将编译好的支持Snappy压缩的hadoop-2.7.2.tar.gz包导入到hadoop102的/opt/software中 4）解压hadoop-2.7.2.tar.gz到当前路径 [kingge@hadoop102 software]$ tar -zxvf hadoop-2.7.2.tar.gz 5）进入到/opt/software/hadoop-2.7.2/lib/native路径可以看到支持Snappy压缩的动态链接库 [kingge@hadoop102 native]$ pwd /opt/software/hadoop-2.7.2/lib/native [kingge@hadoop102 native]$ ll -rw-r–r–. 1 kingge kingge 472950 9月 1 10:19 libsnappy.a -rwxr-xr-x. 1 kingge kingge 955 9月 1 10:19 libsnappy.la lrwxrwxrwx. 1 kingge kingge 18 12月 24 20:39 libsnappy.so -&gt; libsnappy.so.1.3.0 lrwxrwxrwx. 1 kingge kingge 18 12月 24 20:39 libsnappy.so.1 -&gt; libsnappy.so.1.3.0 -rwxr-xr-x. 1 kingge kingge 228177 9月 1 10:19 libsnappy.so.1.3.0 6）拷贝/opt/software/hadoop-2.7.2/lib/native里面的所有内容到开发集群的/opt/module/hadoop-2.7.2/lib/native路径上 ​ [kingge@hadoop102 native]$ cp ../native/* /opt/module/hadoop-2.7.2/lib/native/ 7）分发集群 ​ [kingge@hadoop102 lib]$ xsync native/ 8）再次查看hadoop支持的压缩类型 [kingge@hadoop102 hadoop-2.7.2]$ hadoop checknative 17/12/24 20:45:02 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version 17/12/24 20:45:02 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library Native library checking: hadoop: true /opt/module/hadoop-2.7.2/lib/native/libhadoop.so zlib: true /lib64/libz.so.1 snappy: true /opt/module/hadoop-2.7.2/lib/native/libsnappy.so.1 lz4: true revision:99 bzip2: false 9）重新启动hadoop集群和hive 8.6.2 测试存储和压缩官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC ORC存储方式的压缩： Key Default Notes orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries (must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must &gt;0.0 and &lt;1.0) 1）创建一个非压缩的的ORC存储方式 ​ （1）建表语句 create table log_orc_none( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) row format delimited fields terminated by ‘\\t’ stored as orc tblproperties (“orc.compress”=”NONE”); ​ （2）插入数据 hive (default)&gt; insert into table log_orc_none select * from log_text ; ​ （3）查看插入后数据 hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ; 7.7 M /user/hive/warehouse/log_orc_none/000000_0 2）创建一个SNAPPY压缩的ORC存储方式 ​ （1）建表语句 create table log_orc_snappy( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string ) row format delimited fields terminated by ‘\\t’ stored as orc tblproperties (“orc.compress”=”SNAPPY”); ​ （2）插入数据 hive (default)&gt; insert into table log_orc_snappy select * from log_text ; ​ （3）查看插入后数据 hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ; 3.8 M /user/hive/warehouse/log_orc_snappy/000000_0 3）上一节中默认创建的ORC存储方式，导入数据后的大小为 2.8 M /user/hive/warehouse/log_orc/000000_0 比Snappy压缩的还小。原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。 4**）存储方式和压缩总结：** ​ 在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。 九 企业级调优9.1 Fetch抓取Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。 在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。 hive.fetch.task.conversion more Expects one of [none, minimal, more]. Some select queries can be converted to single FETCH task minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins. 0. none : disable hive.fetch.task.conversion 1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns) 案例实操： ​ 1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。 hive (default)&gt; set hive.fetch.task.conversion=none; hive (default)&gt; select * from emp; hive (default)&gt; select ename from emp; hive (default)&gt; select ename from emp limit 3; ​ 2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。 hive (default)&gt; set hive.fetch.task.conversion=more; hive (default)&gt; select * from emp; hive (default)&gt; select ename from emp; hive (default)&gt; select ename from emp limit 3; 9.2 本地模式大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的（意思就是需要搭建完整的***hadoop**分布式集群*）。不过，有时Hive的*输入数据量是非常小*的**。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。 用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。 set hive.exec.mode.local.auto=true; //开启本地mr //设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M set hive.exec.mode.local.auto.inputbytes.max=50000000; //设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4 set hive.exec.mode.local.auto.input.files.max=10; 案例实操： 1）开启本地模式，并执行查询语句 hive (default)&gt; set hive.exec.mode.local.auto=true; hive (default)&gt; select * from emp cluster by deptno; Time taken: 1.328 seconds, Fetched: 14 row(s) 2）关闭本地模式，并执行查询语句 hive (default)&gt; set hive.exec.mode.local.auto=false; hive (default)&gt; select * from emp cluster by deptno; Time taken: 20.09 seconds, Fetched: 14 row(s) 9.3 表的优化（重要）9.3.1 小表、大表Join将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。 实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。 案例实操 （0）需求：测试大表JOIN小表和小表JOIN大表的效率 （1）建大表、小表和JOIN后表的语句 // 创建大表 create table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by ‘\\t’; // 创建小表 create table smalltable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by ‘\\t’; // 创建join后表的语句 create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by ‘\\t’; （2）分别向大表和小表中导入数据 hive (default)&gt; load data local inpath ‘/opt/module/datas/bigtable’ into table bigtable; hive (default)&gt;load data local inpath ‘/opt/module/datas/smalltable’ into table smalltable; （3）关闭mapjoin功能（默认是打开的-这里为了测试先把他关闭） set hive.auto.convert.join = false; （4）执行小表JOIN大表语句 insert overwrite table jointable select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url from smalltable s left join bigtable b on b.id = s.id; Time taken: 35.921 seconds （5）执行大表JOIN小表语句 insert overwrite table jointable select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url from bigtable b left join smalltable s on s.id = b.id; Time taken: 34.196 seconds 9.3.2 大表Join大表1）空KEY过滤 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下： 案例实操 （1）配置历史服务器 ​ 配置mapred-site.xml mapreduce.jobhistory.address hadoop102:10020 mapreduce.jobhistory.webapp.address hadoop102:19888 ​ 启动历史服务器 sbin/mr-jobhistory-daemon.sh start historyserver ​ 查看jobhistory http://192.168.1.102:19888/jobhistory （2）创建原始数据表、空id表、合并后数据表 // 创建原始表 create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by ‘\\t’; // 创建空id表 create table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by ‘\\t’; // 创建join后表的语句 create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by ‘\\t’; （3）分别加载原始数据和空id数据到对应表中 hive (default)&gt; load data local inpath ‘/opt/module/datas/ori’ into table ori; hive (default)&gt; load data local inpath ‘/opt/module/datas/nullid’ into table nullidtable; （4）测试不过滤空id hive (default)&gt; insert overwrite table jointable select n.* from nullidtable n left join ori o on n.id = o.id; Time taken: 42.038 seconds （5）测试过滤空id hive (default)&gt; insert overwrite table jointable select n. from (select from nullidtable where id is not null ) n left join ori o on n.id = o.id; Time taken: 31.725 seconds 2）空key转换 有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如： 案例实操： 不随机分布空**null**值： （1）设置5个reduce个数 set mapreduce.job.reduces = 5; （2）JOIN两张表 insert overwrite table jointable select n.* from nullidtable n left join ori b on n.id = b.id; 结果：可以看出来，出现了数据倾斜，某些**reducer的资源消耗远大于其他reducer**。 打开历史服务器查看 随机分布空null值 （1）设置5个reduce个数 set mapreduce.job.reduces = 5; （2）JOIN两张表 insert overwrite table jointable select n.* from nullidtable n full join ori o on case when n.id is null then concat(‘hive’, rand()) else n.id end = o.id; 结果：可以看出来，消除了数据倾斜，负载均衡**reducer**的资源消耗 9.3.3 MapJoin如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。 1）开启MapJoin参数设置： （1）设置自动选择Mapjoin set hive.auto.convert.join = true; 默认为true （2）大表小表的阈值设置（默认25M一下认为是小表）： set hive.mapjoin.smalltable.filesize=25000000; 2）MapJoin工作机制 案例实操： （1）开启Mapjoin功能 set hive.auto.convert.join = true; 默认为true （2）执行小表JOIN大表语句 insert overwrite table jointable select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url from smalltable s join bigtable b on s.id = b.id; Time taken: 24.594 seconds （3）执行大表JOIN小表语句 insert overwrite table jointable select b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url from bigtable b join smalltable s on s.id = b.id; Time taken: 24.315 seconds 9.3.4 Group By默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。 1）开启Map端聚合参数设置 ​ （1）是否在Map端进行聚合，默认为True hive.map.aggr = true （2）在Map端进行聚合操作的条目数目 hive.groupby.mapaggr.checkinterval = 100000 （3）有数据倾斜的时候进行负载均衡（默认是false） hive.groupby.skewindata = true 当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。 9.3.5 Count(Distinct) 去重统计数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换： 案例实操 ​ （1）创建一张大表 hive (default)&gt; create table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by ‘\\t’; ​ （2）加载数据 hive (default)&gt; load data local inpath ‘/opt/module/datas/bigtable’ into table bigtable; （3）设置5个reduce个数 set mapreduce.job.reduces = 5; （4）执行去重id查询 hive (default)&gt; select count(distinct id) from bigtable; Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 7.12 sec HDFS Read: 120741990 HDFS Write: 7 SUCCESS Total MapReduce CPU Time Spent: 7 seconds 120 msec OK c0 100001 Time taken: 23.607 seconds, Fetched: 1 row(s) ​ （5）采用GROUP by去重id hive (default)&gt; select count(id) from (select id from bigtable group by id) a; Stage-Stage-1: Map: 1 Reduce: 5 Cumulative CPU: 17.53 sec HDFS Read: 120752703 HDFS Write: 580 SUCCESS Stage-Stage-2: Map: 1 Reduce: 1 Cumulative CPU: 4.29 sec HDFS Read: 9409 HDFS Write: 7 SUCCESS Total MapReduce CPU Time Spent: 21 seconds 820 msec OK _c0 100001 Time taken: 50.795 seconds, Fetched: 1 row(s) 虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。 9.3.6 笛卡尔积尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。 9.3.7 行列过滤列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT 。（**select* *具体的列名，非必要时不要使用**select * 取全列***） 行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。（两个表关联，那么应该先使用***where**过滤自身的数据，然后再**join**两张表，这样数据的操作会更快***） 比如： 案例实操： ​ （1）测试先关联两张表，再用where条件过滤 hive (default)&gt; select o.id from bigtable b join ori o on o.id = b.id where o.id &lt;= 10; Time taken: 34.406 seconds, Fetched: 100 row(s) （2）通过子查询后，再关联表 hive (default)&gt; select b.id from bigtable b join (select id from ori where id &lt;= 10 ) o on b.id = o.id; Time taken: 30.058 seconds, Fetched: 100 row(s) 9.3.8 动态分区调整关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。 1）开启动态分区参数设置 （1）开启动态分区功能（默认true，开启） hive.exec.dynamic.partition=true （2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。） hive.exec.dynamic.partition.mode=nonstrict （3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。 hive.exec.max.dynamic.partitions=1000 ​ （4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。 hive.exec.max.dynamic.partitions.pernode=100 （5）整个MR Job中，最大可以创建多少个HDFS文件。 hive.exec.max.created.files=100000 （6）当有空分区生成时，是否抛出异常。一般不需要设置。 hive.error.on.empty.partition=false 2）案例实操 需求：将ori中的数据按照时间(如：20111230000008)，插入到目标表ori_partitioned_target的相应分区中。 （1）创建分区表 create table ori_partitioned(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) partitioned by (p_time bigint) row format delimited fields terminated by ‘\\t’; （2）加载数据到分区表中 hive (default)&gt; load data local inpath ‘/opt/module/datas/ds1’ into table ori_partitioned partition(p_time=’20111230000010’) ; hive (default)&gt; load data local inpath ‘/opt/module/datas/ds2’ into table ori_partitioned partition(p_time=’20111230000011’) ; （3）创建目标分区表 create table ori_partitioned_target(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) PARTITIONED BY (p_time STRING) row format delimited fields terminated by ‘\\t’; （4）设置动态分区 set hive.exec.dynamic.partition = true; set hive.exec.dynamic.partition.mode = nonstrict; set hive.exec.max.dynamic.partitions = 1000; set hive.exec.max.dynamic.partitions.pernode = 100; set hive.exec.max.created.files = 100000; set hive.error.on.empty.partition = false; hive (default)&gt; insert overwrite table ori_partitioned_target partition (p_time) select id, time, uid, keyword, url_rank, click_num, click_url, p_time from ori_partitioned; （5）查看目标分区表的分区情况 hive (default)&gt; show partitions ori_partitioned_target; 9.3.9 分桶详见6.6章。 9.3.10 分区详见4.6章。 9.4 数据倾斜9.4.1 合理设置Map数1**）通常情况下，作业会通过input的目录产生一个或者多个map任务。** 主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。 2**）是不是map**数越多越好？ 答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。 3**）是不是保证每个map处理接近128m的文件块，就高枕无忧了？** 答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。 针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数； 9.4.2 小文件进行合并在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。 set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 9.4.3 复杂文件增加Map数当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。 增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。 案例实操： （1）执行查询 hive (default)&gt; select count(*) from emp; Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1 ​ （2）设置最大切片值为100个字节 默认256M hive (default)&gt; set mapreduce.input.fileinputformat.split.maxsize=100; hive (default)&gt; select count(*) from emp; Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1 9.4.4 合理设置Reduce数1**）调整reduce**个数方法一 （1）每个Reduce处理的数据量默认是256MB hive.exec.reducers.bytes.per.reducer=256000000 ​ （2）每个任务最大的reduce数，默认为1009 hive.exec.reducers.max=1009 （3）计算reducer数的公式 N=min(参数2，总输入数据量/参数1) 2**）调整reduce**个数方法二 在hadoop的mapred-default.xml文件中修改 设置每个job的Reduce个数 set mapreduce.job.reduces = 15; 3**）reduce**个数并不是越多越好 1）过多的启动和初始化reduce也会消耗时间和资源； 2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题； 在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适； 9.5 并行执行Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。 ​ 通过设置参数hive.exec.parallel值为true（默认是false），就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。 set hive.exec.parallel=true; //打开任务并行执行 set hive.exec.parallel.thread.number=16; //同一个sql允许最大并行度，默认为8。 当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。 9.6 严格模式Hive提供了一个严格模式，可以防止用户执行那些可能意向不到的不好的影响的查询。 ​ 通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。 hive.mapred.mode strict The mode in which the Hive operations are being performed. In strict mode, some risky queries are not allowed to run. They include: Cartesian Product. No partition being picked up for a query. Comparing bigints and strings. Comparing bigints and doubles. Orderby without limit. 1）对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。 2）对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。 3）限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。 9.7 JVM重用(重要)JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。 Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。 mapreduce.job.jvm.numtasks 10 How many tasks to run per jvm. If set to -1, there is no limit. 这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。 9.8 推测执行在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。 设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置 mapreduce.map.speculative true If true, then multiple instances of some map tasks may be executed in parallel. mapreduce.reduce.speculative true If true, then multiple instances of some reduce tasks may be executed in parallel. 不过hive本身也提供了配置项来控制reduce-side的推测执行： ​ hive.mapred.reduce.tasks.speculative.execution true Whether speculative execution for reducers should be turned on. 关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。 9.9 压缩详见第8章。 9.10 执行计划（Explain）1）基本语法 EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query 2）案例实操 （1）查看下面这条语句的执行计划 hive (default)&gt; explain select * from emp; hive (default)&gt; explain select deptno, avg(sal) avg_sal from emp group by deptno; （2）查看详细执行计划 hive (default)&gt; explain extended select * from emp; hive (default)&gt; explain extended select deptno, avg(sal) avg_sal from emp group by deptno; 十 Hive实战敬请期待 十一 数据仓库11.1 什么是数据仓库数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它出于分析性报告和决策支持目的而创建。为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。 11.2 数据仓库能干什么？1）年度销售目标的指定，需要根据以往的历史报表进行决策，不能拍脑袋。 2）如何优化业务流程 ​ 例如：一个电商网站订单的完成包括：浏览、下单、支付、物流，其中物流环节可能和中通、申通、韵达等快递公司合作。快递公司每派送一个订单，都会有订单派送的确认时间，可以根据订单派送时间来分析哪个快递公司比较快捷高效，从而选择与哪些快递公司合作，剔除哪些快递公司，增加用户友好型。 11.3 数据仓库的特点1**）数据仓库的数据是面向主题的** 与传统数据库面向应用进行数据组织的特点相对应，数据仓库中的数据是面向主题进行组织的。什么是主题呢？首先，主题是一个抽象的概念，是较高层次上企业信息系统中的数据综合、归类并进行分析利用的抽象。在逻辑意义上，它是对应企业中某一宏观分析领域所涉及的分析对象。面向主题的数据组织方式，就是在较高层次上对分析对象的数据的一个完整、一致的描述，能完整、统一地刻划各个分析对象所涉及的企业的各项数据，以及数据之间的联系。所谓较高层次是相对面向应用的数据组织方式而言的，是指按照主题进行数据组织的方式具有更高的数据抽象级别。 2**）数据仓库的数据是集成的** 数据仓库的数据是从原有的分散的数据库数据抽取来的。操作型数据与DSS分析型数据之间差别甚大。第一，数据仓库的每一个主题所对应的源数据在原有的各分散数据库中有许多重复和不一致的地方，且来源于不同的联机系统的数据都和不同的应用逻辑捆绑在一起；第二，数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与综合，这一步是数据仓库建设中最关键、最复杂的一步，所要完成的工作有： （1）要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致等。 （2）进行数据综合和计算。数据仓库中的数据综合工作可以在从原有数据库抽取 数据时生成，但许多是在数据仓库内部生成的，即进入数据仓库以后进行综合生成的。 3**）数据仓库的数据是不可更新的** 数据仓库的数据主要供企业决策分析之用，所涉及的数据操作主要是数据查询，一般情况下并不进行修改操作。数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库快照的集合，以及基于这些快照进行统计、综合和重组的导出数据，而不是联机处理的数据。数据库中进行联机处理的数据经过集成输入到数据仓库中，一旦数据仓库存放的数据已经超过数据仓库的数据存储期限，这些数据将从当前的数据仓库中删去。因为数据仓库只进行数据查询操作，所以数据仓库管理系统相比数据库管理系统而言要简单得多。数据库管理系统中许多技术难点，如完整性保护、并发控制等等，在数据仓库的管理中几乎可以省去。但是由于数据仓库的查询数据量往往很大，所以就对数据查询提出了更高的要求，它要求采用各种复杂的索引技术；同时由于数据仓库面向的是商业企业的高层管理者，他们会对数据查询的界面友好性和数据表示提出更高的要求。 4**）数据仓库的数据是随时间不断变化的** 数据仓库中的数据不可更新是针对应用来说的，也就是说，数据仓库的用户进行分析处理时是不进行数据更新操作的。但并不是说，在从数据集成输入数据仓库开始到最终被删除的整个数据生存周期中，所有的数据仓库数据都是永远不变的。 数据仓库的数据是随时间的变化而不断变化的，这是数据仓库数据的第四个特征。这一特征表现在以下3方面： （1）数据仓库随时间变化不断增加新的数据内容。数据仓库系统必须不断捕捉OLTP数据库中变化的数据，追加到数据仓库中去，也就是要不断地生成OLTP数据库的快照，经统一集成后增加到数据仓库中去；但对于确实不再变化的数据库快照，如果捕捉到新的变化数据，则只生成一个新的数据库快照增加进去，而不会对原有的数据库快照进行修改。 （2）数据仓库随时间变化不断删去旧的数据内容。数据仓库的数据也有存储期限，一旦超过了这一期限，过期数据就要被删除。只是数据仓库内的数据时限要远远长于操作型环境中的数据时限。在操作型环境中一般只保存有60~90天的数据，而在数据仓库中则需要保存较长时限的数据（如5~10年），以适应DSS进行趋势分析的要求。 （3）数据仓库中包含有大量的综合数据，这些综合数据中很多跟时间有关，如数据经常按照时间段进行综合，或隔一定的时间片进行抽样等等。这些数据要随着时间的变化不断地进行重新综合。因此，数据仓库的数据特征都包含时间项，以标明数据的历史时期。 11.4 数据仓库发展历程数据仓库的发展大致经历了这样的三个过程： 1**）简单报表阶段：**这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。 2**）数据集市阶段：**这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。 3**）数据仓库阶段：**这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。 通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。 11.5 数据库与数据仓库的区别了解数据库与数据仓库的区别之前，首先掌握三个概念。数据库软件、数据库、数据仓库。 数据库软件：是一种软件，可以看得见，可以操作。用来实现数据库逻辑功能。属于物理层。 数据库：是一种逻辑概念，用来存放数据的仓库。通过数据库软件来实现。数据库由很多表组成，表是二维的，一张表里可以有很多字段。字段一字排开，对应的数据就一行一行写入表中。数据库的表，在于能够用二维表现多维关系。目前市面上流行的数据库都是二维数据库。如：Oracle、DB2、MySQL、Sybase、MS SQL Server等。 数据仓库：是数据库概念的升级。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现的存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大得多。数据仓库主要用于数据挖掘和数据分析，辅助领导做决策。 在IT的架构体系中，数据库是必须存在的。必须要有地方存放数据。比如现在的网购，淘宝，京东等等。物品的存货数量，货品的价格，用户的账户余额之类的。这些数据都是存放在后台数据库中。或者最简单理解，我们现在微博，QQ等账户的用户名和密码。在后台数据库必然有一张user表，字段起码有两个，即用户名和密码，然后我们的数据就一行一行的存在表上面。当我们登录的时候，我们填写了用户名和密码，这些数据就会被传回到后台去，去跟表上面的数据匹配，匹配成功了，你就能登录了。匹配不成功就会报错说密码错误或者没有此用户名等。这个就是数据库，数据库在生产环境就是用来干活的。凡是跟业务应用挂钩的，我们都使用数据库。 数据仓库则是BI下的其中一种技术。由于数据库是跟业务应用挂钩的，所以一个数据库不可能装下一家公司的所有数据。数据库的表设计往往是针对某一个应用进行设计的。比如刚才那个登录的功能，这张user表上就只有这两个字段，没有别的字段了。但是这张表符合应用，没有问题。但是这张表不符合分析。比如我想知道在哪个时间段，用户登录的量最多？哪个用户一年购物最多？诸如此类的指标。那就要重新设计数据库的表结构了。对于数据分析和数据挖掘，我们引入数据仓库概念。数据仓库的表结构是依照分析需求，分析维度，分析指标进行设计的。 数据库与数据仓库的区别实际讲的是OLTP与OLAP的区别。 操作型处理，叫联机事务处理OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。 分析型处理，叫联机分析处理OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。 表 操作型处理与分析型处理的比较 操作型处理 分析型处理 细节的 综合的或提炼的 实体——关系（E-R）模型 星型模型或雪花模型 存取瞬间数据 存储历史数据，不包含最近的数据 可更新的 只读、只追加 一次操作一个单元 一次操作一个集合 性能要求高，响应时间短 性能要求宽松 面向事务 面向分析 一次操作数据量小 一次操作数据量大 支持日常操作 支持决策需求 数据量小 数据量大 客户订单、库存水平和银行账户查询等 客户收益分析、市场细分等 11.6 数据仓库架构分层11.6.1 数据仓库架构数据仓库标准上可以分为四层：ODS（临时存储层）、PDW（数据仓库层）、DM（数据集市层）、APP（应用层）。 1）ODS层： 为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。一般来说ODS层的数据和源系统的数据是同构的，主要目的是简化后续数据加工处理的工作。从数据粒度上来说ODS层的数据粒度是最细的。ODS层的表通常包括两类，一个用于存储当前需要加载的数据，一个用于存储处理完后的历史数据。历史数据一般保存3-6个月后需要清除，以节省空间。但不同的项目要区别对待，如果源系统的数据量不大，可以保留更长的时间，甚至全量保存； 2）PDW层： 为数据仓库层，PDW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。这一层的数据一般是遵循数据库第三范式的，其数据粒度通常和ODS的粒度相同。在PDW层会保存BI系统中所有的历史数据，例如保存10年的数据。 3）DM层： 为数据集市层，这层数据是面向主题来组织数据的，通常是星形或雪花结构的数据。从数据粒度来说，这层的数据是轻度汇总级的数据，已经不存在明细数据了。从数据的时间跨度来说，通常是PDW层的一部分，主要的目的是为了满足用户分析的需求，而从分析的角度来说，用户通常只需要分析近几年（如近三年的数据）的即可。从数据的广度来说，仍然覆盖了所有业务数据。 4）APP层： 为应用层，这层数据是完全为了满足具体的分析需求而构建的数据，也是星形或雪花结构的数据。从数据粒度来说是高度汇总的数据。从数据的广度来说，则并不一定会覆盖所有业务数据，而是DM层数据的一个真子集，从某种意义上来说是DM层数据的一个重复。从极端情况来说，可以为每一张报表在APP层构建一个模型来支持，达到以空间换时间的目的数据仓库的标准分层只是一个建议性质的标准，实际实施时需要根据实际情况确定数据仓库的分层，不同类型的数据也可能采取不同的分层方法。 11.6.2 为什么要对数据仓库分层？1）用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据。 2）如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。 3）通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。 11.7 元数据介绍当需要了解某地企业及其提供的服务时，电话黄页的重要性就体现出来了。元数据（Metadata）类似于这样的电话黄页。 1）元数据的定义 数据仓库的元数据是关于数据仓库中数据的数据。它的作用类似于数据库管理系统的数据字典，保存了逻辑数据结构、文件、地址和索引等信息。广义上讲，在数据仓库中，元数据描述了数据仓库内数据的结构和建立方法的数据。 元数据是数据仓库管理系统的重要组成部分，元数据管理器是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。 （1）构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。 （2）用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。 （3）数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。 元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。 由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体，如图所示 2）元数据的存储方式 元数据有两种常见存储方式：一种是以数据集为基础，每一个数据集有对应的元数据文件，每一个元数据文件包含对应数据集的元数据内容；另一种存储方式是以数据库为基础，即元数据库。其中元数据文件由若干项组成，每一项表示元数据的一个要素，每条记录为数据集的元数据内容。上述存储方式各有优缺点，第一种存储方式的优点是调用数据时相应的元数据也作为一个独立的文件被传输，相对数据库有较强的独立性，在对元数据进行检索时可以利用数据库的功能实现，也可以把元数据文件调到其他数据库系统中操作；不足是如果每一数据集都对应一个元数据文档，在规模巨大的数据库中则会有大量的元数据文件，管理不方便。第二种存储方式下，元数据库中只有一个元数据文件，管理比较方便，添加或删除数据集，只要在该文件中添加或删除相应的记录项即可。在获取某数据集的元数据时，因为实际得到的只是关系表格数据的一条记录，所以要求用户系统可以接受这种特定形式的数据。因此推荐使用元数据库的方式。 元数据库用于存储元数据，因此元数据库最好选用主流的关系数据库管理系统。元数据库还包含用于操作和查询元数据的机制。建立元数据库的主要好处是提供统一的数据结构和业务规则，易于把企业内部的多个数据集市有机地集成起来。目前，一些企业倾向建立多个数据集市，而不是一个集中的数据仓库，这时可以考虑在建立数据仓库（或数据集市）之前，先建立一个用于描述数据、服务应用集成的元数据库，做好数据仓库实施的初期支持工作，对后续开发和维护有很大的帮助。元数据库保证了数据仓库数据的一致性和准确性，为企业进行数据质量管理提供基础。 3）元数据的作用 在数据仓库中，元数据的主要作用如下。 （1）描述哪些数据在数据仓库中，帮助决策分析者对数据仓库的内容定位。 （2）定义数据进入数据仓库的方式，作为数据汇总、映射和清洗的指南。 （3）记录业务事件发生而随之进行的数据抽取工作时间安排。 （4）记录并检测系统数据一致性的要求和执行情况。 （5）评估数据质量。 11.8 星型模型和雪花模型在多维分析的商业智能解决方案中，根据事实表和维度表的关系，又可将常见的模型分为星型模型和雪花型模型。在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型还是雪花型模型进行组织。 11.8.1 星型模型当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型。 星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余，如在地域维度表中，存在国家A 省B的城市C以及国家A省B的城市D两条记录，那么国家A和省B的信息分别存储了两次，即存在冗余。 11.8.2 雪花模型当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的” 层次” 区域，这些被分解的表都连接到主维度表而不是事实表。如图所示，将地域维表又分解为国家，省份，城市等维表。它的优点是：通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。 星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花型模型要高。星型结构不用考虑很多正规化的因素，设计与实现都比较简单。雪花型模型由于去除了冗余，有些统计就需要通过表的联接才能产生，所以效率不一定有星型模型高。正规化也是一种比较复杂的过程，相应的数据库结构设计、数据的 ETL、以及后期的维护都要复杂一些。因此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率。 11.8.3 星型模型和雪花模型对比星形模型和雪花模型是数据仓库中常用到的两种方式，而它们之间的对比要从四个角度来进行讨论。 1**）数据优化** 雪花模型使用的是规范化数据，也就是说数据在数据库内部是组织好的，以便消除冗余，因此它能够有效地减少数据量。通过引用完整性，其业务层级和维度都将存储在数据模型之中。 雪花模型 相比较而言，星形模型使用的是反规范化数据。在星形模型中，维度直接指的是事实表，业务层级不会通过维度之间的参照完整性来部署。 星形模型 2**）业务模型** 主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID将是Account_dimension的一个外键。 在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。 3**）性能** 第三个区别在于性能的不同。雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低。举个例子，如果你想要知道Advertiser 的详细信息，雪花模型就会请求许多信息，比如Advertiser Name、ID以及那些广告主和客户表的地址需要连接起来，然后再与事实表连接。 而星形模型的连接就少的多，在这个模型中，如果你需要上述信息，你只要将Advertiser的维度表和事实表连接即可。 4**）**ETL 雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。 星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。 总结 雪花模型使得维度分析更加容易，比如“针对特定的广告主，有哪些客户或者公司是在线的?”星形模型用来做指标分析更适合，比如“给定的一个客户他们的收入是多少?” 十二 常见错误解析1）SecureCRT 7.3出现乱码或者删除不掉数据，免安装版的SecureCRT 卸载或者用虚拟机直接操作或者换安装版的SecureCRT 2）连接不上mysql数据库 ​ （1）导错驱动包，应该把mysql-connector-java-5.1.27-bin.jar导入/opt/module/hive/lib的不是这个包。错把mysql-connector-java-5.1.27.tar.gz导入hive/lib包下。 ​ （2）修改user表中的主机名称没有都修改为%，而是修改为localhost 3）hive默认的输入格式处理是CombineHiveInputFormat，会对小文件进行合并。 hive (default)&gt; set hive.input.format; hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat 可以采用HiveInputFormat就会根据分区数输出相应的文件。 hive (default)&gt; set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; 4）不能执行mapreduce程序 ​ 可能是hadoop的yarn没开启。 5）启动mysql服务时，报MySQL server PID file could not be found! 异常。 ​ 在/var/lock/subsys/mysql路径下创建hadoop102.pid，并在文件中添加内容：4396 6）报service mysql status MySQL is not running, but lock file (/var/lock/subsys/mysql[失败])异常。 ​ 解决方案：在/var/lib/mysql 目录下创建： -rw-rw—-. 1 mysql mysql 5 12月 22 16:41 hadoop102.pid 文件，并修改权限为 777。 6）hive中文乱码 https://www.cnblogs.com/qingyunzong/p/8724155.html 十三 好的网站https://www.cnblogs.com/qingyunzong/p/8710356.html#_labelTop","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://kingge.top/tags/hive/"},{"name":"数据仓库工具","slug":"数据仓库工具","permalink":"http://kingge.top/tags/数据仓库工具/"}]},{"title":"hadoop在使用中的常用优化手段","slug":"hadoop在使用中的常用优化手段","date":"2018-05-14T13:59:59.000Z","updated":"2019-08-25T02:20:55.395Z","comments":true,"path":"2018/05/14/hadoop在使用中的常用优化手段/","link":"","permalink":"http://kingge.top/2018/05/14/hadoop在使用中的常用优化手段/","excerpt":"","text":"一、前言我们知道影响MapReduce运算的因素很多，主要是机器性能、网络、磁盘读写速度、I/O 操作等等有关。 机器的问题属于外部因素，那么下面主要是介绍关于IO操作引发的性能问题： 主要是有几个以下方面 （1）数据倾斜 - 重点 （2）map和reduce数设置不合理 （3）map运行时间太长，导致reduce等待过久 （4）小文件过多 - 重点 （5）大量的不可分块的超大文件 （6）spill次数过多 （7）merge次数过多。 ​ MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。 下面想讲解小文件的处理方式： 1.1 HDFS小文件优化​ HDFS上每个文件都要在namenode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大是的索引速度变慢。 解决方案1）Hadoop Archive: 是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了namenode的内存使用。 2）Sequence file： sequence file由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。 3）CombineFileInputFormat： CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。（之前hadoop相关的章节讲解道，可以翻翻看看） 4）开启JVM重用 对于大量小文件Job，可以开启JVM重用会减少45%运行时间。 JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他map。 具体设置：mapreduce.job.jvm.numtasks值在10-20之间。 1.2 分阶段优化数据输入阶段 （1）合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致mr运行较慢。 （2）采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。 数据传输阶段1）采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器。 2）使用SequenceFile二进制文件。 进入Map阶段 1）减少溢写（spill）次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘IO。 2）减少合并（merge）次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。 3）在map之后，不影响业务逻辑前提下，先进行combine处理，减少 I/O。 进入Reduce阶段 暂无 数据倾斜 暂无总结 常用参数哟花 暂无","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"hadoop优化","slug":"hadoop优化","permalink":"http://kingge.top/tags/hadoop优化/"}]},{"title":"hadoop大数据(十二)-数据压缩","slug":"hadoop大数据-十二-数据压缩","date":"2018-03-20T14:59:59.000Z","updated":"2019-08-01T13:41:44.559Z","comments":true,"path":"2018/03/20/hadoop大数据-十二-数据压缩/","link":"","permalink":"http://kingge.top/2018/03/20/hadoop大数据-十二-数据压缩/","excerpt":"","text":"4.1 概述压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在Hadoop下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下，I/O操作和网络数据传输要花大量的时间。还有，Shuffle与Merge过程同样也面临着巨大的I/O压力。 ​ 鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。 ​ 如果磁盘I/O和网络带宽影响了MapReduce作业性能，在任意MapReduce阶段启用压缩都可以改善端到端处理时间并减少I/O和网络流量。 压缩Mapreduce的一种优化策略：通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。 注意：压缩特性运用得当能提高性能，但运用不当也可能降低性能。 基本原则： （1）运算密集型的job，少用压缩 （2）IO密集型的job，多用压缩 4.2 MR支持的压缩编码 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFAULT 是，直接使用 DEFAULT .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFAULT .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 4.3 压缩方式选择4.3.1 Gzip压缩优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。 4.3.2 Bzip2压缩优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。 缺点：压缩/解压速度慢；不支持native。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 4.3.3 Lzo压缩优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。 4.3.4 Snappy压缩优点：高速压缩速度和合理的压缩率。 缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装； 应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。 4.4 压缩位置选择​ 压缩可以在MapReduce作用的任意阶段启用。 4.5 压缩配置参数要在Hadoop中启用压缩，可以配置如下参数： 参数 默认值 阶段 建议 io.compression.codecs （在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress（在mapred-site.xml中配置） false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec mapper输出 使用LZO或snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置） false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress. DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置） RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 4.6 压缩实战4.6.1 数据流的压缩和解压缩​ CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据。要想对正在被写入一个输出流的数据进行压缩，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流。相反，要想对从输入流读取而来的数据进行解压缩，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据。 测试一下如下压缩方式： DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec package com.kingge.mapreduce.compress;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.CompressionCodecFactory;import org.apache.hadoop.io.compress.CompressionInputStream;import org.apache.hadoop.io.compress.CompressionOutputStream;import org.apache.hadoop.util.ReflectionUtils;public class TestCompress &#123; public static void main(String[] args) throws Exception &#123; compress(\"e:/hello.txt\",\"org.apache.hadoop.io.compress.BZip2Codec\");// decompress(\"e:/hello.txt.bz2\"); &#125; // 压缩 private static void compress(String filename, String method) throws Exception &#123; // 1 获取输入流 FileInputStream fis = new FileInputStream(new File(filename)); Class codecClass = Class.forName(method); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); // 2 获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename +codec.getDefaultExtension())); CompressionOutputStream cos = codec.createOutputStream(fos); // 3 流的对拷 IOUtils.copyBytes(fis, cos, 1024*1024*5, false); // 4 关闭资源 fis.close(); cos.close(); fos.close(); &#125; // 解压缩 private static void decompress(String filename) throws FileNotFoundException, IOException &#123; // 0 校验是否能解压缩 CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration()); CompressionCodec codec = factory.getCodec(new Path(filename)); if (codec == null) &#123; System.out.println(\"cannot find codec for file \" + filename); return; &#125; // 1 获取输入流 CompressionInputStream cis = codec.createInputStream(new FileInputStream(new File(filename))); // 2 获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename + \".decoded\")); // 3 流的对拷 IOUtils.copyBytes(cis, fos, 1024*1024*5, false); // 4 关闭资源 cis.close(); fos.close(); &#125;&#125; 4.6.2 Map输出端采用压缩​ 即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置： 1）给大家提供的hadoop源码支持的压缩格式有：BZip2Codec 、DefaultCodec package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.BZip2Codec; import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); // 开启map端输出压缩 configuration.setBoolean(&quot;mapreduce.map.output.compress&quot;, true); // 设置map端输出压缩方式 configuration.setClass(&quot;mapreduce.map.output.compress.codec&quot;, BZip2Codec.class, CompressionCodec.class); Job job = Job.getInstance(configuration); job.setJarByClass(WordCountDriver.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 1 : 0); &#125;&#125; 2）Mapper保持不变 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割 String[] words = line.split(&quot; &quot;); // 3 循环写出 for(String word:words)&#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; 3）Reducer保持不变 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; // 1 汇总 for(IntWritable value:values)&#123; count += value.get(); &#125; // 2 输出 context.write(key, new IntWritable(count)); &#125;&#125; 7.10.3 Reduce输出端采用压缩基于wordcount案例处理 1）修改驱动 package com.kingge.mapreduce.compress;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.compress.BZip2Codec;import org.apache.hadoop.io.compress.DefaultCodec;import org.apache.hadoop.io.compress.GzipCodec;import org.apache.hadoop.io.compress.Lz4Codec;import org.apache.hadoop.io.compress.SnappyCodec;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); job.setJarByClass(WordCountDriver.class); job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 设置reduce端输出压缩开启 FileOutputFormat.setCompressOutput(job, true); // 设置压缩的方式 FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); // FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); // FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); boolean result = job.waitForCompletion(true); System.exit(result?1:0); &#125;&#125; 2）Mapper和Reducer保持不变（详见4.6.2）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(十一)-Mapreduce框架原理","slug":"hadoop大数据-十一-Mapreduce框架原理","date":"2018-03-18T10:59:59.000Z","updated":"2019-06-17T13:45:43.508Z","comments":true,"path":"2018/03/18/hadoop大数据-十一-Mapreduce框架原理/","link":"","permalink":"http://kingge.top/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/","excerpt":"","text":"三 MapReduce框架原理3.1 MapReduce工作流程1）流程示意图 2.Submit()方法包含在这里面– 然后接着是切片处理数据（128M为一片）。很明显图例200M的文件需要切成两片处理。分配两个map进行计算操作 3.正式提交任务到yarn上，包含一些job的相关信息。 4．MrAppMaster进行资源调度。根据片块数分配相应数量的MapTask（这里分配两个MapTask） 5.然后MapTask根据InputFormat去读取文本数据。一行一行的经过Mapper程序的map()方法进行计算操作，最后输出到分区中，并有序的存储。 6.等到所有MapTask计算完毕后。启动MrAppMaster启动相对应分区数量的reduce数量进行统计操作。最后生成多个分区对应的统计文件。输出。 2）流程详解 上面的流程是整个mapreduce最全工作流程，但是shuffle过程只是从第7步开始到第16步结束，具体shuffle过程详解，如下： 1）maptask收集我们的map()方法输出的kv对，放到内存缓冲区中 2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 3）多个溢出文件会被合并成大的溢出文件 4）在溢出过程中，及合并的过程中，都要调用partitioner进行分区和针对key进行排序 5）reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据 6）reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序） 7）合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法） 3）注意 Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。 缓冲区的大小可以通过参数调整，参数：io.sort.mb 默认100M。 3.2 InputFormat数据输入3.2.1 Job提交流程和切片源码详解1）job提交流程源码详解 waitForCompletion()submit();// 1建立连接-主要的工作是建立集群环境，以便运行Job任务。同时会根据Configuration配置信息来辨别当前job是需要在本地LocalRunner上运行还是在真实的yarn上运行。 connect(); // 1）创建提交job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交jobsubmitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2）获取jobid ，并创建job路径 JobID jobId = submitClient.getNewJobID(); // 3）拷贝jar包到集群 – 如果是在本地运行那么就不需要提交jar包，但是如果是在远程服务器上运行，那么就需要提交jar包，防止找不到copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir);// 4）计算切片，生成切片规划文件-默认是切一片，会去读取配置文件，获取自定义的最小切片数。切片数最大值也是有一个默认值，最大值是Long.MAX_VALUEwriteSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);// 5）向Stag路径写xml配置文件writeConf(conf, submitJobFile); conf.writeXml(out);// 6）提交job,返回提交状态status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); 2）FileInputFormat源码解析(input.getSplits(job)) （1）找到你数据存储的目录。 ​ （2）开始遍历处理（规划切片）目录下的每一个文件 ​ （3）遍历第一个文件ss.txt ​ a）获取文件大小fs.sizeOf(ss.txt); ​ b）计算切片大小computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M ​ c）默认情况下，切片大小=blocksize ​ d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） ​ e）将切片信息写到一个切片规划文件中 ​ f）整个切片的核心过程在getSplit()方法中完成。 ​ g）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。 ​ h）注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分。 ​ （4）提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。 23.2.2 FileInputFormat切片机制1）FileInputFormat中默认的切片机制： （1）简单地按照文件的内容长度进行切片 （2）切片大小，默认等于block大小 （3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片(他会遍历输入目录里面的文件，一个一个处理，debug查看FileInputFormat的getSplits方法可知) 比如待处理数据有两个文件： file1.txt 320M file2.txt 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下： file1.txt.split1-- 0~128file1.txt.split2-- 128~256file1.txt.split3-- 256~320file2.txt.split1-- 0~10M 2）FileInputFormat切片大小的参数配置 通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize)); 切片主要由这几个值来运算决定 mapreduce.input.fileinputformat.split.minsize=1 默认值为1 mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue 因此，默认情况下，切片大小=blocksize。 maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。 minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 3）获取切片信息API // 根据文件类型获取切片信息FileSplit inputSplit = (FileSplit) context.getInputSplit();// 获取切片的文件名称String name = inputSplit.getPath().getName(); 3.2.3 CombineTextInputFormat切片机制1）关于大量小文件的优化策略1）默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。 2）优化策略​ （1）最好的办法，在数据处理系统的最前端（预处理/采集），将小文件先合并成大文件，再上传到HDFS做后续分析。 ​ （2）补救措施：如果已经是大量小文件在HDFS中了，可以使用另一种InputFormat来做切片（CombineTextInputFormat），它的切片逻辑跟TextFileInputFormat不同：它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个maptask。 ​ （3）优先满足最小切片大小，不超过最大切片大小 ​ CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m ​ CombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m ​ 举例：0.5m+1m+0.3m+5m=2m + 4.8m=2m + 4m + 0.8m ​ 0.5+1+0.3 = 1.8没有满足最小切片大小，所以向5借0.2M,最后合并成2+4.8，但是4.8大于最大切片数，所以拆成4+0.8 ，所以这个四个小文件最后合并成三个文件 3）具体实现步骤注意CombineTextInputFormat的jar包是： // 如果不设置InputFormat,它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class)CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4mCombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m 4）案例​ 大量小文件的切片优化（CombineTextInputFormat）。 4.1 数据准备准备5个小文件（这里准备五个txt文本） 4.2 我们依旧使用我们上一个章节使用的统计文本中单词出现个数的代码代码详见 《hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节案例》 先不进行任何的改造操作，直接用着五个小文件当做输入，运行后查看日志。 （1）不做任何处理，运行需求1中的wordcount程序，观察切片个数为5 （2）在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为1 // 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4mCombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m 3.2.4 InputFormat接口实现类MapReduce任务的输入文件一般是存储在HDFS里面。输入的文件格式包括：基于行的日志文件、二进制格式文件等。这些文件一般会很大，达到数十GB，甚至更大。那么MapReduce是如何读取这些数据的呢？下面我们首先学习InputFormat接口。 InputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。 1）TextInputFormat TextInputFormat是默认的InputFormat。每条记录是一行输入。键是LongWritable类型，存储该行在整个文件中的字节偏移量。值是这行的内容，不包括任何行终止符（换行符和回车符）。 以下是一个示例，比如，一个分片包含了如下4条文本记录。 Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 每条记录表示为以下键/值对： (0,Rich learning form)(19,Intelligent learning engine)(47,Learning more convenient)(72,From the real demand for more close to the enterprise) 很明显，键并不是行号。一般情况下，很难取得行号，因为文件按字节而不是按行切分为分片。 2）KeyValueTextInputFormat 每一行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “ “);来设定分隔符。默认分隔符是tab（\\t）。 以下是一个示例，输入是一个包含4条记录的分片。其中——&gt;表示一个（水平方向的）制表符。 line1 ——&gt;Rich learning formline2 ——&gt;Intelligent learning engineline3 ——&gt;Learning more convenientline4 ——&gt;From the real demand for more close to the enterprise 每条记录表示为以下键/值对： (line1,Rich learning form)(line2,Intelligent learning engine)(line3,Learning more convenient)(line4,From the real demand for more close to the enterprise) 此时的键是每行排在制表符之前的Text序列。 3）NLineInputFormat 如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数/N=切片数，如果不整除，切片数=商+1。 以下是一个示例，仍然以上面的4行输入为例。 Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 例如，如果N是2，则每个输入分片包含两行。开启2个maptask。 (0,Rich learning form) (19,Intelligent learning engine) 另一个 mapper 则收到后两行： (47,Learning more convenient) (72,From the real demand for more close to the enterprise) ​ 这里的键和值与TextInputFormat生成的一样。 3.2.5 自定义InputFormat1）概述（1）自定义一个类继承FileInputFormat。 （2）改写RecordReader，实现一次读取一个完整文件封装为KV。 （3）在输出时使用SequenceFileOutPutFormat输出合并文件。 2）案例​ 无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。将多个小文件合并成一个文件SequenceFile，SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。 小文件的优化无非以下几种方式： （1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS （2）在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并 （3）在mapreduce处理时，可采用CombineTextInputFormat提高效率 2.1 数据准备准备三个文本文件。 aa.txt:包含以下内容yongpeng weidong weinansanfeng luozong xiaomingbb.txt:包含以下内容longlong fanfanmazong kailun yuhang yixinlonglong fanfanmazong kailun yuhang yixincc.txt:包含以下内容shuaige changmo zhenqiang dongli lingu xuanxuan 最终预期文件格式： part-r-00000 2.2 代码实现使用自定义InputFormat的方式，处理输入小文件的问题。 （1）自定义一个类继承FileInputFormat （2）改写RecordReader，实现一次读取一个完整文件封装为KV （3）在输出时使用SequenceFileOutPutFormat输出合并文件 （1）自定义InputFromatpackage com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;// 定义类继承FileInputFormatpublic class WholeFileInputformat extends FileInputFormat&lt;NullWritable, BytesWritable&gt;&#123; @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; （2）自定义RecordReader package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class WholeRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt;&#123; private Configuration configuration; private FileSplit split; private boolean processed = false; private BytesWritable value = new BytesWritable(); @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.split = (FileSplit)split; configuration = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; // 1 定义缓存区 byte[] contents = new byte[(int)split.getLength()]; FileSystem fs = null; FSDataInputStream fis = null; try &#123; // 2 获取文件系统 Path path = split.getPath(); fs = path.getFileSystem(configuration); // 3 读取数据 fis = fs.open(path); // 4 读取文件内容 IOUtils.readFully(fis, contents, 0, contents.length); // 5 输出文件内容 value.set(contents, 0, contents.length); &#125; catch (Exception e) &#123; &#125;finally &#123; IOUtils.closeStream(fis); &#125; processed = true; return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return processed? 1:0; &#125; @Override public void close() throws IOException &#123; &#125;&#125; （3）SequenceFileMapper处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class SequenceFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;&#123; Text k = new Text(); @Override protected void setup(Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 1 获取文件切片信息 FileSplit inputSplit = (FileSplit) context.getInputSplit(); // 2 获取切片名称 String name = inputSplit.getPath().toString(); // 3 设置key的输出 k.set(name); &#125; @Override protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(k, value); &#125;&#125; （4）SequenceFileReducer处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, values.iterator().next()); &#125;&#125; （5）SequenceFileDriver处理流程 package com.kingge.mapreduce.inputformat;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;public class SequenceFileDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[] &#123; &quot;e:/input/inputinputformat&quot;, &quot;e:/output1&quot; &#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SequenceFileDriver.class); job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 设置输入的inputFormat job.setInputFormatClass(WholeFileInputformat.class); // 设置输出的outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.3 MapTask工作机制3.3.1 并行度决定机制1）问题引出 maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度。那么，mapTask并行任务是否越多越好呢？ 2）MapTask并行度决定机制 ​ 一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。 3.3.2 MapTask工作机制 ​ （1）Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。 ​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。 ​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner—调用用户自定义getPartition方法），并写入一个环形内存缓冲区中。 ​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 ​ 溢写阶段详情： ​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 ​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 ​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 ​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 ​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 ​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 https://blog.csdn.net/qq_41455420/article/details/79288764 好的总结： 3.4 Shuffle机制3.4.1 Shuffle机制Mapreduce确保每个reducer的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reducer）称为shuffle。 3.4.2 Partition分区 分区的行为在每一次的map操作都会调用一或者多次 0）问题引出：要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区） 默认只输出到一个分区，也就是结果输出到一个文件 1）默认partition分区 public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; ​ 默认分区是根据key的hashCode对reduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。（numReduceTasks默认是1，也就是说，默认返回0，也就是只创建一个分区，所以是part-r-00000） 2）自定义Partitioner步骤 ​ （1）自定义类继承Partitioner，重写getPartition()方法 public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123;// 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; ​ （2）在job驱动中，设置自定义partitioner： ​ job.setPartitionerClass(CustomPartitioner.class); ​ （3）自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce task ​ job.setNumReduceTasks(5); 3）注意： 如果reduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx； 如果1&lt;reduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception； 如果reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000； ​ 例如：假设自定义分区数为5，则 （1）job.setNumReduceTasks(1);会正常运行，只不过会产生一个输出文件 （2）job.setNumReduceTasks(2);会报错 （3）job.setNumReduceTasks(6);大于5，程序会正常运行，会产生空文件 4）案例4.1 案例1​ 将统计结果按照手机归属地不同省份输出到不同文件中（分区） 1）数据准备 phone.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 2）分析 （1）Mapreduce中会将map输出的kv对，按照相同key分组，然后分发给不同的reducetask。默认的分发规则为：根据key的hashcode%reducetask数来分发 （2）如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner 自定义一个CustomPartitioner继承抽象类：Partitioner （3）在job驱动中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class) 3）在&lt;hadoop大数据(十)-Mapreduce基础 章节的2.6.2 案例&gt;的基础上，增加一个分区类 package com.kingge.mapreduce.flowsum;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;//他的key和value就是map输出的kvpublic class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 在驱动函数中增加自定义数据分区设置和reduce task设置 package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 8 指定自定义数据分区 job.setPartitionerClass(ProvincePartitioner.class); // 9 同时指定相应数量的reduce task job.setNumReduceTasks(5); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 4.2 案例2​ 把单词按照ASCII码奇偶分区（Partitioner），结合&lt;hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节–统计一堆文件中单词出现的个数&gt; 只需要在此代码的基础上，添加自定义分区 package com.kingge.mapreduce.wordcount;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class WordCountPartitioner extends Partitioner&lt;Text, IntWritable&gt;&#123; @Override public int getPartition(Text key, IntWritable value, int numPartitions) &#123; // 1 获取单词key String firWord = key.toString().substring(0, 1); char[] charArray = firWord.toCharArray(); int result = charArray[0]; // int result = key.toString().charAt(0); // 2 根据奇数偶数分区 if (result % 2 == 0) &#123; return 0; &#125;else &#123; return 1; &#125; &#125;&#125; 在驱动类中配置加载分区，设置reducetask个数 job.setPartitionerClass(WordCountPartitioner.class);job.setNumReduceTasks(2);//想分多少个区，这里必须开多少个reduce，否则默认只会生成一个分区，那么自定义分区失效 5）总结l 结果输出文件，跟分区数量和reduce数量有关系 l getPartition方法是在map调用之后才会进入，而且是每一次map可能会调用多次getPartition。为什么说是多次调用分区方法呢？我们知道每一次进入map方法都是一行数据（例如 hello.txt的第一行hello kingge），那么经过分割后生成两个单词，调用两次**context.write（）所以为了确定这两个单词所属那个分区，那么就需要调用两次getPartition。也就说在这个例子中，一次map调用处理完后需要调用两次getPartition。（即：context.write（）内部会进行分区） l 如果job.setNumReduceTasks(1)（也就是保持默认值），那么就是生成一个分区，不会进入自定义的分区方法。Redeucetask必须大于1，自定义分区方法才会生效。 3.4.3 WritableComparable排序排序是MapReduce框架中最重要的操作之一。Map Task和Reduce Task均会对数据（按照key）进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 ​ 对于Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行一次合并，以将这些文件合并成一个大的有序文件。 ​ 对于Reduce Task，它从每个Map Task上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上，否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task统一对内存和磁盘上的所有数据进行一次合并。 每个阶段的默认排序 1）排序的分类：​ （1）部分排序： MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。例如输出文件到五个分区，那么部分排序能够保证各个五个分区的数据都是有序的。 ​ （2）全排序： 如何用Hadoop产生一个全局排序的文件？最简单的方法是使用一个分区，那么这个分区里面的数据全局都是排序的。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。 ​ 替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为上述文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。这种方式可以达到全排序的功能 （3）辅助排序：（GroupingComparator分组） ​ Mapreduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。 ​ （4）二次排序： ​ 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 2）自定义排序WritableComparable（1）原理分析 bean对象实现WritableComparable接口重写compareTo方法，就可以实现排序 @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 3）案例3.1 案例1在&lt;hadoop大数据(十)-Mapreduce基础 章节的2.6.2 案例&gt;输出结果的基础上增加一个新的需求 根据2.6.2 案例输出的结果：再次对总流量进行排序 1）数据准备 phone.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 2）分析 ​ （1）把程序分两步走，第一步正常统计总流量，第二步再把结果进行排序 ​ （2）context.write(总流量，手机号) ​ （3）FlowBean实现WritableComparable接口重写compareTo方法 @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 3）代码实现 （1）FlowBean对象在在需求2.6.2基础上增加了比较功能（compareTo） package com.kingge.mapreduce.sort;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; private long upFlow; private long downFlow; private long sumFlow; // 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public void set(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; /** * 序列化方法 * @param out * @throws IOException */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; /** * 反序列化方法 注意反序列化的顺序和序列化的顺序完全一致 * @param in * @throws IOException */ @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong(); &#125; @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; &#125; @Override public int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1; &#125;&#125; （2）编写mapper package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;&#123; FlowBean bean = new FlowBean(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 String phoneNbr = fields[0]; long upFlow = Long.parseLong(fields[1]); long downFlow = Long.parseLong(fields[2]); bean.set(upFlow, downFlow); v.set(phoneNbr); // 4 输出 context.write(bean, v); &#125;&#125; （3）编写reducer package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt;&#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; // 循环输出，避免总流量相同情况 for (Text text : values) &#123; context.write(text, key); &#125; &#125;&#125; （4）编写driver package com.kingge.mapreduce.sort;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowCountSortDriver &#123; public static void main(String[] args) throws ClassNotFoundException, IOException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCountSortDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.2 案例2改造案例1的需求 ​ 要求每个省份手机号输出的文件中按照总流量内部排序。（部分排序） 2）做法 在案例1的基础上增加自定义分区类即可。 package com.kingge.mapreduce.sort;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; &#123; @Override public int getPartition(FlowBean key, Text value, int numPartitions) &#123; // 1 获取手机号码前三位 String preNum = value.toString().substring(0, 3); int partition = 4; // 2 根据手机号归属地设置分区 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; （2）在驱动类中添加分区类 // 加载自定义分区类job.setPartitionerClass(FlowSortPartitioner.class);// 设置Reducetask个数 job.setNumReduceTasks(5); 3.4.4 GroupingComparator分组（辅助排序）1）对reduce阶段的数据根据某一个或几个字段进行分组。 2）案例 ​ 求出每一个订单中最贵的商品（GroupingComparator） 1）需求 有如下订单数据 订单id 商品id 成交金额 0000001 Pdt_01 222.8 0000001 Pdt_06 25.8 0000002 Pdt_03 522.8 0000002 Pdt_04 122.4 0000002 Pdt_05 722.4 0000003 Pdt_01 222.8 0000003 Pdt_02 33.8 现在需要求出每一个订单中最贵的商品。 2）输入数据 goods.txt 0000001 Pdt_01 222.80000002 Pdt_06 722.40000001 Pdt_05 25.80000003 Pdt_01 222.80000003 Pdt_01 33.80000002 Pdt_03 522.80000002 Pdt_04 122.4 输出数据预期： ​ 3 222.8 2 722.4 1 222.8 3）分析 （1）利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce。 （2）在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值。 4）代码实现 （1）定义订单信息OrderBean package com.kingge.mapreduce.order;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int order_id; // 订单id号 private double price; // 价格 public OrderBean() &#123; super(); &#125; public OrderBean(int order_id, double price) &#123; super(); this.order_id = order_id; this.price = price; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; order_id = in.readInt(); price = in.readDouble(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + price; &#125; public int getOrder_id() &#123; return order_id; &#125; public void setOrder_id(int order_id) &#123; this.order_id = order_id; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; // 二次排序 @Override public int compareTo(OrderBean o) &#123; int result; if (order_id &gt; o.getOrder_id()) &#123; result = 1; &#125; else if (order_id &lt; o.getOrder_id()) &#123; result = -1; &#125; else &#123; // 价格倒序排序 result = price &gt; o.getPrice() ? -1 : 1; &#125; return result; &#125;&#125; （2）编写OrderSortMapper package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; OrderBean k = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 k.setOrder_id(Integer.parseInt(fields[0])); k.setPrice(Double.parseDouble(fields[2])); // 4 写出 context.write(k, NullWritable.get()); &#125;&#125; （3）编写OrderSortPartitioner package com.kingge.mapreduce.order;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Partitioner;public class OrderPartitioner extends Partitioner&lt;OrderBean, NullWritable&gt; &#123; @Override public int getPartition(OrderBean key, NullWritable value, int numReduceTasks) &#123; return (key.getOrder_id() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; （4）编写OrderSortGroupingComparator package com.kingge.mapreduce.order;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; //可以查看super的源代码，true是必须要传的，否则汇报空指针，因为我们在下面的compare方法中使用了强转的操作，那么如果不注明比较的bean的类型，那么就会有问题。 super(OrderBean.class, true); &#125; @SuppressWarnings(&quot;rawtypes&quot;) @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123; result = -1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125; （5）编写OrderSortReducer package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125;&#125; （6）编写OrderSortDriver package com.kingge.mapreduce.order;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class OrderDriver &#123; public static void main(String[] args) throws Exception, IOException &#123; // 1 获取配置信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 设置jar包加载路径 job.setJarByClass(OrderDriver.class); // 3 加载map/reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4 设置map输出数据key和value类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5 设置最终输出数据的key和value类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); // 6 设置输入数据和输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 10 设置reduce端的分组 job.setGroupingComparatorClass(OrderGroupingComparator.class); // 7 设置分区 job.setPartitionerClass(OrderPartitioner.class); // 8 设置reduce个数 job.setNumReduceTasks(3); // 9 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;//如果不使用GroupingComparator方法，那么就无法实现功能，因为我们知道进入reduce的数据，他们key一定是一样的。那么上面的OrderBean作为key很明显是不一样的，就算order_id相同，但是他们的price不相同。那么GroupingComparator就可以帮我们做到，假设某个值是相同的，那么他就认为整个key是相同的。那么OrderBean作为key就可以分组处理也就是说，我们通过在GroupingComparator方法中指明了，相同key的规则，那么就可以实现进入reduce的数据的分组情况尖叫提示： Map阶段结束后，马上进入GroupingComparator方法，进行判断key的逻辑。每判断一次完后，就调用reduce一次。循环此操作直到数据统计结束。 在进入GroupingComparator之前，map阶段输出的数据，已经按照订单分区，分区内的价格也已经按照大到小排序。 3.4.5 Combiner合并1）combiner是MR程序中Mapper和Reducer之外的一种组件。 2）combiner组件的父类就是Reducer。 3）combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行; Reducer是接收全局所有Mapper的输出结果； 4）combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。 5）combiner能够应用的前提是不能影响最终的业务逻辑，而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来。 Mapper3 5 7 -&gt;(3+5+7)/3=5 2 6 -&gt;(2+6)/2=4Reducer(3+5+7+2+6)/5=23/5 不等于 (5+4)/2=9/2 很明显，combiner不适合做求平均值这样的操作。他适合做汇总这样的业务场景。 6）自定义Combiner实现步骤： （1）自定义一个combiner继承Reducer，重写reduce方法 public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1 汇总操作 int count = 0; for(IntWritable v :values)&#123; count = v.get(); &#125; // 2 写出 context.write(key, new IntWritable(count)); &#125;&#125; （2）在job驱动类中设置： job.setCombinerClass(WordcountCombiner.class); 7）案例 ​ 前提：结合&lt;hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节–统计一堆文件中单词出现的个数&gt; 代码 数据输入也是同上 ​ 需求：统计过程中对每一个maptask的输出进行局部汇总，以减小网络传输量即采用Combiner功能。 方案一 1）增加一个WordcountCombiner类继承Reducer package com.kingge.mr.combiner;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1 汇总 int count = 0; for(IntWritable v :values)&#123; count += v.get(); &#125; // 2 写出 context.write(key, new IntWritable(count)); &#125;&#125; // 9 指定需要使用combiner，以及用哪个类作为combiner的逻辑 job.setCombinerClass(WordcountCombiner.class); // 9 指定需要使用combiner，以及用哪个类作为combiner的逻辑job.setCombinerClass(WordcountCombiner.class); 方案二 1）将WordcountReducer作为combiner在WordcountDriver驱动类中指定 // 指定需要使用combiner，以及用哪个类作为combiner的逻辑job.setCombinerClass(WordcountReducer.class); 运行程序 总结自定义Combiner的调用时机：是在MapTask阶段的split溢写阶段，需要写入到磁盘的之前进行。将有相同 key 的 key/value 对的 value 加起来，减少溢写到磁盘的数据量。调用完后进入**reduce**方法 ​ 3.5 ReduceTask工作机制1）设置ReduceTask并行度（个数） reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置： //默认值是1，手动设置为4 job.setNumReduceTasks(4); 2）注意 （1）reducetask=0 ，表示没有reduce阶段，输出文件个数和map个数一致。 ​ 例子7.1.1 job.setNumReduceTasks(0); 输出 ​ 生成一个分区，但是分区内的单词没有汇总 ​ （2）reducetask默认值就是1，所以输出文件个数为一个。 （3）如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜（也就是说，相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况） （4）reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask。 （5）具体多少个reducetask，需要根据集群性能而定。 （6）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在maptask的源码中，执行分区的前提是先判断reduceNum个数是否大于1。不大于1肯定不执行。 3）实验：测试reducetask多少合适。 （1）实验环境：1个master节点，16个slave节点：CPU:8GHZ，内存: 2G （2）实验结论： ​ 表1 改变reduce task （数据量为1GB） Map task =16 Reduce task 1 5 10 15 16 20 25 30 45 60 总时间 892 146 110 92 88 100 128 101 145 104 4）ReduceTask工作机制 ​ （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 ​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 ​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。 ​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 3.6 OutputFormat数据输出3.6.1 OutputFormat接口实现类 OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。 1）文本输出TextOutputFormat ​ 默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串。 2）SequenceFileOutputFormat SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。 3）自定义OutputFormat ​ 根据用户需求，自定义实现输出。 3.6.2 自定义OutputFormat为了实现控制最终文件的输出路径，可以自定义OutputFormat。 要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现。 1）自定义OutputFormat步骤（1）自定义一个类继承FileOutputFormat。 （2）改写recordwriter，具体改写输出数据的方法write()。 2）案例​ 修改日志内容及自定义日志输出路径（自定义OutputFormat）。 1）需求 ​ 过滤输入的log日志中是否包含kingge ​ （1）包含kingge的网站输出到e:/kingge.log ​ （2）不包含kingge的网站输出到e:/other.log 2）输入数据（pp.txt） http://www.baidu.comhttp://www.google.comhttp://cn.bing.comhttp://www.kingge.comhttp://www.sohu.comhttp://www.sina.comhttp://www.sin2a.comhttp://www.sin2desa.comhttp://www.sindsafa.com 输出预期： kingge.log文件包含： http://www.kingge.com other.log文件包含： http://cn.bing.comhttp://www.baidu.comhttp://www.google.comhttp://www.sin2a.comhttp://www.sin2desa.comhttp://www.sina.comhttp://www.sindsafa.comhttp://www.sohu.com 3）代码实现： （1）自定义一个outputformat package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; // 创建一个RecordWriter return new FilterRecordWriter(job); &#125;&#125; （2）具体的写数据RecordWriter package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream kinggeOut = null; FSDataOutputStream otherOut = null; public FilterRecordWriter(TaskAttemptContext job) &#123; // 1 获取文件系统 FileSystem fs; try &#123; fs = FileSystem.get(job.getConfiguration()); // 2 创建输出文件路径 Path kinggePath = new Path(&quot;e:/kingge.log&quot;); Path otherPath = new Path(&quot;e:/other.log&quot;); // 3 创建输出流 kinggeOut = fs.create(kinggePath); otherOut = fs.create(otherPath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断是否包含“kingge”输出到不同文件 if (key.toString().contains(&quot;kingge&quot;)) &#123; kinggeOut.write(key.toString().getBytes()); &#125; else &#123; otherOut.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; // 关闭资源 if (kinggeOut != null) &#123; kinggeOut.close(); &#125; if (otherOut != null) &#123; otherOut.close(); &#125; &#125;&#125; （3）编写FilterMapper package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FilterMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); k.set(line); // 3 写出 context.write(k, NullWritable.get()); &#125;&#125; （4）编写FilterReducer package com.kingge.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FilterReducer extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; String k = key.toString(); k = k + &quot;\\r\\n&quot;; context.write(new Text(k), NullWritable.get()); &#125;&#125; （5）编写FilterDriver package com.kingge.mapreduce.outputformat;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FilterDriver &#123; public static void main(String[] args) throws Exception &#123;args = new String[] &#123; &quot;e:/input/inputoutputformat&quot;, &quot;e:/output2&quot; &#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FilterDriver.class); job.setMapperClass(FilterMapper.class); job.setReducerClass(FilterReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要将自定义的输出格式组件设置到job中 job.setOutputFormatClass(FilterOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); // 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat // 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.7 Join多种应用3.7.1 Reduce join1）原理： Map端的主要工作：为来自不同表(文件)的key/value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。 Reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行合并就ok了。 2）该方法的缺点 这种方式的缺点很明显就是会造成map和reduce端也就是shuffle阶段出现大量的数据传输，效率很低。 3）案例​ reduce端表合并（数据倾斜） 通过将关联条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reducetask，在reduce中进行数据的串联。 1）代码实现 ​ 1.1 创建商品和订合并后的bean类 package com.kingge.mapreduce.table;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class TableBean implements Writable &#123; private String order_id; // 订单id private String p_id; // 产品id private int amount; // 产品数量 private String pname; // 产品名称 private String flag;// 表的标记 public TableBean() &#123; super(); &#125; public TableBean(String order_id, String p_id, int amount, String pname, String flag) &#123; super(); this.order_id = order_id; this.p_id = p_id; this.amount = amount; this.pname = pname; this.flag = flag; &#125; public String getFlag() &#123; return flag; &#125; public void setFlag(String flag) &#123; this.flag = flag; &#125; public String getOrder_id() &#123; return order_id; &#125; public void setOrder_id(String order_id) &#123; this.order_id = order_id; &#125; public String getP_id() &#123; return p_id; &#125; public void setP_id(String p_id) &#123; this.p_id = p_id; &#125; public int getAmount() &#123; return amount; &#125; public void setAmount(int amount) &#123; this.amount = amount; &#125; public String getPname() &#123; return pname; &#125; public void setPname(String pname) &#123; this.pname = pname; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(order_id); out.writeUTF(p_id); out.writeInt(amount); out.writeUTF(pname); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.order_id = in.readUTF(); this.p_id = in.readUTF(); this.amount = in.readInt(); this.pname = in.readUTF(); this.flag = in.readUTF(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + pname + &quot;\\t&quot; + amount + &quot;\\t&quot; ; &#125;&#125; 2）编写TableMapper程序 package com.kingge.mapreduce.table;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt;&#123; TableBean bean = new TableBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取输入文件类型 FileSplit split = (FileSplit) context.getInputSplit(); String name = split.getPath().getName(); // 2 获取输入数据 String line = value.toString(); // 3 不同文件分别处理 if (name.startsWith(&quot;order&quot;)) &#123;// 订单表处理 // 3.1 切割 String[] fields = line.split(&quot;\\t&quot;); // 3.2 封装bean对象 bean.setOrder_id(fields[0]); bean.setP_id(fields[1]); bean.setAmount(Integer.parseInt(fields[2])); bean.setPname(&quot;&quot;); bean.setFlag(&quot;0&quot;); k.set(fields[1]); &#125;else &#123;// 产品表处理 // 3.3 切割 String[] fields = line.split(&quot;\\t&quot;); // 3.4 封装bean对象 bean.setP_id(fields[0]); bean.setPname(fields[1]); bean.setFlag(&quot;1&quot;); bean.setAmount(0); bean.setOrder_id(&quot;&quot;); k.set(fields[0]); &#125; // 4 写出 context.write(k, bean); &#125;&#125; 3）编写TableReducer程序 package com.kingge.mapreduce.table;import java.io.IOException;import java.util.ArrayList;import org.apache.commons.beanutils.BeanUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 1准备存储订单的集合 ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;(); // 2 准备bean对象 TableBean pdBean = new TableBean(); for (TableBean bean : values) &#123; if (&quot;0&quot;.equals(bean.getFlag())) &#123;// 订单表 // 拷贝传递过来的每条订单数据到集合中 TableBean orderBean = new TableBean(); try &#123; BeanUtils.copyProperties(orderBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; orderBeans.add(orderBean); &#125; else &#123;// 产品表 try &#123; // 拷贝传递过来的产品表到内存中 BeanUtils.copyProperties(pdBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 3 表的拼接 for(TableBean bean:orderBeans)&#123; bean.setPname (pdBean.getPname()); // 4 数据写出去 context.write(bean, NullWritable.get()); &#125; &#125;&#125; 4）编写TableDriver程序 package com.kingge.mapreduce.table;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 指定本程序的jar包所在的本地路径 job.setJarByClass(TableDriver.class); // 3 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 4 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 5 指定最终输出的数据的kv类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 6 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3）运行程序查看结果 1001 小米 1 1001 小米 1 1002 华为 2 1002 华为 2 1003 格力 3 1003 格力 3 缺点：这种方式中，合并的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜 解决方案： map端实现数据合并 3.7.2 Map join（Distributedcache分布式缓存）1）使用场景：一张表十分小、一张表很大。 2）解决方案 在map端缓存多张表，提前处理业务逻辑，这样增加map端业务，减少reduce端数据的压力，尽可能的减少数据倾斜。 3）具体办法：采用distributedcache ​ （1）在mapper的setup阶段，将文件读取到缓存集合中。 ​ （2）在驱动函数中加载缓存。 job.addCacheFile(new URI(“file:/e:/mapjoincache/pd.txt”));// 缓存普通文件到task运行节点 4）案例：​ map端表合并（Distributedcache） - 结合上个案例代码（3.7.1 3 案例） 1）分析 适用于关联表中有小表的情形； 可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行合并并输出最终结果，可以大大提高合并操作的并发度，加快处理速度。 2）实操案例 （1）先在驱动模块中添加缓存文件 package test;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class DistributedCacheDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取job信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置加载jar包路径 job.setJarByClass(DistributedCacheDriver.class); // 3 关联map job.setMapperClass(DistributedCacheMapper.class); // 4 设置最终输出数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5 设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 加载缓存数据 job.addCacheFile(new URI(&quot;file:///e:/inputcache/pd.txt&quot;)); // 7 map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); // 8 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; （2）读取缓存的文件数据 package test;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(); @Override protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 1 获取缓存的文件 BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;pd.txt&quot;),&quot;UTF-8&quot;)); String line; while(StringUtils.isNotEmpty(line = reader.readLine()))&#123; // 2 切割 String[] fields = line.split(&quot;\\t&quot;); // 3 缓存数据到集合 pdMap.put(fields[0], fields[1]); &#125; // 4 关流 reader.close(); &#125; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 获取产品id String pId = fields[1]; // 4 获取商品名称 String pdName = pdMap.get(pId); // 5 拼接 k.set(line + &quot;\\t&quot;+ pdName); // 6 写出 context.write(k, NullWritable.get()); &#125;&#125; 3.8 数据清洗（ETL）1）概述 在运行核心业务Mapreduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行mapper程序，不需要运行reduce程序。 2）案例日志清洗（数据清洗）。 简单解析版1）需求： 去除日志中字段长度小于等于11的日志。 2）输入数据 里面的内容就是我们平时网站输出的日志。例如： 194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;101.226.68.137 - - [18/Sep/2013:06:49:45 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;60.208.6.156 - - [18/Sep/2013:06:49:48 +0000] &quot;GET /wp-content/uploads/2013/07/rcassandra.png HTTP/1.0&quot; 200 185524 &quot;http://cos.name/category/software/packages/&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] &quot;GET /images/my.jpg HTTP/1.1&quot; 200 19939 &quot;http://www.angularjs.cn/A00n&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;222.68.172.190 - - [18/Sep/2013:06:50:08 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot; 3）实现代码： （1）编写LogMapper package com.kingge.mapreduce.weblog;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取1行数据 String line = value.toString(); // 2 解析日志 boolean result = parseLog(line,context); // 3 日志不合法退出 if (!result) &#123; return; &#125; // 4 设置key k.set(line); // 5 写出数据 context.write(k, NullWritable.get()); &#125; // 2 解析日志 private boolean parseLog(String line, Context context) &#123; // 1 截取 String[] fields = line.split(&quot; &quot;); // 2 日志长度大于11的为合法 if (fields.length &gt; 11) &#123; // 系统计数器 context.getCounter(&quot;map&quot;, &quot;true&quot;).increment(1); return true; &#125;else &#123; context.getCounter(&quot;map&quot;, &quot;false&quot;).increment(1); return false; &#125; &#125;&#125; （2）编写LogDriver package com.kingge.mapreduce.weblog;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class LogDriver &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; &quot;e:/input/inputlog&quot;, &quot;e:/output1&quot; &#125;; // 1 获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 加载jar包 job.setJarByClass(LogDriver.class); // 3 关联map job.setMapperClass(LogMapper.class); // 4 设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置reducetask个数为0 job.setNumReduceTasks(0); // 5 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 提交 job.waitForCompletion(true); &#125;&#125; 复杂解析版1）需求： 对web访问日志中的各字段识别切分 去除日志中不合法的记录 根据统计需求，生成各类访问请求过滤数据 2）输入数据 输入同上一个案例 3）实现代码： （1）定义一个bean，用来记录日志数据中的各数据字段 package com.kingge.mapreduce.log;public class LogBean &#123; private String remote_addr;// 记录客户端的ip地址 private String remote_user;// 记录客户端用户名称,忽略属性&quot;-&quot; private String time_local;// 记录访问时间与时区 private String request;// 记录请求的url与http协议 private String status;// 记录请求状态；成功是200 private String body_bytes_sent;// 记录发送给客户端文件主体内容大小 private String http_referer;// 用来记录从那个页面链接访问过来的 private String http_user_agent;// 记录客户浏览器的相关信息 private boolean valid = true;// 判断数据是否合法 public String getRemote_addr() &#123; return remote_addr; &#125; public void setRemote_addr(String remote_addr) &#123; this.remote_addr = remote_addr; &#125; public String getRemote_user() &#123; return remote_user; &#125; public void setRemote_user(String remote_user) &#123; this.remote_user = remote_user; &#125; public String getTime_local() &#123; return time_local; &#125; public void setTime_local(String time_local) &#123; this.time_local = time_local; &#125; public String getRequest() &#123; return request; &#125; public void setRequest(String request) &#123; this.request = request; &#125; public String getStatus() &#123; return status; &#125; public void setStatus(String status) &#123; this.status = status; &#125; public String getBody_bytes_sent() &#123; return body_bytes_sent; &#125; public void setBody_bytes_sent(String body_bytes_sent) &#123; this.body_bytes_sent = body_bytes_sent; &#125; public String getHttp_referer() &#123; return http_referer; &#125; public void setHttp_referer(String http_referer) &#123; this.http_referer = http_referer; &#125; public String getHttp_user_agent() &#123; return http_user_agent; &#125; public void setHttp_user_agent(String http_user_agent) &#123; this.http_user_agent = http_user_agent; &#125; public boolean isValid() &#123; return valid; &#125; public void setValid(boolean valid) &#123; this.valid = valid; &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append(this.valid); sb.append(&quot;\\001&quot;).append(this.remote_addr); sb.append(&quot;\\001&quot;).append(this.remote_user); sb.append(&quot;\\001&quot;).append(this.time_local); sb.append(&quot;\\001&quot;).append(this.request); sb.append(&quot;\\001&quot;).append(this.status); sb.append(&quot;\\001&quot;).append(this.body_bytes_sent); sb.append(&quot;\\001&quot;).append(this.http_referer); sb.append(&quot;\\001&quot;).append(this.http_user_agent); return sb.toString(); &#125;&#125; （2）编写LogMapper程序 package com.kingge.mapreduce.log;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取1行 String line = value.toString(); // 2 解析日志是否合法 LogBean bean = pressLog(line); if (!bean.isValid()) &#123; return; &#125; k.set(bean.toString()); // 3 输出 context.write(k, NullWritable.get()); &#125; // 解析日志 private LogBean pressLog(String line) &#123; LogBean logBean = new LogBean(); // 1 截取 String[] fields = line.split(&quot; &quot;); if (fields.length &gt; 11) &#123; // 2封装数据 logBean.setRemote_addr(fields[0]); logBean.setRemote_user(fields[1]); logBean.setTime_local(fields[3].substring(1)); logBean.setRequest(fields[6]); logBean.setStatus(fields[8]); logBean.setBody_bytes_sent(fields[9]); logBean.setHttp_referer(fields[10]); if (fields.length &gt; 12) &#123; logBean.setHttp_user_agent(fields[11] + &quot; &quot;+ fields[12]); &#125;else &#123; logBean.setHttp_user_agent(fields[11]); &#125; // 大于400，HTTP错误 if (Integer.parseInt(logBean.getStatus()) &gt;= 400) &#123; logBean.setValid(false); &#125; &#125;else &#123; logBean.setValid(false); &#125; return logBean; &#125;&#125; （3）编写LogDriver程序 package com.kingge.mapreduce.log;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class LogDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 加载jar包 job.setJarByClass(LogDriver.class); // 3 关联map job.setMapperClass(LogMapper.class); // 4 设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6 提交 job.waitForCompletion(true); &#125;&#125; 3.9 计数器应用​ Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。 1）API ​ （1）采用枚举的方式统计计数 enum MyCounter{MALFORORMED,NORMAL} //对枚举定义的自定义计数器加1 context.getCounter(MyCounter.MALFORORMED).increment(1); （2）采用计数器组、计数器名称的方式统计 context.getCounter(“counterGroup”, “countera”).increment(1); ​ 组名和计数器名称随便起，但最好有意义。 ​ （3）计数结果在程序运行后的控制台上查看。 2）案例 ​ 数据清洗的两个案例 3.10 MapReduce开发总结在编写mapreduce程序时，需要考虑的几个方面： 1）输入数据接口：InputFormat 默认使用的实现类是：TextInputFormat TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。 KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\\t）。 NlineInputFormat按照指定的行数N来划分切片。 CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。 用户还可以自定义InputFormat。 2）逻辑处理接口：Mapper 用户根据业务需求实现其中三个方法：map() setup() cleanup () 3）Partitioner分区 ​ 有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces ​ 如果业务上有特别的需求，可以自定义分区。 4）Comparable排序 ​ 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。 ​ 部分排序：对最终输出的每一个文件进行内部排序。 ​ 全排序：对所有数据进行排序，通常只有一个Reduce。 ​ 二次排序：排序的条件有两个。 5）Combiner合并 Combiner合并可以提高程序执行效率，减少io传输。但是使用时必须不能影响原有的业务处理结果。 6）reduce端分组：Groupingcomparator ​ reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数。 ​ 利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑。 ​ 自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果。然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）。这样，我们要取的最大值就是reduce()方法中传进来key。 7）逻辑处理接口：Reducer ​ 用户根据业务需求实现其中三个方法：reduce() setup() cleanup () 8）输出数据接口：OutputFormat ​ 默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对向目标文本文件中输出为一行。 SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。 用户还可以自定义OutputFormat。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(十)-Mapreduce基础","slug":"hadoop大数据-十-Mapreduce基础","date":"2018-03-16T11:59:59.000Z","updated":"2019-06-17T12:46:35.921Z","comments":true,"path":"2018/03/16/hadoop大数据-十-Mapreduce基础/","link":"","permalink":"http://kingge.top/2018/03/16/hadoop大数据-十-Mapreduce基础/","excerpt":"","text":"一 MapReduce入门1.1 MapReduce定义Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。 Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 1.2 MapReduce优缺点1.2.1 优点1**）MapReduce 易于编程。**它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 2**）良好的扩展性。**当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 3**）高容错性。**MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。 4**）适合PB**级以上海量数据的离线处理（他跟其他的分布式运行框架不同，例如spark等等）。这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。 1.2.2 缺点MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。 1）实时计算。MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 2）流式计算。流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 3）DAG（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 1.3 MapReduce核心思想 下面根据一个小小的案例来体现 mapreduce的运转流程。 根据块大小（128M）进行分片运算，每个maptask负责处理自己所属的块数据，把每个单词出现个数计算统计然后放到hashmap（实际上是放到磁盘上）中，key是单词，value是单词出现次数。 1）分布式的运算程序往往需要分成至少2个阶段。（map阶段和reduce阶段） 2）第一个阶段的maptask并发实例，完全并行运行，互不相干。 3）第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。 4）MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。 1.4 MapReduce进程一个完整的mapreduce程序在分布式运行时有三类实例进程： 1）MrAppMaster：负责整个程序的过程调度及状态协调。 2）MapTask：负责map阶段的整个数据处理流程。 3）ReduceTask：负责reduce阶段的整个数据处理流程。 1.5 MapReduce编程规范用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) 1）Mapper阶段​ （1）用户自定义的Mapper要继承自己的父类 ​ （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） ​ （3）Mapper中的业务逻辑写在map()方法中 ​ （4）Mapper的输出数据是KV对的形式（KV的类型可自定义） ​ （5）map()方法（maptask进程）对每一个调用一次 2）Reducer阶段​ （1）用户自定义的Reducer要继承自己的父类 ​ （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV ​ （3）Reducer的业务逻辑写在reduce()方法中 ​ （4）Reducetask进程对每一组相同k的组调用一次reduce()方法 3）Driver阶段整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象 4）案例​ 统计一堆文件中单词出现的个数（WordCount案例）。 在一堆给定的文本文件中统计输出每一个单词出现的总次数 1.数据准备 anly.text 包涵一下数据。hello worldkingge kinggehadoop sparkhello worldkingge kinggehadoop sparkhello worldhadoop spark 2.按照mapreduce编程规范，分别编写Mapper，Reducer，Driver。 简单案例分析 3.书写java代码（1）编写mapper类package com.kingge.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;//四个参数：前两个是map的输入参数类型，后两个数输出参数类型//很明显，执行一个map，数据的key值是long类型代表着数据所属的行号，那么value值就是string类型，对应Hadoop的序列化类型是text.//输出的结果是，每个单词对应的个数。那么输出的key应该是Text,代表单词,value应该是Int类型，代表这个单词的个数public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行-因为map是一行一行进行处理的 String line = value.toString(); // 2 切割 String[] words = line.split(&quot; &quot;); // 3 输出 for (String word : words) &#123; k.set(word); context.write(k, v); &#125; &#125;&#125; （2）编写reducer类package com.kingge.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; value, Context context) throws IOException, InterruptedException &#123; // 1 累加求和 int sum = 0; for (IntWritable count : value) &#123; sum += count.get(); &#125; // 2 输出 context.write(key, new IntWritable(sum)); &#125;&#125;执行到reduce阶段，那么经过map的计算和排序，最终会形成了一组一组的相同key的KV键值对（key group）。然后相同组的会进行reduce统计。一组接着一组进行计算。并不是所有组都通过reduce。//例如假设最终返回的KV值是：//hello 1//hello 1//word 1//word 1 那么 前两个hello为一组，经过reduce运算，然后返回，同时word为一组也经过统计返回。这两组并不会都由同一个reduce处理 （3）编写驱动类package com.kingge.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 设置map和Reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置Reduce输出 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 4）集群上测试 （1）将程序打成jar包，然后拷贝到hadoop集群中。 （2）启动hadoop集群 （3）执行wordcount程序 [kingge@hadoop102 software]$ hadoop jar wc.jar com.kingge.wordcount.WordcountDriver /user/kingge/input /user/kingge/output1 5）本地测试 （1）在windows环境上配置HADOOP_HOME环境变量。 （2）在eclipse上运行程序 （3）注意：如果eclipse打印不出日志，在控制台上只显示 1.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). 2.log4j:WARN Please initialize the log4j system properly. 3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 需要在项目的src目录下，新建一个文件，命名为“log4j.properties”，在文件中填入 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 经过debug发现，只有当map处理完所有数据，才会进入reduce，map处理数据是一行一行进行处理的，每一行数据的处理都会经过一次map方法，直到所有数据处理完毕。Map处理完所有数据后，会排序所有的key，进行分组。然后一组一组的经过reduce，进行统计操作。直到所有组统计完毕，然后输出数据。 二 Hadoop序列化2.1 为什么要序列化？​ 一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。 2.2 什么是序列化？序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。 反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。 2.3 为什么不用Java的序列化？​ Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效。 2.4 为什么序列化对Hadoop很重要？​ 因为Hadoop在集群之间进行通讯或者RPC调用的时候，需要序列化，而且要求序列化要快，且体积要小，占用带宽要小。所以必须理解Hadoop的序列化机制。 ​ 序列化和反序列化在分布式数据处理领域经常出现：进程通信和永久存储。然而Hadoop中各个节点的通信是通过远程调用（RPC）实现的，那么RPC序列化要求具有以下特点： 1）紧凑：紧凑的格式能让我们充分利用网络带宽，而带宽是数据中心最稀缺的资源 2）快速：进程通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本的； 3）可扩展：协议为了满足新的需求变化，所以控制客户端和服务器过程中，需要直接引进相应的协议，这些是新协议，原序列化方式能支持新的协议报文； 4）互操作：能支持不同语言写的客户端和服务端进行交互； 2.5 常用数据序列化类型常用的数据类型对应的hadoop数据序列化类型 Java**类型** Hadoop Writable**类型** boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable string Text map MapWritable array ArrayWritable 2.6 自定义bean对象实现序列化接口（Writable）1）自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项。 （1）必须实现Writable接口 （2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造 ​ public FlowBean() { super(); } （3）重写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; （4）重写反序列化方法 ​ @Overridepublic void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong();&#125; （5）注意反序列化的顺序和序列化的顺序完全一致 （6）要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。 （7）如果需要将自定义的bean放在key中传输，则还需要实现WritableComparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序。 ​ 《自定义的bean放在key中传输》是什么意思呢？因为我们知道map操作中输入数据的存储结构是-key-value的形式.上面的例子中统计文本单词数，那么文本文件中每一行的文本的序号就是key（0,1,2,3）每一行的文本，就是value的值。Map操作完后输出的数据结构也是key-value的形式。而且输出的数据会根据key排序，以便reduce处理。那么怎么排序在hadoop中有一个默认规则（如果key是2.5中的常用数据类型），如果使我们自定义的序列化数据类型作为key。那么默认排序规则就会失效，那么就需要我们制定一个排序规则就需要覆盖compareTo方法。** ​ @Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 2）案例​ 每一个手机号耗费的总上行流量、下行流量、总流量（序列化）。 2.1 数据准备pd.txt 1363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13560436666 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 输入数据格式： 输出数据格式 2.2 分析基本思路： Map阶段： （1）读取一行数据，切分字段 （2）抽取手机号、上行流量、下行流量 （3）以手机号为key，bean对象为value输出，即context.write(手机号,bean); Reduce阶段： （1）累加上行流量和下行流量得到总流量。 （2）实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输 （3）MR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key 所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable。然后重写key的compareTo方法。 2.3 编写mapreduce程序（1）编写流量统计的bean对象 package com.kingge.mapreduce.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;// 1 实现writable接口public class FlowBean implements Writable&#123; private long upFlow ; private long downFlow; private long sumFlow; //2 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; //3 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; //4 反序列化方法 //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 6 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125; （2）编写mapper package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); v.set(downFlow, upFlow); // 4 写出 context.write(new Text(phoneNum), new FlowBean(upFlow, downFlow)); &#125;&#125; （3）编写reducer package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_downFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sum_upFlow += flowBean.getSumFlow(); sum_downFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow); // 3 写出 context.write(key, resultBean); &#125;&#125; （4）编写驱动 package com.kingge.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kingge.top/tags/MapReduce/"}]},{"title":"hadoop大数据(九)-yarn","slug":"hadoop大数据-九-yarn","date":"2018-03-14T14:59:59.000Z","updated":"2019-08-01T13:33:46.678Z","comments":true,"path":"2018/03/14/hadoop大数据-九-yarn/","link":"","permalink":"http://kingge.top/2018/03/14/hadoop大数据-九-yarn/","excerpt":"","text":"5.1 Hadoop1.x和Hadoop2.x架构区别在Hadoop1.x时代，Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。 ResourceManagement 资源管理 JobScheduling/JobMonitoring 任务调度监控 在Hadoop2.x时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。这样就能够各司其职 ResourceManger ApplicationMaster ​ 需要注意的是，在Yarn中我们把job的概念换成了application，因为在新的Hadoop2.x中，运行的应用不只是MapReduce了，还有可能是其它应用如一个DAG（有向无环图Directed Acyclic Graph，例如storm应用）。Yarn的另一个目标就是拓展Hadoop，使得它不仅仅可以支持MapReduce计算，还能很方便的管理诸如Hive、Hbase、Pig、Spark/Shark等应用。这种新的架构设计能够使得各种类型的应用运行在Hadoop上面，并通过Yarn从系统层面进行统一的管理，也就是说，有了Yarn，各种应用就可以互不干扰的运行在同一个Hadoop系统中，共享整个集群资源。 5.2 Yarn概述Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 5.3 Yarn基本架构​ YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。 5.4 Yarn工作机制1）Yarn运行机制 2）工作机制详解 ​ （0）Mr程序提交到客户端所在的节点。 ​ （1）Yarnrunner向Resourcemanager申请一个Application。 ​ （2）rm将该应用程序的资源路径返回给yarnrunner。 ​ （3）该程序将运行所需资源提交到HDFS上。 ​ （4）程序资源提交完毕后，申请运行mrAppMaster。 ​ （5）RM将用户的请求初始化成一个task。 ​ （6）其中一个NodeManager领取到task任务。 ​ （7）该NodeManager创建容器Container，并产生MRAppmaster。 ​ （8）Container从HDFS上拷贝资源到本地。 ​ （9）MRAppmaster向RM 申请运行maptask资源。 ​ （10）RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 ​ （11）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。 （12）MrAppMaster等待所有maptask运行完毕后，向RM申请容器，运行reduce task。 ​ （13）reduce task向maptask获取相应分区的数据。 ​ （14）程序运行完毕后，MR会向RM申请注销自己。 5.5 作业提交全过程1）作业提交过程之YARN 作业提交全过程详解 （1）作业提交第0步：client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。 第1步：client向RM申请一个作业id。 第2步：RM给client返回该job资源的提交路径和作业id。 第3步：client提交jar包、切片信息和配置文件到指定的资源提交路径。 第4步：client提交完资源后，向RM申请运行MrAppMaster。 （2）作业初始化第5步：当RM收到client的请求后，将该job添加到容量调度器中。 第6步：某一个空闲的NM领取到该job。 第7步：该NM创建Container，并产生MRAppmaster。 第8步：下载client提交的资源到本地。 （3）任务分配第9步：MrAppMaster向RM申请运行多个maptask任务资源。 第10步：RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 （4）任务运行第11步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。 第12步：MrAppMaster等待所有maptask运行完毕后，向RM申请容器，运行reduce task。 第13步：reduce task向maptask获取相应分区的数据。 第14步：程序运行完毕后，MR会向RM申请注销自己。 （5）进度和状态更新YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。 （6）作业完成除了向应用管理器请求作业进度外, 客户端每5分钟都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。 2）作业提交过程之MapReduce 3）作业提交过程之读数据 4）作业提交过程之写数据 5.6 资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2默认的资源调度器是Capacity Scheduler。 具体设置详见：yarn-default.xml文件 The class to use as the resource scheduler. yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler &lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 1）先进先出调度器（FIFO） 2）容量调度器（Capacity Scheduler） 3）公平调度器（Fair Scheduler） 5.7 任务的推测执行1）作业完成时间取决于最慢的任务完成时间 一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。 典型案例：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？ 2）推测执行机制： 发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。 3）执行推测任务的前提条件 （1）每个task只能有一个备份任务； （2）当前job已完成的task必须不小于0.05（5%） （3）开启推测执行参数设置。Hadoop2.7.2 mapred-site.xml文件中默认是打开的。 &lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; mapreduce.map.speculative true If true, then multiple instances of some map tasks may be executed in parallel. mapreduce.reduce.speculative true If true, then multiple instances of some reduce tasks may be executed in parallel. 4）不能启用推测执行机制情况 （1）任务间存在严重的负载倾斜； （2）特殊任务，比如任务向数据库中写数据。 5）算法原理：","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"yarn","slug":"yarn","permalink":"http://kingge.top/tags/yarn/"}]},{"title":"hadoop大数据(八)-namenode和resourcemanager高可用","slug":"hadoop大数据-八-namenode和resourcemanager高可用","date":"2018-03-12T12:59:59.000Z","updated":"2019-06-10T13:20:52.956Z","comments":true,"path":"2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/","link":"","permalink":"http://kingge.top/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/","excerpt":"","text":"HDFS 高可用高可用概述1）所谓HA（high available），即高可用（7*24小时不中断服务）。 2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。 3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。 4）NameNode主要在以下两个方面影响HDFS集群 ​ NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启 ​ NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用 HDFS HA功能通过配置Active/Standby两个nameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。 HDFS-HA工作机制1）通过双namenode消除单点故障 HDFS-HA工作要点1）元数据管理方式需要改变： 内存中各自保存一份元数据； Edits日志只有Active状态的namenode节点可以做写操作； 两个namenode都可以读取edits； 共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）； 2）需要一个状态管理功能模块 实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split（脑裂）现象的发生。 脑裂：集群中存在两台active状态的namenode。 3）必须保证两个NameNode之间能够ssh无密码登录。 4）隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务 怎么能够保证两台namenode，有一台是active另一台是standby，而不出现脑裂现象呢？ 假想一：两台namenode进行通信，周期请求对面，告知自己状态。在一定的条件下可以实现高可用，但是存在如下问题：1.两台namenode直接通信，如果namenode1（active）处理client请求时，没空响应namenode2那么nn2等待了一段时间，就认为nn1已经碟机，那么nn2启动（切换为active）。这个时候集群出现脑裂现象。2.nn1和nn2 因为网络问题，可能存在一定的延迟，无法实时的切换（nn1碟机，切换到nn2的时候，可能会等待一两分钟）。 假想二：使用zookeeper记录namenode 的状态。也会出现上面的问题。如果网络出现问题，nn1（active）无法正确汇报自己的状态到zookeeper，那么nn2启动，也会出现脑裂问题。 假想三：zkfailover，一个namenode的内部进程（解决网络交互问题） HDFS-HA自动故障转移工作机制前面学习了使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能： 1）故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。 2）现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。 ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责： 1）健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。 2）ZooKeeper会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。 3）基于ZooKeeper的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为active状态。 ​ zookeeper服务端 HDFS-HA集群配置环境准备1）修改IP 2）修改主机名及主机名和IP地址的映射 3）关闭防火墙 4）ssh免密登录 5）安装JDK，配置环境变量等 规划集群hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager NodeManager NodeManager NodeManager 配置Zookeeper集群0）集群规划 在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 1）解压安装 （1）解压zookeeper安装包到/opt/module/目录下 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ （2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData ​ mkdir -p zkData （3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg ​ mv zoo_sample.cfg zoo.cfg 2）配置zoo.cfg文件 ​ （1）具体配置 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 增加如下配置 ​ #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 （2）配置参数解读 Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器； B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 3）集群操作 （1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件 ​ touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 （2）编辑myid文件 ​ vi myid ​ 在文件中添加与server对应的编号：如2 （3）拷贝配置好的zookeeper到其他机器上 ​ scp -r zookeeper-3.4.10/ root@hadoop103.kingge.com:/opt/app/ ​ scp -r zookeeper-3.4.10/ root@hadoop104.kingge.com:/opt/app/ ​ 并分别修改myid文件中内容为3、4 （4）分别启动zookeeper ​ [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start （5）查看状态 [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower 配置HDFS-HA集群（手动故障转移配置） 为什么说是手动，因为假设某一台namenode 出现了问题，并不会自动的切换另一台namenode为active状态，需要我们手动切换 1）官方地址：http://hadoop.apache.org/ 2）在opt目录下创建一个ha文件夹 mkdir ha 3）将/opt/app/下的 hadoop-2.7.2拷贝到/opt/ha目录下 cp -r hadoop-2.7.2/ /opt/ha/ 4）配置hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 5）配置core-site.xml fs.defaultFS hdfs://mycluster //任意名字，代表着整个namenode集群，至于调用那个namenode，他会自己分配 hadoop.tmp.dir /opt/ha/hadoop-2.7.2/data/tmp &lt;configuration&gt;&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; //任意名字，代表着整个namenode集群，至于调用那个namenode，他会自己分配 &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6）配置hdfs-site.xml（因为有了热备namenode，那么就可以把secondary-namenode关闭，功能重复） &lt;configuration&gt; &lt;!—可以配置文件块备份数--&gt; &lt;!-- 完全分布式集群名称 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; //这个名字必须与上面的mycluster一致 &lt;/property&gt; &lt;!-- 集群中NameNode节点都有哪些 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode元数据（edit.log）在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 防止脑裂--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/kingge/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 声明journalnode服务器存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/jn&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭权限检查--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; &lt;!—可以配置文件块备份数–&gt; dfs.nameservices mycluster //这个名字必须与上面的mycluster一致 dfs.ha.namenodes.mycluster nn1,nn2 dfs.namenode.rpc-address.mycluster.nn1 hadoop102:9000 dfs.namenode.rpc-address.mycluster.nn2 hadoop103:9000 dfs.namenode.http-address.mycluster.nn1 hadoop102:50070 dfs.namenode.http-address.mycluster.nn2 hadoop103:50070 dfs.namenode.shared.edits.dir qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster dfs.ha.fencing.methods sshfence dfs.ha.fencing.ssh.private-key-files /home/atguigu/.ssh/id_rsa dfs.journalnode.edits.dir /opt/ha/hadoop-2.7.2/data/jn dfs.permissions.enable false dfs.client.failover.proxy.provider.mycluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider 7）可以关闭secondary namenode和关闭原先的namenode 的http访问模式 8）拷贝配置好的hadoop环境到其他节点 启动HDFS-HA集群1）在各个JournalNode节点上，输入以下命令启动journalnode服务：（在这里是hadoop102、hadoop103.、hadoop104 三台服务器都需要执行下面命令） ​ sbin/hadoop-daemon.sh start journalnode 2）在[nn1]上，对其进行格式化，并启动： ​ bin/hdfs namenode –format // ​ sbin/hadoop-daemon.sh start namenode //开启active namenode 3）在[nn2]上，同步nn1的元数据信息： ​ bin/hdfs namenode -bootstrapStandby 4）启动[nn2]：启动备用namenode ​ sbin/hadoop-daemon.sh start namenode 5）查看web页面显示 6）在[nn1]上，启动所有datanode ​ sbin/hadoop-daemons.sh start datanode 7）将[nn1]切换为Active ​ bin/hdfs haadmin -transitionToActive nn1 8）查看是否Active ​ bin/hdfs haadmin -getServiceState nn1 9）尝试kill 掉nn1的namenode Kill 7575 查看nn1和nn2的namenode 状态， 你会发现nn1挂掉后，nn2不还是standby状态，没有自动切换为active，需要手动切换为Active 配置HDFS-HA自动故障转移（上面的是手动故障转移，就是需要手动启动某个namenode为active）1）具体配置 ​ （1）在hdfs-site.xml中增加 &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; dfs.ha.automatic-failover.enabled true ​ （2）在core-site.xml文件中增加 &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;&lt;/property&gt; ha.zookeeper.quorum hadoop102:2181,hadoop103:2181,hadoop104:2181 2）启动 ​ （1）关闭所有HDFS服务： ​ sbin/stop-dfs.sh ​ （2）启动Zookeeper集群： ​ bin/zkServer.sh start ​ （3）初始化HA在Zookeeper中状态： ​ bin/hdfs zkfc -formatZK ​ （4）启动HDFS服务： ​ sbin/start-dfs.sh ​ （5）在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode ​ sbin/hadoop-daemin.sh start zkfc 3）验证 ​ （1）将Active NameNode进程kill ​ kill -9 namenode的进程id ​ （2）将Active NameNode机器断开网络 ​ service network stop YARN-高可用配置YARN-HA工作机制1）官方文档： http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html 2）YARN-HA工作机制 配置YARN-HA集群（也就是开两台resourcemanager）0）环境准备 （1）修改IP （2）修改主机名及主机名和IP地址的映射 （3）关闭防火墙 （4）ssh免密登录 （5）安装JDK，配置环境变量等 ​ （6）配置Zookeeper集群 1）规划集群 hadoop102 hadoop103 hadoop104 NameNode NameNode JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager ResourceManager NodeManager NodeManager NodeManager 2）具体配置 （1）yarn-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--启用resourcemanager ha--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--声明两台resourcemanager的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster-yarn1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop102&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper集群的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复 – 当resourcemanager碟机后自动重启--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; yarn.nodemanager.aux-services mapreduce_shuffle yarn.resourcemanager.ha.enabled true yarn.resourcemanager.cluster-id cluster-yarn1 yarn.resourcemanager.ha.rm-ids rm1,rm2 yarn.resourcemanager.hostname.rm1 hadoop102 yarn.resourcemanager.hostname.rm2 hadoop103 yarn.resourcemanager.zk-address hadoop102:2181,hadoop103:2181,hadoop104:2181 yarn.resourcemanager.recovery.enabled true yarn.resourcemanager.store.class org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore ​ （2）同步更新其他节点的配置信息 3）启动hdfs （1）在各个JournalNode节点上，输入以下命令启动journalnode服务： ​ sbin/hadoop-daemon.sh start journalnode （2）在[nn1]上，对其进行格式化，并启动： ​ bin/hdfs namenode -format ​ sbin/hadoop-daemon.sh start namenode （3）在[nn2]上，同步nn1的元数据信息： ​ bin/hdfs namenode -bootstrapStandby （4）启动[nn2]： ​ sbin/hadoop-daemon.sh start namenode （5）启动所有datanode ​ sbin/hadoop-daemons.sh start datanode （6）将[nn1]切换为Active ​ bin/hdfs haadmin -transitionToActive nn1 4）启动yarn （1）在hadoop102中执行： sbin/start-yarn.sh （2）在hadoop103中执行： sbin/yarn-daemon.sh start resourcemanager （3）查看服务状态 bin/yarn rmadmin -getServiceState rm1 HDFS Federation架构设计1） NameNode架构的局限性 （1）Namespace（命名空间）的限制 由于NameNode在内存中存储所有的元数据（metadata），因此单个namenode所能存储的对象（文件+块）数目受到namenode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个datanode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个datanode从4T增长到36T，集群的尺寸增长到8000个datanode。存储的需求从12PB增长到大于100PB。 （2）隔离问题 由于HDFS仅有一个namenode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。 ​ （3）性能的瓶颈 ​ 由于是单个namenode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个namenode的吞吐量。 2）HDFS Federation架构设计 能不能有多个NameNode NameNode NameNode NameNode 元数据 元数据 元数据 Log machine 电商数据/话单数据 3）HDFS Federation应用思考 不同应用可以使用不同NameNode进行数据管理 ​ 图片业务、爬虫业务、日志审计业务 Hadoop生态系统中，不同的框架使用不同的namenode进行管理namespace。（隔离性）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"},{"name":"hadoop高可用","slug":"hadoop高可用","permalink":"http://kingge.top/tags/hadoop高可用/"}]},{"title":"hadoop大数据(七)-HDFS的Namenode和Datanode","slug":"hadoop大数据-七-HDFS的Namenode和Datanode","date":"2018-03-10T07:38:59.000Z","updated":"2019-06-10T12:56:33.609Z","comments":true,"path":"2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/","link":"","permalink":"http://kingge.top/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/","excerpt":"","text":"五 NameNode工作机制5.1 NameNode&amp;Secondary NameNode工作机制 1）第一阶段：namenode启动（1）第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存（初始化系统为上一次退出时的最新状态）。 （2）客户端对元数据进行增删改的请求 （3）namenode记录操作日志，更新滚动日志。 （4）namenode在内存中对数据进行增删改查 对于namenode而言最新的操作日志是 edits.in.progress(正在执行的日志) 2）第二阶段：Secondary NameNode工作 核心工作：检查是否需要合并namenode的编辑日志和镜像文件（checkpoint） ​ （1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。定时时间默认1小时，edits默认一百万次 ​ （2）Secondary NameNode请求执行checkpoint。（是否需要合并两个文件） ​ （3）namenode滚动正在写的edits日志（edits.in.progress） ​ （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode ​ （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 ​ （6）生成新的镜像文件fsimage.chkpoint ​ （7）拷贝fsimage.chkpoint到namenode ​ （8）namenode将fsimage.chkpoint重新命名成fsimage 也就是说，secondarynamenode的主要作用是帮助namenode分担他的压力，主要是帮助namenode合并镜像和操作日志，合并后，推给namenode。 总结正如上面所分析的，Hadoop文件系统会出现编辑日志（edits）不断增长的情况，尽管在NameNode运行期间不会对文件系统造成影响，但是如果NameNode重新启动，它将会花费大量的时间运行编辑日志中的每个操作，在此期间也就是我们前面所说的安全模式下，文件系统是不可用的。为了解决上述问题，Hadoop会运行一个Secondary NameNode进程，它的任务就是为原NameNode内存中的文件系统元数据产生检查点。其实说白了，就是辅助NameNode来处理fsimage文件与edits文件的一个进程。它从NameNode中复制fsimage与edits到临时目录并定期合并成一个新的fsimage并且删除原来的编辑日志edits。具体 步骤如下：（1）Secondary NameNode首先请求原NameNode进行edits的滚动，这样会产生一个新的编辑日志文件edits来保存对文件系统的操作（例如：上传新文件，删除文件，修改文件）。（2）Secondary NameNode通过Http方式读取原NameNode中的fsimage及edits。（3）Secondary NameNode将fsimage及edits进行合并产生新的fsimage（4）Secondary NameNode通过Http方式将新生成的fsimage发送到原来的NameNode中（5）原NameNode用新生成的fsimage替换掉旧的fsimage文件，新生成的edits文件也就是（1）生成的滚动编辑日志文件替换掉之前的edits文件 3）web端访问SecondaryNameNode​ （1）启动集群 ​ （2）浏览器中输入：http://hadoop102:50090/status.html ​ （3）查看SecondaryNameNode信息 4）chkpoint检查时间参数设置（1）通常情况下，SecondaryNameNode每隔一小时执行一次。 ​ [hdfs-default.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt; （2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt;&lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt;&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;&lt;/property&gt; 5.2 镜像文件和编辑日志文件1）概念​ namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件 edits_0000000000000000000 fsimage_0000000000000000000.md5 seen_txid VERSION （1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。 （2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。 （3）seentxid文件保存的是一个数字，就是最后一个edits的数字（最后一次操作的序号） （4）每次Namenode启动的时候都会将fsimage文件读入内存，并从00001开始到seen_txid中记录的数字依次执行每个edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成Namenode启动的时候就将fsimage和edits文件进行了合并。 2）oiv查看fsimage文件（1）查看oiv和oev命令 [kingge@hadoop102 current]$ hdfs oiv apply the offline fsimage viewer to an fsimage oev apply the offline edits viewer to an edits file （2）基本语法 hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径 （3）案例实操 [kingge@hadoop102 current]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current [kingge@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml [kingge@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml 将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。 ​ 总结 查看XML你会发现，里面存储了HDFS中文件或者文件夹的创建日期，权限，名称等等元数据信息。但是并没有存储文件保存的位置，也就是：并没有发现文件存储的DataNode节点信息信息那么当客户端请求读数据的时候，namenode是怎么返回数据所在块信息呢？原来他会一直跟datanode进行交互，获取数据所在块信息。 3）oev查看edits文件edits包括两类，edits_XXX,edits_inprogress_XXX edits_XXX：保存文件系统的操作，查看方式见下面语法&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;-&lt;EDITS&gt;&lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt;-&lt;RECORD&gt;&lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt;-&lt;DATA&gt;&lt;TXID&gt;7170&lt;/TXID&gt;&lt;/DATA&gt;&lt;/RECORD&gt;-&lt;RECORD&gt;&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;-&lt;DATA&gt;&lt;TXID&gt;7171&lt;/TXID&gt;&lt;LENGTH&gt;0&lt;/LENGTH&gt;&lt;INODEID&gt;17787&lt;/INODEID&gt;&lt;PATH&gt;/user/zpx/a.txt&lt;/PATH&gt;&lt;REPLICATION&gt;3&lt;/REPLICATION&gt;&lt;MTIME&gt;1489118864779&lt;/MTIME&gt;&lt;ATIME&gt;1489118864779&lt;/ATIME&gt;&lt;BLOCKSIZE&gt;数据的大小&lt;/BLOCKSIZE&gt;&lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_1295720148_1&lt;/CLIENT_NAME&gt;&lt;CLIENT_MACHINE&gt;192.168.231.1&lt;/CLIENT_MACHINE&gt;&lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt;-&lt;PERMISSION_STATUS&gt;&lt;USERNAME&gt;Administrator&lt;/USERNAME&gt;&lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt;&lt;MODE&gt;420&lt;/MODE&gt;&lt;/PERMISSION_STATUS&gt;&lt;RPC_CLIENTID&gt;0c9a5af9-26a8-45d9-8754-cd0e7e47f65b&lt;/RPC_CLIENTID&gt;&lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt;&lt;/DATA&gt;&lt;/RECORD&gt;&lt;/EDITS&gt;对Hdfs文件系统的每一个操作都保存在了edits文件中，每一个操作都是事务，有事务id——&lt;TXID&gt;7171&lt;/TXID&gt;，还有当前操作做了什么&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;，副本数，以及大小edits_inprogress_XXX：正在使用的过程，当前正在向前滚动。查看方式见下面语法 （1）基本语法 hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 （2）案例实操 [kingge@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml [kingge@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml 将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。 总结 你会发现，edits_inprogress，记录的是当前客户端请求执行的操作（增量记录当前操作） 5.3 滚动编辑日志正常情况HDFS文件系统有更新操作时，就会滚动编辑日志。也可以用命令强制滚动编辑日志。 1）滚动编辑日志（前提必须启动集群） [kingge@hadoop102 current]$ hdfs dfsadmin -rollEdits 2）镜像文件什么时候产生 Namenode启动时加载镜像文件和编辑日志 5.4 Namenode版本号1）查看namenode版本号 在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current这个目录下查看VERSION namespaceID=1933630176 clusterID=CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175 cTime=0 storageType=NAME_NODE blockpoolID=BP-97847618-192.168.10.102-1493726072779 layoutVersion=-63 2）namenode版本号具体解释 （1） namespaceID在HDFS上，会有多个Namenode，所以不同Namenode的namespaceID是不同的，分别管理一组blockpoolID。 （2）clusterID集群id，全局唯一 （3）cTime属性标记了namenode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 （4）storageType属性说明该存储目录包含的是namenode的数据结构。 （5）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。 （6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 5.5 SecondaryNameNode目录结构Secondary NameNode用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。 也即是说存在两种情况secondarynamenode会向namenode请求合并镜像文件和日志文件。（1）当上次请求时间已经间隔了一个小时后，会去请求（2）当操作数（edits，操作日志数）到达一百万次时，会去请求那么他怎么知道操作次数到达一百万次呢？答案是，一分钟请求namenode一次，查询操作次数是否到达一百万次。注意，这个检查操作数的时间设置最好不要跟 一致，不然他会默认执行第一种场景（间隔一个小时） 在/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/current这个目录中查看SecondaryNameNode目录结构。 edits_0000000000000000001-0000000000000000002 fsimage_0000000000000000002 fsimage_0000000000000000002.md5 VERSION SecondaryNameNode的namesecondary/current目录和主namenode的current目录的布局相同。 好处：在主namenode**发生故障时（假设没有及时备份数据），可以从SecondaryNameNode**恢复数据。 根据secondarynamenode恢复namenode方法一：将SecondaryNameNode中数据拷贝到namenode存储数据的目录； 方法二：使用-importCheckpoint选项启动namenode守护进程，从而将SecondaryNameNode中数据拷贝到namenode目录中。 1）案例实操（一）： 模拟namenode故障，并采用方法一，恢复namenode数据 （1）kill -9 namenode进程 （2）删除namenode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* （3）拷贝SecondaryNameNode中数据到原namenode存储数据目录 ​ [kingge@hadoop102 hadoop-2.7.2]$ scp -R /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* /opt/module/hadoop-2.7.2/data/tmp/dfs/name/ （4）重新启动namenode [kingge@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 2）案例实操（二）： 模拟namenode故障，并采用方法二，恢复namenode数据 （0）修改hdfs-site.xml中的 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;# 120秒checkpoint一次&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt;# namenode镜像文件和操作日志存放目录 （1）kill -9 namenode进程 （2）删除namenode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* （3）如果SecondaryNameNode不和Namenode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到Namenode存储数据的平级目录。Scp命令拷贝过来 [kingge@hadoop102 dfs]$ pwd/opt/module/hadoop-2.7.2/data/tmp/dfs[kingge@hadoop102 dfs]$ lsdata name namesecondary （4）导入检查点数据（等待一会ctrl+c结束掉） [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint （5）启动namenode [kingge@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode （6）如果提示文件锁了，可以删除in_use.lock ​ [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/in_use.lock 5.5.5 设置checkpoint检查时间默认的checkpoint period是1个小时。可以去hdfs-site.xml中修改 5.6 集群安全模式操作1）概述 Namenode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的fsimage文件和一个空的编辑日志。此时，namenode开始监听datanode请求。但是此刻，namenode运行在安全模式，即namenode的文件系统对于客户端来说是只读的。（可以解释为什么在namenode启动的时候，我们put数据到hdfs会提示，安全模式错误）因为这个时候namenode和datanode还没有联通对方，需要等待连通后，安全模式自动关闭，然后就可以上传文件了 系统中的数据块的位置并不是由namenode维护的，而是以块列表的形式存储在datanode中。在系统的正常操作期间，namenode会在内存中保留所有块位置的映射信息。在安全模式下，各个datanode会向namenode发送最新的块列表信息，namenode了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”，namenode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以namenode不会进入安全模式。 2）基本语法 集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。 （1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） （2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） （3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） （4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态） 3）案例 ​ 模拟等待安全模式 ​ 1）先进入安全模式 [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter ​ 2）执行下面的脚本 编辑一个脚本 #!/bin/bashbin/hdfs dfsadmin -safemode waitbin/hdfs dfs -put ~/hello.txt /root/hello.txt ​ 3）再打开一个窗口，执行 [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave 5.7 Namenode多目录配置1）namenode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。 2）具体配置如下： ​ hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 2.停止集群，删除数据文件 -- rm -rf data/logs (集群里有多少台服务器就删除多少台)3.格式化namenode4.调用xsync 分发脚本到各个集群 5.启动集群6.查看设置的本地目录name1、name2 你会发现里面的数据一模一样 测试NameNode场景：关闭namenode（stop-dfs.sh），关闭yarn（stop-yarn.sh），删除hadoop目录下的data目录和log目录。 1.格式化namenode – bin/hdfs namenode -format 2.启动hdfs和yarn 六 DataNode工作机制6.1 DataNode工作机制 1）一个数据块在datanode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 2）DataNode启动后向namenode注册，通过后，周期性（1小时）的向namenode上报所有的块信息。 3）心跳是每3秒一次，心跳返回结果带有namenode给该datanode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个datanode的心跳，则认为该节点不可用。 4）集群运行中可以安全加入和退出一些机器（在不关闭集群的情况下服役和退役服务器） 6.2 数据完整性1）当DataNode读取block的时候，它会计算checksum 2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。 3）client读取其他DataNode上的block。 4）datanode在其文件创建后周期验证checksum 6.3 掉线时限参数设置datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： ​ timeout = 2 dfs.namenode.heartbeat.recheck-interval + 10 dfs.heartbeat.interval。 ​ 而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。 ​ 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 &lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt; dfs.heartbeat.interval &lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 6.4 DataNode的目录结构和namenode不同的是，datanode的存储目录是初始阶段自动创建的，不需要额外格式化。 1）在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current这个目录下查看版本号 [kingge@hadoop102 current]$ cat VERSION storageID=DS-1b998a1d-71a3-43d5-82dc-c0ff3294921b clusterID=CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175 cTime=0 datanodeUuid=970b2daf-63b8-4e17-a514-d81741392165 storageType=DATA_NODE layoutVersion=-56 2）具体解释 ​ （1）storageID：存储id号 ​ （2）clusterID集群id，全局唯一 ​ （3）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 ​ （4）datanodeUuid：datanode的唯一识别码 ​ （5）storageType：存储类型 ​ （6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 3）在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-97847618-192.168.10.102-1493726072779/current这个目录下查看该数据块的版本号 [kingge@hadoop102 current]$ cat VERSION #Mon May 08 16:30:19 CST 2017 namespaceID=1933630176 cTime=0 blockpoolID=BP-97847618-192.168.10.102-1493726072779 layoutVersion=-56 4）具体解释 （1）namespaceID：是datanode首次访问namenode的时候从namenode处获取的storageID对每个datanode来说是唯一的（但对于单个datanode中所有存储目录来说则是相同的），namenode可用这个属性来区分不同datanode。 （2）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。 （3）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。 （4）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。 6.5 服役新数据节点0）需求： 随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。 1）环境准备 ​ （1）克隆一台虚拟机 ​ （2）修改ip地址和主机名称 ​ （3）修改xcall和xsync文件，增加新`增节点的同步ssh ​ （4）删除原来HDFS文件系统留存的文件 ​ /opt/module/hadoop-2.7.2/data 和 /opt/module/hadoop-2.7.2/log目录 2）服役新节点具体步骤（下面的操作建议在namenode所在节点进行操作） ​ （1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件 [kingge@hadoop105 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop105 hadoop]$ touch dfs.hosts （名字任意） [kingge@hadoop105 hadoop]$ vi dfs.hosts 添加如下主机名称（包含新服役的节点） hadoop102 hadoop103 hadoop104 hadoop105 ​ （2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性 &lt;property&gt;&lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; ​ （3）刷新namenode [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful ​ （4）更新resourcemanager节点 [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 操作完后，打开**hdfs文件系统，发现已经服役了一个新的data**节点 ​ （5）在namenode的slaves文件中增加新主机名称 ​ 增加105 不需要分发 hadoop102 hadoop103 hadoop104 hadoop105 ​ （6）单独命令启动新的数据节点和节点管理器 [kingge@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-datanode-hadoop105.out [kingge@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-kingge-nodemanager-hadoop105.out ​ （7）在web浏览器上检查是否ok 3）如果数据不均衡，可以用命令实现集群的再平衡 ​ [kingge@hadoop102 sbin]$ ./start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 6.6 退役旧数据节点1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件 ​ [kingge@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop102 hadoop]$ touch dfs.hosts.exclude [kingge@hadoop102 hadoop]$ vi dfs.hosts.exclude 添加如下主机名称（要退役的节点） hadoop105 2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 &lt;property&gt;&lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 3）刷新namenode、刷新resourcemanager [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 4）检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点。 5）等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役。· [kingge@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode stopping datanode [kingge@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager stopping nodemanager 6）从include文件中删除退役节点，再运行刷新节点的命令 ​ （1）从namenode的dfs.hosts文件中删除退役节点hadoop105 hadoop102 hadoop103 hadoop104 ​ （2）刷新namenode，刷新resourcemanager [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 7）从namenode的slave文件中删除退役节点hadoop105 hadoop102 hadoop103 hadoop104 8）如果数据不均衡，可以用命令实现集群的再平衡 [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved 6.7 Datanode多目录配置1）datanode也可以配置成多个目录，每个目录存储的数据不一样，即是上传一个文本，那么文本存储在data，但是data2什么都没有（跟namenode多目录区别）。即：数据不是副本。 2）具体配置如下： ​ hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; 七 HDFS其他功能7.1 集群间数据拷贝1）scp实现两个远程主机之间的文件复制 ​ scp -r hello.txt root@hadoop103:/user/kingge/hello.txt // 推 push ​ scp -r root@hadoop103:/user/kingge/hello.txt hello.txt // 拉 pull ​ scp -r root@hadoop103:/user/kingge/hello.txt root@hadoop104:/user/kingge //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。 2）采用discp命令实现两个hadoop集群之间的递归数据复制 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp hdfs://haoop102:9000/user/kingge/hello.txt hdfs://hadoop103:9000/user/kingge/hello.txt 7.2 Hadoop存档1）理论概述 每个文件均按块存储，每个块的元数据存储在namenode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽namenode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。 Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少namenode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件可以用作MapReduce的输入。 2）案例实操 （1）需要启动yarn进程 ​ [kingge@hadoop102 hadoop-2.7.2]$ start-yarn.sh （2）归档文件 ​ 归档成一个叫做xxx.har的文件夹，该文件夹下有相应的数据文件。Xx.har目录是一个整体，该目录看成是一个归档文件即可。 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p /user/kingge /user/my （3）查看归档 ​ [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///myhar.har （4）解归档文件 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har /* /user/kingge 7.3 快照管理快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 1）基本语法 ​ （1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能） ​ （2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用） ​ （3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照） ​ （4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照） ​ （5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照） ​ （6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有可快照目录） ​ （7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处） ​ （8）hdfs dfs -deleteSnapshot （功能描述：删除快照） 2）案例实操 ​ （1）开启/禁用指定目录的快照功能 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/kingge/data [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/kingge/data ​ （2）对目录创建快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/kingge/data 通过web访问hdfs://hadoop102:9000/user/kingge/data/.snapshot/s…..// 快照和源文件使用相同数据块 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/kingge/data/.snapshot/ ​ （3）指定名称创建快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/kingge/data miao170508 ​ （4）重命名快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/kingge/data/ miao170508 kingge170508 ​ （5）列出当前用户所有可快照目录 [kingge@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir ​ （6）比较两个快照目录的不同之处 [kingge@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/kingge/data/ . .snapshot/kingge170508 ​ （7）恢复快照 [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/kingge/input/.snapshot/s20170708-134303.027 /user 7.4 回收站1）默认回收站 默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。 默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。 要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。 2）启用回收站 修改core-site.xml，配置垃圾回收时间为1分钟。 &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 3）查看回收站 回收站在集群中的；路径：/user/kingge/.Trash/…. 4）修改访问垃圾回收站用户名称(如果不修改为想要查看该回收站的用户的名称，那么该用户试图进入回收站时会提示权限问题) ​ 进入垃圾回收站用户名称，默认是dr.who，修改为kingge用户 ​ [core-site.xml] &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;kingge&lt;/value&gt;&lt;/property&gt; 5）通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 Trash trash = New Trash(conf); trash.moveToTrash(path); 6）恢复回收站数据 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/kingge/.Trash/Current/user/kingge/input /user/kingge/input 7）清空回收站（他并不是真正删除文件，而是生成一个当前时间戳的文件夹然后把回收站里面的文件都放到这个文件夹里面） [kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -expunge","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(六)-HDFS的读写数据流程","slug":"hadoop大数据-六-HDFS的读写数据流程","date":"2018-03-08T14:38:59.000Z","updated":"2019-06-09T04:48:50.181Z","comments":true,"path":"2018/03/08/hadoop大数据-六-HDFS的读写数据流程/","link":"","permalink":"http://kingge.top/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/","excerpt":"","text":"四 HDFS的数据流4.1 HDFS写数据流程4.1.1 剖析文件写入 1）客户端通过Distributed FileSystem模块向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。（存在覆盖，不存在创建） 2）namenode返回是否可以上传。 3）客户端请求第一个 block上传到哪几个datanode服务器上。 4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。（根据配置文件中指定的备份数量及机架感知原理进行文件分配） 5）客户端通过FSDataOutputStream模块请求dn1上传数据（建立RPC请求），dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。-应答成功，开始传输数据 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet（默认 64K）为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 128M他并不是一次性写入dn1，而是分包的形式，dn1接收到一个包，然后保存在自己所在服务器的本地缓存中。然后再写入自己磁盘（7_blk_1）的同时，传输给dn2，以此类推。直到传输完整个128M。第一块传输完毕。 8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器。（重复执行3-7步）。 4.1.2 网络拓扑概念​ 在本地网络中，两个节点被称为“彼此近邻”是什么意思？在海量数据处理中，其主要限制因素是节点之间数据的传输速率——带宽很稀缺。这里的想法是将两个节点间的带宽作为距离的衡量标准。 ​ 节点距离：两个节点到达最近的共同祖先的距离总和。 例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 Distance(/d1/r1/n1, /d1/r1/n1)=0（同一节点上的进程） Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点） Distance(/d1/r1/n1, /d1/r3/n2)=4（同一数据中心不同机架上的节点） Distance(/d1/r1/n1, /d2/r4/n2)=6（不同数据中心的节点） 大家算一算每两个节点之间的距离。 4.1.3 机架感知（副本节点选择） 一份数据如果存在三个副本，那么副本存放服务器的选择，应该采取怎么样的策略 1）官方ip地址： http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication 2）低版本Hadoop副本节点选择 第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于不相同机架的随机节点上。 第三个副本和第二个副本位于相同机架，节点随机。 3）Hadoop2.7.2副本节点选择 ​ 第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。 ​ 第二个副本和第一个副本位于相同机架，随机节点。 ​ 第三个副本位于不同机架，随机节点。 为什么第一个副本选择在客户端所在的节点，因为这样client**请求数据的时候，可以做到更快的响应，优先读取当前节点副本信息（设计网络拓扑概念），距离客户端越近的节点，数据传输速率越快** 4.2 HDFS读数据流程 1）客户端通过Distributed FileSystem向namenode请求下载文件，namenode通过查询元数据，找到文件块所在的datanode地址。 2）挑选一台datanode（就近原则，然后随机）服务器，请求读取数据。 3）datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验）。 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 4.3 一致性模型1）debug调试如下代码 @Test public void writeFile() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); fs = FileSystem.get(configuration); // 2 创建文件输出流 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”); FSDataOutputStream fos = fs.create(path); // 3 写数据 fos.write(“hello”.getBytes()); // 4 一致性刷新 fos.hflush(); fos.close(); } @Test public void writeFile() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); fs = FileSystem.get(configuration); // 2 创建文件输出流 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;); FSDataOutputStream fos = fs.create(path); // 3 写数据 fos.write(&quot;hello&quot;.getBytes()); // 4 一致性刷新 fos.hflush(); fos.close(); &#125; 2）总结 写入数据时，如果希望数据被其他client立即可见，调用如下方法 FsDataOutputStream. hflush (); //清理客户端缓冲区数据，被其他client立即可见 因为传统的流操作，只有在关闭流的时候，才会去执行flush**操作，那么可能在关闭流之前发生错误，导致数据没有存储到对应的节点。为了避免这种问题，可以手动显式的执行flush**写入磁盘操作。（IOUtils. copyBytes 内部已经实现刷新机制，不需要手动刷新）","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(五)-HDFS概念和基本操作","slug":"hadoop大数据-五-HDFS概念和基本操作","date":"2018-03-06T12:31:59.000Z","updated":"2019-06-09T04:01:11.154Z","comments":true,"path":"2018/03/06/hadoop大数据-五-HDFS概念和基本操作/","link":"","permalink":"http://kingge.top/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/","excerpt":"","text":"一 HDFS简单介绍1.1 HDFS产生背景随着数据量越来越大，在一个操作系统管辖的范围内存不下了，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。 1.2 HDFS概念HDFS**，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的**，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS的设计适合一次写入，多次读出的场景，且不支持文件的修改（也是可以修改的，但是不建议）。适合用来做数据分析，并不适合用来做网盘应用。 1.3 HDFS 优缺点1.3.1 优点1）高容错性 ​ （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。 ​ （2）某一个副本丢失以后，它可以自动恢复，这是由 HDFS 内部机制实现的，我们不必关心。 2）适合大数据处理 ​ （1）数据规模：能够处理数据规模达到 GB、TB、甚至PB级别的数据。（因为他会切块存储，所以可以存储大文件-**参见HDFS**写数据流程） ​ （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 3）流式数据访问**（一点一点的处理数据，而不是一次性读取整个数据，这样会极大消耗内存）** ​ （1）一次写入，多次读取，不能修改，只能追加。 ​ （2）它能保证数据的一致性。 4）可构建在廉价机器上 ​ （1）它通过多副本机制，提高可靠性。 ​ （2）它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。 1.3.2 缺点1）不适合低延时数据访问 ​ （1）比如毫秒级的来存储数据，这是不行的，它做不到。 ​ （2）它适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况下是不行的，比如毫秒级以内读取数据，这样它是很难做到的。 2）无法高效的对大量小文件进行存储（HDFS默认的最基本的存储单位是128M的数据块。） ​ （1）存储大量小文件;)的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。 ​ （2）小文件存储的寻道时间会超过读取时间，它违反了HDFS的设计目标。 3）并发写入、文件随机修改 ​ （1）一个文件只能有一个写，不允许多个线程同时写。 ​ （2）仅支持数据 append（追加），不支持文件的随机修改。 1.4 HDFS架构HDFS的架构图 ​ 这种架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分。 1）Client：就是客户端。 ​ （1）文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的Block，然后进行存储。 ​ （2）与NameNode交互，获取文件的位置信息。 ​ （3）与DataNode交互，读取或者写入数据。 ​ （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS。 ​ （5）Client可以通过一些命令来访问HDFS。 2）NameNode：就是master，它是一个主管、管理者。 ​ （1）管理HDFS的名称空间。 ​ （2）管理数据块（Block）;)映射信息 ​ （3）配置副本策略;) ​ （4）处理客户端读写请求。 3） DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 ​ （1）存储实际的数据块。 ​ （2）执行数据块的读/写操作。 4） Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 ​ （1）辅助NameNode，分担其工作量。 ​ （2）定期合并fsimage和fsedits;)，并推送给NameNode。 ​ （3）在紧急情况下，可辅助恢复NameNode。 1.5 HDFS 文件块大小HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。 HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。 如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。 块的大小： 10ms*100*100M/s = 100M 块大小取决于磁盘传输速率 二 HFDS命令行操作操作HDFS的命令其实有三个： ​ Hadoop fs：使用面最广，可以操作任何文件系统。 ​ hadoop dfs与hdfs dfs：只能操作HDFS文件系统相关（包括与Local FS间的操作），前者已经Deprecated，一般使用后者。 推荐使用：hadoop fs 1）基本语法bin/hadoop fs 具体命令 2）参数大全​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs [-appendToFile … ] [-cat [-ignoreCrc] …] [-checksum …] [-chgrp [-R] GROUP PATH…] [-chmod [-R] PATH…] [-chown [-R] [OWNER][:[GROUP]] PATH…] [-copyFromLocal [-f] [-p] … ] [-copyToLocal [-p] [-ignoreCrc] [-crc] … ] [-count [-q] …] [-cp [-f] [-p] … ] [-createSnapshot []] [-deleteSnapshot ] [-df [-h] [ …]] [-du [-s] [-h] …] [-expunge] [-get [-p] [-ignoreCrc] [-crc] … ] [-getfacl [-R] ] [-getmerge [-nl] ] [-help [cmd …]] [-ls [-d] [-h] [-R] [ …]] [-mkdir [-p] …] [-moveFromLocal … ] [-moveToLocal ] [-mv … ] [-put [-f] [-p] … ] [-renameSnapshot ] [-rm [-f] [-r|-R] [-skipTrash] …] [-rmdir [–ignore-fail-on-non-empty] …] [-setfacl [-R] [{-b|-k} {-m|-x } ]|[–set ]] [-setrep [-R] [-w] …] [-stat [format] …] [-tail [-f] ] [-test -[defsz] ] [-text [-ignoreCrc] …] [-touchz …] [-usage [cmd …]] [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] &lt;path&gt; ...] [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] &lt;path&gt; ...] [-expunge] [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] &lt;file&gt;] [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]] 3）常用命令 （0）启动Hadoop集群（方便后续的测试） ​ [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -help rm ​ （2）-ls: 显示目录信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -ls / （3）-mkdir：在hdfs上创建目录 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /user/kingge/test （4）-moveFromLocal从本地剪切粘贴到hdfs [kingge@hadoop102 hadoop-2.7.2]$ touch jinlian.txt [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./jinlian.txt /user/kingge/test （5）–appendToFile ：追加一个文件到已经存在的文件末尾 [kingge@hadoop102 hadoop-2.7.2]$ touch ximen.txt [kingge@hadoop102 hadoop-2.7.2]$ vi ximen.txt 输入 wo ai jinlian [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile ximen.txt /user/kingge/test/jinlian.txt （6）-cat ：显示文件内容 （7）-tail：显示一个文件的末尾 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /user/kingge/test/jinlian.txt （8）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /hello.txt [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -chown someuser:somegrp /hello.txt （9）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /user/kingge/test （10）-copyToLocal：从hdfs拷贝到本地 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /user/kingge/test/jinlian.txt ./jinlian.txt （11）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /user/kingge/test/jinlian.txt /jinlian2.txt （12）-mv：在hdfs目录中移动文件 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /jinlian2.txt /user/kingge/test/ （13）-get：等同于copyToLocal，就是从hdfs下载文件到本地 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -get /user/kingge/test/jinlian2.txt ./ （14）-getmerge ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,… [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/kingge/test/* ./zaiyiqi.txt （15）-put：等同于copyFromLocal [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/kingge/test/ （16）-rm：删除文件或文件夹 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/kingge/test/jinlian2.txt （17）-rmdir：删除空目录 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test （18）-df ：统计文件系统的可用空间信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -df -h / （19）-du统计文件夹的大小信息 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/kingge/test 2.7 K /user/kingge/test ###查看文件夹下文件总大小 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/kingge/test 1.3 K /user/kingge/test/README.txt 15 /user/kingge/test/jinlian.txt 1.4 K /user/kingge/test/zaiyiqi.txt ##查看文件夹下各个文件大小 （20）-setrep：设置hdfs中文件的副本数量 [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 2 /user/kingge/test/jinlian.txt 这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。 三 HDFS客户端操作-eclipse3.1 Eclipse环境准备注意：以下操作，是在hadoop集群中中进行的，也就是说，需要先启动linux的hadoop集群 3.1.1 jar包准备1）解压hadoop-2.7.2.tar.gz到非中文目录 2）进入share文件夹，查找所有jar包，并把jar包拷贝到_lib文件夹下 3）在全部jar包中查找sources.jar，并剪切到_source文件夹。 4）在全部jar包中查找tests.jar，并剪切到_test文件夹。 3.1.2 Eclipse准备1）配置HADOOP_HOME环境变量 2）采用Hadoop编译后的bin 、lib两个文件夹（如果不生效，重新启动eclipse） 3）创建第一个java工程 4）导入 编译后目录的jar包（可以在文章下方回复我，我私信给你们） public class HdfsClientDemo1 { public static void main(String[] args) throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 configuration.set(“fs.defaultFS”, “hdfs://hadoop102:9000”); FileSystem fileSystem = FileSystem.get(configuration); // 直接配置访问集群的路径和访问集群的用户名称 // FileSystem fileSystem = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 把本地文件上传到文件系统中 fileSystem.copyFromLocalFile(new Path(“f:/hello.txt”), new Path(“/hello1.copy.txt”)); // 3 关闭资源 fileSystem.close(); System.out.println(“over”); } } public class HdfsClientDemo1 &#123; public static void main(String[] args) throws Exception &#123; // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 - 如果不配置，默认使用的是本地的hadoop运行环境。 configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); FileSystem fileSystem = FileSystem.get(configuration); // 直接配置访问集群的路径和访问集群的用户名称// FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 把本地文件上传到文件系统中 fileSystem.copyFromLocalFile(new Path(&quot;f:/hello.txt&quot;), new Path(&quot;/hello1.copy.txt&quot;)); // 3 关闭资源 fileSystem.close(); System.out.println(&quot;over&quot;); &#125;&#125; 4）执行程序 运行时需要配置用户名称 客户端去操作hdfs时，是有一个用户身份的。默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=kingge，kingge为用户名称。 3.2 通过API操作HDFS3.2.1 HDFS获取文件系统1）详细代码 ​ @Test public void initHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); // 2 获取文件系统 FileSystem fs = FileSystem.get(configuration); // 3 打印文件系统 System.out.println(fs.toString()); } @Testpublic void initHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); // 2 获取文件系统 FileSystem fs = FileSystem.get(configuration); // 3 打印文件系统 System.out.println(fs.toString());&#125; 3.2.2 HDFS文件上传 @Test public void putFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration configuration = new Configuration(); // 2 设置参数 // 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 3 创建要上传文件所在的本地路径 Path src = new Path(&quot;e:/hello.txt&quot;); // 4 创建要上传到hdfs的目标路径 Path dst = new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;); // 5 拷贝文件 fs.copyFromLocalFile(src, dst); fs.close(); &#125; ​ @Test public void putFileToHDFS() throws Exception{ // 1 创建配置信息对象 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration configuration = new Configuration(); // 2 设置参数 // 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 configuration.set(“dfs.replication”, “2”); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 3 创建要上传文件所在的本地路径 Path src = new Path(“e:/hello.txt”); // 4 创建要上传到hdfs的目标路径 Path dst = new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”); // 5 拷贝文件 fs.copyFromLocalFile(src, dst); fs.close(); } 2）将core-site.xml拷贝到项目的根目录下 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; fs.defaultFS hdfs://hadoop102:9000 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）将hdfs-site.xml拷贝到项目的根目录下 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt; dfs.replication 1 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 4）测试参数优先级 参数优先级： 1**、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3**、然后是服务器的默认配置 3.2.3 HDFS文件下载 @Test public void getFileFromHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // fs.copyToLocalFile(new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“d:/hello.txt”)); // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件效验 // 2 下载文件 fs.copyToLocalFile(false, new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“e:/hellocopy.txt”), true); fs.close(); } @Testpublic void getFileFromHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // fs.copyToLocalFile(new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;d:/hello.txt&quot;)); // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件效验 // 2 下载文件 fs.copyToLocalFile(false, new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;e:/hellocopy.txt&quot;), true); fs.close(); &#125; 3.2.4 HDFS目录创建​ @Test public void mkdirAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 创建目录 fs.mkdirs(new Path(“hdfs://hadoop102:9000/user/atguigu/output”)); } @Testpublic void mkdirAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 创建目录 fs.mkdirs(new Path(&quot;hdfs://hadoop102:9000/user/kingge/output&quot;));&#125; 3.2.5 HDFS文件夹删除@Testpublic void deleteAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 删除文件夹 ，如果是非空文件夹，参数2是否递归删除，true递归 fs.delete(new Path(&quot;hdfs://hadoop102:9000/user/kingge/output&quot;), true);&#125; ​ @Test public void deleteAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 删除文件夹 ，如果是非空文件夹，参数2是否递归删除，true递归 fs.delete(new Path(“hdfs://hadoop102:9000/user/atguigu/output”), true); } 3.2.6 HDFS文件名更改​ @Test public void renameAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); //2 重命名文件或文件夹 fs.rename(new Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), new Path(“hdfs://hadoop102:9000/user/atguigu/hellonihao.txt”)); } @Testpublic void renameAtHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); //2 重命名文件或文件夹 fs.rename(new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;hdfs://hadoop102:9000/user/kingge/hellonihao.txt&quot;));&#125; 3.2.7 HDFS文件详情查看查看文件名称、权限、长度、块信息-不是文件夹，是文件 @Test public void readListFiles() throws Exception { // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 思考：为什么返回迭代器，而不是List之类的容器 RemoteIterator listFiles = fs.listFiles(new Path(“/“), true); while (listFiles.hasNext()) { LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) { System.out.println(“block-offset:” + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) { System.out.println(host); } } System.out.println(“————–李冰冰的分割线————–”); } } @Testpublic void readListFiles() throws Exception &#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 思考：为什么返回迭代器，而不是List之类的容器 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName()); System.out.println(fileStatus.getBlockSize()); System.out.println(fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation bl : blockLocations) &#123; System.out.println(&quot;block-offset:&quot; + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(&quot;--------------李冰冰的分割线--------------&quot;); &#125; &#125; 3.2.8 HDFS文件和文件夹判断 不支持递归，只能查询当前某个目录下的文件或者或者文件夹，然后判断 @Testpublic void findAtHDFS() throws Exception, IllegalArgumentException, IOException&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 获取查询路径下的文件状态信息 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); // 3 遍历所有文件状态 for (FileStatus status : listStatus) &#123; if (status.isFile()) &#123; System.out.println(&quot;f--&quot; + status.getPath().getName()); &#125; else &#123; System.out.println(&quot;d--&quot; + status.getPath().getName()); &#125; &#125;&#125; @Test public void findAtHDFS() throws Exception, IllegalArgumentException, IOException{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 获取查询路径下的文件状态信息 FileStatus[] listStatus = fs.listStatus(new Path(“/“)); // 3 遍历所有文件状态 for (FileStatus status : listStatus) { if (status.isFile()) { System.out.println(“f–” + status.getPath().getName()); } else { System.out.println(“d–” + status.getPath().getName()); } } } 3.3 通过IO流操作HDFS3.3.1 HDFS文件上传​ @Test public void putFileToHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 创建输入流 FileInputStream inStream = new FileInputStream(new File(“e:/hello.txt”)); // 3 获取输出路径 String putFileName = “hdfs://hadoop102:9000/user/atguigu/hello1.txt”; Path writePath = new Path(putFileName); // 4 创建输出流 FSDataOutputStream outStream = fs.create(writePath); // 5 流对接 try{ IOUtils.copyBytes(inStream, outStream, 4096, false); }catch(Exception e){ e.printStackTrace(); }finally{ IOUtils.closeStream(inStream); IOUtils.closeStream(outStream); } } @Testpublic void putFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 创建输入流 FileInputStream inStream = new FileInputStream(new File(&quot;e:/hello.txt&quot;)); // 3 获取输出路径 String putFileName = &quot;hdfs://hadoop102:9000/user/kingge/hello1.txt&quot;; Path writePath = new Path(putFileName); // 4 创建输出流 FSDataOutputStream outStream = fs.create(writePath); // 5 流对接 try&#123; IOUtils.copyBytes(inStream, outStream, 4096, false); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; IOUtils.closeStream(inStream); IOUtils.closeStream(outStream); &#125;&#125; 3.3.2 HDFS文件下载​ @Test public void getFileToHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration, “atguigu”); // 2 获取读取文件路径 String filename = “hdfs://hadoop102:9000/user/atguigu/hello1.txt”; // 3 创建读取path Path readPath = new Path(filename); // 4 创建输入流 FSDataInputStream inStream = fs.open(readPath); // 5 流对接输出到控制台 try{ IOUtils.copyBytes(inStream, System.out, 4096, false); }catch(Exception e){ e.printStackTrace(); }finally{ IOUtils.closeStream(inStream); } } @Testpublic void getFileToHDFS() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;); // 2 获取读取文件路径 String filename = &quot;hdfs://hadoop102:9000/user/kingge/hello1.txt&quot;; // 3 创建读取path Path readPath = new Path(filename); // 4 创建输入流 FSDataInputStream inStream = fs.open(readPath); // 5 流对接输出到控制台 try&#123; IOUtils.copyBytes(inStream, System.out, 4096, false); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; IOUtils.closeStream(inStream); &#125;&#125; 3.3.3 定位文件读取1）下载第一块 @Test // 定位下载第一块内容 public void readFileSeek1() throws Exception { // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”), configuration, “atguigu”); // 2 获取输入流路径 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/tmp/hadoop-2.7.2.tar.gz”); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(“e:/hadoop-2.7.2.tar.gz.part1”); // 5 流对接 byte[] buf = new byte[1024]; for (int i = 0; i &lt; 128 1024; i++) { fis.read(buf); fos.write(buf); } // 6 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream*(fos); } @Test// 定位下载第一块内容public void readFileSeek1() throws Exception &#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kingge&quot;); // 2 获取输入流路径 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/tmp/hadoop-2.7.2.tar.gz&quot;); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;); // 5 流对接 byte[] buf = new byte[1024]; for (int i = 0; i &lt; 128 * 1024; i++) &#123; fis.read(buf); fos.write(buf); &#125; // 6 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos); &#125; 2）下载第二块 ​ @Test // 定位下载第二块内容 public void readFileSeek2() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”), configuration, “atguigu”); // 2 获取输入流路径 Path path = new Path(“hdfs://hadoop102:9000/user/atguigu/tmp/hadoop-2.7.2.tar.gz”); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(“e:/hadoop-2.7.2.tar.gz.part2”); // 5 定位偏移量（第二块的首位） fis.seek(1024 1024 128); // 6 流对接 IOUtils.copyBytes(fis, fos, 1024); // 7 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } @Test// 定位下载第二块内容public void readFileSeek2() throws Exception&#123; // 1 创建配置信息对象 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kingge&quot;); // 2 获取输入流路径 Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/tmp/hadoop-2.7.2.tar.gz&quot;); // 3 打开输入流 FSDataInputStream fis = fs.open(path); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;); // 5 定位偏移量（第二块的首位） fis.seek(1024 * 1024 * 128); // 6 流对接 IOUtils.copyBytes(fis, fos, 1024); // 7 关闭流 IOUtils.closeStream(fis); IOUtils.closeStream(fos);&#125; 3）合并文件 在window命令窗口中执行 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 解压，发现，就是我们上传的压缩文件。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kingge.top/tags/HDFS/"}]},{"title":"hadoop大数据(四)-Hadoop编译源码","slug":"hadoop大数据-四-Hadoop编译源码","date":"2018-02-28T11:31:59.000Z","updated":"2019-08-25T04:43:10.303Z","comments":true,"path":"2018/02/28/hadoop大数据-四-Hadoop编译源码/","link":"","permalink":"http://kingge.top/2018/02/28/hadoop大数据-四-Hadoop编译源码/","excerpt":"","text":"五 Hadoop编译源码5.1 前期准备工作1）CentOS联网 配置CentOS能连接外网。Linux虚拟机ping www.baidu.com 是畅通的 注意：采用root角色编译，减少文件夹权限出现问题 2）jar包准备(hadoop源码、JDK7 、 maven、 ant 、protobuf) （1）hadoop-2.7.2-src.tar.gz （2）jdk-7u79-linux-x64.gz （3）apache-ant-1.9.9-bin.tar.gz （4）apache-maven-3.0.5-bin.tar.gz （5）protobuf-2.5.0.tar.gz 5.2 jar包安装0）注意：所有操作必须在root用户下完成 1）JDK解压、配置环境变量 JAVA_HOME和PATH，验证java-version(如下都需要验证是否配置成功) [root@hadoop101 software] # tar -zxf jdk-7u79-linux-x64.gz -C /opt/module/ [root@hadoop101 software]# vi /etc/profile JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：java -version 2）Maven解压、配置 MAVEN_HOME和PATH。 [root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile MAVEN_HOME export MAVEN_HOME=/opt/module/apache-maven-3.0.5 export PATH=$PATH:$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn -version 3）ant解压、配置 ANT _HOME和PATH。 [root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-ant-1.9.9]# vi /etc/profile ANT_HOME export ANT_HOME=/opt/module/apache-ant-1.9.9 export PATH=$PATH:$ANT_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：ant -version 4）安装 glibc-headers 和 g++ 命令如下: [root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers [root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++ 5）安装make和cmake [root@hadoop101 apache-ant-1.9.9]# yum install make [root@hadoop101 apache-ant-1.9.9]# yum install cmake 6）解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0 然后相继执行命令： [root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/ [root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/ [root@hadoop101 protobuf-2.5.0]#./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make check [root@hadoop101 protobuf-2.5.0]# make install [root@hadoop101 protobuf-2.5.0]# ldconfig [root@hadoop101 hadoop-dist]# vi /etc/profile LD_LIBRARY_PATH export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0 export PATH=$PATH:$LD_LIBRARY_PATH [root@hadoop101 software]#source /etc/profile 验证命令：protoc –version 7）安装openssl库 [root@hadoop101 software]#yum install openssl-devel 8）安装 ncurses-devel库： [root@hadoop101 software]#yum install ncurses-devel 到此，编译工具安装基本完成。 5.3 编译源码1）解压源码到/opt/tools目录 [root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/ 2）进入到hadoop源码主目录 [root@hadoop101 hadoop-2.7.2-src]# pwd /opt/hadoop-2.7.2-src 3）通过maven执行编译命令 [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 等待时间30分钟左右，最终成功是全部SUCCESS。 4）成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下。 [root@hadoop101 target]# pwd /opt/hadoop-2.7.2-src/hadoop-dist/target 5.4 常见的问题及解决方案1）MAVEN install时候JVM内存溢出 处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method） 2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）： [root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar 3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐 2.7.0版本的问题汇总帖子 http://www.tuicool.com/articles/IBn63qf","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(三)-Hadoop三种部署和运行方式","slug":"hadoop大数据-三-Hadoop三种部署和运行方式","date":"2018-02-26T15:21:59.000Z","updated":"2019-06-09T02:46:10.040Z","comments":true,"path":"2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/","link":"","permalink":"http://kingge.top/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/","excerpt":"","text":"四 Hadoop运行模式1）官方网址 （1）官方网站： http://hadoop.apache.org/ （2）各个版本归档库地址 https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/ （3）hadoop2.7.2版本详情介绍 http://hadoop.apache.org/docs/r2.7.2/ 2）Hadoop运行模式 （1）本地模式（默认模式）： 不需要启用单独进程，直接可以运行，测试和开发时使用。 （2）伪分布式模式： 等同于完全分布式，只有一个节点。 （3）完全分布式模式： 多个节点一起运行。 4.1 本地运行Hadoop 案例4.1.1 官方grep案例1）创建在hadoop-2.7.2文件下面创建一个input文件夹 [kingge@hadoop101 hadoop-2.7.2]$mkdir input 2）将hadoop的xml配置文件复制到input [kingge@hadoop101 hadoop-2.7.2]$cp etc/hadoop/*.xml input 3）执行share目录下的mapreduce程序 [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output ‘dfs[a-z.]+’ 4）查看输出结果 [kingge@hadoop101 hadoop-2.7.2]$ cat output/* 4.1.2 官方wordcount案例1）创建在hadoop-2.7.2文件下面创建一个wcinput文件夹 [kingge@hadoop101 hadoop-2.7.2]$mkdir wcinput 2）在wcinput文件下创建一个wc.input文件 [kingge@hadoop101 hadoop-2.7.2]$cd wcinput [kingge@hadoop101 wcinput]$touch wc.input 3）编辑wc.input文件 [kingge@hadoop101 wcinput]$vim wc.input在文件中输入如下内容hadoop yarnhadoop mapreduce kinggekingge保存退出：：wq 4）回到hadoop目录/opt/module/hadoop-2.7.2 5）执行程序： [kingge@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput 6）查看结果： [kingge@hadoop101 hadoop-2.7.2]$cat wcoutput/part-r-00000 kingge 2 hadoop 2 mapreduce 1 yarn 1 4.1.3 总结第一个案例：统计input文件里面的文件，文件内容包含’dfs[a-z.]+’ 规则的文字，筛选出来。 第二个案例：统计wc.input 文件中单词出现的个数，特别注意，第二次运行之前必须删除已有的结果输出目录（wcoutput）（rm -rf wcoutput/），否则执行wordcount指令就会报错，提示文件已经存在 —- 操作设计的文件都是存储在linux 的文件系统中。本地操作，不支持联网操作 4.2 伪分布式运行Hadoop案例4.2.1 启动HDFS并运行MapReduce程序1）分析： ​ （1）准备1台客户机 ​ （2）安装jdk ​ （3）配置环境变量 ​ （4）安装hadoop ​ （5）配置环境变量 ​ （6）配置集群 ​ （7）启动、测试集群增、删、查 ​ （8）执行wordcount案例 2）执行步骤 需要配置hadoop文件如下 （1）配置集群 （a）配置：hadoop-env.sh ​ 1.Linux系统中获取jdk的安装路径： [kingge@hadoop100 hadoop-2.7.2]$ cd etc/hadoop/ [root@ hadoop101 ~]# echo $JAVA_HOME /opt/module/jdk1.8.0_144 ​ 2.修改Jhadoop-env.sh的JAVA_HOME 路径： export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置：core-site.xml &lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; （c）配置：hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; （2）启动集群（a）格式化namenode（第一次启动时格式化，以后就不要总格式化） ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs namenode –format ​ 会在上面配置配置的存储目录生成 这两个文件 （b）启动namenode ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode ​ c）启动datanode ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode （3）查看集群​ （a）查看是否启动成功 [kingge@hadoop101 hadoop-2.7.2]$ jps 13586 NameNode 13668 DataNode 13786 Jps ​ （b）查看产生的log日志 当前目录：/opt/module/hadoop-2.7.2/logs [kingge@hadoop101 logs]$ ls hadoop-kingge-datanode-hadoop.kingge.com.log hadoop-kingge-datanode-hadoop.kingge.com.out hadoop-kingge-namenode-hadoop.kingge.com.log hadoop-kingge-namenode-hadoop.kingge.com.out SecurityAuth-root.audit [kingge@hadoop101 logs]# cat hadoop-kingge-datanode-hadoop101.log ​ （c）web端查看HDFS文件系统 ​ http://192.168.1.101:50070/dfshealth.html#tab-overview ​ 注意：如果不能查看，看如下帖子处理 http://www.cnblogs.com/zlslch/p/6604189.html （4）操作集群​ （a）在hdfs文件系统上创建一个input文件夹 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/kingge/input ​ （b）将测试文件内容上传到文件系统上 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -put wcinput/wc.input /user/kingge/input/ ​ （c）查看上传的文件是否正确 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -ls /user/kingge/input/ [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/ input/wc.input ​ （d）运行mapreduce程序(所有数据在HDFS上) ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input/ /user/kingge/output ​ （e）查看输出结果 命令行查看： [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/output/* 浏览器查看 ​ （f）将测试文件内容下载到本地 [kingge@hadoop101 hadoop-2.7.2]$ hadoop fs -get /user/kingge/ output/part-r-00000 ./wcoutput/ （g）删除输出结果 [kingge@hadoop101 hadoop-2.7.2]$ hdfs dfs -rmr /user/kingge/output 4.2.2 YARN上运行MapReduce 程序1）分析：​ （1）准备1台客户机 ​ （2）安装jdk ​ （3）配置环境变量 ​ （4）安装hadoop ​ （5）配置环境变量 ​ （6）配置集群yarn上运行 ​ （7）启动、测试集群增、删、查 ​ （8）在yarn上执行wordcount案例 2）执行步骤（1）配置集群 ​（a）配置yarn-env.sh ​ 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 （b）配置yarn-site.xml &lt;!-- reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt; ​ （c）配置：mapred-env.sh ​ 配置一下JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ （d）配置： (对mapred-site.xml.template重新命名为) mapred-site.xml [kingge@hadoop101 hadoop]$ mv mapred-site.xml.template mapred-site.xml [kingge@hadoop101 hadoop]$ vi mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; ​ （2）启动集群 （a）启动resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager （b）启动nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager ​ （3）集群操作 （a）yarn的浏览器页面查看 http://192.168.1.101:8088/cluster ​ （b）删除文件系统上的output文件 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/kingge/output ​ （c）执行mapreduce程序 ​ [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input /user/kingge/output ​ （d）查看运行结果 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/output/* 4.2.3 配置临时文件存储路径1）停止进程 [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop namenode 2）修改hadoop.tmp.dir ​ [core-site.xml] &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 3）将/opt/module/hadoop-2.7.2路径中的logs文件夹删除掉 [kingge@hadoop101 hadoop-2.7.2]$ rm -rf logs/ 4）进入到tmp目录将tmp目录中hadoop-kingge目录删除掉 [kingge@hadoop101 tmp]$ rm -rf hadoop-kingge/ 5）格式化NameNode ​ [kingge@hadoop101 hadoop-2.7.2]$ hadoop namenode -format 6）启动所有进程 ​ [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager ​ 7）查看/opt/module/hadoop-2.7.2/data/tmp这个目录下的内容。 4.2.4 配置历史服务器​ 1）配置mapred-site.xml [kingge@hadoop101 hadoop]$ vi mapred-site.xml &lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt; ​ 2）查看启动历史服务器文件目录： [kingge@hadoop101 hadoop-2.7.2]$ ls sbin/ | grep mr mr-jobhistory-daemon.sh ​ 3）启动历史服务器 [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver ​ 4）查看历史服务器是否启动 ​ [kingge@hadoop101 hadoop-2.7.2]$ jps ​ 5）查看jobhistory http://192.168.1.101:19888/jobhistory 4.2.5 配置日志的聚集日志聚集概念：应用运行完成以后，将日志信息上传到HDFS系统上。 开启日志聚集功能步骤： （1）配置yarn-site.xml [kingge@hadoop101 hadoop]$ vi yarn-site.xml &lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; （2）关闭nodemanager 、resourcemanager和historymanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh stop historyserver （3）启动nodemanager 、resourcemanager和historymanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager [kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver （4）删除hdfs上已经存在的hdfs文件 [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/kingge/output （5）执行wordcount程序 [kingge@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input /user/kingge/output （6）查看日志 http://192.168.1.101:19888/jobhistory 4.2.6 配置文件说明Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。 （1）默认配置文件：存放在hadoop相应的jar包中 [core-default.xml] ​ hadoop-common-2.7.2.jar/ core-default.xml ​ [hdfs-default.xml] hadoop-hdfs-2.7.2.jar/ hdfs-default.xml ​ [yarn-default.xml] hadoop-yarn-common-2.7.2.jar/ yarn-default.xml ​ [core-default.xml] hadoop-mapreduce-client-core-2.7.2.jar/ core-default.xml ​ （2）自定义配置文件：存放在$HADOOP_HOME/etc/hadoop ​ core-site.xml ​ hdfs-site.xml ​ yarn-site.xml ​ mapred-site.xml 4.2.7 总结相比于本地运行模式，伪分布式模式支持互联网操作，不过集群的副本是1（不配置的话默认是3，详情查看hdfs-default.xml的dfs.replication属性）。 4.3 完全分布式部署Hadoop分析： ​ 1）准备3台客户机（关闭防火墙、静态ip、主机名称） ​ 2）安装jdk ​ 3）配置环境变量 ​ 4）安装hadoop ​ 5）配置环境变量 ​ 6）安装ssh ​ 7）配置集群 ​ 8）启动测试集群 4.3.1 虚拟机准备详见3.2-3.3章。 Hadoop运行环境搭建3.2-3.3 4.3.2 主机名设置Hadoop运行环境搭建3.4 4.3.3 scp1）scp可以实现服务器与服务器之间的数据拷贝。 操作一：hadoop101 主动推送数据到hadoop102 操作二：hadoop102主动从hadoop101获取数据到本地 操作三：hadoop101 控制将hadoop102的数据拷贝到hadoop103 ​ 接收方一般使用root**用户接受，因为有些文件夹的权限只有root用户才有，为保证传输成功，双方最好都切换到root用户** 2）案例实操 （1）将hadoop101中/opt/module和/opt/software文件拷贝到hadoop102、hadoop103和hadoop104上。 [root@hadoop101 /]# scp -r /opt/module/ root@hadoop102:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop102:/opt [root@hadoop101 /]# scp -r /opt/module/ root@hadoop103:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop103:/opt [root@hadoop101 /]# scp -r /opt/module/ root@hadoop104:/opt [root@hadoop101 /]# scp -r /opt/software/ root@hadoop105:/opt （2）将hadoop101服务器上的/etc/profile文件拷贝到hadoop102上。 [root@hadoop102 opt]# scp root@hadoop101:/etc/profile /etc/profile 例子1 [root@hadoop102 opt]# scp -r root@192.168.1.101:/opt/module/ /opt/–例子二 ​ （3）实现两台远程机器之间的文件传输（hadoop103主机文件拷贝到hadoop104主机上） ​ [kingge@hadoop102 test]$ scp kingge@hadoop103:/opt/test/haha kingge@hadoop104:/opt/test/ 注意：如果传递环境变量配置文件后需要source /etc/profile 一下，让其生效。同时可能需要修改一下文件的权限或者文件所属（chmod chown） 4.3.4 SSH无密码登录 针对执行ssh命令的 无密码操作 1）配置ssh （1）基本语法 ssh 另一台电脑的ip地址 （2）ssh连接时出现Host key verification failed的解决方法 [root@hadoop102 opt]# ssh 192.168.1.103 The authenticity of host ‘192.168.1.103 (192.168.1.103)’ can’t be established. RSA key fingerprint is cf:1e:de:d7:d0:4c:2d:98:60:b4:fd:ae:b1:2d:ad:06. Are you sure you want to continue connecting (yes/no)? Host key verification failed. （3）解决方案如下：直接输入yes 2）无密钥配置 （1）进入到我的home目录 ​ [kingge@hadoop102 opt]$ cd ~/.ssh （2）生成公钥和私钥： [kingge@hadoop102 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） （3）将公钥拷贝到要免密登录的目标机器上 [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop102 （给自己授权免密登录） [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop103 [kingge@hadoop102 .ssh]$ ssh-copy-id hadoop104 查看103或者104的~/.ssh，发现多了authorized_keys 个文件 需求： A服务器需要访问B服务器，不需要输入密码 3）.ssh文件夹下的文件功能解释 ​ （1）~/.ssh/known_hosts ：记录ssh访问过计算机的公钥(public key) ​ （2）id_rsa ：生成的私钥 ​ （3）id_rsa.pub ：生成的公钥 ​ （4）authorized_keys ：存放授权过得无秘登录服务器公钥 注意： 如果你授权的免密登录用户，被切换了，那么还是需要输入密码才能够登录。 例子：hadoop100服务器的kingge用户生成了密匙，然后发给你了hadoop101，那么100服务器就可以免密登录101服务器，但是假设100服务器su root（切换为了root用户），那么当执行ssh hadoop101 操作时，就需要输入101服务器密码，而不能免密登录，所以要想在root用户下也能免密登录101服务器，就需要在root用户下重新走一遍免密登录流程 4.3.5 rsyncrsync远程同步工具，主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync**和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp**是把所有文件都复制过去。 （1）查看rsync使用说明 man rsync | more ​ （2）基本语法 rsync -rvl $pdir/$fname $user@hadoop$host:$pdir ​ 命令 命令参数 要拷贝的文件路径/名称 目的用户@主机:目的路径 ​ 选项 -r 递归 -v 显示复制过程 -l 拷贝符号连接 ​ （3）案例实操 ​ 把本机/opt/tmp目录同步到hadoop103服务器的root用户下的/opt/tmp目录 [kingge@hadoop102 opt]$ rsync -rvl /opt/tmp root@hadoop103:/opt/ 4.3.6 编写集群分发脚本xsync 场景：分布式系统中假设有6900**台服务器，那么假设我们需要同步配置信息，我们不可能一台台的执行scp/rsync 命令，效率极低，那怎么办呢？** 1）需求分析：循环复制文件到所有节点的相同目录下。 ​ （1）原始拷贝： rsync -rvl /opt/module root@hadoop103:/opt/ ​ （2）期望脚本： xsync 要同步的文件名称 ​ （3）在/usr/local/bin这个目录下存放的脚本，可以在系统任何地方直接执行。（就是在执行xsync命令时可以不用输入/usr/local/bin这样前缀） 2）案例实操： （1）在/usr/local/bin目录下创建xsync文件，文件内容如下： [root@hadoop102 bin]# touch xsync [root@hadoop102 bin]# vi xsync #!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do #echo $pdir/$fname $user@hadoop$host:$pdir echo --------------- hadoop$host ---------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone （2）修改脚本 xsync 具有执行权限 [root@hadoop102 bin]# chmod 777 xsync [root@hadoop102 bin]# chown kingge:kingge -R xsync （3）调用脚本形式：xsync 文件名称 [kingge@hadoop102 opt]$ xsync tmp/ 4.3.7 编写集群操作脚本xcall1）需求分析：在所有主机上同时执行相同的命令 xcall +命令 2）具体实现 （1）在/usr/local/bin目录下创建xcall文件，文件内容如下： [root@hadoop102 bin]# touch xcall [root@hadoop102 bin]# vi xcall #!/bin/bashpcount=$#if((pcount==0));then echo no args; exit;fiecho -------------localhost----------$@for((host=101; host&lt;=108; host++)); do echo ----------hadoop$host--------- ssh hadoop$host $@done （2）修改脚本xcall具有执行权限 ​ [root@hadoop102 bin]# chmod 777 xcall [root@hadoop102 bin]# chown kingge:kingge xcall （3）调用脚本形式： xcall 操作命令 [root@hadoop102 ~]# xcall rm -rf /opt/tmp/ 4.3.8 配置集群1）集群部署规划 hadoop102 hadoop103 hadoop104 HDFS NameNode DataNode DataNode SecondaryNameNode DataNode YARN NodeManager ResourceManager NodeManager NodeManager 集群规划的原则：NameNode/SecondaryNameNode/ ResourceManager必须要单独占据一个服务器(或者这三个不能在同一个节点上运行)，因为他是相当于目录，请求他的次数最大，所以不能跟其他插件部署在一起，不能跟NameNode抢占资源，因为他不能挂掉。Datanode就没有这些限制，挂掉也无所谓。 但是下面的例子中NameNode和DataNode都部署在hadoop102这个节点，因为我们是属于测试搭建环境下，所以无所谓，但是生产环境下必须按照规则 2）配置文件​ （1）core-site.xml [kingge@hadoop102 hadoop]$ vi core-site.xml &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; ​ （2）Hdfs ​ 2.1 hadoop-env.sh [kingge@hadoop102 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ 2.2 hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;# 如果不配置。默认是跟namenode同个位置 &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ​ 2.3 Slaves ​ 配置文件里面不能存在多余的空格或者换行 [kingge@hadoop102 hadoop]$ vi slaves hadoop102 hadoop103 hadoop104 ​ （3）yarn ​ yarn-env.sh [kingge@hadoop102 hadoop]$ vi yarn-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ yarn-site.xml ​ [kingge@hadoop102 hadoop]$ vi yarn-site.xml &lt;configuration&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ​ （4）mapreduce ​ mapred-env.sh [kingge@hadoop102 hadoop]$ vi mapred-env.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 ​ mapred-site.xml [kingge@hadoop102 hadoop]$ vi mapred-site.xml &lt;configuration&gt; &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）在集群上分发以上所有文件[kingge@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml [kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/slaves 4）查看文件分发情况 [kingge@hadoop102 hadoop]$ xcall cat /opt/module/hadoop-2.7.2/etc/hadoop/slaves 4.3.9 集群启动及测试1）启动集群 ​ 清空之前启动namenode的数据 ​ Rm –rf data/ log/ ​ Data就是在core-site.xml中配置的文件存储目录，log就是hadoop的log目录。 ​ （0）如果集群是第一次启动，需要格式化namenode ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -format （1）启动HDFS： [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh –这一步跟我们之前的单个启动不一样（sbin/hadoop-daemon.sh start namenode**），这个操作是启动整个集群的namenode**和datanode—那么就需要配置ssh 无密码登录 [kingge@hadoop102 hadoop-2.7.2]$ jps 4166 NameNode 4482 Jps 4263 DataNode [kingge@hadoop103 hadoop-2.7.2]$ jps 3218 DataNode 3288 Jps [kingge@hadoop104 hadoop-2.7.2]$ jps 3221 DataNode 3283 SecondaryNameNode 3364 Jps （2）启动yarn （启动namemanager 和 resourceManager） [kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh 注意：Namenode和ResourceManger如果不是同一台机器，不能在NameNode上启动 yarn，应该在ResouceManager所在的机器上启动yarn。 因为我们在这个集群中ResourceManager是在hadoop103上面的，所以在103上启动 2）集群基本测试 （1）上传文件到集群 ​ 1.上传小文件 ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/kingge/input ​ [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -put etc/hadoop/wc.input /user/kingge/input 上传完后 hadoop103和hadoop104 上面也会同步副本（即是，也会存在上面的/user/kingge/tep/conf 和 *-site.xml 这些东西） 查看HDFS系统的文件结构 我们可以看到他是存储在了块0，而且有三个副本101/102/103 ​ 接下来我们再上传一个大文件看看 ​ 2.上传大文件 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -put /opt/software/hadoop-2.7.2.tar.gz /user/kingge/input 为什么他这里会分为两块存储呢？因为默认块大小是128，当超过这个大小时就需要分块存储 （2）上传文件后查看文件存放在什么位置 ​ 文件存储路径 ​ [kingge@hadoop102 subdir0]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-938951106-192.168.10.107-1495462844069/current/finalized/subdir0/subdir0 ​ 查看文件内容(wc.input) [kingge@hadoop102 subdir0]$ cat blk_1073741825 hadoop kingge kingge 然后26 27 就是那个大文件，分为了两块 （3）拼接 -rw-rw-r–. 1 kingge kingge 134217728 5月 23 16:01 blk_1073741836 -rw-rw-r–. 1 kingge kingge 1048583 5月 23 16:01 blk_1073741836_1012.meta -rw-rw-r–. 1 kingge kingge 63439959 5月 23 16:01 blk_1073741837 -rw-rw-r–. 1 kingge kingge 495635 5月 23 16:01 blk_1073741837_1013.meta [kingge@hadoop102 subdir0]$ cat blk_1073741836&gt;&gt;tmp.file [kingge@hadoop102 subdir0]$ cat blk_1073741837&gt;&gt;tmp.file [kingge@hadoop102 subdir0]$ tar -zxvf tmp.file 已解压发现就是我们上传那个tar**大文件的解压版本，说明这两个文件就是存放着大文件** （4）下载 [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -get /user/kingge/input/hadoop-2.7.2.tar.gz 3）性能测试集群 ​ 写海量数据 ​ 读海量数据 4.3.10 Hadoop启动停止方式1）各个服务组件逐一启动 ​ （1）分别启动hdfs组件 ​ hadoop-daemon.sh start|stop namenode|datanode|secondarynamenode ​ （2）启动yarn ​ yarn-daemon.sh start|stop resourcemanager|nodemanager 2）各个模块分开启动（配置ssh是前提）常用 ​ （1）整体启动/停止hdfs ​ start-dfs.sh ​ stop-dfs.sh ​ （2）整体启动/停止yarn ​ start-yarn.sh ​ stop-yarn.sh 3）全部启动（不建议使用） ​ start-all.sh ​ stop-all.sh 4.3.11 集群时间同步时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 配置时间同步实操： 1）时间服务器配置（必须root用户） （1）检查ntp是否安装 [root@hadoop102 桌面]# rpm -qa|grep ntp ntp-4.2.6p5-10.el6.centos.x86_64 fontpackages-filesystem-1.41-1.1.el6.noarch ntpdate-4.2.6p5-10.el6.centos.x86_64 （2）修改ntp配置文件 [root@hadoop102 桌面]# vi /etc/ntp.conf 修改内容如下 a）修改1 #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap为 restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap ​ b）修改2 注释时间服务器 server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst为 #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst ​ c）添加3 自己的时间服务器 server 127.127.1.0 fudge 127.127.1.0 stratum 10 （3）修改/etc/sysconfig/ntpd 文件 [root@hadoop102 桌面]# vim /etc/sysconfig/ntpd 增加内容如下 SYNC_HWCLOCK=yes ​ （4）重新启动ntpd [root@hadoop102 桌面]# service ntpd status ntpd 已停 [root@hadoop102 桌面]# service ntpd start 正在启动 ntpd： [确定] ​ （5）执行： ​ [root@hadoop102 桌面]# chkconfig ntpd on 2）其他机器配置（必须root用户） ​ （1）在其他机器配置10分钟与时间服务器同步一次 ​ [root@hadoop103 hadoop-2.7.2]# crontab -e ​ 编写脚本 ​ /10 * /usr/sbin/ntpdate hadoop102 ​ （2）修改任意机器时间 ​ [root@hadoop103 hadoop]# date -s “2017-9-11 11:11:11” ​ （3）十分钟后查看机器是否与时间服务器同步 ​ [root@hadoop103 hadoop]# date 4.3.12 配置集群常见问题1）防火墙没关闭、或者没有启动yarn INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032 2）主机名称配置错误 3）ip地址配置错误 4）ssh没有配置好 5）root用户和kingge两个用户启动集群不统一 6）配置文件修改不细心 7）未编译源码 Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 17/05/22 15:38:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032 8）datanode不被namenode识别问题 Namenode在format初始化的时候会形成两个标识，blockPoolId和clusterId。新的datanode加入时，会获取这两个标识作为自己工作目录中的标识。 一旦namenode重新format后，namenode的身份标识已变，而datanode如果依然持有原来的id，就不会被namenode识别。 解决办法，删除datanode节点中的数据后，再次重新格式化namenode。 9）不识别主机名称 java.net.UnknownHostException: hadoop102: hadoop102 at java.net.InetAddress.getLocalHost(InetAddress.java:1475) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:146) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) 解决办法： （1）在/etc/hosts文件中添加192.168.1.102 hadoop102 ​ （2）主机名称不要起hadoop hadoop000等特殊名称 10）datanode和namenode进程同时只能工作一个。 11）执行命令 不生效，粘贴word中命令时，遇到-和长–没区分开。导致命令失效 解决办法：尽量不要粘贴word中代码。 12）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。 13）jps不生效。 原因：全局变量hadoop java没有生效，需要source /etc/profile文件。 14）8088端口连接不上 [kingge@hadoop102 桌面]$ cat /etc/hosts 注释掉如下代码 #127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 #::1 hadoop102","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(二)-运行环境搭建","slug":"hadoop大数据-二-运行环境搭建","date":"2018-02-24T02:21:59.000Z","updated":"2019-06-07T09:12:38.271Z","comments":true,"path":"2018/02/24/hadoop大数据-二-运行环境搭建/","link":"","permalink":"http://kingge.top/2018/02/24/hadoop大数据-二-运行环境搭建/","excerpt":"","text":"三 Hadoop运行环境搭建3.0 前置准备需要linux相关的知识，和安装虚拟机。所以还不了解的请看：Linux基础 3.1 虚拟机网络模式设置为NAT ​ 最后，重新启动系统。 ​ [root@hadoop101 ~]# sync ​ [root@hadoop101 ~]# reboot 3.2 克隆虚拟机1）克隆虚拟机 2）启动虚拟机 3.3 修改为静态ip1）在终端命令窗口中输入[root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules 进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址 2）修改IP地址[root@hadoop101 /]# vim /etc/sysconfig/network-scripts/ifcfg-eth0 需要修改的内容有5项： IPADDR=192.168.1.101 GATEWAY=192.168.1.2 ONBOOT=yes BOOTPROTO=static DNS1=192.168.1.2 ​ （1）修改前 ​ （2）修改后 ：wq 保存退出 3）执行[root@hadoop101 /]# service network restart 4）如果报错，reboot，重启虚拟机。​ [root@hadoop101 /]# reboot 3.4 修改主机名1）修改linux的hosts文件（1）进入Linux系统查看本机的主机名。通过hostname命令查看。 [root@hadoop100 /]# hostname hadoop100 （2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件。 [root@hadoop100~]# vi /etc/sysconfig/network 修改文件中主机名称 NETWORKING=yes NETWORKING_IPV6=no HOSTNAME= hadoop101 注意：主机名称不要有“_”下划线 （3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。 （4）保存退出。 （5）打开/etc/hosts [root@hadoop100 ~]# vim /etc/hosts 添加如下内容 192.168.1.100 hadoop100 192.168.1.101 hadoop101 192.168.1.102 hadoop102 192.168.1.103 hadoop103 192.168.1.104 hadoop104 192.168.1.105 hadoop105 192.168.1.106 hadoop106 192.168.1.107 hadoop107 192.168.1.108 hadoop108 192.168.1.109 hadoop109 192.168.1.110 hadoop110 （6）并重启设备，重启后，查看主机名，已经修改成功 2）修改window7的hosts文件(可以不改)只不过是方便于在windows服务器中，使用域名的方式访问hadopp相关的组件。 ​ （1）进入C:\\Windows\\System32\\drivers\\etc路径 ​ （2）打开hosts文件并添加如下内容 192.168.1.100 hadoop100 192.168.1.101 hadoop101 192.168.1.102 hadoop102 192.168.1.103 hadoop103 192.168.1.104 hadoop104 192.168.1.105 hadoop105 192.168.1.106 hadoop106 192.168.1.107 hadoop107 192.168.1.108 hadoop108 192.168.1.109 hadoop109 192.168.1.110 hadoop110 3.5 关闭防火墙1）查看防火墙开机启动状态 [root@hadoop101 ~]# chkconfig iptables –list 2）关闭防火墙 [root@hadoop101 ~]# chkconfig iptables off 3.6 在opt目录下创建文件1）创建kingge用户​ 在root用户里面执行如下操作 [root@hadoop101 opt]# adduser atguigu [root@hadoop101 opt]# passwd atguigu 更改用户 test 的密码 。 新的 密码： 无效的密码： 它没有包含足够的不同字符 无效的密码： 是回文 重新输入新的 密码： passwd： 所有的身份验证令牌已经成功更新。 2）设置kingge用户具有root权限修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： [root@hadoop101 kingge]# vi /etc/sudoers ## Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) ALL 修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。 3）在/opt目录下创建文件夹（1）在root用户下创建module、software文件夹 [root@hadoop101 opt]# mkdir module [root@hadoop101 opt]# mkdir software （2）修改module、software文件夹的所有者 [root@hadoop101 opt]# chown kingge:kingge module [root@hadoop101 opt]# chown kingge:kingge sofrware [root@hadoop101 opt]# ls -al 总用量 16 drwxr-xr-x. 6 root root 4096 4月 24 09:07 . dr-xr-xr-x. 23 root root 4096 4月 24 08:52 .. drwxr-xr-x. 4 kingge kingge 4096 4月 23 16:26 module drwxr-xr-x. 2 kingge kingge 4096 4月 23 16:25 software 3.7 安装jdk1）卸载现有jdk（1）查询是否安装java软件： [root@hadoop101 opt]# rpm -qa|grep java （2）如果安装的版本低于1.7，卸载该jdk： [root@hadoop101 opt]# rpm -e 软件包 2）复制文件用SecureCRT工具将jdk、Hadoop-2.7.2.tar.gz导入到opt目录下面的software文件夹下面 3）在linux系统下的opt目录中查看软件包是否导入成功。[root@hadoop101opt]# cd software/ [root@hadoop101software]# ls hadoop-2.7.2.tar.gz jdk-8u144-linux-x64.tar.gz 4）解压jdk到/opt/module目录下​ [root@hadoop101software]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/ 5）配置jdk环境变量​ （1）先获取jdk路径： [root@hadoop101 jdk1.8.0_144]# pwd /opt/module/jdk1.8.0_144 ​ （2）打开/etc/profile文件： [root@hadoop101 jdk1.8.0_144]# vi /etc/profile ​ 在profie文件末尾添加jdk路径： ​ ##JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin ​ （3）保存后退出： :wq ​ （4）让修改后的文件生效： [root@hadoop101 jdk1.8.0_144]# source /etc/profile ​ （5）重启（如果java -version可以用就不用重启）： [root@hadoop101 jdk1.8.0_144]# sync ​ [root@hadoop101 jdk1.8.0_144]# reboot 6）测试jdk安装成功[root@hadoop101 jdk1.8.0_144]# java -version java version “1.8.0_144” 3.8 安装Hadoop 这里使用的是已经编译过后的hadoop源码，官网下载的是非编译过后的，需要编译。如何编译参见。 Hadoop编译源码 1）进入到Hadoop安装包路径下： [root@hadoop101 ~]# cd /opt/software/ 2）解压安装文件到/opt/module下面 [root@hadoop101 software]# tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/ 3）查看是否解压成功 [root@hadoop101 software]# ls /opt/module/ hadoop-2.7.2 4）在/opt/module/hadoop-2.7.2/etc/hadoop路径下配置hadoop-env.sh （1）Linux系统中获取jdk的安装路径： [root@hadoop101 jdk1.8.0_144]# echo $JAVA_HOME /opt/module/jdk1.8.0_144 （2）修改hadoop-env.sh文件中JAVA_HOME 路径： [root@hadoop101 hadoop]# vi hadoop-env.sh 修改JAVA_HOME如下 export JAVA_HOME=/opt/module/jdk1.8.0_144 5）将hadoop添加到环境变量 （1）获取hadoop安装路径：[root@ hadoop101 hadoop-2.7.2]# pwd/opt/module/hadoop-2.7.2 （2）打开/etc/profile文件：[root@ hadoop101 hadoop-2.7.2]# vi /etc/profile 在profie文件末尾添加jdk路径：（shitf+g）\\##HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin （3）保存后退出：:wq （4）让修改后的文件生效：[root@ hadoop101 hadoop-2.7.2]# source /etc/profile（5）重启(如果hadoop命令不能用再重启)： [root@ hadoop101 hadoop-2.7.2]# sync [root@ hadoop101 hadoop-2.7.2]# reboot 6）修改/opt目录下的所有文件所有者为kingge ​ [root@hadoop101 opt]# chown kingge:kingge -R /opt/ 7）切换到kingge用户 ​ [root@hadoop101 opt]# su kingge","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"hadoop大数据(一)-理论知识了解","slug":"hadoop大数据-一-理论知识了解","date":"2018-02-20T16:31:59.000Z","updated":"2019-06-07T08:31:03.225Z","comments":true,"path":"2018/02/21/hadoop大数据-一-理论知识了解/","link":"","permalink":"http://kingge.top/2018/02/21/hadoop大数据-一-理论知识了解/","excerpt":"","text":"前言首先本人要先声明的是，这一系列的大数据总结，只是对于整个大数据生态的一个稍微深入的总结。并非是非常深入的，但是能够满足大部分人的需求。 一 大数据概念大数据的概念： 指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 这个概念的解释来源于百度百科，这样的解释太过空洞，那么就用更浅显易懂的解释是： 解决海量数据的存储和海量数据的分析计算 1.2 大数据的特点 图片来源于网上 1.3 大数据应用场景 4.给顾客推荐访问过的商品，我们使用淘宝之类的，当我们浏览某个商品之后，下次进来时，他会默认给你推荐你上次浏览过的商品。 第八：人工智能，目前最火的的风口 二 Hadoop框架2.1 Hadoop是什么1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构 2）主要解决，海量数据的存储和海量数据的分析计算问题（很明显只是一句废话，哈哈哈）。 3）广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈 2.2 Hadoop发展历史1）Lucene–Doug Cutting开创的开源软件，用java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎 2）2001年年底成为apache基金会的一个子项目 3）对于大数量的场景，Lucene面对与Google同样的困难 4）学习和模仿Google解决这些问题的办法 ：微型版Nutch 5）可以说Google是hadoop的思想之源(Google在大数据方面的三篇论文) ​ GFS —&gt;HDFS ​ Map-Reduce —&gt;MR ​ BigTable —&gt;Hbase 6）2003-2004年，Google公开了部分GFS和Mapreduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和Mapreduce机制，使Nutch性能飙升 7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。2006 年 3 月份，Map-Reduce和Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中 8）名字来源于Doug Cutting儿子的玩具大象 9）Hadoop就此诞生并迅速发展，标志这云计算时代来临 2.3 Hadoop三大发行版本Hadoop 三大发行版本: Apache、Cloudera、Hortonworks。 Apache版本最原始（最基础）的版本，对于入门学习最好。 Cloudera在大型互联网企业中用的较多。（因为他解决了hadoop各个版本和其他框架的兼容问题） Hortonworks文档较好。 1）Apache Hadoop 官网地址：http://hadoop.apache.org/releases.html 下载地址：https://archive.apache.org/dist/hadoop/common/ 2）Cloudera Hadoop 官网地址：https://www.cloudera.com/downloads/cdh/5-10-0.html 下载地址：http://archive-primary.cloudera.com/cdh5/cdh/5/ （1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。 （2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support （3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。 （4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。 （5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。 3）Hortonworks Hadoop 官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform （1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。 （2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。 （3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。 （4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。 （5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 （6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的microsoft Windows平台上本地运行。定价以集群为基础，每10个节点每年为12500美元。 2.4 Hadoop的优势1）高可靠性：因为Hadoop假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理。2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度（根据文件快开启相应数量的map任务处理，reduce的并行数量是可控的）。4）高容错性：自动保存多份副本数据，并且能够自动将失败的任务重新分配。 2.5 Hadoop组成2.5.0 四个组成1）Hadoop HDFS：一个高可靠、高吞吐量的分布式文件系统。2）Hadoop MapReduce：一个分布式的离线并行计算框架。3）Hadoop YARN：作业调度与集群资源管理的框架。4）Hadoop Common：支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作）。 2.5.1 HDFS 组成： namenode：存储文件元数据。例如文件名称，文件目录结构，文件属性（生成时间、副本数，文件权限），以及每个文件的快列表和快所在的datanode。hdfs的文件目录维护中心。datanode：真正存储文件的单位，也就是统称的块。secondary namenode：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS的元数据的快照。帮助namenode合并镜像文件和操作日志，合并完成后同步给namenode，减少namenode的处理任务的压力。他不能够替代namenode，只能够做备份（方便namenode意外挂掉后，能够恢复数据） 2.5.2 YARN1）ResourceManager(rm)：处理客户端请求、启动/监控ApplicationMaster、监控NodeManager、资源分配与调度； 2）NodeManager(nm)：单个节点上的资源管理、处理来自ResourceManager的命令、处理来自ApplicationMaster的命令； 3）ApplicationMaster：数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错。 4）Container：对任务运行环境的抽象，封装了CPU、内存等多维资源以及环境变量、启动命令等任务运行相关的信息。 2.5.3 MapReduce主要是获取HDFS存储的数据，通过拆分块的形式进行数据获取分析处理，最后输出自己想要的结果。 MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据 2）Reduce阶段对Map结果进行汇总 上图简单的阐明了map和reduce的两个过程或者作用，虽然不够严谨，但是足以提供一个大概的认知，map过程是一个蔬菜到制成食物前的准备工作，reduce将准备好的材料合并进而制作出食物的过程。 2.6 大数据整个结构 图片来源于网上 图中涉及的技术名词解释如下：1）Sqoop：sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 一般是将关系型数据库的数据导入到hive或者HDFS中，目的就是为了分析这些数据，因为我们知道关系型数据库当数据量变得很大时，通过sql去统计查询，那么就会卡死。就是因为整个架构他们本省就是不支持的，他们通常进行的是扫表操作。2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息（3）支持通过Kafka服务器和消费机集群来分区消息。（4）支持Hadoop并行数据加载。4）Storm：Storm为分布式实时计算提供了一组通用原语，可被用于“流处理”之中，实时处理消息并更新数据库。这是管理队列及工作者集群的另一种方式。 Storm也可被用于“连续计算”（continuous computation），对数据流做连续查询，在计算时就将结果以流的形式输出给用户。5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。Oozie协调作业就是通过时间（频率）和有效数据触发当前的Oozie工作流程。7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。8）Hive：hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。11）Mahout:Apache Mahout是个可扩展的机器学习和数据挖掘库，当前Mahout支持主要的4个用例：推荐挖掘：搜集用户动作并以此给用户推荐可能喜欢的事物。聚集：收集文件并进行相关文件分组。分类：从现有的分类文档中学习，寻找文档中的相似特征，并为无标签的文档进行正确的归类。频繁项集挖掘：将一组项分组，并识别哪些个别项会经常一起出现。12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 好了到这里我们已经了解了hadoop整个生态相关的概念和组成，以及架构。那么接下来让我们进行hadoop环境的搭建。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kingge.top/tags/大数据/"}]},{"title":"关于2018年的计划","slug":"关于2018年的计划","date":"2018-02-18T16:00:00.000Z","updated":"2019-06-04T16:28:14.626Z","comments":true,"path":"2018/02/19/关于2018年的计划/","link":"","permalink":"http://kingge.top/2018/02/19/关于2018年的计划/","excerpt":"","text":"因为公司业务的转移和相关产品的开发，再加上大数据的火热，所以本人决定更新一些hadoop生态链相关的文章。所以敬请期待吧，哈哈哈哈哈。 Comming Soon！！！","categories":[{"name":"新年计划","slug":"新年计划","permalink":"http://kingge.top/categories/新年计划/"}],"tags":[{"name":"新年计划","slug":"新年计划","permalink":"http://kingge.top/tags/新年计划/"}]},{"title":"zookeeper知识学习","slug":"zookeeper知识学习","date":"2018-02-02T11:12:44.000Z","updated":"2019-06-02T13:24:38.033Z","comments":true,"path":"2018/02/02/zookeeper知识学习/","link":"","permalink":"http://kingge.top/2018/02/02/zookeeper知识学习/","excerpt":"","text":"引言 最近公司开发saas模式的企业应用软件服务，所以下面是我个人使用和总结（掺杂了大数据相关的总结） 一、正文0.1 下载1）官网首页： https://zookeeper.apache.org/ 2）下载截图 1.1 概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 1.2 模型构造和特点 1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。 2）Leader负责进行投票的发起和决议，更新系统状态。 3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票。 4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。（例如现在zookeeper集群现在有四台，那么挂掉两台后就不能正常工作了。假设初始时只有三台，那么最多也是挂掉两台后就不能工作了。也就是说，部署三台和部署四台的效用其实是一样的，所以一般都是部署奇数台zookeeper，节省资源） 5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。 6）更新请求顺序进行（全局数据一致性的提现），来自同一个client的更新请求按其发送顺序依次执行。 7）数据更新原子性，一次数据更新要么成功，要么失败。 8）实时性，在一定时间范围内（数据一致性的更新会有延迟），client能读到最新数据。 9）一次性监听（缺点）-这个缺点我们在后面可以用代码解决。 1.3 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。每个节点的存储的数据量，也决定了他的应用场景并不是存储大量的数据。下面的1.4章节会阐述到他的作用-应用场景 1.4 应用场景如果某个需求：当某个节点发生变化，通知其他关注这个节点的其他节点。那么就可以使用**zookeeper** 项目中常用到分布式锁和配置管理 1.4.1 统一命名服务 意思就是：我们通常使用域名来访问某个网站，但是域名对应的ip我们是不需要关注的为了系统的容错性，我们访问Baidu的这个请求是会随机寻找一个正常运行的服务器去处理。那么我们就可以使用zookeeper来进行管理。管理可以访问到Baidu这个网址的ip列表。客户端每次请求百度时，只需要去请求这个zookeeper获取可访问的ip即可。实现动态ip的上下线管理 1.4.2 统一配置管理 1.4.3 统一集群管理集群管理结构图如下所示。 1.4.4 服务器节点动态上下线 1.4.5 软负载均衡 控制某个服务器的访问数，达到资源合理分配 1.4.6 分布式锁（主要原理是同一路径下的节点名称不能重复，不能重复创建）有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。（在下面的3.2章节会讲到zookeeper顺序节点的相关内容） 好的博客： https://my.oschina.net/aidelingyu/blog/1600979 https://www.jianshu.com/p/5d12a01018e1 1.4.7 队列管理两种类型的队列： 1、 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 2、队列按照 FIFO 方式进行入队和出队操作。 第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。 二 Zookeeper安装2.1 本地模式安装部署1）安装前准备： （1）安装jdk （2）通过cshell工具拷贝zookeeper到linux系统下 （3）修改tar包权限 [kingge@hadoop102 software]$ chmod u+x zookeeper-3.4.10.tar.gz （4）解压到指定目录 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 2）配置修改 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； ​ 进入zoo.cfg文件：vim zoo.cfg ​ 修改dataDir路径为 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹 ​ [kingge@hadoop102 zookeeper-3.4.10]$ mkdir zkData 3）操作zookeeper （1）启动zookeeper [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start （2）查看进程是否启动 ​ [kingge@hadoop102 zookeeper-3.4.10]$ jps 4020 Jps 4001 QuorumPeerMain （3）查看状态： [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: standalone （4）启动客户端： [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkCli.sh （5）退出客户端： [zk: localhost:2181(CONNECTED) 0] quit （6）停止zookeeper [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh stop 2.2 配置参数解读解读zoo.cfg文件中参数含义 1）tickTime=2000：通信心跳数，Zookeeper服务器心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) 2）initLimit=10：Leader和Follower初始通信时限（10* tickTime – 也就是不要超过二十秒） 集群中的follower跟随者服务器与leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 投票选举新leader的初始化时间 Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 Leader允许Follower在initLimit时间内完成这个工作。 3）syncLimit=5：Leader和Follower同步通信时限（5* tickTime – 也就是不要超过十秒） 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。（默认超过十秒，leader就认为follwer已经碟机） 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 4）dataDir：数据文件目录+数据持久化路径 保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。 5）clientPort=2181：客户端连接端口 监听客户端连接的端口 2.3 分布式模式下的安装详见第四章节 三 Zookeeper内部原理3.1 选举机制1）半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。所以zookeeper**适合装在奇数台机器上**。 2）Zookeeper虽然在配置文件中并没有指定master和slave。但是，zookeeper工作时，是有一个节点为leader，其他则为follower，Leader是通过内部的选举机制临时产生的。 3）以一个简单的例子来说明整个选举的过程。 假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。 （1）服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是LOOKING状态。 （2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3 5/2=2.5 向上取整3)，所以服务器1、2还是继续保持LOOKING状态。 （3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的leader。 （4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。 （5）服务器5启动，同4一样当小弟。 3.2 节点类型1）Znode有两种类型： 短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 2）Znode有四种形式的目录节点（默认是persistent ） （1）持久化目录节点（PERSISTENT） ​ 客户端与zookeeper断开连接后，该节点依旧存在。 （2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） ​ 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。（保证创建的节点名称不会重复） （3）临时目录节点（EPHEMERAL） 客户端与zookeeper断开连接后，该节点被删除。 （4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 3）创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 4）在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序（应用场景1.4.6 分布式锁 第二种方式） 3.3 stat结构体1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid 每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。 事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。 2）ctime - znode被创建的毫秒数(从1970年开始) 3）mzxid - znode最后更新的zxid 4）mtime - znode最后修改的毫秒数(从1970年开始) 5）pZxid-znode最后更新的子节点zxid 6）cversion - znode子节点变化号，znode子节点修改次数 7）dataversion - znode数据变化号 8）aclVersion - znode访问控制列表的变化号 9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。 10）dataLength- znode的数据长度 11）numChildren - znode子节点数量 3.4 监听器原理 3.5 写数据流程 1.收到请求，先找到leader节点 2.广播请求给其他follower 3.哥哥follower写入数据，写入成功后，通知leader写入成功。（半数以上follower写入成功即为写入数据成功） 4.leader通知最初收到客户请求的server，数据写入成功，该server通知客户端写入数据成功 四 Zookeeper实战4.1 分布式安装部署0）集群规划 在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 1）解压安装 （1）解压zookeeper安装包到/opt/module/目录下 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ （2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData ​ mkdir -p zkData （3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg ​ mv zoo_sample.cfg zoo.cfg 2）配置zoo.cfg文件 ​ （1）具体配置 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 增加如下配置 ​ #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 （2）配置参数解读 Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器；（必须唯一） B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 3）集群操作 （1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件 ​ touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 （2）编辑myid文件 ​ vi myid ​ 在文件中添加与server对应的编号：如2 （3）拷贝配置好的zookeeper到其他机器上（可以用shell脚本进行分发数据） ​ scp -r zookeeper-3.4.10/ root@hadoop103.kingge.com:/opt/app/ ​ scp -r zookeeper-3.4.10/ root@hadoop104.kingge.com:/opt/app/ ​ 并分别修改myid文件中内容为3、4 （4）分别启动zookeeper ​ [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start （5）查看状态 [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower 分析：当第二个zookeeper启动时，因为2 &gt; 3/2=1.5，所以他被投票为了Leader，所以其他的节点就是follwer 4.2 客户端命令行操作 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建 -s 含有序列 -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 1）启动客户端（随便连接那个zookeeper都可以，因为他们内容都是一样的，下面连接的是103服务器） [kingge@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh 2）显示所有操作命令 [zk: localhost:2181(CONNECTED) 1] help 3）查看当前znode中所包含的内容 [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper] 4）查看当前节点数据并能看到更新次数等数据 [zk: localhost:2181(CONNECTED) 1] ls2 / [zookeeper] cZxid = 0x0 ctime = Thu Jan 01 08:00:00 CST 1970 mZxid = 0x0 mtime = Thu Jan 01 08:00:00 CST 1970 pZxid = 0x0 cversion = -1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 5）创建普通节点（注意创建节点时需要写入一些数据，否则创建不成功-例如create /app1 这样的话创建是没有效果的） [zk: localhost:2181(CONNECTED) 2] create /app1 “hello app1” Created /app1 [zk: localhost:2181(CONNECTED) 4] create /app1/server101 “192.168.1.101” Created /app1/server101 1.不支持递归创建节点，比如你要创建/app1/a,如果app1不存在，你就不能创建a( KeeperException.NoNode)。2.不可以再ephemeral类型的节点下创建子节点(KeeperException.NoChildrenForEphemerals)。（因为他本身是临时节点）3.如果指定的节点已经存在，会触发KeeperException.NodeExists 异常,当然了对于sequential类型的，不会抛出这个异常。（有编号类型的节点名称会自动递增）4.数据内容不能超过1M,否则将抛出KeeperException异常。 6）获得节点的值 [zk: localhost:2181(CONNECTED) 6] get /app1 hello app1 cZxid = 0x20000000a ctime = Mon Jul 17 16:08:35 CST 2017 mZxid = 0x20000000a mtime = Mon Jul 17 16:08:35 CST 2017 pZxid = 0x20000000b cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 10 numChildren = 1 [zk: localhost:2181(CONNECTED) 8] get /app1/server101 192.168.1.101 cZxid = 0x20000000b ctime = Mon Jul 17 16:11:04 CST 2017 mZxid = 0x20000000b mtime = Mon Jul 17 16:11:04 CST 2017 pZxid = 0x20000000b cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 13 numChildren = 0 7）创建短暂节点 [zk: localhost:2181(CONNECTED) 9] create -e /app-emphemeral 8888 （1）在当前客户端是能查看到的 [zk: localhost:2181(CONNECTED) 10] ls / [app1, app-emphemeral, zookeeper] （2）退出当前客户端然后再重启客户端 ​ [zk: localhost:2181(CONNECTED) 12] quit [kingge@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh （3）再次查看根目录下短暂节点已经删除 ​ [zk: localhost:2181(CONNECTED) 0] ls / [app1, zookeeper] 8）创建带序号的节点 ​ （1）先创建一个普通的根节点app2 ​ [zk: localhost:2181(CONNECTED) 11] create /app2 “app2” ​ （2）创建带序号的节点 ​ [zk: localhost:2181(CONNECTED) 13] create -s /app2/aa 888 Created /app2/aa0000000000 [zk: localhost:2181(CONNECTED) 14] create -s /app2/bb 888 Created /app2/bb0000000001 [zk: localhost:2181(CONNECTED) 15] create -s /app2/cc 888 Created /app2/cc0000000002 如果原节点下有1个节点，则再排序时从1开始，以此类推。 [zk: localhost:2181(CONNECTED) 16] create -s /app1/aa 888 Created /app1/aa0000000001 9）修改节点数据值 [zk: localhost:2181(CONNECTED) 2] set /app1 999 10）节点的值变化监听（一次性触发器）（Watch**的通知事件是从服务器发送给客户端的，是异步的**） ​ （1）在104主机上注册监听/app1节点数据变化（） 需要注意的是，注册一次监听，只能够响应一次，如果/app1节点的数据修改了两次，那么只显示第一次监听的信息，第二次不会有任何响应，想要得到响应，需要再次监听 [zk: localhost:2181(CONNECTED) 26] get /app1 watch ​ （2）在103主机上修改/app1节点的数据 [zk: localhost:2181(CONNECTED) 5] set /app1 777 ​ （3）观察104主机收到数据变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1 11）节点的子节点变化监听（路径变化） ​ （1）在104主机上注册监听/app1节点的子节点变化 [zk: localhost:2181(CONNECTED) 1] ls /app1 watch [aa0000000001, server101] ​ （2）在103主机/app1节点上创建子节点 [zk: localhost:2181(CONNECTED) 6] create /app1/bb 666 Created /app1/bb ​ （3）观察104主机收到子节点变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 12）删除节点 [zk: localhost:2181(CONNECTED) 4] delete /app1/bb 13）递归删除节点 [zk: localhost:2181(CONNECTED) 7] rmr /app2 14）查看节点状态 [zk: localhost:2181(CONNECTED) 12] stat /app1 cZxid = 0x20000000a ctime = Mon Jul 17 16:08:35 CST 2017 mZxid = 0x200000018 mtime = Mon Jul 17 16:54:38 CST 2017 pZxid = 0x20000001c cversion = 4 dataVersion = 2 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 3 numChildren = 2 15）exists 节点 这个函数很特殊，因为他可以监听一个尚未存在的节点，这是getData，getChildren不能做到的。exists可以监听一个节点的生命周期：从无到有，节点数据的变化，从有到无。 在传递给exists的watcher里，当path指定的节点被成功创建后，watcher会收到NodeCreated事件通知。当path所指定的节点的数据内容发送了改变后，wather会受到NodeDataChanged事件通知。 这里最需要注意的就是，exists可以监听一个未存在的节点，这是他与getData，getChildren本质的区别。 注意看上面的代码，其实我们已经实现了多次监听，解决了zookeeper单次监听的缺点。关键代码，我们在监听器里面，又再次声明了一次监听---zkClient.exists(&quot;eclipse&quot;,true) 16） getData 16） getChildren 4.3 API应用4.3.1 Eclipse环境搭建1）创建一个工程 2）解压zookeeper-3.4.10.tar.gz文件 3）拷贝zookeeper-3.4.10.jar、jline-0.9.94.jar、log4j-1.2.16.jar、netty-3.10.5.Final.jar、slf4j-api-1.6.1.jar、slf4j-log4j12-1.6.1.jar到工程的lib目录。并build一下，导入工程。 4）拷贝log4j.properties文件到项目根目录 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 4.3.2 创建ZooKeeper客户端private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void init() throws Exception &#123;//创建zookeeper连接的时候同时注册一个全局的默认的事件监听器 – // event.getType() 永远为null默认监听到None事件// //默认监听也可以使用register方法注册 //zkClient.register(watcherDefault); zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（用户的业务逻辑） System.out.println(event.getType() + &quot;--&quot; + event.getPath()); // 再次启动监听 - 解决zookeeper单次监听的缺点 try &#123; zkClient.getChildren(&quot;/&quot;, true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125;这里的watcher是该客户端总的监听方法，任何操作都会执行，而且是可以多次执行，并非单次。 4.3.3 创建子节点// 创建子节点@Testpublic void create() throws Exception &#123; // 数据的增删改查 // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型 String nodeCreated = zkClient.create(&quot;/eclipse&quot;, &quot;hello zk&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT);&#125; 4.3.4 获取子节点并监听// 获取子节点 @Test public void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren(&quot;/&quot;, true); for (String child : children) &#123; System.out.println(child); &#125; // 延时阻塞 Thread.sleep(Long.MAX_VALUE); &#125; 4.3.5 判断znode是否存在// 判断znode是否存在 @Test public void exist() throws Exception &#123; Stat stat = zkClient.exists(&quot;/eclipse&quot;, false); System.out.println(stat == null ? &quot;not exist&quot; : &quot;exist&quot;); &#125; 4.3.6 事件类型对照表 本表总结：exits和getData设置数据监视，而getChildren设置子节点监视 4.3.7 实现永久监听（伪）我们知道zookeeper的监听是一次性监听（on-time-trriger） 详情可查看 4.3.2代码 和 4.2 的15）exists 节点 4.4 案例总结1）需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线 2）需求分析 3）具体实现： （0）现在集群上创建/servers节点 [zk: localhost:2181(CONNECTED) 10] create /servers “servers” Created /servers （1）服务器端代码 package com.kingge.zkcase;import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.ZooDefs.Ids;public class DistributeServer &#123; private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到zk的客户端连接 public void getConnect() throws IOException&#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); &#125; // 注册服务器 public void registServer(String hostname) throws Exception&#123; String create = zk.create(parentNode + \"/server\", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +\" is noline \"+ create); &#125; // 业务功能 public void business(String hostname) throws Exception&#123; System.out.println(hostname+\" is working ...\"); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 利用zk连接注册服务器信息 server.registServer(args[0]); // 启动业务功能 server.business(args[0]); &#125;&#125; （2）客户端代码 package com.kingge.zkcase;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = &quot;/servers&quot;; private volatile ArrayList&lt;String&gt; serversList = new ArrayList&lt;&gt;(); // 创建到zk的客户端连接 public void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 再次启动监听 try &#123; getServerList(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // public void getServerList() throws Exception &#123; // 获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); for (String child : children) &#123; byte[] data = zk.getData(parentNode + &quot;/&quot; + child, false, null); servers.add(new String(data)); &#125; // 把servers赋值给成员serverList，已提供给各业务线程使用 serversList = servers; System.out.println(serversList); &#125; // 业务功能 public void business() throws Exception &#123; System.out.println(&quot;client is working ...&quot;);Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 业务进程启动 client.business(); &#125;&#125; 4.5 zookeeper核心原理（事件） https://blog.csdn.net/yinwenjie/article/details/47685077 五 好的总结网站\\1. https://blog.csdn.net/liu857279611/article/details/70495413 \\2. https://www.jianshu.com/p/a1d7826073e6 \\3. https://blog.csdn.net/yinwenjie/article/details/47685077 \\4. https://www.jianshu.com/p/5d12a01018e1","categories":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://kingge.top/categories/zookeeper/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"hadoop，linux","slug":"hadoop，linux","permalink":"http://kingge.top/tags/hadoop，linux/"}]},{"title":"聊聊分布式事务，再说说解决方案-cap","slug":"聊聊分布式事务，再说说解决方案-cap","date":"2017-10-18T09:57:58.000Z","updated":"2017-10-18T10:00:42.654Z","comments":true,"path":"2017/10/18/聊聊分布式事务，再说说解决方案-cap/","link":"","permalink":"http://kingge.top/2017/10/18/聊聊分布式事务，再说说解决方案-cap/","excerpt":"","text":"数据库事务 在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会经常使用到。但是即使如此，可能对于一些细节问题，很多人仍然不清楚。比如很多人都知道数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。但是再往下比如问到隔离性指的是什么的时候可能就不知道了，或者是知道隔离性是什么但是再问到数据库实现隔离的都有哪些级别，或者是每个级别他们有什么区别的时候可能就不知道了。 本文并不打算介绍这些数据库事务的这些东西，有兴趣可以搜索一下相关资料。不过有一个知识点我们需要了解，就是假如数据库在提交事务的时候突然断电，那么它是怎么样恢复的呢？ 为什么要提到这个知识点呢？ 因为分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，所以我们在做分布式系统的时候，最先考虑的就是这种情况。这些异常可能有 机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常等等… 我们接着说本地事务数据库断电的这种情况，它是怎么保证数据一致性的呢？我们使用SQL Server来举例，我们知道我们在使用 SQL Server 数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多。数据库进行任何写入操作的时候都是要先写日志的，同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。 接着，我们就说一下分布式事务。 分布式理论 当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况了，而在这种ACID的集群环境下，再想保证集群的ACID几乎是很难达到，或者即使能达到那么效率和性能会大幅下降，最为关键的是再很难扩展新的分区了，这个时候如果再追求集群的ACID会导致我们的系统变得很差，这时我们就需要引入一个新的理论原则来适应这种集群的情况，就是 CAP 原则或者叫CAP定理，那么CAP定理指的是什么呢？ CAP定理 CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性： 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成 具体地讲在分布式系统中，在任何数据库设计中，一个Web应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。 这个定理在迄今为止的分布式系统中都是适用的！ 为什么这么说呢？ 转载链接描述的很到位：http://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html","categories":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"数据库","slug":"数据库","permalink":"http://kingge.top/tags/数据库/"}]},{"title":"数据库中的undo和redo日志","slug":"数据库中的undo和redo日志","date":"2017-10-18T09:52:46.000Z","updated":"2017-10-18T09:55:01.852Z","comments":true,"path":"2017/10/18/数据库中的undo和redo日志/","link":"","permalink":"http://kingge.top/2017/10/18/数据库中的undo和redo日志/","excerpt":"","text":"转载好的博客解释1： http://blog.csdn.net/kobejayandy/article/details/50885693 转载好的博客解释2： http://www.cnblogs.com/Bozh/archive/2013/03/18/2966494.html","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://kingge.top/tags/分布式/"},{"name":"数据库","slug":"数据库","permalink":"http://kingge.top/tags/数据库/"}]},{"title":"vSphere与Workstation虚拟机交互的几种方法","slug":"vSphere与Workstation虚拟机交互的几种方法","date":"2017-10-18T08:11:28.000Z","updated":"2017-10-18T08:14:18.402Z","comments":true,"path":"2017/10/18/vSphere与Workstation虚拟机交互的几种方法/","link":"","permalink":"http://kingge.top/2017/10/18/vSphere与Workstation虚拟机交互的几种方法/","excerpt":"","text":"参见转载链接： http://wangchunhai.blog.51cto.com/225186/1884052","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"centos","slug":"centos","permalink":"http://kingge.top/tags/centos/"},{"name":"vmware","slug":"vmware","permalink":"http://kingge.top/tags/vmware/"}]},{"title":"查看虚拟机里的Centos7的IP","slug":"查看虚拟机里的Centos7的IP","date":"2017-10-18T07:48:43.000Z","updated":"2017-10-18T08:09:04.483Z","comments":true,"path":"2017/10/18/查看虚拟机里的Centos7的IP/","link":"","permalink":"http://kingge.top/2017/10/18/查看虚拟机里的Centos7的IP/","excerpt":"","text":"登录虚拟机 输入用户名和密码（用户名一般是root） 查看ip 指令 ip addr 指令： 查看当前虚拟机ip 我们发现ens32 没有 inet 这个属性，没有出现ip，那么说明在设置的时候没有开启，需要先去设置。 当前位置：[root@localhost ~]# pwd/root[root@localhost ~]# 接着来查看ens32网卡的配置： vi /etc/sysconfig/network-scripts/ifcfg-ens32 注意vi后面加空格. etc 文件夹的位置在于 [root@localhost ~]# cd ..[root@localhost /]# lsbin dev home lib64 mnt proc run srv tmp varboot etc lib media opt root sbin sys usr 查看 ifcfg-ens32 的内容 从配置清单中可以发现 CentOS 7 默认是不启动网卡的（ONBOOT=no）。 把这一项改为YES（ONBOOT=yes） – (按 i 进入编辑模式 ，修改完，按 esc退出编辑模式，然后 按 ctrl + shift + : 输入 wq 完成编辑) 然后重启网络服务： sudo service network restart 然后我们再输入 ip addr 命令 使用第三方工具登录 这里是用的是 xshell，你也可以用winscp（这个一般是用来传文件的） 然后点击连接，输入用户名和密码，便可以进入命令界面","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"centos","slug":"centos","permalink":"http://kingge.top/tags/centos/"},{"name":"vmware","slug":"vmware","permalink":"http://kingge.top/tags/vmware/"}]},{"title":"activity工作流框架——数据库表结构说明","slug":"activity工作流框架——数据库表结构说明","date":"2017-10-12T06:54:33.000Z","updated":"2017-10-12T07:08:08.890Z","comments":true,"path":"2017/10/12/activity工作流框架——数据库表结构说明/","link":"","permalink":"http://kingge.top/2017/10/12/activity工作流框架——数据库表结构说明/","excerpt":"","text":"本文转载于： http://www.jianshu.com/p/f9fd1cc02eae activity一共23张表 表的命名第一部分都是以 ACT_开头的。 表的命名第二部分是一个两个字符用例表的标识 act_ge_*： ‘ge’代表general（一般）。普通数据，各种情况都使用的数据。 act_gebytearray：二进制数据表，用来保存部署文件的大文本数据1.ID:资源文件编号，自增长2.REVINT:版本号3.NAME:资源文件名称4.DEPLOYMENTID:来自于父表act_redeployment的主键5.BYTES:大文本类型，存储文本字节流 act_geproperty：属性数据表，存储这整个流程引擎级别的数据。在初始化表结构时，会默认插入三条记录。1.NAME:属性名称2.VALUE_:属性值3.REV_INT:版本号 act_hi_*： hi’代表 history（历史）。就是这些表包含着历史的相关数据，如结束的流程实例、变量、任务、等等。 act_hiactinst：历史节点表1.ID : 标识2.PROC_DEFID :流程定义id3.PROC_INSTID : 流程实例id4.EXECUTIONID : 执行实例5.ACTID : 节点id6.ACTNAME : 节点名称7.ACTTYPE : 节点类型8.ASSIGNEE_ : 节点任务分配人9.STARTTIME : 开始时间10.ENDTIME : 结束时间11.DURATION : 经过时长 act_hi_attachment：历史附件表 act_hicomment：历史意见表1.ID :标识2.TYPE : 意见记录类型 为comment 时 为处理意见3.TIME : 记录时间4.USERID :5.TASKID ： 对应任务的id6.PROC_INSTID : 对应的流程实例的id7.ACTION ： 为AddComment 时为处理意见8.MESSAGE : 处理意见9.FULLMSG : act_hidetail：历史详情表，启动流程或者在任务complete之后,记录历史流程变量1.ID : 标识2.TYPE_ : variableUpdate 和 formProperty 两种值3.PROC_INSTID : 对应流程实例id4.EXECUTIONID : 对应执行实例id5.TASKID : 对应任务id6.ACT_INSTID : 对应节点id7.NAME : 历史流程变量名称，或者表单属性的名称8.VARTYPE : 定义类型9.REV : 版本10.TIME : 导入时间11.BYTEARRAYID12.DOUBLE : 如果定义的变量或者表单属性的类型为double，他的值存在这里13.LONG : 如果定义的变量或者表单属性的类型为LONG ,他的值存在这里14.TEXT : 如果定义的变量或者表单属性的类型为string，值存在这里15.TEXT2: act_hi_identitylink：历史流程人员表 act_hiprocinst： 历史流程实例表1.ID : 唯一标识2.PROC_INSTID : 流程ＩＤ3.BUSINESSKEY : 业务编号4.PROC_DEFID ： 流程定义id5.STARTTIME : 流程开始时间6.ENT_TIME : 结束时间7.DURATION : 流程经过时间8.START_USERID : 开启流程用户id9.START_ACTID : 开始节点10.END_ACTID： 结束节点11.SUPER_PROCESS_INSTANCEID : 父流程流程id12.DELETEREASON : 从运行中任务表中删除原因 act_hitaskinst： 历史任务实例表1.ID ： 标识2.PROC_DEFID ： 流程定义id3.TASK_DEFKEY : 任务定义id4.PROC_INSTID : 流程实例ｉｄ5.EXECUTIONID : 执行实例id6.PARENT_TASKID : 父任务id7.NAME : 任务名称8.DESCRIPTION : 说明9.OWNER : 拥有人（发起人）10.ASSIGNEE : 分配到任务的人11.START_TIME : 开始任务时间12.ENDTIME : 结束任务时间13.DURATION_ : 时长14.DELETEREASON :从运行时任务表中删除的原因15.PRIORITY_ : 紧急程度16.DUEDATE : act_hi_varinst：历史变量表 act_id_*： id’代表 identity（身份）。这些表包含着标识的信息，如用户、用户组、等等。 act_idgroup:用户组信息表，用来存储用户组信息。1.ID：用户组名2.REVINT:版本号3.NAME:用户组描述信息4.TYPE_:用户组类型 act_id_info：用户扩展信息表 act_id_membership：用户与用户组对应信息表，用来保存用户的分组信息1.USERID:用户名2.GROUPID:用户组名 act_iduser：用户信息表1.ID:用户名2.REVINT:版本号3.FIRST:用户名称4.LAST:用户姓氏5.EMAIL:邮箱6.PWD_:密码 act_re_*： ’re’代表 repository（仓库）。带此前缀的表包含的是静态信息，如，流程定义、流程的资源（图片、规则，等）。 act_redeployment:部署信息表,用来存储部署时需要持久化保存下来的信息1.ID:部署编号，自增长2.NAME_:部署包的名称3.DEPLOYTIME:部署时间 act_re_model 流程设计模型部署表 act_reprocdef:业务流程定义数据表1.ID:流程ID，由“流程编号：流程版本号：自增长ID”组成2.CATEGORY:流程命名空间（该编号就是流程文件targetNamespace的属性值）3.NAME:流程名称（该编号就是流程文件process元素的name属性值）4.KEY:流程编号（该编号就是流程文件process元素的id属性值）5.VERSION:流程版本号（由程序控制，新增即为1，修改后依次加1来完成的）6.DEPLOYMENTID:部署编号7.RESOURCENAME:资源文件名称8.DGRM_RESOURCENAME:图片资源文件名称9.HAS_START_FROMKEY:是否有Start From Key 注：此表和ACT_RE_DEPLOYMENT是多对一的关系，即，一个部署的bar包里可能包含多个流程定义文件，每个流程定义文件都会有一条记录在ACT_REPROCDEF表内，每个流程定义的数据，都会对于ACT_GE_BYTEARRAY表内的一个资源文件和PNG图片文件。和ACT_GE_BYTEARRAY的关联是通过程序用ACT_GE_BYTEARRAY.NAME与ACT_REPROCDEF.NAME完成的，在数据库表结构中没有体现。 act_ru_*： ’ru’代表 runtime（运行时）。就是这个运行时的表存储着流程变量、用户任务、变量、作业，等中的运行时的数据。 activiti 只存储流程实例执行期间的运行时数据，当流程实例结束时，将删除这些记录。这就使这些运行时的表保持 的小且快。 act_ru_event_subscr act_ruexecution：运行时流程执行实例表1.ID：主键，这个主键有可能和PROC_INSTID相同，相同的情况表示这条记录为主实例记录。2.REV_：版本，表示数据库表更新次数。3.PROC_INSTID：流程实例编号，一个流程实例不管有多少条分支实例，这个ID都是一致的。4.BUSINESSKEY：业务编号，业务主键，主流程才会使用业务主键，另外这个业务主键字段在表中有唯一约束。5.PARENTID：找到该执行实例的父级，最终会找到整个流程的执行实例6.PROC_DEFID：流程定义ID7.SUPEREXEC： 引用的执行模板，这个如果存在表示这个实例记录为一个外部子流程记录，对应主流程的主键ID。8.ACTID： 节点id，表示流程运行到哪个节点9.ISACTIVE： 是否活动流程实例10.ISCONCURRENT：是否并发。上图同步节点后为并发，如果是并发多实例也是为1。11.ISSCOPE： 主实例为1，子实例为0。12.TENANTID : 这个字段表示租户ID。可以应对多租户的设计。13.IS_EVENT_SCOPE: 没有使用到事件的情况下，一般都为0。14.SUSPENSIONSTATE：是否暂停。 act_ruidentitylink：运行时流程人员表，主要存储任务节点与参与者的相关信息1.ID： 标识2.REV_： 版本3.GROUPID： 组织id4.TYPE_： 类型5.USERID： 用户id6.TASKID： 任务id act_ru_job act_rutask：运行时任务节点表1.ID：2.REV_：3.EXECUTIONID： 执行实例的id4.PROC_INSTID： 流程实例的id5.PROC_DEFID： 流程定义的id,对应act_reprocdef 的id6.NAME_： 任务名称，对应 task 的name7.PARENT_TASKID : 对应父任务8.DESCRIPTION_：9.TASK_DEFKEY： task 的id10.OWNER : 发起人11.ASSIGNEE： 分配到任务的人12.DELEGATION : 委托人13.PRIORITY： 紧急程度14.CREATETIME： 发起时间15.DUETIME：审批时长 act_ruvariable：运行时流程变量数据表1.ID：标识2.REV：版本号3.TYPE：数据类型4.NAME_：变量名5.EXECUTIONID： 执行实例id6.PROC_INSTID： 流程实例id7.TASKID： 任务id8.BYTEARRAYID：9.DOUBLE：若数据类型为double ,保存数据在此列10.LONG： 若数据类型为Long保存数据到此列11.TEXT： string 保存到此列12.TEXT2： 结论及总结: 流程文件部署主要涉及到3个表，分别是：ACT_GE_BYTEARRAY、ACT_RE_DEPLOYMENT、ACT_RE_PROCDEF。主要完成“部署包”–&gt;“流程定义文件”–&gt;“所有包内文件”的解析部署关系。从表结构中可以看出，流程定义的元素需要每次从数据库加载并解析，因为流程定义的元素没有转化成数据库表来完成，当然流程元素解析后是放在缓存中的，具体的还需要后面详细研究。 流程定义中的java类文件不保存在数据库里 。 组织机构的管理相对较弱，如果要纳入单点登录体系内还需要改造完成，具体改造方法有待研究。 运行时对象的执行与数据库记录之间的关系需要继续研究 历史数据的保存及作用需要继续研究。","categories":[{"name":"activity","slug":"activity","permalink":"http://kingge.top/categories/activity/"}],"tags":[{"name":"activity","slug":"activity","permalink":"http://kingge.top/tags/activity/"},{"name":"工作流","slug":"工作流","permalink":"http://kingge.top/tags/工作流/"}]},{"title":"关于web.xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序","slug":"关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序","date":"2017-10-10T08:22:44.000Z","updated":"2017-10-10T09:25:05.991Z","comments":true,"path":"2017/10/10/关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序/","link":"","permalink":"http://kingge.top/2017/10/10/关于web-xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序/","excerpt":"","text":"前言 今天跑一个web项目，想做一些初始化工作，于是使用Filter来实现，但是发现ServletContextListener，Servlet也是能够实现的。但是肯定会有先后顺序执行的问题，那么接下来探讨这个问题。 作者规则：为了节省部分人的时间，先说结论。结论就是标题的顺序：ServletContext - ServletContextListener- Filter、Servlet web加载 启动一个WEB项目的时候，WEB容器会去读取它的配置文件web.xml。 加载产生Servlet上下文实例，ServletContext 这个web项目的所有部分都将共享这个上下文。容器将转换为键值对，并交给servletContext。L例如我们在使用spring的时候，会配置applicationContext.xml &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; 依次加载Servlet的事件监听器 - ServletContextListener 并依次调用public void contextInitialized(ServletContextEvent sce)方法。加载和调用多个Listener的顺序由在web.xml中配置的依次顺序决定的。 &lt;listener&gt; &lt;listener-class&gt;com.wlx.core.application.ApplicaltionListener&lt;/listener-class&gt;&lt;/listener&gt;&lt;listener&gt; &lt;listener-class&gt;com.wlx.core.application.ApplicaltionListener2&lt;/listener-class&gt;&lt;/listener&gt;先执行 ApplicaltionListener的contextInitialized方法后执行ApplicaltionListener2的contextInitialized方法 我们可以通过这个方法做一些初始化工作：例如初始化数据库连接池，初始化redis，启动定时器服务，启动线程池做一些socket通讯服务等等工作。 然后在contextDestroyed方法关闭这些服务即可。 .依次加载Servlet的过滤器-Filter 并依次调用public void init(FilterConfig filterConfig) throws ServletException;方法加载和调用多个filter的顺序由在web.xml中配置的依次顺序决定的。 &lt;filter&gt; &lt;filter-name&gt;appFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.wlx.core.application.AppFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;appFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 依次加载Servlet Load-on-startup元素在web应用启动的时候指定了servlet被加载的顺序，它的值必须是一个整数。如果它的值是一个负整数或是这个元素不存在，那么容器会在该servlet被调用的时候(例如下面代码访问-/servlet/UploadFile 为后缀的时候才会去初始化init，并不会在项目启动时候访问init)，加载这个servlet。如果值是正整数或零，容器在配置的时候就加载并初始化这个servlet，容器必须保证值小的先被加载。如果值相等，容器可以自动选择先加载谁。 在servlet的配置当中，&lt;load-on-startup&gt;5&lt;/load-on-startup&gt;的含义是：标记容器是否在启动的时候就加载这个servlet。当值为0或者大于0时，表示容器在应用启动时就加载这个servlet；当是一个负数时或者没有指定时，则指示容器在该servlet被选择时才加载。正数的值越小，启动该servlet的优先级越高。 项目启动时会去调用 UploadFile的init方法&lt;servlet&gt; &lt;servlet-name&gt;UploadFile&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wlx.core.application.servlet.UploadFile&lt;/servlet-class&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;UploadFile&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/UploadFile&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 项目启动时不会去调用 EServlet的init方法，访问匹配规则的网址时才会去调用init，而且只调用一次 &lt;servlet&gt; &lt;servlet-name&gt;EServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wlx.core.application.servlet.EServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;EServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/EServlet&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 总结 以上是Web容器在启动时加载的顺序，启动加载只会加载一次。web.xml 的加载顺序是：ServletContext-&gt; context-param -&gt;listener -&gt; filter -&gt; servlet. 扩展知识-请求执行循序 在上面中我们总结web加载的执行顺序，那么一个请求的执行循序呢？实际上就是一个责任链模式的问题 依次执行过滤器filter的方法public void doFilter(ServletRequest request, ServletResponse response,FilterChain chain)，这个方法应用了责任链模式，当在该方法中使用chain.doFilter(request, response);则这个过滤器就调用下一个过滤器，直到过滤器链条完成调用，进入Servlet处理，这个时候doFilter并未执行完成，仅仅在servlet之前进行一连串的过滤处理。 进入相应Servlet并调用public void service(ServletRequest req, ServletResponse res)方法，或者说是GET和POST方法。public void doGet(HttpServletRequest request, HttpServletResponse respose)进行请求响应的业务处理。 Servlet处理完成后，执行chain.doFilter(request, response);执行其他过滤器链条的后置过滤处理，然后执行自己的后置处理。 以上Filter和Servlet的执行顺序有点像Spring AOP 的前置通知和后置通知与业务方法关系。在Filter的doFilter方法中的chain.doFilter(request, response);之前做的业务逻辑就像前置通知，之后的逻辑像后置通知。业务方法是Sevlet中的public void service(ServletRequest req, ServletResponse res)方法。并且可以由多个有序的过滤链条进行Servlet的过滤。 Filter的过滤请求的Servlet的范围与配置有关,Filter在每次访问Servlet时都会拦截过滤。 代码例子： public class MyFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;执行MyFilter init&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;执行MyFilter doFilter&quot;); System.out.println(&quot;执行MyFilter doFilter before&quot;); chain.doFilter(request, response); System.out.println(&quot;执行MyFilter doFilter after&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;执行MyFilter destroy&quot;); &#125;&#125;-------------------------------------------------------------public class MyFilter1 implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;执行MyFilter1 init&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;执行MyFilter1 doFilter &quot;); System.out.println(&quot;执行MyFilter1 doFilter before&quot;); chain.doFilter(request, response); System.out.println(&quot;执行MyFilter1 doFilter after&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;执行MyFilter1 destroy&quot;); &#125;&#125;------------------------------------------------------------------------public class MyServlet1 extends HttpServlet &#123; private static final long serialVersionUID = 1L; public void init() throws ServletException &#123; System.out.println(&quot;执行Servlet1 init()&quot;); &#125; public void destroy() &#123; System.out.println(&quot;执行Servlet1 destroy()&quot;); &#125; public void doGet(HttpServletRequest request, HttpServletResponse respose) throws ServletException, IOException &#123; System.out.println(&quot;执行Servlet1 service&quot;); &#125;&#125; 省略在web.xml中的配置 输出： 执行MyFilter doFilter执行MyFilter doFilter before执行MyFilter1 doFilter执行MyFilter1 doFilter before执行Servlet service执行MyFilter1 doFilter after执行MyFilter doFilter after","categories":[{"name":"javaweb","slug":"javaweb","permalink":"http://kingge.top/categories/javaweb/"}],"tags":[{"name":"javaweb","slug":"javaweb","permalink":"http://kingge.top/tags/javaweb/"},{"name":"web.xml","slug":"web-xml","permalink":"http://kingge.top/tags/web-xml/"}]},{"title":"软技能-代码之外的生存指南-把自己当做一个企业去思考","slug":"软技能-代码之外的生存指南-把自己当做一个企业去思考","date":"2017-10-09T00:46:26.000Z","updated":"2017-10-09T00:51:47.087Z","comments":true,"path":"2017/10/09/软技能-代码之外的生存指南-把自己当做一个企业去思考/","link":"","permalink":"http://kingge.top/2017/10/09/软技能-代码之外的生存指南-把自己当做一个企业去思考/","excerpt":"","text":"《软技能》—— 把自己当做一个企业去思考","categories":[{"name":"读书系统","slug":"读书系统","permalink":"http://kingge.top/categories/读书系统/"}],"tags":[{"name":"软技能","slug":"软技能","permalink":"http://kingge.top/tags/软技能/"},{"name":"代码之外的生存指南","slug":"代码之外的生存指南","permalink":"http://kingge.top/tags/代码之外的生存指南/"}]},{"title":"java到底是值传递还是引用传递","slug":"java到底是值传递还是引用传递","date":"2017-09-26T07:13:17.000Z","updated":"2017-09-26T07:56:21.045Z","comments":true,"path":"2017/09/26/java到底是值传递还是引用传递/","link":"","permalink":"http://kingge.top/2017/09/26/java到底是值传递还是引用传递/","excerpt":"","text":"引言 我们先给本文定下基调，java是值传递 有一种说法，引用传递实际上也就是值传递。这个说法很有意思，实际上这种说法也是有道理的，传递引用，这个引用实际上就是一个地址，也即是一个值。 什么是值传递和引用传递 首先，不要纠结于 Pass By Value 和 Pass By Reference 的字面上的意义，否则很容易陷入所谓的“一切传引用其实本质上是传值”这种并不能解决问题无意义论战中。更何况，要想知道Java到底是传值还是传引用，起码你要先知道传值和传引用含义。 一：搞清楚 基本类型 和 引用类型的不同之处 int num = 10;String str = &quot;hello&quot;; num是基本类型，值就直接保存在变量中。而str是引用类型，变量中保存的只是实际对象的地址。一般称这种变量为”引用”，引用指向实际对象，实际对象中保存着内容。 二：搞清楚赋值运算符（=）的作用 num = 20;str = &quot;java&quot;; 对于基本类型 num ，赋值运算符会直接改变变量的值，原来的值被覆盖掉。对于引用类型 str，赋值运算符会改变引用中所保存的地址，原来的地址被覆盖掉。但是原来的对象不会被改变（重要）。 例子 参数传递基本上就是赋值操作 第一个例子：基本类型void foo(int value) &#123; value = 100;&#125;foo(num); // num 没有被改变第二个例子：没有提供改变自身方法的引用类型void foo(String text) &#123; text = &quot;windows&quot;;&#125;foo(str); // str 也没有被改变第三个例子：提供了改变自身方法的引用类型StringBuilder sb = new StringBuilder(&quot;iphone&quot;);void foo(StringBuilder builder) &#123; builder.append(&quot;4&quot;);&#125;foo(sb); // sb 被改变了，变成了&quot;iphone4&quot;。第四个例子：提供了改变自身方法的引用类型，但是不使用，而是使用赋值运算符。StringBuilder sb = new StringBuilder(&quot;iphone&quot;);void foo(StringBuilder builder) &#123; builder = new StringBuilder(&quot;ipad&quot;);&#125;foo(sb); // sb 没有被改变，还是 &quot;iphone&quot;。 重点理解为什么，第三个例子和第四个例子结果不同？ 例子5 public class Employee &#123; public int age;&#125;public class Main &#123; public static void changeEmployee(Employee employee3) &#123; employee3 = new Employee(); // flag 1 employee3.age = 1000; &#125; public static void main(String[] args) &#123; Employee employee = new Employee(); employee.age = 100; changeEmployee(employee); System.out.println(employee.age); &#125;&#125;输出： 100如果把 flag 1 位置代码注释，那么程序结果输出1000---原因同上 总结 = 号的理解是最重要的，他是一个动词，可能会引起左边变量值的改变 java中方法参数传递方式是按值传递。 如果参数是基本类型，传递的是基本类型的字面量值的拷贝。也就是你我没有半毛钱关系 如果参数是引用类型，传递的是该参量所引用的对象在堆中地址值的拷贝。你我可能存在关系 = 是赋值操作（任何包含=的如+=、-=、 /=等等，都内含了赋值操作）。不再是你以前理解的数学含义了，而+ - /和 = 在java中更不是一个级别，换句话说， = 是一个动作，一个可以改变内存状态的操作，一个可以改变变量的符号，而+ - /却不会。这里的赋值操作其实是包含了两个意思：1、放弃了原有的值或引用；2、得到了 = 右侧变量的值或引用。Java中对 = 的理解很重要啊！！可惜好多人忽略了，或者理解了却没深思过。 对于基本数据类型变量，= 操作是完整地复制了变量的值。换句话说，“=之后，你我已无关联”；至于基本数据类型，就不在这科普了。 对于非基本数据类型变量，= 操作是复制了变量的引用。换句话说，“嘿，= 左侧的变量，你丫别给我瞎动！咱俩现在是一根绳上的蚂蚱，除非你再被 = 一次放弃现有的引用！！上面说了 = 是一个动作，所以我把 = 当作动词用啦！！”。而非基本数据类型变量你基本上可以参数本身是变量 参数传递本质就是一种 = 操作。参数是变量，所有我们对变量的操作、变量能有的行为，参数都有。所以把C语言里参数是传值啊、传指针啊的那套理论全忘掉，参数传递就是 = 操作。","categories":[{"name":"java","slug":"java","permalink":"http://kingge.top/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java深入理解","slug":"java深入理解","permalink":"http://kingge.top/tags/java深入理解/"}]},{"title":"继承之上溯造型和下溯造型","slug":"继承之上溯造型和下溯造型","date":"2017-09-12T03:26:59.000Z","updated":"2020-05-09T08:57:26.343Z","comments":true,"path":"2017/09/12/继承之上溯造型和下溯造型/","link":"","permalink":"http://kingge.top/2017/09/12/继承之上溯造型和下溯造型/","excerpt":"","text":"前言 我们在平时的开发编码中，都会用到上溯造型和下溯造型，只是我们并不知道他的官方叫法而已， 上溯造型跟继承和多态，以及动态绑定的关系很密切 ，关于这几个概念后面会有涉及到他们的概念。 继承和合成 继承：它的本质就是为了使得代码复用（可以基于已经存在的类构造一个新类。继承已经存在的类就可以复用这些类的方法和域。在此基础上，可以添加新的方法和域，从而扩充了类的功能。） 合成：在新类里创建原有的对象称为合成。这种方式可以重复利用现有的代码而不更改它的形式。 -----继承关键字extends表明新类派生于一个已经存在的类。已存在的类称为父类或基类，新类称为子类或派生类。例如:class Dog extends Animal &#123;&#125;类Dog继承了Animal，Animal类称为父类或基类，Dog类称为子类或派生类。---合成合成比较简单，就是在一个类中创建一个已经存在的类。class Dog &#123; Animal animal;&#125; 上溯造型 这个术语缘于继承关系图的传统画法：将基类至于顶部，而向下发展的就是派生类(子类)，发送给父类的消息亦可发给衍生类，父类包含子类。假设把子类赋值给父类，这个过程就称之为上溯造型— 这个时候只能够调用父类父类的方法，子类特有的方法不能够调用，子类变窄 //父类abstract class Animal &#123; public abstract void speak(); public void eat()&#123; &#125; &#125;//子类特有方法interface DoorGod &#123; void guard(); &#125; //Dog 子类和 Cat 子类class Cat extends Animal &#123; @Override public void eat() &#123; try &#123; Thread.sleep( 1000 ); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // super .eat(); System.out.println(&quot;cat eat&quot;); &#125; @Override public void speak() &#123; System.out.println( &quot; 喵喵 &quot; ); &#125; &#125; class Dog extends Animal implements DoorGod&#123; @Override public void speak() &#123; System.out.println( &quot; 汪汪 &quot; ); &#125; public void guard() &#123; while ( true )&#123; System.out.println( &quot; 汪汪 &quot; ); &#125; &#125; &#125; //测试方法public class TestShangSu&#123; public static void upcasting(Animal animal)&#123; animal.speak(); animal.eat(); &#125; @Test public void test1()&#123; Animal dog1 = new Dog(); upcasting(dog1); Animal cat = new Cat(); upcasting(cat); &#125; &#125;//输出 汪汪 喵喵 cat eat 这个时候为什么输出是：子类覆盖父类的方法，而不是父类的方法，这个涉及到动态绑定。后面再讲 由于upcasting(Animal animal)方法的参数是 Animal类型的，因此如果传入的参数是 Animal的子类，传入的参数就会被转换成父类Animal类型，这样你创建的Dog对象能使用的方法只是Animal中的签名方法；也就是说，在上溯的过程中，Dog的接口变窄了，它本身的一些方法（例如实现了 DoorGod的guard方法）就不可见了。如果你想使用Dog中存在而Animal中不存在的方法（比如guard方法），编译时不能通过的。由此可见，上溯造型是安全的类型转换。 如果Dog在上溯造型过程中想使用 DoorGod的guard方法，那么需要配合下溯造型和安全检查，来进行强制转换，讲Animal 下溯为 Dog类型。 注意的是：下溯是不安全的，由父类转化为子类，所以需要加上判断。 下溯造型 将基类转化为衍生类，不安全的操作，可能会引发ClassCastException。 上面的例子只需要加上这一层判断即可 public static void upcasting(Animal animal)&#123; if( animal instanceof Dog )&#123;//下溯造型判断 Dog dog = (Dog) animal; dog.guard(); &#125; animal.speak(); animal.eat(); &#125; 我们在使用注解实现请求方法的登录控制 登录拦截器里面有段关键代码使用的就是下溯造型 为什么使用上溯和下溯造型 上面的例子我们发现，关键的代码是upcasting方法，为什么在调用upcasting方法时要有意忽略调用它的对象类型呢？如果让upcasting方法简单地获取Dog句柄似乎更加直观易懂，但是那样会使衍生自Animal类的每一个新类都要实现专属自己的upcasting方法：例如Cat会实现一个重复的upcasting(Cat cat )这样的方法。 实现多态的好处和代码复利用。 动态绑定 在上面的upcasting方法，测试例子输出的是子类的方法，而非是父类的方法，但是我们使用的是父类去调用这些方法，为什么输出不是父类的呢？ upcasting它接收的是Animal句柄，当执行speak和eat方法时时，它是如何知道Animal句柄指向的是一个Dog对象而不是Cat对象呢？编译器是无从得知的，这涉及到接下来要说明的绑定问题。 Java实现了一种方法调用机制，可在运行期间判断对象的类型，然后调用相应的方法，这种在运行期间进行，以对象的类型为基础的绑定称为动态绑定。除非一个方法被声明为final，Java中的所有方法都是动态绑定的。 静态方法的绑定 他跟普通的方法不同，子类和父类方法都是静态的，子类如果去掉父类编译会错误 package Test;class Person &#123; static void eat() &#123; System.out.println(&quot;Person.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Person.speak()&quot;); &#125;&#125;class Boy extends Person &#123; static void eat() &#123; System.out.println(&quot;Boy.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Boy.speak()&quot;); &#125;&#125;class Girl extends Person &#123; static void eat() &#123; System.out.println(&quot;Girl.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Girl.speak()&quot;); &#125;&#125;public class Persons &#123; public static Person randPerson() &#123; switch ((int)(Math.random() * 2)) &#123; default: case 0: return new Boy(); case 1: return new Girl(); &#125; &#125; public static void main(String[] args) &#123; Person[] p = new Person[4]; for (int i = 0; i &lt; p.length; i++) &#123; p[i] = randPerson(); // 随机生成Boy或Girl &#125; for (int i = 0; i &lt; p.length; i++) &#123; p[i].eat(); &#125; &#125;&#125;//输出Person.eat()Person.eat()Person.eat()Person.eat() 对于静态方法而言，不管父类引用指向的什么子类对象，调用的都是父类的方法。 总结 上溯造型和动态绑定实际上就是多态的体现，下溯造型是为了解决因为上溯而导致衍生类功能变小的问题，继承则是上溯和下溯以及动态编译的基础。","categories":[{"name":"java","slug":"java","permalink":"http://kingge.top/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"继承","slug":"继承","permalink":"http://kingge.top/tags/继承/"},{"name":"多态","slug":"多态","permalink":"http://kingge.top/tags/多态/"}]},{"title":"注解实现请求方法的登录控制","slug":"注解实现请求方法的登录控制","date":"2017-09-06T02:37:08.000Z","updated":"2017-09-06T03:56:00.492Z","comments":true,"path":"2017/09/06/注解实现请求方法的登录控制/","link":"","permalink":"http://kingge.top/2017/09/06/注解实现请求方法的登录控制/","excerpt":"","text":"前言 之前一直使用的是，拦截器来统一验证当前用户是否登录，通过验证cookie或者session里面的是否存在已经登录标识来完成登录逻辑判断。但是会发现，这个很麻烦，而且有很多配置需要配置，例如免验证URL等等配置，无法实现可拔插式方法级别的控制。 public class RequestInterceptor extends HandlerInterceptorAdapter &#123; public String[] allowUrls;//配置不拦截的资源，所以在代码里面来排除. public void setAllowUrls(String[] allowUrls) &#123; this.allowUrls = allowUrls; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; // TODO Auto-generated method stub &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // TODO Auto-generated method stub request.setCharacterEncoding(&quot;UTF8&quot;); HttpSession session=request.getSession();//获取登录的SESSION String sessionid=request.getSession().getId();//获取登录的SESSIONID String requestPath=request.getServletPath();//获取客户请求页面 //先过滤掉不需要判断SESSION的请求 for(String url : allowUrls) &#123; if(requestPath.contains(url)) &#123; return true; &#125; &#125; Object attribute = request.getSession().getAttribute(&quot;sys_user&quot;); if( attribute == null )&#123; response.sendRedirect(&quot;/index.jsp&quot;); &#125; return true; &#125; 大体上是这样的，通过allowUrls来控制免登录url（上面的代码其实可以使用配置文件的方式来配置allowUrls的值，可以不通过setAllowUrls的方式来赋值，但是为了方面扩展就加入了。） 这里会面临一个问题，那就是如果网站网页多的话，那么allowUrls的值会变得很庞大，可能会缺漏。所以下面讲解本人用到的解决方式—-注解 和 spring配置方式（跟数组形式没有什么区别） spring 配置方式path 对所有的请求拦截使用/**，对某个模块下的请求拦截使用：/myPath/*&lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot; /&gt; &lt;bean class=&quot;com.kingge.oa.user.LoginInterceptor&quot; /&gt;&lt;/mvc:interceptor&gt; 或者 &lt;!-- 拦截是否登录 &lt;mvc:interceptor&gt; 需拦截的地址 二级目录 &lt;mvc:mapping path=&quot;/*/*&quot;/&gt; &lt;bean class=&quot;com.jk.ssm.interceptor.RequestInterceptor&quot; &gt; &lt;property name=&quot;allowUrls&quot;&gt; //回去调用拦截器的 setAllowUrls 方法 &lt;list&gt; 如果请求中包含以下路径，则不进行拦截 &lt;value&gt;/account/login.html&lt;/value&gt; &lt;value&gt;/captcha/image.html&lt;/value&gt; &lt;value&gt;/register/register.html&lt;/value&gt; &lt;value&gt;/error/400.html&lt;/value&gt; &lt;value&gt;/error/404.html&lt;/value&gt; &lt;value&gt;/error/500.html&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/mvc:interceptor&gt; 使用注解关于注解 官方说辞：JDK5开始，java增加了对元数据(MetaData)的支持，怎么支持？答：通过Annotation(注解）来实现。Annotation提供了为程序元素设置元数据的方法。元数据：描述数据的数据。 个人理解：首先什么是元数据，元数据就是对一类事物的统称，他不仅限于某个事物的描述。例如我们有ABC三个系统，分别使用oracle，mysql，db2，都有登录功能，他们的用户表字段名称是不一样的。那么有个需求，我想把A系统的用户数据pour到B系统中，那么进行映射操作？这个时候就需要一个描述用户数据的一个统一标识（元数据）这样我们就可以先把，A系统数据映射到元数据，然后再从元数据取数据映射到B系统中。 粗俗的理解，元数据就是一个类的属性，但是他所具备的职能的而应用范围，跟真正意义上类的属性数不一样的。传统的类的属性他只描述这个类，元数据可以描述多个具有共性的类。 再举个例子，我们现在常用的数据中心（DC）就是使用了元数据来作为数据传输的媒介。 元数据作用：：Annotation就像代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取。读取到了程序元素的元数据，就可以执行相应的处理。通过注解，程序开发人员可以在不改变原有逻辑的情况下，在源代码文件中嵌入一些补充信息。代码分析工具、开发工具和部署工具可以通过解析这些注解获取到这些补充信息，从而进行验证或者进行部署等。 到java8为止一共提供了五个 注解 unchecked异常：运行时异常。是RuntimeException的子类，不需要在代码中显式地捕获unchecked异常做处理。Java异常 @SafeVarargs (java7新增）：java7的“堆污染”警告与@SafeVarargs堆污染：把一个不带泛型的对象赋给一个带泛型的变量是，就会发生堆污染。例如：下面代码引起堆污染，会给出警告List l2 = new ArrayList&lt;Number&gt;();List&lt;String&gt; ls = l2;3中方式去掉这个警告 3种方式去掉这个警告：使用注解@SafeVarargs修饰引发该警告的方法或构造器。使用@SuppressWarnings(“unchecked”) 修饰。使用编译器参数命令：-Xlint:varargs @Functionlnterface （java8新增）：修饰函数式接口使用该注解修饰的接口必须是函数式接口，不然编译会出错。那么什么是函数式接口？答：如果接口中只有一个抽象方法（可以包含多个默认方法或static方法），就是函数式接口。 五个基本元注解 元注解：描述注解的注解（概念跟元数据类似）。 java提供了6个元注解（Meta Annotation)，在java.lang.annotation中。其中5个用于修饰其他的Annonation定义。而@Repeatable专门用于定义Java8新增的重复注解。所以要定义注解必须使用到5个元注解来定义( 五个注解用法 详情百度 ) @Inherited @Documented @Retention（英文：保留） @Target ( 目标) 自定义注解 参见下面，例子或者白度，具体就不阐述了。 使用注解解决登录问题定义一个枚举类 作用： 是否进行验证权限（因为后期可能会增加权限判断注解，而且是否登录也可以说是权限判断的一种，所以这里的枚举类的作用就是保存是否进行权限判断信息） public enum Action&#123; Normal(&quot;0&quot;,&quot;执行权限验证&quot;), Skip(&quot;1&quot;, &quot;跳过权限验证&quot;); private final String key; private final String desc; private Action(String key, String desc) &#123; this.key = key; this.desc = desc; &#125; //省略get set方法 定义登录和权限注解 Login属性是action ，属性类型是Action（上面的枚举类） @Target(ElementType.METHOD)@Documented@Retention(RetentionPolicy.RUNTIME)public @interface Login&#123; Action action() default Action.Normal;&#125; @Target(&#123;java.lang.annotation.ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Permission&#123; String value() default &quot;&quot;; // 这里我是保存一个权限代码，例如赋值为4000，表示当前用户的必须具备4000的权限才能够访问方法 Action action() default Action.Normal;&#125; 拦截器public class LoginInterceptor extends HandlerInterceptorAdapter&#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception&#123; if(handler instanceof HandlerMethod)&#123; //是否为请求方法 HandlerMethod handlerMethod = (HandlerMethod) handler; Login login = handlerMethod.getMethodAnnotation(Login.class);//当前请求方法是否添加了Login注解 if( login != null &amp;&amp; &quot;0&quot;.equals(login.action().getKey()) )&#123;//判断属性的值是否是0-表示需要进行登录验证 Object attribute = request.getSession().getAttribute(&quot;sys_user&quot;); if( attribute == null )&#123; response.sendRedirect(&quot;/index.jsp&quot;); &#125; &#125; return true; &#125; return true; &#125;&#125; 在spring中配置拦截器&lt;mvc:interceptors&gt;&lt;bean class=&quot;com.kingge.oa.user.LoginInterceptor&quot;&gt;&lt;/bean&gt;&lt;/mvc:interceptors&gt; 给请求方法添加权限控制 @Login(action=Action.Skip) //不需要进行登录校验 @Permission(value=&quot;4000&quot;,action=Action.normal)//需要进行权限号为4000的权限校验 @RequestMapping(&quot;/list&quot;) public String list(Model model,HttpServletRequest request) &#123; request.getSession().setAttribute(&quot;sys_user&quot;, &quot;denglule&quot;); List&lt;User&gt; userList = userService.findAllObjects(); System.out.println( userList ); model.addAttribute(&quot;userList&quot;,userList ); return &quot;list&quot;; &#125; @Login(action=Action.Normal)//添加操作，需要校验是否登录 @RequestMapping(value=&quot;/add&quot;, method=RequestMethod.POST) public String add( User user ) &#123; System.out.println( user ); userService.insert(user); return &quot;forward:/user/list&quot;; &#125;","categories":[{"name":"java","slug":"java","permalink":"http://kingge.top/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java注解","slug":"java注解","permalink":"http://kingge.top/tags/java注解/"},{"name":"登录控制","slug":"登录控制","permalink":"http://kingge.top/tags/登录控制/"}]},{"title":"java8新特性","slug":"java8新特性","date":"2017-08-29T04:27:16.000Z","updated":"2017-09-01T02:08:20.226Z","comments":true,"path":"2017/08/29/java8新特性/","link":"","permalink":"http://kingge.top/2017/08/29/java8新特性/","excerpt":"","text":"Java 8可谓是自Java 5以来最具革命性的版本了，她在语言、编译器、类库、开发工具以及Java虚拟机等方面都带来了不少新特性。我们来一一回顾一下这些特性。 一、Lambda表达式 Lambda表达式可以说是Java 8最大的卖点，她将函数式编程引入了Java。Lambda允许把函数作为一个方法的参数，或者把代码看成数据。Lambda 是一个匿名函数。 一个Lambda表达式可以由用逗号分隔的参数列表、–&gt;符号与函数体三部分表示。例如： 例子1 需求： 比较TreeSet中数据，按小到大输出 使用匿名内部类实现一个排序功能 //采用匿名内部类的方式-实现比较器 Comparator&lt;Integer&gt; comparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return Integer.compare(o1, o2);//关键代码 &#125; &#125;;//传入比较器 TreeSet&lt;Integer&gt; tree2 = new TreeSet&lt;&gt;(comparator ); tree2.add(12); tree2.add(-12); tree2.add(100);System.out.println(tree2) //输出 -12 12 100 我们不难发现上面的代码存在一个问题：其实关键代码只有第七行，其他代码都是冗余的 使用Lambda表达式实现同样功能 //使用Lambda表达式，抽取关键代码，减少代码量Comparator&lt;Integer&gt; comparator2 = (x, y) -&gt; Integer.compare(x, y); //关键代码 TreeSet&lt;Integer&gt; tree = new TreeSet&lt;&gt;(comparator2 ); tree.add(12); tree.add(-12); tree.add(100); tree.forEach(System.out::println);//代替System.out.println 代码瞬间就变得很简短，你可能觉得这个有什么，没什么感觉。那么我们在进入第二个例子 例子2 需求：1.获取公司中年龄小于 35 的员工信息2.获取公司中工资大于 5000 的员工信息。。。。。。 前期准备实现一个Employee类,有四个属性 private int id;private String name;private int age;private double salary;忽略get/set方法和构造器 初始化一个List： List&lt;Employee&gt; emps = Arrays.asList( new Employee(101, &quot;张三&quot;, 18, 9999.99), new Employee(102, &quot;李四&quot;, 59, 6666.66), new Employee(103, &quot;王五&quot;, 28, 3333.33), new Employee(104, &quot;赵六&quot;, 8, 7777.77), new Employee(105, &quot;田七&quot;, 38, 5555.55)); 常规方法实现实现两个方法，然后传入需要过滤的源数据，返回过滤后的结果集 //需求：获取公司中年龄小于 35 的员工信息public List&lt;Employee&gt; filterEmployeeAge(List&lt;Employee&gt; emps)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee emp : emps) &#123; if(emp.getAge() &lt;= 35)&#123;//比较代码 list.add(emp); &#125; &#125; return list;&#125;//需求：获取公司中工资大于 5000 的员工信息public List&lt;Employee&gt; filterEmployeeSalary(List&lt;Employee&gt; emps)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee emp : emps) &#123; if(emp.getSalary() &gt;= 5000)&#123;//比较代码 list.add(emp); &#125; &#125; return list;&#125; 我们不难发现上面的代码存在一个问题：那就是两个方法除了比较部分不同，其他逻辑是一样的，存在大量冗余，假设有新的需求（例如求得求得名字姓王的员工）那么就需要再创建一个 filterEmployee**方法对应新的需求。 使用策略设计模式实现 提供父借口 和 两个 实现类（两个需求对应的逻辑实现类） // 父接口 @FunctionalInterfacepublic interface MyPredicate&lt;T&gt; &#123; public boolean test(T t); &#125;//需求1 实现类-年龄小于35public class FilterEmployeeForAge implements MyPredicate&lt;Employee&gt;&#123; @Override public boolean test(Employee t) &#123; return t.getAge() &lt;= 35; &#125;&#125;//需求1 实现类-工资大于5000public class FilterEmployeeForSalary implements MyPredicate&lt;Employee&gt; &#123; @Override public boolean test(Employee t) &#123; return t.getSalary() &gt;= 5000; &#125;&#125; 测试代码 // 通用过滤方法 public List&lt;Employee&gt; filterEmployee(List&lt;Employee&gt; emps, MyPredicate&lt;Employee&gt; mp)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee employee : emps) &#123; if(mp.test(employee))&#123; list.add(employee); &#125; &#125; return list; &#125; @Test public void test4()&#123; //传入实现年龄过滤的实现类 List&lt;Employee&gt; list = filterEmployee(emps, new FilterEmployeeForAge()); for (Employee employee : list) &#123; System.out.println(employee); &#125; System.out.println(&quot;------------------------------------------&quot;); List&lt;Employee&gt; list2 = filterEmployee(emps, new FilterEmployeeForSalary()); for (Employee employee : list2) &#123; System.out.println(employee); &#125; &#125; 使用策略模式比上一个的好处是：代码很清晰，便于维护，新的需求我们只需要再实现对应的需求实现类即可，然后传入MyPredicate```接口即可。缺点是：需要实现对应的需求类然后实现``` MyPredicate&lt;T&gt;```接口### **匿名内部类**这种方法类似于例子1中的 Comparator这个接口的实现```JAVA//直接使用 MyPredicate&lt;Employee&gt;接口，不去实现对应的需求类（上面的FilterEmployeeForSalary 和 FilterEmployeeForAge ） @Test public void test5()&#123; List&lt;Employee&gt; list = filterEmployee(emps, new MyPredicate&lt;Employee&gt;() &#123; @Override public boolean test(Employee t) &#123; return t.getId() &lt;= 103; &#125; &#125;); for (Employee employee : list) &#123; System.out.println(employee); &#125; &#125; 我们不难发现上面的代码存在一个问题：跟例子1一样，存在大量的冗余。 Lambda 表达式实现前期准备public List&lt;Employee&gt; filterEmployee(List&lt;Employee&gt; emps, MyPredicate&lt;Employee&gt; mp)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee employee : emps) &#123; if(mp.test(employee))&#123; list.add(employee); &#125; &#125; return list;&#125; @Testpublic void test6()&#123; List&lt;Employee&gt; list = filterEmployee(emps, (e) -&gt; e.getAge() &lt;= 35); list.forEach(System.out::println); System.out.println(\"------------------------------------------\"); List&lt;Employee&gt; list2 = filterEmployee(emps, (e) -&gt; e.getSalary() &gt;= 5000); list2.forEach(System.out::println);&#125; 我们不难发现上面的代码存在一个问题：这个代码，是不是已经非常简短了，感觉已经是终极的最简代码。但是实际上还有更简短的代码（使用stream api）缺点：太过依赖 MyPredicate 这个接口，假设这个接口不存在，该怎么办呢？（我们这里仅仅是做个假设） 终极实现方式：Stream API@Testpublic void test7()&#123; emps.stream() .filter((e) -&gt; e.getAge() &lt;= 35) .forEach(System.out::println); System.out.println(&quot;----------------------------------------------&quot;); emps.stream() .filter((e) -&gt; e.getSalary() &gt;= 5000) .forEach(System.out::println); System.out.println(&quot;----------------------------------------------&quot;); // 可以使用map 指定输出那个属性的值，代替普通的便利输出 emps.stream() .map(Employee::getName) .limit(3)// 输出前三个 .sorted()//排序 .forEach(System.out::println); &#125; 输出 Employee [id=101, name=张三, age=18, salary=9999.99]Employee [id=103, name=王五, age=28, salary=3333.33]Employee [id=104, name=赵六, age=8, salary=7777.77]----------------------------------------------Employee [id=101, name=张三, age=18, salary=9999.99]Employee [id=102, name=李四, age=59, salary=6666.66]Employee [id=104, name=赵六, age=8, salary=7777.77]Employee [id=105, name=田七, age=38, salary=5555.55]----------------------------------------------张三李四王五 我们不难发现上面的代码存在一个问题：这个代码，是非常潇洒，舒服的，不依赖我们上面所说的接口。 函数式接口 为了使现有函数更好的支持Lambda表达式，Java 8引入了函数式接口的概念。函数式接口就是只有一个方法的普通接口。java.lang.Runnable与java.util.concurrent.Callable是函数式接口最典型的例子。为此，Java 8增加了一种特殊的注解@FunctionalInterface： –也就是说：这个接口里面只能够存在一个接口方法，多个就会报错 例子：@FunctionalInterfacepublic interface Functional &#123; void method();&#125; 认识Lambda表达式概念 一、Lambda 表达式的基础语法：Java8中引入了一个新的操作符 “-&gt;” 该操作符称为箭头操作符或 Lambda 操作符 箭头操作符将 Lambda 表达式拆分成两部分： 左侧：Lambda 表达式的参数列表 右侧：Lambda 表达式中所需执行的功能， 即 Lambda 体 上面的例子：List list = filterEmployee(emps, (e) -&gt; e.getAge() &lt;= 35); 第二个参数他会去找 MyPredicate&lt;T&gt; 接口里面的 public boolean test(T t);test方法，lambda表达式左边的(e) 对应的是test方法的入参, ambda表达式右边的e.getAge() &lt;= 35 对应得是test方法的实现 那么你可能会有疑问，假设MyPredicate接口里面有很多个接口方法，那么他会去调用那个呢？他怎么知道去找test方法呢？ 引入了：@FunctionalInterface这个函数式接口的概念，解决了这个问题。 * 语法格式一：无参数，无返回值 * () -&gt; System.out.println(&quot;Hello Lambda!&quot;); &gt; 例如 Runnable接口的 run方法就是无参数无返回值： @Test public void test1()&#123; int num = 0;//jdk 1.7 前，我们知道匿名内部引用局部变量必须声明为final //但jdk1.8，它默认给我们添加了final，不用显示声明。 Runnable r = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Hello World!&quot; + num); //这里如果改为 num++是会报错的，因为他本质上是一个final &#125; &#125;; r.run(); System.out.println(&quot;-------------------------------&quot;); Runnable r1 = () -&gt; System.out.println(&quot;Hello Lambda!&quot;); r1.run(); &#125;这两个是等效的 * * 语法格式二：有一个参数，并且无返回值* (x) -&gt; System.out.println(x)* 例子：Consumer这个类jdk自带--有参数无返回值@Testpublic void test2()&#123; Consumer&lt;String&gt; con = x -&gt; System.out.println(x); con.accept(&quot;我是你泽精哥！&quot;);&#125; * 语法格式三：若只有一个参数，小括号可以省略不写* x -&gt; System.out.println(x)* * 语法格式四：有两个以上的参数，有返回值，并且 Lambda 体中有多条语句* Comparator&lt;Integer&gt; com = (x, y) -&gt; &#123;* System.out.println(&quot;函数式接口&quot;);* return Integer.compare(x, y);* &#125;; * 语法格式五：若 Lambda 体中只有一条语句， return 和 大括号都可以省略不写* Comparator&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x, y);* * 语法格式六：Lambda 表达式的参数列表的数据类型可以省略不写，因为JVM编译器通过上下文推断出，数据类型，即“类型推断”* (Integer x, Integer y) -&gt; Integer.compare(x, y); 类型推断 : jdk1.8后，添加了这个功能String[] strs = {“aaa”, “bbb”, “ccc”} ; 它自动会转换里面的数据为String类型的数据改为： String[] strs;strs = &#123;&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;&#125;;//会报错--因为这样无法进行类型推断 类型推断例子2 public void show(Map&lt;String, Integer&gt; map)&#123;&#125;//方法 show(new HashMap&lt;&gt;());//调用方法我们发现在调用方法的时候入参我们并没有明确声明类型，但是在jdk1.8中是可以编译通过的。这里也是运用了类型推断（注意：jdk1.7中编译会失败） 热身例子一 //函数是接口@FunctionalInterfacepublic interface MyFun &#123; public Integer getValue(Integer num);&#125;//测试 //需求：对一个数进行运算 @Test public void test6()&#123; Integer num = operation(100, (x) -&gt; x * x); System.out.println(num); System.out.println(operation(200, (y) -&gt; y + 200)); &#125; public Integer operation(Integer num, MyFun mf)&#123; return mf.getValue(num); &#125; 热身例子二//函数接口 @FunctionalInterface //约束当前接口只能有一个方法public interface CalcLong&lt;K,T&gt;&#123; // public K getMultiply(T t, T tt); K getMultiply(T t, T tt);&#125;//需求：求得两个数的和 String result = getMuyl(10L,10L,(e,ee)-&gt;&#123; System.out.println(e+ &quot; &quot; + ee); return e+ee+&quot;&quot;; &#125;); System.out.println(result); public String getMuyl(Long l,Long ll,CalcLong&lt;String,Long&gt; mf)&#123; return mf.getMultiply(l, ll); &#125; 看到这里可能会有疑惑？我靠，使用lambda表达式还得声明一个函数接口，这么麻烦。实际上，java内部已经帮我们实现了很多个接口供我们使用，不需要重新自己定义，除非有特别操作。 java8内置四大函数式接口 为了解决接口需要自定义问题 /* * Java8 内置的四大核心函数式接口 * * Consumer&lt;T&gt; : 消费型接口 * void accept(T t); * * Supplier&lt;T&gt; : 供给型接口 * T get(); * * Function&lt;T, R&gt; : 函数型接口 * R apply(T t); * * Predicate&lt;T&gt; : 断言型接口 * boolean test(T t); * */ 例子消费型接口//Consumer&lt;T&gt; 消费型接口 :@Testpublic void test1()&#123; String p; happy(10000, (m) -&gt; System.out.println(&quot;桑拿，每次消费：&quot; + m + &quot;元&quot;));&#125; public void happy(double money, Consumer&lt;Double&gt; con)&#123; con.accept(money);&#125; Supplier 供给型接口 //Supplier&lt;T&gt; 供给型接口 :@Testpublic void test2()&#123; List&lt;Integer&gt; numList = getNumList(10, () -&gt; (int)(Math.random() * 100)); for (Integer num : numList) &#123; System.out.println(num); &#125;&#125;//需求：产生指定个数的整数，并放入集合中public List&lt;Integer&gt; getNumList(int num, Supplier&lt;Integer&gt; sup)&#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; num; i++) &#123; Integer n = sup.get(); list.add(n); &#125; return list;&#125; Function 函数型接口 //Function&lt;T, R&gt; 函数型接口：@Testpublic void test3()&#123; String newStr = strHandler(&quot;\\t\\t\\t 去除前后空格 &quot;, (str) -&gt; str.trim()); System.out.println(newStr); String subStr = strHandler(&quot;截取字符串你知不知道&quot;, (str) -&gt; str.substring(2, 5)); System.out.println(subStr);&#125;//需求：用于处理字符串public String strHandler(String str, Function&lt;String, String&gt; fun)&#123; return fun.apply(str);&#125; Predicate 断言型接口 //Predicate&lt;T&gt; 断言型接口：@Testpublic void test4()&#123; List&lt;String&gt; list = Arrays.asList(&quot;Hello&quot;, &quot;atguigu&quot;, &quot;Lambda&quot;, &quot;www&quot;, &quot;ok&quot;); List&lt;String&gt; strList = filterStr(list, (s) -&gt; s.length() &gt; 3); for (String str : strList) &#123; System.out.println(str); &#125;&#125;//需求：将满足条件的字符串，放入集合中public List&lt;String&gt; filterStr(List&lt;String&gt; list, Predicate&lt;String&gt; pre)&#123; List&lt;String&gt; strList = new ArrayList&lt;&gt;(); for (String str : list) &#123; if(pre.test(str))&#123; strList.add(str); &#125; &#125; return strList;&#125; 四大内置函数衍生的子函数 二、接口的默认方法与静态方法 我们可以在接口中定义默认方法，使用default关键字，并提供默认的实现。所有实现这个接口的类都会接受默认方法的实现，除非子类提供的自己的实现。例如：public interface DefaultFunctionInterface &#123; default String defaultFunction() &#123; return &quot;default function&quot;; &#125;&#125; 我们还可以在接口中定义静态方法，使用static关键字，也可以提供实现。例如：public interface StaticFunctionInterface &#123; static String staticFunction() &#123; return &quot;static function&quot;; &#125;&#125; 接口的默认方法和静态方法的引入，其实可以认为引入了C＋＋中抽象类的理念，以后我们再也不用在每个实现类中都写重复的代码了。 三、方法引用 通常与Lambda表达式联合使用，可以直接引用已有Java类或对象的方法。一般有四种不同的方法引用： 构造器引用 构造器引用。语法是Class::new，构造器的参数列表，需要与函数式接口中参数列表保持一致！也就是说，决定Class::new调用那一个构造器得是：接口函数的方法的参数 //构造器引用@Testpublic void test7()&#123; // Supplier 的接口方法 T get(); --所以调用无参构造器 Supplier&lt;Employee&gt; fun0 = Employee::new; //Function 的接口方法 R apply(T t);-调用一个参数构造器 Function&lt;String, Employee&gt; fun = Employee::new; //BiFunction 的接口方法 R apply(T t, U u); -调用二参构造器 BiFunction&lt;String, Integer, Employee&gt; fun2 = Employee::new;&#125; 对象静态方法引用（类名::静态方法） 静态方法引用。语法是Class::static_method，要求接受一个Class类型的参数； //类名 :: 静态方法名//max和compare 都是静态方法@Testpublic void test4()&#123; Comparator&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x, y); System.out.println(&quot;-------------------------------------&quot;); Comparator&lt;Integer&gt; com2 = Integer::compare; BiFunction&lt;Double, Double, Double&gt; fun = (x, y) -&gt; Math.max(x, y); System.out.println(fun.apply(1.5, 22.2)); System.out.println(&quot;------------------------------------&quot;); BiFunction&lt;Double, Double, Double&gt; fun2 = Math::max; System.out.println(fun2.apply(1.2, 1.5));&#125; 对象实例方法引用（对象引用::实例方法名） 特定类的任意对象方法引用。它的语法是Class::method。要求方法是没有参数的； //对象的引用 :: 实例方法名@Testpublic void test2()&#123; Employee emp = new Employee(101, &quot;张三&quot;, 18, 9999.99); Supplier&lt;String&gt; sup = () -&gt; emp.getName(); System.out.println(sup.get()); System.out.println(&quot;----------------------------------&quot;); Supplier&lt;String&gt; sup2 = emp::getName; System.out.println(sup2.get());&#125; 类名实例方法引用(类名::实例方法名) 我们知道一般是有对象才能够引用实例方法，但是有种特殊情况是可以直接使用类名引用实例方法若Lambda 的参数列表的第一个参数，是实例方法的调用者，第二个参数(或无参)是实例方法的参数时，格式： ClassName::MethodName //类名 :: 实例方法名//按照常规是String st = new String(&quot;123&quot;); st::equals,//对象调用实例方法，但是下面因为符合第四种引用的规则，//所以可以使用类名调用实例方法@Testpublic void test5()&#123;//第一个参数为实例方法调用者，第二个参数为为实例方法参数 BiPredicate&lt;String, String&gt; bp = (x, y) -&gt; x.equals(y); System.out.println(bp.test(&quot;abcde&quot;, &quot;abcde&quot;)); System.out.println(&quot;-------------------------------------&quot;); BiPredicate&lt;String, String&gt; bp2 = String::equals;System.out.println(bp2.test(&quot;abc&quot;, &quot;abc&quot;)); System.out.println(&quot;---------------------------------------&quot;); //第一个参数为实例方法调用者，第二个参数为空Function&lt;Employee, String&gt; fun = (e) -&gt; e.show();System.out.println(fun.apply(new Employee()));System.out.println(&quot;--------------------------------------&quot;); Function&lt;Employee, String&gt; fun2 = Employee::show; System.out.println(fun2.apply(new Employee())); &#125; 注意： ①方法体所引用的方法的参数列表与返回值类型，需要与函数式接口中抽象方法的参数列表和返回值类型保持一致！ ②若Lambda 的参数列表的第一个参数，是实例方法的调用者，第二个参数(或无参)是实例方法的参数时，格式：ClassName::MethodName (针对于第四种方法引用) 数组引用（类型[] :: new）//数组引用@Testpublic void test8()&#123; Function&lt;Integer, String[]&gt; fun = (args) -&gt; new String[args]; String[] strs = fun.apply(10); System.out.println(strs.length); System.out.println(&quot;--------------------------&quot;); Function&lt;Integer, Employee[]&gt; fun2 = Employee[] :: new; Employee[] emps = fun2.apply(20); System.out.println(emps.length);&#125; 四、重复注解在Java 5中使用注解有一个限制，即相同的注解在同一位置只能声明一次。Java 8引入重复注解，这样相同的注解在同一地方也可以声明多次。重复注解机制本身需要用@Repeatable注解。Java 8在编译器层做了优化，相同注解会以集合的方式保存，因此底层的原理并没有变化。 五、扩展注解的支持Java 8扩展了注解的上下文，几乎可以为任何东西添加注解，包括局部变量、泛型类、父类与接口的实现，连方法的异常也能添加注解。 六、OptionalJava 8引入Optional类来防止空指针异常，Optional类最先是由Google的Guava项目引入的。Optional类实际上是个容器：它可以保存类型T的值，或者保存null。使用Optional类我们就不用显式进行空指针检查了。 七、Stream前言 Stream API是把真正的函数式编程风格引入到Java中。其实简单来说可以把Stream理解为MapReduce，当然Google的MapReduce的灵感也是来自函数式编程。她其实是一连串支持连续、并行聚集操作的元素。从语法上看，也很像linux的管道、或者链式编程，代码写起来简洁明了，非常酷帅！ 八、Date/Time API (JSR 310)Java 8新的Date-Time API (JSR 310)受Joda-Time的影响，提供了新的java.time包，可以用来替代 java.util.Date和java.util.Calendar。一般会用到Clock、LocaleDate、LocalTime、LocaleDateTime、ZonedDateTime、Duration这些类，对于时间日期的改进还是非常不错的。 九、JavaScript引擎NashornNashorn允许在JVM上开发运行JavaScript应用，允许Java与JavaScript相互调用。 十、Base64在Java 8中，Base64编码成为了Java类库的标准。Base64类同时还提供了对URL、MIME友好的编码器与解码器。 除了这十大新特性之外，还有另外的一些新特性： 更好的类型推测机制：Java 8在类型推测方面有了很大的提高，这就使代码更整洁，不需要太多的强制类型转换了。 编译器优化：Java 8将方法的参数名加入了字节码中，这样在运行时通过反射就能获取到参数名，只需要在编译时使用-parameters参数。 并行（parallel）数组：支持对数组进行并行处理，主要是parallelSort()方法，它可以在多核机器上极大提高数组排序的速度。 并发（Concurrency）：在新增Stream机制与Lambda的基础之上，加入了一些新方法来支持聚集操作。 Nashorn引擎jjs：基于Nashorn引擎的命令行工具。它接受一些JavaScript源代码为参数，并且执行这些源代码。 类依赖分析器jdeps：可以显示Java类的包级别或类级别的依赖。 JVM的PermGen空间被移除：取代它的是Metaspace（JEP 122），元空间直接采用的是物理空间，也即是我们电脑的内存，电脑内存多大，元空间就有多大。","categories":[{"name":"Java8","slug":"Java8","permalink":"http://kingge.top/categories/Java8/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java8","slug":"java8","permalink":"http://kingge.top/tags/java8/"},{"name":"java8新特性","slug":"java8新特性","permalink":"http://kingge.top/tags/java8新特性/"}]},{"title":"哈希表冲突解决方式之开放地址法和链地址法","slug":"哈希表冲突解决方式之开放地址法和链地址法","date":"2017-08-29T03:07:51.000Z","updated":"2017-08-29T03:56:03.815Z","comments":true,"path":"2017/08/29/哈希表冲突解决方式之开放地址法和链地址法/","link":"","permalink":"http://kingge.top/2017/08/29/哈希表冲突解决方式之开放地址法和链地址法/","excerpt":"","text":"基本定义 散列技术是在记录的存储位置和它的关键字之间建立一个确定的对应关系 f，使得每个关键字key对应一个存储位置f(key)。 这种对应关系f称为散列或哈希函数 采用上述思想将数据存储在一块连续的存储空间中，这块连续的存储空间称为散列或哈希表 关键字对应的存储位置称为散列地址 如果碰到两个不同的关键字key1≠key2，但却有相同的f(key1)=f(key2)，这种现象称为冲突， 并把key1和key2 称为这个散列函数的同义词（synonym） 散列函数构造方法好的散列函数参考如下两个原则： 计算简单 散列地址分布均匀 最常用的方法是除留余数法，对于散列表长度为m的散列函数是 f(key)=key mod p (p≤m) 处理散列冲突 开放地址法 开放地址法就是一旦发生冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列表总能找到，并存入。开放地址法又分为线性探测法，二次探测法和随机探测法。 链地址法 将所有同义词的关键字存储在同一个单链表中，称这个单链表为同义词子表，在散列表中只存储同义词子表的头指针。只要有冲突，就在同义词的子表中增加结点。(java中的HashMap就是采用这种方法) 开放地址法","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://kingge.top/categories/数据结构/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"http://kingge.top/tags/数据结构/"},{"name":"哈希表","slug":"哈希表","permalink":"http://kingge.top/tags/哈希表/"},{"name":"哈希解决冲突","slug":"哈希解决冲突","permalink":"http://kingge.top/tags/哈希解决冲突/"}]},{"title":"java8之Hashmap","slug":"java8之Hashmap","date":"2017-08-28T14:27:16.000Z","updated":"2017-08-29T08:08:38.273Z","comments":true,"path":"2017/08/28/java8之Hashmap/","link":"","permalink":"http://kingge.top/2017/08/28/java8之Hashmap/","excerpt":"","text":"Java8-HashMap变化 数据的存储结构从：数组+链表 演变为了 数组+链表+红黑树 Map 家庭族谱 HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 总结对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 HashMap简介 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 我们知道HashMap的数据存储结构就是：数组加上链表。通过对于key的值做hash运算，获得对应的值找到对应的数组下标，然后再存储值。存储值的过程中可能当前数组已经存在值（这个称之为冲突） 然后再生成一个链表存储冲突的值。 HashCode() 和 Hash() 方法实现得足够好，能够尽可能地减少冲突的产生，那么对 HashMap 的操作几乎等价于对数组的随机访问操作，具有很好的性能。但是，如果 HashCode() 或者 Hash() 方法实现较差，在大量冲突产生的情况下，HashMap 事实上就退化为几个链表，对 HashMap 的操作等价于遍历链表，此时性能很差。 解决冲突的方法：开放地址法和链地址法 HashMap特点 允许null为key 输出无序 如果想要输出有序，那以使用继承他的LinkedHashMap，元素输出顺序跟输入顺序一致,他提供了一个节点保存输入的元素的顺序。想要对元素的值进行排序 推荐TreeMap（因为他继承了SortedMap) 非线程安全 数组+链表存储方式 Java8特性 HashMap是数组+链表+红黑树 存储算法： map.put(&quot;kingge&quot;,&quot;shuai&quot;)系统将调用kingge”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 好的hash算法和扩容机制是解决冲突和高效存取的命题 HashMap 重要的几个属性int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; Node[] table(Hash桶)始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 分析HashMap的put方法put方法图解，详情可以去看源码 public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 为什么说HashMap是线程不安全的 个人觉得有两个表现，如果还有其他的希望大家补充，或者以后等楼主源码研究透了再补充 表现一 我们知道当插入数据超过了threshold(threshold=length * Load factor),那么就会扩容，扩容会去调用resize和transfer方法，这个时候原先hash桶里面的所有数据都会重新计算，对应的位置–称之为rehash，这个成本很大 最根本的原因是出现时死循环-也就是在死锁问题出现在了transfer方法上面,而且是因为在扩容转换的过程中采用的是链表的头插法的形式进行插入数据。例如原来在数组arr[0]的位置又链表1–&gt;2–&gt;3 那么扩容后，采用头插法就变成了arr[0]：3–&gt;2–&gt;1 为什么采用头插法，因为时间复杂度为O(1)，想象一下尾插法，那么需要遍历找到最尾元素然后插入时间复杂度是O(n) 具体源码分析参见：http://www.importnew.com/22011.html 表现二多个线程同时操作一个hashmap就可能出现不安全的情况：比如A B两个线程(A线程获数据 B线程存数据) 同时操作myHashMap1.B线程执行存放数据modelHashMap.put(“1”,”2”);2.A线程执行get获取数据modelHashMap.get(“1”)A线程获取的值本来应该是2，但是如果A线程在刚到达获取的动作还没执行的时候，线程执行的机会又跳到线程B，此时线程B又对modelHashMap赋值 如：modelHashMap.put(“1”,”3”);然后线程虚拟机又执行线程A，A取到的值为3，这样map中第一个存放的值 就会丢失。。。。。—原子性 解决HashMap非线程安全其实上面我已经有提过了： 三个方法： Hashtable替换HashMap Collections.synchronizedMap将HashMap包装起来 private Map map = Collections.synchronizedMap(new HashMap());替换private HashMap map = new HashMap(); ConcurrentHashMap替换HashMap private ConcurrentHashMap map = new ConcurrentHashMap();替换private HashMap map = new HashMap(); 好的博文http://blog.csdn.net/lyg468088/article/details/49464121","categories":[{"name":"Java8","slug":"Java8","permalink":"http://kingge.top/categories/Java8/"}],"tags":[{"name":"java","slug":"java","permalink":"http://kingge.top/tags/java/"},{"name":"java8","slug":"java8","permalink":"http://kingge.top/tags/java8/"},{"name":"java8新特性","slug":"java8新特性","permalink":"http://kingge.top/tags/java8新特性/"}]},{"title":"程序员未来规划","slug":"程序员未来规划","date":"2017-08-28T02:00:36.000Z","updated":"2017-08-28T02:06:13.437Z","comments":true,"path":"2017/08/28/程序员未来规划/","link":"","permalink":"http://kingge.top/2017/08/28/程序员未来规划/","excerpt":"","text":"http://www.jianshu.com/p/9d29a441ee17?utm_source=desktop&amp;utm_medium=timeline","categories":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/categories/心情/"}],"tags":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/tags/心情/"},{"name":"大龄程序员","slug":"大龄程序员","permalink":"http://kingge.top/tags/大龄程序员/"},{"name":"规划","slug":"规划","permalink":"http://kingge.top/tags/规划/"}]},{"title":"java之ClassLoader源码分析","slug":"java之ClassLoader源码分析","date":"2017-08-24T06:36:35.000Z","updated":"2017-08-24T08:36:55.128Z","comments":true,"path":"2017/08/24/java之ClassLoader源码分析/","link":"","permalink":"http://kingge.top/2017/08/24/java之ClassLoader源码分析/","excerpt":"","text":"类加载器ClassLoader的含义 不论多么简单的java程序，都是由一个或者多个java文件组成，java内部实现了程序所需要的功能逻辑，类之间可能还存在着依赖关系。当程序运行的时候，类加载器会把一部分类编译为class后加载到内存中，这样程序才能够调用里面的方法并运行。 类之间如果存在依赖关系，那么类加载会去帮你加载相关的类到内存中，这样才能够完成调用。如果找不到相关的类，那么他就会抛出我们在开发经常见到的异常：ClassNotFoundException Java中的所有类，必须被装载到jvm中才能运行，这个装载工作是由jvm中的类装载器完成的，类装载器所做的工作实质是把类文件从硬盘读取到内存中，JVM在加载类的时候，都是通过ClassLoader的loadClass（）方法来加载class的,与此同时在loadClass中存在着三种加载策略，loadClass使用双亲委派模式。 所以Classloader就是用来动态加载Class文件到内存当中用的。 Java默认提供的三个ClassLoader1.Bootstrap ClassLoader 称为启动类加载器，是Java类加载层次中最顶层的类加载器，负责加载JDK中的核心类库，预设上它负责搜寻JRE所在目录的classes或lib目录下（实际上是由系统参数sun.boot.class.path指定）。如：rt.jar、resources.jar、charsets.jar等，可通过如下程序获得该类加载器从哪些地方加载了相关的jar或class文件： URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i &lt; urls.length; i++) &#123; System.out.println(urls[i].toExternalForm()); &#125; &gt; 这两种输出的内容都是一样的。 System.out.println(System.getProperty(\"sun.boot.class.path\")); 输出 file:/F:/JDK/jdk1.8/jre/lib/resources.jarfile:/F:/JDK/jdk1.8/jre/lib/rt.jarfile:/F:/JDK/jdk1.8/jre/lib/sunrsasign.jarfile:/F:/JDK/jdk1.8/jre/lib/jsse.jarfile:/F:/JDK/jdk1.8/jre/lib/jce.jarfile:/F:/JDK/jdk1.8/jre/lib/charsets.jarfile:/F:/JDK/jdk1.8/jre/lib/jfr.jarfile:/F:/JDK/jdk1.8/jre/classesF:\\JDK\\jdk1.8\\jre\\lib\\resources.jar;F:\\JDK\\jdk1.8\\jre\\lib\\rt.jar;F:\\JDK\\jdk1.8\\jre\\lib\\sunrsasign.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jsse.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jce.jar;F:\\JDK\\jdk1.8\\jre\\lib\\charsets.jar;F:\\JDK\\jdk1.8\\jre\\lib\\jfr.jar;F:\\JDK\\jdk1.8\\jre\\classes 2.Extension ClassLoader 称为扩展类加载器，负责加载Java的扩展类库，默认加载JAVA_HOME/jre/lib/ext/目下的所有jar（实际上是由系统参数java.ext.dirs指定） 3.App ClassLoader 称为系统类加载器，负责加载应用程序classpath目录下的所有jar和class文件（由系统参数java.class.path指定） 总结 Extension ClassLoader和App ClassLoader 这两个类加载器实际上都是继承了ClassLoader类，但是Bootstrap ClassLoader不继承自ClassLoader，因为它不是一个普通的Java类，底层由C++编写，已嵌入到了JVM内核当中，当JVM启动后，Bootstrap ClassLoader也随着启动，负责加载完核心类库后，并构造Extension ClassLoader和App ClassLoader类加载器，也就是说： Bootstrap Loader会在JVM启动之后载入，之后它会载入ExtClassLoader并将ExtClassLoader的parent设为Bootstrap Loader，然后BootstrapLoader再加载AppClassLoader，并将AppClassLoader的parent设定为 ExtClassLoader。 ClassLoader加载类的原理双亲委托加载模式 我们知道除了顶级的 Bootstrap Loader他的parent属性为null之外，其他的两个或者自定义的类加载器都是存在parent 的。 当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader 进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象 为什么要使用双亲委托这种模型呢 java是一门具有很高的安全性的语言，使用这种加载策略的目的是为了：防止重载，父类如果已经找到了需要的类并加载到了内存，那么子类加载器就不需要再去加载该类。安全性问题。两个原因 JVM在搜索类的时候，又是如何判定两个class是相同的 类名是否相同 否由同一个类加载器实例加载 看一个例子public class TestClassLoader&#123; public static void main(String[] args) &#123; ClassLoader loader = TestClassLoader.class.getClassLoader(); //获得加载ClassLoaderTest.class这个类的类加载器 while(loader != null) &#123; System.out.println(loader); loader = loader.getParent(); //获得父类加载器的引用 &#125; System.out.println(loader); &#125;&#125; 输出 sun.misc.Launcher$AppClassLoader@2a139a55sun.misc.Launcher$ExtClassLoader@7852e922null 结论 第一行结果说明：ClassLoaderTest的类加载器是AppClassLoader。 第二行结果说明：AppClassLoader的类加器是ExtClassLoader，即parent=ExtClassLoader。 第三行结果说明：ExtClassLoader的类加器是Bootstrap ClassLoader，因为Bootstrap ClassLoader不是一个普通的Java类，所以ExtClassLoader的parent=null，所以第三行的打印结果为null就是这个原因。 测试2 将ClassLoaderTest.class打包成ClassLoaderTest.jar，放到Extension ClassLoader的加载目录下（JAVA_HOME/jre/lib/ext），然后重新运行这个程序，得到的结果会是什么样呢？ 输出 sun.misc.Launcher$ExtClassLoader@7852e922null 结果分析： 为什么第一行的结果是ExtClassLoader呢？ 因为ClassLoader的委托模型机制，当我们要用ClassLoaderTest.class这个类的时候，AppClassLoader在试图加载之前，先委托给Bootstrcp ClassLoader，Bootstracp ClassLoader发现自己没找到，它就告诉ExtClassLoader，兄弟，我这里没有这个类，你去加载看看，然后Extension ClassLoader拿着这个类去它指定的类路径（JAVA_HOME/jre/lib/ext）试图加载，唉，它发现在ClassLoaderTest.jar这样一个文件中包含ClassLoaderTest.class这样的一个文件，然后它把找到的这个类加载到内存当中，并生成这个类的Class实例对象，最后把这个实例返回。所以ClassLoaderTest.class的类加载器是ExtClassLoader。 第二行的结果为null，是因为ExtClassLoader的父类加载器是Bootstrap ClassLoader。 测试3: 用Bootstrcp ClassLoader来加载ClassLoaderTest.class，有两种方式：1、在jvm中添加-Xbootclasspath参数，指定Bootstrcp ClassLoader加载类的路径，并追加我们自已的jar（ClassTestLoader.jar）2、将class文件放到JAVA_HOME/jre/classes/目录下（上面有提到）(将ClassLoaderTest.jar解压后，放到JAVA_HOME/jre/classes目录下，如下图所示：)提示：jre目录下默认没有classes目录，需要自己手动创建一个提供者：Java团长 自定义ClassLoader前言 实现自定义类加载的目的是，假设我们的类他不是存在特定的位置，可能是某个磁盘或者某个远程服务器上面，那么我们就需要自定义类加载器去加载这些类。 继承继承java.lang.ClassLoader 重写父类的findClass方法 在findClass()方法中调用defineClass()。 这个方法在编写自定义classloader的时候非常重要，它能将class二进制内容转换成Class对象，如果不符合要求的会抛出各种异常 注意： 一个ClassLoader创建时如果没有指定parent，那么它的parent默认就是AppClassLoader。 为什么不去重定义loadClass方法呢？其实也可以，但是loadClass方法内部已经实现了搜索类的策略。除非你是非常熟悉否则还是不建议这样去做。这里建议重载findClass方法，因为在loadClass中最后会去调用findClass方法去加载类。而且这个方法内部默认是空的。 分析loadClass方法源码/*** A class loader is an object that is responsible for loading classes. The class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to locate or generate data that constitutes a definition for the class. A typical strategy is to transform the name into a file name and then read a &quot;class file&quot; of that name from a file system.**/ 大致意思如下：class loader是一个负责加载classes的对象，ClassLoader类是一个抽象类，需要给出类的二进制名称，class loader尝试定位或者产生一个class的数据，一个典型的策略是把二进制名字转换成文件名然后到文件系统中找到该文件。 protected synchronized Class loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; // 首先检查该name指定的class是否有被加载 Class c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; //如果parent不为null，则调用parent的loadClass进行加载 c = parent.loadClass(name, false); &#125;else&#123; //parent为null，则调用BootstrapClassLoader进行加载 c = findBootstrapClass0(name); &#125; &#125;catch(ClassNotFoundException e) &#123; //如果仍然无法加载成功，则调用自身的findClass进行加载 c = findClass(name); // &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; 自定义类加载器package com.kingge.com;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.InputStream;import java.net.URL;public class PersonalClassLoader extends ClassLoader&#123; private String rootUrl; public PersonalClassLoader(String rootUrl) &#123; this.rootUrl = rootUrl; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; Class clazz = null;//this.findLoadedClass(name); // 父类已加载 //if (clazz == null) &#123; //检查该类是否已被加载过 byte[] classData = getClassData(name); //根据类的二进制名称,获得该class文件的字节码数组 if (classData == null) &#123; throw new ClassNotFoundException(); &#125; clazz = defineClass(name, classData, 0, classData.length); //将class的字节码数组转换成Class类的实例 //ClassLoader内置方法 /* * Converts an array of bytes into an instance of class * Before the &lt;tt&gt;Class&lt;/tt&gt; can be used it must be resolved.*/ //&#125; return clazz; &#125; private byte[] getClassData(String name) &#123; InputStream is = null; try &#123; String path = classNameToPath(name); URL url = new URL(path); byte[] buff = new byte[1024*4]; int len = -1; is = url.openStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); while((len = is.read(buff)) != -1) &#123; baos.write(buff,0,len); &#125; return baos.toByteArray(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (is != null) &#123; try &#123; is.close(); &#125; catch(IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return null; &#125; private String classNameToPath(String name) &#123; return rootUrl + &quot;/&quot; + name.replace(&quot;.&quot;, &quot;/&quot;) + &quot;.class&quot;; &#125;&#125; 测试类： package com.kingge.com;public class ClassLoaderTest&#123; public static void main(String[] args) &#123; try &#123; /*ClassLoader loader = ClassLoaderTest.class.getClassLoader(); //获得ClassLoaderTest这个类的类加载器 while(loader != null) &#123; System.out.println(loader); loader = loader.getParent(); //获得父加载器的引用 &#125; System.out.println(loader);*/ String rootUrl = &quot;http://localhost:8080/console/res&quot;; PersonalClassLoader networkClassLoader = new PersonalClassLoader(rootUrl); String classname = &quot;HelloWorld&quot;; Class clazz = networkClassLoader.loadClass(classname); System.out.println(clazz.getClassLoader()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出： com.kingge.com.PersonalClassLoader@65b54208 其中HelloWorld.class文件的位置在于： 其实很多服务器都自定义了类加载器 用于加载web应用指定目录下的类库（jar或class），如：Weblogic、Jboss、tomcat等，下面我以Tomcat为例，展示该web容器都定义了哪些个类加载器： 下面以tomcat为例子 1、新建一个web工程httpweb 2、新建一个ClassLoaderServletTest，用于打印web容器中的ClassLoader层次结构 一下代码来自网上：import java.io.IOException; import java.io.PrintWriter; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; public class ClassLoaderServletTest extends HttpServlet &#123; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.setContentType(&quot;text/html&quot;); PrintWriter out = response.getWriter(); ClassLoader loader = this.getClass().getClassLoader(); while(loader != null) &#123; out.write(loader.getClass().getName()+&quot;&lt;br/&gt;&quot;); loader = loader.getParent(); &#125; out.write(String.valueOf(loader)); out.flush(); out.close(); &#125; public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125; &#125; 3、配置Servlet，并启动服务 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app version=&quot;2.4&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;&gt; &lt;servlet&gt; &lt;servlet-name&gt;ClassLoaderServletTest&lt;/servlet-name&gt; &lt;servlet-class&gt;ClassLoaderServletTest&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ClassLoaderServletTest&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/ClassLoaderServletTest&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;/web-app&gt; 运行截图： 总结 这种自定义的方式目的就是为了，能够控制类的加载流程，那么这种远程加载类的方式类似于我们常用的Hessian 来访问多个系统获取类 好的网站http://blog.csdn.net/briblue/article/details/54973413","categories":[{"name":"java深入理解","slug":"java深入理解","permalink":"http://kingge.top/categories/java深入理解/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"ClassLoader","slug":"ClassLoader","permalink":"http://kingge.top/tags/ClassLoader/"},{"name":"类加载器","slug":"类加载器","permalink":"http://kingge.top/tags/类加载器/"},{"name":"自定义类加载器","slug":"自定义类加载器","permalink":"http://kingge.top/tags/自定义类加载器/"},{"name":"类加载器源码分析","slug":"类加载器源码分析","permalink":"http://kingge.top/tags/类加载器源码分析/"}]},{"title":"为什么毕业后选择留在小城市","slug":"为什么毕业后选择留在小城市","date":"2017-08-21T06:13:44.000Z","updated":"2019-09-03T13:23:45.780Z","comments":true,"path":"2017/08/21/为什么毕业后选择留在小城市/","link":"","permalink":"http://kingge.top/2017/08/21/为什么毕业后选择留在小城市/","excerpt":"","text":"前言经常看到大学要毕业的学生，会有一种纠结感。越是临近毕业，这种感觉就越是强烈这种感觉实际上就是一种选择恐惧症，又或者说是小城综合征。他们对于毕业后究竟是选择去一线城市就业还是去二三线城市就业，产生了很大的选择恐惧。 为什么会产生这种心理大致的原因有那么几个： 身边的认识的人，都是选择回到自己的家乡进行就业，自己收到了影响。 大城市的生活节奏会比小城市更加的紧凑，你会很忙。 父母希望自己回去，离家近的地方工作。 恋人不跟随自己，她或他选择了回到了故乡就业，自己左右为难。 有些人已经在大城市实习过，对于大城市已经厌倦。 作者的选择 本人就是属于最后一种，大三的时候去的深圳实习，在一家IT互联网公司上班，加班很多，虽然加班这种现象在深圳是一种常态。但是每天加班到晚上一点多，也是很累，所以毕业后也就选择回到了自己的家乡就业。 回到了","categories":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/categories/心情/"}],"tags":[{"name":"心情","slug":"心情","permalink":"http://kingge.top/tags/心情/"},{"name":"总结","slug":"总结","permalink":"http://kingge.top/tags/总结/"},{"name":"有感","slug":"有感","permalink":"http://kingge.top/tags/有感/"}]},{"title":"max-allowed-packet的问题","slug":"max-allowed-packet的问题","date":"2017-08-17T02:37:15.000Z","updated":"2017-08-17T09:44:39.015Z","comments":true,"path":"2017/08/17/max-allowed-packet的问题/","link":"","permalink":"http://kingge.top/2017/08/17/max-allowed-packet的问题/","excerpt":"","text":"前言 近期，因启动项目有个批量插入的sql结果太大，超过了mysql自带的缓存，报了这个错误 修改： 定位到mysql的安装目录下面，然后修改my.ini 的max_allowed_packet = 8M默认是1M","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"异常","slug":"异常","permalink":"http://kingge.top/tags/异常/"},{"name":"sql异常","slug":"sql异常","permalink":"http://kingge.top/tags/sql异常/"}]},{"title":"YUM仓库配置","slug":"YUM仓库配置","date":"2017-08-06T14:12:30.000Z","updated":"2019-06-02T06:17:59.046Z","comments":true,"path":"2017/08/06/YUM仓库配置/","link":"","permalink":"http://kingge.top/2017/08/06/YUM仓库配置/","excerpt":"","text":"1.1 概述YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。-需要联网 1.2 为什么要制作本地YUM源YUM源虽然可以简化我们在Linux上安装软件的过程，但是生产环境通常无法上网，不能连接外网的YUM源，说以就无法使用yum命令安装软件了。为了在内网中也可以使用yum安装相关的软件，就要配置yum源。 YUM源其实就是一个保存了多个RPM包的服务器，可以通过http的方式来检索、下载并安装相关的RPM包。 1.3 yum的常用命令1）基本语法： yum install -y rpm软件包 （功能描述：安装httpd并确认安装） yum list （功能描述：列出所有可用的package和package组） yum clean all （功能描述：清除所有缓冲数据） yum deplist rpm软件包 （功能描述：列出一个包所有依赖的包） yum remove rpm软件包 （功能描述：删除httpd） 2）案例实操 ​ yum install -y tree //安装文档树结构展示插件 1.4 关联网络yum源1）前期文件准备 （1）前提条件linux系统必须可以联网 （2）在Linux环境中访问该网络地址：http://mirrors.163.com/.help/centos.html，在使用说明中点击CentOS6-&gt;再点击保存 （3）查看文件保存的位置 在打开的终端中输入如下命令，就可以找到文件的保存位置。 [kingge@hadoop101 下载]$ pwd /home/kingge/下载 2）替换本地yum文件 ​ （1）把下载的文件移动到/etc/yum.repos.d/目录 [root@hadoop101 下载]# mv CentOS6-Base-163.repo /etc/yum.repos.d/ ​ （2）进入到/etc/yum.repos.d/目录 [root@hadoop101 yum.repos.d]# pwd /etc/yum.repos.d ​ （3）用CentOS6-Base-163.repo替换CentOS-Base.rep [root@hadoop101 yum.repos.d]# mv CentOS6-Base-163.repo CentOS-Base.rep 3）安装命令 ​ （1）[root@hadoop101 yum.repos.d]#yum clean all ​ （2）[root@hadoop101 yum.repos.d]#yum makecache ​ （3）[root@hadoop101 yum.repos.d]# yum install -y createrepo （4）[root@hadoop101 yum.repos.d]#yum install -y httpd 1.5 制作本地yum源1.5.1 制作只有本机能访问的本地YUM源（1）准备一台Linux服务器，版本CentOS-6.8-x86_64-bin-DVD1.iso （2）配置好这台服务器的IP地址 （3）将CentOS-6.8-x86_64-bin-DVD1.iso镜像挂载到/mnt/cdrom目录 [root@hadoop101 /]# mkdir /mnt/cdrom [root@hadoop101 /]# mount -t iso9660 /dev/cdrom /mnt/cdrom （4）安装相应的软件 [root@hadoop101 yum.repos.d]#yum install -y httpd ​ （5）启动httpd服务 [root@hadoop101 yum.repos.d]#service httpd start （6）使用浏览器访问http://192.168.1.101:80（如果访问不通，检查防火墙是否开启了80端口或关闭防火墙），测试网络是否畅通 （7）将YUM源配置到httpd（Apache Server）中 [root@hadoop101 html]# mkdir Packages [root@hadoop101 html]# chown kingge:kingge Packages/ [root@hadoop101 html]# cp -r /mnt/cdrom/Packages/* /var/www/html/Packages/ （8）执行创建仓库命令：createrepo 路径 [root@hadoop101 Packages]# createrepo ./ （9）修改本机上的YUM源配置文件，将源指向自己 备份原有的YUM源的配置文件 [root@hadoop101 /]# cd /etc/yum.repos.d/ [root@hadoop101 yum.repos.d]# cp CentOS-Base.repo CentOS-Base.repo.bak ​ 编辑CentOS-Base.repo文件 [root@hadoop101 yum.repos.d]# vi CentOS-Base.repo [base] name=CentOS-Local baseurl=file:///var/www/html/Packages gpgcheck=0 enabled=1 #增加改行，使能 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 添加上面内容保存退出 （10）清除YUM缓存 [root@hadoop101 yum.repos.d]# yum clean all （11）列出可用的YUM仓库 [root@hadoop101 yum.repos.d]# yum repolist （12）安装相应的软件 [root@hadoop101 yum.repos.d]# yum install -y tree [root@hadoop101 Packages]# yum install -y firefox-45.0.1-1.el6.centos.x86_64.rpm 1.5.2 制作其他主机通过网络能访问的本地YUM源前期准备：检查yum源服务器的httpd服务是否启动： ​ Ps aux | grep httpd –查看该进程是否存在 或者 netstat –anp | grep 80 直接查看是否监听80端口 ​ 启动 httpd服务： service httpd start/restart 启动后，可能使用yum源的服务器访问不到yum仓库（例如下面的101服务器访问不到102服务器的yum），那么有可能是101服务器防火墙屏蔽了80端口，应该设置一下防火墙规则 （1）让其他需要安装RPM包的服务器指向这个YUM源，准备一台新的服务器，备份或删除原有的YUM源配置文件 备份原有的YUM源的配置文件 [root@hadoop102 /]#cd /etc/yum.repos.d/ [root@hadoop102 yum.repos.d]# cp CentOS-Base.repo CentOS-Base.repo.bak ​ 编辑CentOS-Base.repo文件 [root@hadoop102 yum.repos.d]# vi CentOS-Base.repo [base] name=CentOS-hadoop101 baseurl=http://192.168.1.101/Packages gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 添加上面内容保存退出 （2）在这台新的服务器上执行YUM的命令 [root@hadoop102 yum.repos.d]# yum clean all [root@hadoop102 yum.repos.d]# yum repolist （3）安装软件 [root@hadoop102 yum.repos.d]# yum install -y httpd","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"Shell编程","slug":"Shell编程","date":"2017-08-05T16:00:00.000Z","updated":"2019-06-02T06:10:23.295Z","comments":true,"path":"2017/08/06/Shell编程/","link":"","permalink":"http://kingge.top/2017/08/06/Shell编程/","excerpt":"","text":"一、引言因为公司遜要开发一款saas系统，需要使用自动部署脚本，部署web项目，所以就去了解了一下相关的知识，这里并不做深入的研究。 1.1 概述Shell是一个命令行解释器，它为用户提供了一个向Linux内核发送请求以便运行程序的界面系统级程序，用户可以用Shell来启动、挂起、停止甚至是编写一些程序。 Shell还是一个功能相当强大的编程语言，易编写、易调试、灵活性强。Shell是解释执行的脚本语言，在Shell中可以调用Linux系统命令。 1.2 shell脚本的执行方式1）echo输出内容到控制台 ​ （1）基本语法：​ echo [选项] [输出内容] 选项： -e： 支持反斜线控制的字符转换 控制字符 作 用 \\ 输出\\本身 \\a 输出警告音 \\b 退格键，也就是向左删除键 \\c 取消输出行末的换行符。和“-n”选项一致 \\e ESCAPE键 \\f 换页符 \\n 换行符 \\r 回车键 \\t 制表符，也就是Tab键 \\v 垂直制表符 \\0nnn 按照八进制ASCII码表输出字符。其中0为数字零，nnn是三位八进制数 \\xhh 按照十六进制ASCII码表输出字符。其中hh是两位十六进制数 ​（2）案例 ​ [kingge@hadoop101 sbin]$ echo &quot;helloworld&quot; helloworld 2）脚本格式 （1）脚本以 #!/bin/bash 开头（2）脚本必须有可执行权限 3）第一个Shell脚本 （1）需求：创建一个Shell脚本，输出helloworld （2）实操： [kingge@hadoop101 datas]$ touch helloworld.sh [kingge@hadoop101 datas]$ vi helloworld.sh ​ 在helloworld.sh中输入如下内容 #!**/**bin**/**bash echo &quot;helloworld&quot; 4）脚本的常用执行方式 第一种：输入脚本的绝对路径或相对路径 （1）首先要赋予helloworld.sh 脚本的+x权限 [kingge@hadoop101 datas]$ chmod 777 helloworld.sh （2）执行脚本 ​ /root/helloWorld.sh ​ ./helloWorld.sh 第二种：bash或sh+脚本（不用赋予脚本+x权限） ​ sh /root/helloWorld.sh ​ sh helloWorld.sh 1.3 shell中的变量1）Linux Shell中的变量分为，系统变量和用户自定义变量。 2）系统变量：$HOME、$PWD、$SHELL、$USER等等 3）显示当前shell中所有变量：set 1.3.1 定义变量1）基本语法： ​ （1）定义变量：变量=值 （2）撤销变量：unset 变量 （3）声明静态变量：readonly变量，注意：不能unset 2）变量定义规则 ​ （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头。 ​ （2）等号两侧不能有空格 ​ （3）变量名称一般习惯为大写 3）案例 ​ （1）定义变量A A=8 ​ （2）撤销变量A unset A ​ （3）声明静态的变量B=2，不能unset readonly B=2 ​ （4）可把变量提升为全局环境变量，可供其他shell程序使用 export 变量名 1.3.2 将命令的返回值赋给变量​ （1）A=ls -la 反引号，运行里面的命令，并把结果返回给变量A ​ （2）A=$(ls -la) 等价于反引号 1.3.3 设置环境变量1）基本语法： ​ （1）export 变量名=变量值 （功能描述：设置环境变量的值） （2）source 配置文件 （功能描述：让修改后的配置信息立即生效） （3）echo $变量名 （功能描述：查询环境变量的值） 2）案例： ​ （1）在/etc/profile文件中定义JAVA_HOME环境变量 ​ export JAVA_HOME=/opt/module/jdk1.7.0_79 ​ export PATH=$PATH:$JAVA_HOME/bin （2）查看环境变量JAVA_HOME的值 ​ [kingge@hadoop101 datas]$ echo $JAVA_HOME /opt/module/jdk1.7.0_79 1.3.4 位置参数变量1）基本语法 ​ $n （功能描述：n为数字，$0代表命令本身，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如$&#123;10&#125;）$* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）$@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）$# （功能描述：这个变量代表命令行中所有参数的个数） 2）案例 （1）输出输入的的参数1，参数2，所有参数，参数个数 #!/bin/bashecho &quot;$0 $1 $2&quot;echo &quot;$*&quot;echo &quot;$@&quot;echo &quot;$#&quot; （2）$*与$@的区别 #!/bin/bash for i in &quot;$*&quot; #$*中的所有参数看成是一个整体，所以这个for循环只会循环一次 do echo &quot;The parameters is: $i&quot; done x=1 for y in &quot;$@&quot; #$@中的每个参数都看成是独立的，所以“$@”中有几个参数，就会循环几次 do echo &quot;The parameter$x is: $y&quot; x=$(( $x +1 )) done a）$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数 b）当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“​$1 $2 …​$n”的形式输出所有参数；“$@”会将各个参数分开，以“​$1” “$2”…”​$n”的形式输出所有参数 1.3.5 预定义变量1）基本语法： （1）“$((运算式))”或“$[运算式]”（2）expr m + n 注意expr运算符间要有空格（3）expr m - n（4）expr \\*, /, % 乘，除，取余 2）案例 计算（2+3）X4的值 （1）采用$[运算式]方式[root@hadoop101 datas]# S=$[(2+3)*4] [root@hadoop101 datas]# echo $S （2）expr分布计算 S=`expr 2 + 3` expr $S \\* 4 （3）expr一步完成计算 expr `expr 2 + 3` \\* 4 1.4 运算符1）基本语法： （1）“$((运算式))”或“$[运算式]” （2）expr m + n 注意expr运算符间要有空格 （3）expr m - n （4）expr *, /, % 乘，除，取余 2）案例：计算（2+3）X4的值 ​ （1）采用$[运算式]方式 ​ [root@hadoop101 datas]# S=$[(2+3)*4] ​ [root@hadoop101 datas]# echo $S ​ （2）expr分布计算 ​ S=expr 2 + 3 ​ expr $S * 4 ​ （3）expr一步完成计算 ​ expr expr 2 + 3 * 4 1.5 条件判断1.5.1 判断语句1）基本语法： [ condition ]（注意condition前后要有空格） #非空返回true，可使用$?验证（0为true，&gt;1为false） 2）案例： [kingge] 返回true [] 返回false [condition] &amp;&amp; echo OK || echo notok 条件满足，执行后面的语句 1.5.2 常用判断条件1）两个整数之间比较 = 字符串比较 -lt 小于 -le 小于等于 -eq 等于 -gt 大于 -ge 大于等于 -ne 不等于 2）按照文件权限进行判断 -r 有读的权限 -w 有写的权限 -x 有执行的权限 3）按照文件类型进行判断 -f 文件存在并且是一个常规的文件 -e 文件存在 -d 文件存在并是一个目录 4）案例 ​ （1）23是否大于等于22 ​ [ 23 -ge 22 ] ​ （2）student.txt是否具有写权限 ​ [ -w student.txt ] ​ （3）/root/install.log目录中的文件是否存在 ​ [ -e /root/install.log ] 1.6 流程控制1.6.1 if判断1）基本语法： if [ 条件判断式 ];then 程序 fi 或者 if [ 条件判断式 ] then ​ 程序 fi ​ 注意事项：（1）[ 条件判断式 ]，中括号和条件判断式之间必须有空格 2）案例 #!/bin/bashif [ $1 -eq &quot;123&quot; ]then echo &quot;123&quot;elif [ $1 -eq &quot;456&quot; ]then echo &quot;456&quot;fi 1.6.2 case语句1）基本语法： case $变量名 in “值1”） ​ 如果变量的值等于值1，则执行程序1 ​ ;; “值2”） ​ 如果变量的值等于值2，则执行程序2 ​ ;; …省略其他分支… *） ​ 如果变量的值都不是以上的值，则执行此程序 ​ ;; esac 2）案例 !/bin/bashcase $1 in&quot;1&quot;) echo &quot;1&quot;;;&quot;2&quot;) echo &quot;2&quot;;;*) echo &quot;other&quot;;;esac 1.6.3 for循环1）基本语法1： for 变量 in 值1 值2 值3… do ​ 程序 done 2）案例： ​ （1）打印输入参数 #!/bin/bash#打印数字for i in &quot;$*&quot; do echo &quot;The num is $i &quot; donefor j in &quot;$@&quot; do echo &quot;The num is $j&quot; done 3）基本语法2： ​ for (( 初始值;循环控制条件;变量变化 )) do ​ 程序 done 4）案例 （1）从1加到100 #!/bin/bashs=0for((i=0;i&lt;=100;i++))do s=$[$s+$i]doneecho &quot;$s&quot; 1.6.4 while循环1）基本语法： while [ 条件判断式 ] do ​ 程序 done 2）案例 ​ （1）从1加到100 #!/bin/bashs=0i=1while [ $i -le 100 ]do s=$[$s+$i] i=$[$i+1]doneecho $s 1.7 read读取控制台输入1）基本语法： ​ read(选项)(参数) ​ 选项： -p：指定读取值时的提示符； -t：指定读取值时等待的时间（秒）。 参数 ​ 变量：指定读取值的变量名 2）案例 ​ 读取控制台输入的名称 #!/bin/bashread -t 7 -p &quot;please 7 miao input your name &quot; NAMEecho $NAME 1.8 函数1.8.1 系统函数1）basename基本语法 basename [pathname] [suffix] basename [string] [suffix] （功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。 选项： suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。 2）案例 [kingge@hadoop101 opt]$ basename /opt/test.txt test.txt [kingge@hadoop101 opt]$ basename /opt/test.txt .txt test 3）dirname基本语法 ​ dirname 文件绝对路径 （功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）） 4）案例 ​ [kingge@hadoop101 opt]$ dirname /opt/test.txt /opt 1.8.2 自定义函数1）基本语法： [ function ] funname[()]&#123; Action; [return int;]&#125; funname 注意： ​ （1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。 ​ （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255) 2）案例 ​ （1）计算输入参数的和 #!/bin/bashfunction sum()&#123; s=0 s=$[ $1 + $2 ] echo &quot;$s&quot;&#125;read -p &quot;Please input the number1: &quot; n1;read -p &quot;Please input the number2: &quot; n2;sum $n1 $n2;","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"实现多数据量的导入数据库","slug":"实现多数据量的导入数据库","date":"2017-08-01T06:37:15.000Z","updated":"2017-08-17T08:21:15.695Z","comments":true,"path":"2017/08/01/实现多数据量的导入数据库/","link":"","permalink":"http://kingge.top/2017/08/01/实现多数据量的导入数据库/","excerpt":"","text":"引言 在做一个项目的时候，涉及到需要从一个表格中获取百万条数据然后插入到数据库中，最后采用JDBC的executeBantch方法实现这个功能。 采取的策略 尽量关闭字段索引（因为再插入数据的时候还是需要维护索引的，在创建索引和维护索引 会耗费时间,随着数据量的增加而增加，可以在插入数据后再去为字段创建索引） 虽然索引可以提高查询速度但是，插入数据的时候会导致索性的更新。索性越多，插入会越慢。可以看文档描述:Although it can be tempting to create an indexes for every possible column used in a query, unnecessary indexes waste space and waste time for MySQL to determine which indexes to use. Indexes also add to the cost of inserts, updates, and deletes because each index must be updated. You must find the right balance to achieve fast queries using the optimal set of indexes. 分批次提交数据 在分布式条件下，还可以考虑在不同的数据库结点提交，有底层的消息系统完成数据扩展 过滤预处理数据 预处理数据的场景：为了避免插入的数据（假设ListA）跟数据库中某些数据重复，那么我们会把要插入的数据去数据库中查询是否已经存在，获得返回的已经存在数据（ListB）。然后在插入数据的时候判断当前数据是否在ListB中，那么当前数据不能够插入数据库。过滤出来，最后得到一个可以插入数据库的ListC 代码关键代码/*数据分析结束*/ /*往数据库写数据开始*/ Connection conn=null; PreparedStatement idsUserAdd=null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\") ; conn = DriverManager.getConnection(ConfigTool.getProperty(\"jdbc.url\").toString() , ConfigTool.getProperty(\"jdbc.username\").toString() , ConfigTool.getProperty(\"jdbc.password\").toString()); conn.setAutoCommit(false); //构造预处理statement idsUserAdd = conn.prepareStatement(\"INSERT INTO dc_matedata (\"+ \" ID,`NAME`, DATATYPE,`CODE`,TYPE_ID,`LENGTH`, \"+ \" DATANAME, VALUEAREA,`RESTRICT`, REMARK,MD_DATE)\"+ \" values(?,?,?,?,?,?,?,?,?,?,now())\"); //最大列表的数目当做循环次数 int xhcs=addMetadataList.size();//addMetadataList需要插入的数据 for(int i=0;i&lt;xhcs;i++)&#123; idsUserAdd.setString(1,addMetadataList.get(i).get(\"id\").toString()); idsUserAdd.setString(2,addMetadataList.get(i).get(\"name\").toString()); idsUserAdd.setString(3,addMetadataList.get(i).get(\"dataType\").toString()); idsUserAdd.setString(4,addMetadataList.get(i).get(\"code\").toString()); idsUserAdd.setString(5,addMetadataList.get(i).get(\"typeId\").toString()); idsUserAdd.setString(6,addMetadataList.get(i).get(\"dataLength\").toString()); idsUserAdd.setString(7,addMetadataList.get(i).get(\"dataName\").toString()); idsUserAdd.setString(8,addMetadataList.get(i).get(\"valueArea\").toString()); idsUserAdd.setString(9,addMetadataList.get(i).get(\"dataRestrict\").toString()); idsUserAdd.setString(10,addMetadataList.get(i).get(\"dataRemark\").toString()); idsUserAdd.addBatch(); //每10000次提交一次 if(i%10000==0||i==xhcs-1)&#123;//可以设置不同的大小；如50，100，500，1000等等 i==xhcs-1（最后一次） idsUserAdd.executeBatch(); conn.commit(); idsUserAdd.clearBatch(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw e; &#125;finally &#123; try &#123; if(idsUserAdd!=null) idsUserAdd.close(); if(conn!=null) conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); throw e; &#125; &#125; /*往数据库写数据结束*/ 完整代码/** * 校验需要导入的元数据信息，封装错误信息并批量插入数据库 */ @Override public List&lt;Map&lt;String, Object&gt;&gt; saveDCMetadataBatch(List&lt;Map&lt;String, Object&gt;&gt; list, boolean valid, boolean addError) throws Exception&#123; List&lt;Map&lt;String,Object&gt;&gt; errorList=new ArrayList&lt;Map&lt;String,Object&gt;&gt;();//获得不能够添加成功的数据 Map&lt;String,Object&gt; map=new HashMap&lt;String,Object&gt;();//查询条件 Map&lt;String,String&gt; codeMap=new HashMap&lt;String,String&gt;();//每个分类对应的元数据的编号最大值 Map&lt;String,Object&gt; metaName=new HashMap&lt;String,Object&gt;();//查询数据库中是否存在相同的数据（这里校验的是：元数据的中文简称） Map&lt;String,Object&gt; metaDataName=new HashMap&lt;String,Object&gt;();//查询数据库中是否存在相同的数据（这里校验的是：元数据的数据项名称） map.put(\"metaName\",list);//需要查询的元数据中文名称 map.put(\"metaDataTypeId\",list);//导入的元数据的编号 List&lt;Map&lt;String, Object&gt;&gt; metaExistList = dCMatedataDao.getDCMetadata(map);//根据元数据名称查询当前分类下是否存在同样元数据 map.put(\"metaName\",null);//置空 map.put(\"metaDataName\",list); List&lt;Map&lt;String, Object&gt;&gt; metaExistListTwo = dCMatedataDao.getDCMetadata(map);//根据元数据数据项名称查询存在的元数据 //保存重复的信息 for(int i=0;i&lt;metaExistList.size();i++) metaName.put(metaExistList.get(i).get(\"name\").toString()+metaExistList.get(i).get(\"code\").toString() ,metaExistList.get(i).get(\"id\"));//添加父类的编号为后缀-唯一性保证 for(int i=0;i&lt;metaExistListTwo.size();i++) metaDataName.put(metaExistListTwo.get(i).get(\"dataname\").toString()+metaExistListTwo.get(i).get(\"code\").toString(), metaExistListTwo.get(i).get(\"id\")); /*整理出来的数据-开始*/ List&lt;Map&lt;String,Object&gt;&gt; addMetadataList=new ArrayList&lt;Map&lt;String,Object&gt;&gt;(); /*整理出来的数据-结束*/ for (int i = 0; i &lt; list.size(); i++) &#123; Map&lt;String, Object&gt; MetadataObj = list.get(i); try &#123; String metadatId = StringUtil.getUUID();//元数据id /*校验开始*/ if (valid)&#123; if(validUser(MetadataObj,\"name\",addError)!=null)&#123;//验证输入的数据是否符合格式和必填。 errorList.add(MetadataObj); continue; &#125; &#125; /*前端校验结束*/ /*校验是否存在同名的元数据*/ String dataCodeCheck = MetadataObj.get(\"dataCode\").toString().trim(); //元数据父分类编号 String name = MetadataObj.get(\"name\").toString().trim();//元数据中文简称 if (metaName.containsKey(name+dataCodeCheck)) &#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"中文简称已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; /*校验是否存在相同数据项的元数据*/ String dataName = MetadataObj.get(\"dataName\").toString().trim();//数据项名 if (metaDataName.containsKey(dataName+dataCodeCheck)) &#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"数据项名已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; String dataCode = MetadataObj.get(\"dataCode\").toString().trim(); //元数据父分类编号 List&lt;Map&lt;String, Object&gt;&gt; footCount = dCMatedataDao.getFootCount(dataCode); if( footCount.size() &gt; 0)&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"分类编码不是最后一级分类\"); &#125; errorList.add(MetadataObj); continue; &#125; Map&lt;String, Object&gt; typeByCode = dCMatedataDao.getMetadataTypeByCode(dataCode); if( typeByCode == null || typeByCode.size() &lt; 1)&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"分类编码不存在，请先添加分类\"); &#125; errorList.add(MetadataObj); continue; &#125; //校验是在添加的List中是否存在相同的数据项名或者中文简称 //校验导入文件中是否存在一样的中文简称或者数据项名 boolean nameExist = false; boolean dataNameExist = false; for (int j = 0; j &lt; addMetadataList.size(); j++)&#123; Map&lt;String, Object&gt; map2 = addMetadataList.get(j); String typeId = map2.get(\"typeId\").toString(); String nameE = map2.get(\"name\").toString(); String dataNameE = map2.get(\"dataName\").toString(); if( typeId.equals(typeByCode.get(\"id\").toString()) &amp;&amp; nameE.equals(name))&#123; nameExist=true; break; &#125; if( typeId.equals(typeByCode.get(\"id\").toString()) &amp;&amp; dataNameE.equals(dataName))&#123; dataNameExist=true; break; &#125; &#125; if( nameExist )&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"中文简称已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; if( dataNameExist )&#123; if (addError) &#123; MetadataObj.put(\"errInfo\", \"数据项名已存在\"); &#125; errorList.add(MetadataObj); continue; &#125; //进入这里说明校验结束，开始填充添加的数据 String type_id = typeByCode.get(\"id\").toString();//元数据所属分类id String dataType = MetadataObj.get(\"dataType\").toString().trim(); //元数据类型 String dataLength = MetadataObj.get(\"dataLength\").toString().trim(); //元数据长度 String code = \"\"; //// if( codeMap.get(dataCode) == null||StringUtil.isEmpty(codeMap.get(dataCode)) )&#123;//表示当前分类不存在已经添加的元数据--因为编码map中不存在对应分类的最大编码 Map maxCodeByPid = this.selectMetadataMaxCode(type_id); if( maxCodeByPid == null )&#123;//表示当前分类下不存在任何子分类 code = StringUtil.getCode(\"0\", dataCode);//则从01开始编号 codeMap.put(dataCode, \"01\");//保存当前分类下元数据编号最大值 &#125;else&#123; String object = (String) maxCodeByPid.get(\"codeNum\");//当前分类节点下的元数据的编号最大值。 int pSituation = object.indexOf(dataCode); int pLength = pSituation+dataCode.length() ; String substring = object.substring(pLength); //截取出最大编号值得最大值 code = StringUtil.getCode(substring, dataCode); int temp = Integer.parseInt(substring);//保存当前分类下元数据编号最大值 temp+=1; codeMap.put(dataCode, temp+\"\"); &#125; &#125;else&#123; String maxCode = codeMap.get(dataCode); code = StringUtil.getCode(maxCode, dataCode); //保存当前分类下元数据编号最大值 int temp = Integer.parseInt(maxCode); temp+=1; codeMap.put(dataCode, temp+\"\"); &#125; /// Map&lt;String, Object&gt; metadatList = new LinkedHashMap&lt;String, Object&gt;(); metadatList.put(\"id\", metadatId); metadatList.put(\"name\",name); metadatList.put(\"dataType\",dataType); metadatList.put(\"code\",code); metadatList.put(\"typeId\",type_id); metadatList.put(\"dataLength\",dataLength); metadatList.put(\"dataName\",dataName); metadatList.put(\"valueArea\", MetadataObj.get(\"valueArea\")==null?\"\":MetadataObj.get(\"valueArea\") ); metadatList.put(\"dataRestrict\",MetadataObj.get(\"dataRestrict\")==null?\"\":MetadataObj.get(\"dataRestrict\")); metadatList.put(\"dataRemark\",MetadataObj.get(\"dataRemark\")==null?\"\":MetadataObj.get(\"dataRemark\")); metadatList.put(\"mdDate\",new Date()); addMetadataList.add(metadatList); &#125; catch (Exception e)&#123; if(addError) &#123; MetadataObj.put(\"errInfo\", e.getMessage()); &#125; errorList.add(MetadataObj); &#125; &#125; /*数据分析结束*/ /*往数据库写数据开始*/ Connection conn=null; PreparedStatement idsUserAdd=null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\") ; conn = DriverManager.getConnection(ConfigTool.getProperty(\"jdbc.url\").toString() , ConfigTool.getProperty(\"jdbc.username\").toString() , ConfigTool.getProperty(\"jdbc.password\").toString()); conn.setAutoCommit(false); //构造预处理statement idsUserAdd = conn.prepareStatement(\"INSERT INTO dc_matedata (\"+ \" ID,`NAME`, DATATYPE,`CODE`,TYPE_ID,`LENGTH`, \"+ \" DATANAME, VALUEAREA,`RESTRICT`, REMARK,MD_DATE)\"+ \" values(?,?,?,?,?,?,?,?,?,?,now())\"); //最大列表的数目当做循环次数 int xhcs=addMetadataList.size(); for(int i=0;i&lt;xhcs;i++)&#123; idsUserAdd.setString(1,addMetadataList.get(i).get(\"id\").toString()); idsUserAdd.setString(2,addMetadataList.get(i).get(\"name\").toString()); idsUserAdd.setString(3,addMetadataList.get(i).get(\"dataType\").toString()); idsUserAdd.setString(4,addMetadataList.get(i).get(\"code\").toString()); idsUserAdd.setString(5,addMetadataList.get(i).get(\"typeId\").toString()); idsUserAdd.setString(6,addMetadataList.get(i).get(\"dataLength\").toString()); idsUserAdd.setString(7,addMetadataList.get(i).get(\"dataName\").toString()); idsUserAdd.setString(8,addMetadataList.get(i).get(\"valueArea\").toString()); idsUserAdd.setString(9,addMetadataList.get(i).get(\"dataRestrict\").toString()); idsUserAdd.setString(10,addMetadataList.get(i).get(\"dataRemark\").toString()); idsUserAdd.addBatch(); //每10000次提交一次 if(i%10000==0||i==xhcs-1)&#123;//可以设置不同的大小；如50，100，500，1000等等 idsUserAdd.executeBatch(); conn.commit(); idsUserAdd.clearBatch(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw e; &#125;finally &#123; try &#123; if(idsUserAdd!=null) idsUserAdd.close(); if(conn!=null) conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); throw e; &#125; &#125; /*往数据库写数据结束*/ return errorList; &#125; 总结 有些网友发现使用StringBuffer 来拼接入参，不通过prepareStatement的预处理，虽然前者速度很快，但是使用prepareStatement可以防止SQL注入 有的好的建议大家都可以提出来","categories":[{"name":"JDBC","slug":"JDBC","permalink":"http://kingge.top/categories/JDBC/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"JDBC","slug":"JDBC","permalink":"http://kingge.top/tags/JDBC/"},{"name":"批量导入","slug":"批量导入","permalink":"http://kingge.top/tags/批量导入/"},{"name":"SSM","slug":"SSM","permalink":"http://kingge.top/tags/SSM/"},{"name":"项目经验","slug":"项目经验","permalink":"http://kingge.top/tags/项目经验/"}]},{"title":"Java工程师书单（初级、中级、高级）","slug":"Java工程师书单（初级、中级、高级）","date":"2017-06-21T06:13:44.000Z","updated":"2017-08-24T07:33:21.781Z","comments":true,"path":"2017/06/21/Java工程师书单（初级、中级、高级）/","link":"","permalink":"http://kingge.top/2017/06/21/Java工程师书单（初级、中级、高级）/","excerpt":"","text":"当你的能力承受不住你的欲望，你就应该静下心来读书 初级书籍《编写高质量代码——改善Java程序的151个建议》 这是一本值得入门java的人放在床头的书。此书内容广泛、要点翔实。大多数优秀程序设计书籍都需要看老外写的，但是这本讲述提高java编程水平的书还是不错的，适合具有基本java编程能力的人。对于程序猿而言，工作久了，就感觉编程习惯对一个人很重要。习惯好，不仅工作效率告，而且bug少。这本书对提高个人的好的编程习惯很有帮助。 《Java程序员修炼之道》 此书涵盖了Java7的新特性和Java开发的关键技术，对当前大量开源技术并存，多核处理器、并发以及海量数据给Java开发带来的挑战作出了精辟的分析，提供了实践前沿的深刻洞见，涉及依赖注入、现代并发、类与字节码、性能调优等底层概念的剖析。**书中的道理很浅显，可是对于菜鸟却是至理名言。基本为你勾勒了一个成熟软件程序员专家所需要的所有特性。。 《Java8实战》 没看过。嘻嘻嘻 《有效的单元测试》 此书由敏捷技术实践专家撰写，系统且深入地阐释单元测试用于软件设计的工具、方法、原则和佳实践；深入剖析各种测试常见问题，包含大量实践案例，可操作性强，能为用户高效编写测试提供系统实践指南。**介绍了单元测试的各个方面，TDD、test double、测试的坏味道、可测试的设计等等，每个主题需要深入的话，还需要配合其它书籍和实践，非常适合入门单元测试。书中例子非常全面，看完对使用 Junit 进行单元测试会有一个大的长进，而且用java语言编写，内容很新 《Java核心技术：卷1》 不推荐卷2，因为这个作为初级书单来讲，太难了。 《代码整洁之道》 没看过 《数据结构与算法分析-Java语言描述》 本书是java数据结构与算法方面的三宝之一，除了这三本其他的已经没有意义了。这三宝分别是:**黑宝书《数据结构与算法分析java语言描述》mark allen weiss蓝宝书《java数据结构和算法》robert lafore红宝书《算法》robert sedgewick黑宝书胜在公式推理和证明以及算法的简洁和精炼，此外习题较多。蓝宝书胜在对算法的深入浅出的讲解，演示和举例，让艰涩的理论变得很容易理解。红宝书胜在系出名门斯坦福，演示通俗易懂，内容丰富。有了这三宝，算法不用愁，学完以后再看《算法导论》就容易多了。本书从讲解什么是数据结构开始，延伸至高级数据结构和算法分析，强调数据结构和问题求解技术。本书的目的是从抽象思维和问题求解的观点提供对数据结构的实用介绍，试图包含有关数据结构、算法分析及其Java实现的所有重要的细节 中级书单《重构：改善既有代码的设计》 重构，绝对是写程序过程中最重要的事之一。在写程序之前我们不可能事先了解所有的需求，设计肯定会有考虑不周的地方，而且随着项目需求的修改，也有可能原来的设计已经被改得面目全非了。更何况，我们很少有机会从头到尾完成一个项目，基本上都是接手别人的代码，我们要做的是重构，从小范围的重构开始。**重构是设计,设计是art,重构也是art. 一个函数三行只是语不惊人死不休的说法,是对成百上千行代码的矫枉过正。 更一个般的看法是一个函数应该写在一页纸内。 《Effective Java》 必读 《Java并发编程实战》 没看过： 本书深入浅出地介绍了Java线程和并发，是一本完美的Java并发参考手册。书中从并发性和线程安全性的基本概念出发，介绍了如何使用类库提供的基本并发构建块，用于避免并发危险、构造线程安全的类及验证线程安全的规则，如何将小的线程安全类组合成更大的线程安全类，如何利用线程来提高并发应用程序的吞吐量。**java进阶必看，多线程的最佳书籍。 实战Java高并发程序设计》 没看过 《算法》 没看过 《Head First 设计模式》 这是我看过最幽默最搞笑最亲切同时又让我收获巨大的技术书籍！ 森森的膜拜Freeman(s)！Amen！ 深入浅出，娓娓道来，有的地方能笑死你！写得很有趣，图文并茂，比起四人帮的那本，好懂了不知道多少倍。计算机世界的head first系列基本都是经典。不过只看书学明白设计模式是不可能的，这些只是前人的总结，我们唯有实践实践再实践了。**读这本书不仅仅是学习知识，而是在学习一种思考的方法，学习一种认知的技巧，学习一种成长的阶梯。 总之，用你闲暇的时间来读这本书，并不亚于你专注的工作或学习。笔者强烈推荐此书，要成长为一名高级程序员，设计模式已经是必备技能了。 《Java编程思想》 没看过 高级书单 《深入理解Java虚拟机》 没看过 《Java性能权威指南》 没看过 《深入分析Java Web技术内幕》 没看过 《大型网站系统与Java中间件实践》 没看过 《大型网站技术架构：核心原理与案例分析》 没看过 《企业应用架构模式》 没看过 Spring3.x企业应用开发实战 这本书适合初学者看或者当做一本参考书。对于提高者而言，略看就行 Spring揭秘 没看过 Java程序性能优化:让你的Java程序更快、更稳定 没看过 总结talk is less show me your code，希望大家有好的书籍也可以推荐","categories":[{"name":"读书系统","slug":"读书系统","permalink":"http://kingge.top/categories/读书系统/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://kingge.top/tags/Java/"},{"name":"书籍推荐","slug":"书籍推荐","permalink":"http://kingge.top/tags/书籍推荐/"}]},{"title":"linux-PRM软件包管理工具","slug":"linux-PRM软件包管理工具","date":"2017-06-13T12:12:30.000Z","updated":"2019-06-02T05:47:38.887Z","comments":true,"path":"2017/06/13/linux-PRM软件包管理工具/","link":"","permalink":"http://kingge.top/2017/06/13/linux-PRM软件包管理工具/","excerpt":"","text":"1.1 概述RPM（RedHat Package Manager），Rethat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。 RPM包的名称格式 Apache-1.3.23-11.i386.rpm - “apache” 软件名称 - “1.3.23-11”软件的版本号，主版本和此版本 - “i386”是软件所运行的硬件平台 - “rpm”文件扩展名，代表RPM包 1.2 常用命令1.2.1 查询（rpm -qa）1）基本语法： rpm -qa （功能描述：查询所安装的所有rpm软件包） 过滤 rpm -qa | grep rpm软件包 2）案例 [root@hadoop101 Packages]# rpm -qa |grep firefox firefox-45.0.1-1.el6.centos.x86_64 1.2.2 卸载（rpm -e）1）基本语法： （1）rpm -e RPM软件包 或者（2） rpm -e –nodeps 软件包 –nodeps 如果该RPM包的安装依赖其它包，即使其它包没装，也强迫安装。 2）案例 [root@hadoop101 Packages]# rpm -e firefox 1.2.3 安装（rpm -ivh）1）基本语法： ​ rpm -ivh RPM包全名 ​ -i=install，安装 ​ -v=verbose，显示详细信息 ​ -h=hash，进度条 ​ –nodeps，不检测依赖进度 2）案例 [root@hadoop101 Packages]# pwd /media/CentOS_6.8_Final/Packages [root@hadoop101 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm warning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY Preparing… ########################################### [100%] 1:firefox ########################################### [100%]","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"linux基本操作命令","slug":"linux基本操作命令","date":"2017-06-13T07:34:03.000Z","updated":"2019-06-02T05:52:23.561Z","comments":true,"path":"2017/06/13/linux基本操作命令/","link":"","permalink":"http://kingge.top/2017/06/13/linux基本操作命令/","excerpt":"","text":"一、常用基本命令此文章紧接linux基础文章，如果没有安装linux，那么情先参见（linux基础 ） 1.1 帮助命令1.1.1 man 获得帮助信息1）基本语法： ​ man [命令或配置文件] （功能描述：获得帮助信息） ​ （1）显示说明 NAME 命令的名称和单行描述 SYNOPSIS 怎样使用命令 DESCRIPTION 命令功能的深入讨论 EXAMPLES 怎样使用命令的例子 SEE ALSO 相关主题（通常是手册页） （2）数字说明 1.用户在shell环境中可以操作的命令或是可执行的文件 2.系统内核(kernel)可以调用的函数 3.常用的函数or函数库 4.设备配置文件 5.配置文件的格式 6.游戏相关 7.linux网络协议和文件系统 8.系统管理员可以用的命令 9.跟内核有关系的文件 2）案例 [root@hadoop106 home]# man ls 1.1.2 help 获得shell内置命令的帮助信息1）基本语法： ​ help 命令 （功能描述：获得shell内置命令的帮助信息） 2）案例： ​ [root@hadoop101 bin]# help cd 1.1.3 常用快捷键1）ctrl + c：停止进程 2）ctrl+l：清屏 3）ctrl + q：退出 4）善于用tab键 5）上下键：查找执行过的命令 6）ctrl +alt：linux和Windows之间切换 1.2 文件目录类1.2.1 pwd 显示当前工作目录的绝对路径1）基本语法： ​ pwd （功能描述：显示当前工作目录的绝对路径） ​ 2）案例 [root@hadoop101 home]# pwd /home 1.2.2 ls 列出目录的内容1）基本语法： ls [选项] [目录或是文件] 选项： -a ：全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用) -l ：长数据串列出，包含文件的属性与权限等等数据；(常用) 每行列出的信息依次是： 文件类型与权限 链接数 文件属主 文件属组 文件大小用byte来表示 建立或最近修改的时间 名字 2）案例 [kingge@hadoop101 ~]$ ls -al 总用量 44 drwx——. 5 kingge kingge 4096 5月 27 15:15 . drwxr-xr-x. 3 root root 4096 5月 27 14:03 .. drwxrwxrwx. 2 root root 4096 5月 27 14:14 hello -rwxrw-r–. 1 kingge kingge 34 5月 27 14:20 test.txt 1.2.3 mkdir 创建一个新的目录1）基本语法： ​ mkdir [-p] 要创建的目录 ​ 选项： -p：创建多层目录 2）案例 [root@hadoop101 opt]# mkdir test [root@hadoop101 opt]# mkdir -p user/kingge 1.2.4 rmdir 删除一个空的目录1）基本语法： ​ rmdir 要删除的空目录 2）案例 [root@hadoop101 opt]# mkdir test [root@hadoop101 opt]# rmdir test 1.2.5 touch 创建空文件1）基本语法： ​ touch 文件名称 2）案例 [root@hadoop101 opt]# touch test.java 1.2.6 cd 切换目录1）基本语法： ​ （1）cd 绝对路径 ​ （2）cd 相对路径 ​ （3）cd ~或者cd （功能描述：回到自己的家目录） ​ （4）cd - （功能描述：回到上一次所在目录） ​ （5）cd .. （功能描述：回到当前目录的上一级目录） ​ （6）cd -P （功能描述：跳转到实际物理路径，而非快捷方式路径） 2）案例 （1）使用 mkdir 命令创建kingge目录 [root@hadoop101 ~]# mkdir kingge （2）使用绝对路径切换到kingge目录 [root@hadoop101 ~]# cd /root/kingge/ （3）使用相对路径切换到kingge目录 [root@hadoop101 ~]# cd ./kingge/ （4）表示回到自己的家目录，亦即是 /root 这个目录 [root@hadoop101 kingge]# cd ~ （5）cd- 回到上一次所在目录 [root@hadoop101 kingge]# cd - （6）表示回到当前目录的上一级目录，亦即是 /root 的上一级目录的意思； [root@hadoop101 ~]# cd .. 1.2.7 cp 复制文件或目录1）基本语法： （1）cp source dest （功能描述：复制source文件到dest） （2）cp -r sourceFolder targetFolder （功能描述：递归复制整个文件夹） 2）案例 （1）复制文件 [root@hadoop101 opt]# cp test.java test （2）递归复制整个文件夹 [root@hadoop101 opt]# cp -r test test1 1.2.8 rm 移除文件或目录1）基本语法 ​ （1）rmdir deleteEmptyFolder （功能描述：删除空目录） （2）rm -rf deleteFile （功能描述：递归删除目录中所有内容） 2）案例 ​ 1）删除空目录 [root@hadoop101 opt]# rmdir test 2）递归删除目录中所有内容 [root@hadoop101 opt]# rm -rf test1 1.2.9 mv 移动文件与目录或重命名1）基本语法： ​ （1）mv oldNameFile newNameFile （功能描述：重命名） ​ （2）mv /temp/movefile /targetFolder （功能描述：递归移动文件） 2）案例： ​ 1）重命名 [root@hadoop101 opt]# mv test.java test1.java 2）移动文件 [root@hadoop101 opt]# mv test1.java test1 1.2.10 cat 查看文件内容查看文件内容，从第一行开始显示。 1）基本语法 ​ cat [选项] 要查看的文件 选项： -A ：相当于 -vET 的整合选项，可列出一些特殊字符而不是空白而已； -b ：列出行号，仅针对非空白行做行号显示，空白行不标行号！ -E ：将结尾的断行字节 $ 显示出来； -n ：列出行号，连同空白行也会有行号，与 -b 的选项不同； -T ：将 [tab] 按键以 ^I 显示出来； -v ：列出一些看不出来的特殊字符 2）案例 [kingge@hadoop101 ~]$ cat -A test.txt hellda $ dasadf ^I$ da^I^I^I$ das$ 1.2.11 tac查看文件内容查看文件内容，从最后一行开始显示，可以看出 tac 是 cat 的倒著写。 1）基本语法： ​ tac [选项参数] 要查看的文件 2）案例 [root@hadoop101 test1]# cat test1.java hello kingge kingge1 [root@hadoop101 test1]# tac test1.java kingge1 kingge hello 1.2.12 more 查看文件内容查看文件内容，一页一页的显示文件内容。 1）基本语法： ​ more 要查看的文件 2）功能使用说明 空白键 (space)：代表向下翻一页； Enter:代表向下翻『一行』； q:代表立刻离开 more ，不再显示该文件内容。 Ctrl+F 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 :f 输出文件名和当前行的行号 3）案例 [root@hadoop101 test1]# more test1.java 1.2.13 less 查看文件内容less 的作用与 more 十分相似，都可以用来浏览文字档案的内容，不同的是 less 允许使用[pageup] [pagedown]往回滚动。 1）基本语法： ​ less 要查看的文件 2）功能使用说明 空白键 ：向下翻动一页； [pagedown]：向下翻动一页； [pageup] ：向上翻动一页； /字串 ：向下搜寻『字串』的功能；n：向下查找；N：向上查找； ?字串 ：向上搜寻『字串』的功能；n：向上查找；N：向下查找； q ：离开 less 这个程序； 3）案例 [root@hadoop101 test1]# less test1.java 1.2.14 head查看文件内容查看文件内容，只看头几行。 1）基本语法 head -n 10 文件 （功能描述：查看文件头10行内容，10可以是任意行数） 2）案例 [root@hadoop101 test1]# head -n 2 test1.java hello kingge 1.2.15 tail 查看文件内容查看文件内容，只看尾巴几行。 1）基本语法 （1）tail -n 10 文件 （功能描述：查看文件头10行内容，10可以是任意行数） （2）tail -f 文件 （功能描述：实时追踪该文档的所有更新） 2）案例 （1）查看文件头1行内容 [root@hadoop101 test1]# tail -n 1 test1.java kingge （2）实时追踪该档的所有更新 [root@hadoop101 test1]# tail -f test1.java hello kingge kingge 1.2.16 重定向命令1）基本语法： （1）ls -l &gt;文件 （功能描述：列表的内容写入文件a.txt中（覆盖写）） （2）ls -al &gt;&gt;文件 （功能描述：列表的内容追加到文件aa.txt的末尾） 2）案例 ​ （1）[root@hadoop101 opt]# ls -l&gt;t.txt （2）[root@hadoop101 opt]# ls -l&gt;&gt;t.txt （3）[root@hadoop101 test1]# echo hello&gt;&gt;test1.java 1.2.17 echo1）基本语法： （1）echo 要显示的内容 &gt;&gt; 存储内容的的文件 （功能描述：将要显示的内容，存储到文件中） ​ （2）echo 变量 （功能描述：显示变量的值） 2）案例 [root@hadoop101 test1]# echo $JAVA_HOME /opt/module/jdk1.7.0_79 1.2.18 ln软链接1）基本语法： ln -s [原文件] [目标文件] （功能描述：给原文件创建一个软链接，软链接存放在目标文件目录） 删除软链接： rm -rf kingge，而不是rm -rf kingge/ 2）案例： [root@hadoop101 module]# ln -s /opt/module/test.txt /opt/t.txt [root@hadoop101 opt]# ll lrwxrwxrwx. 1 root root 20 6月 17 12:56 t.txt -&gt; /opt/module/test.txt 创建一个软链接 [kingge@hadoop101 opt]$ ln -s /opt/module/hadoop-2.7.2/ /opt/software/hadoop cd不加参数进入是软链接的地址 [kingge@hadoop101 software]$ cd hadoop [kingge@hadoop101 hadoop]$ pwd /opt/software/hadoop cd加参数进入是实际的物理地址 [kingge@hadoop101 software]$ cd -P hadoop [kingge@hadoop101 hadoop-2.7.2]$ pwd /opt/module/hadoop-2.7.2 1.2.19 history查看所敲命令历史1）基本语法： ​ history 2）案例 [root@hadoop101 test1]# history 1.3 时间日期类1）基本语法 date [OPTION]… [+FORMAT] 1.3.1 date显示当前时间1）基本语法： ​ （1）date （功能描述：显示当前时间） ​ （2）date +%Y （功能描述：显示当前年份） （3）date +%m （功能描述：显示当前月份） （4）date +%d （功能描述：显示当前是哪一天） （5）date +%Y%m%d date +%Y/%m/%d … （功能描述：显示当前年月日各种格式 ） ​ （6）date “+%Y-%m-%d %H:%M:%S” （功能描述：显示年月日时分秒） 2）案例 [root@hadoop101 /]# date 2017年 06月 19日 星期一 20:53:30 CST [root@hadoop101 /]# date +%Y%m%d 20170619 [root@hadoop101 /]# date “+%Y-%m-%d %H:%M:%S” 2017-06-19 20:54:58 1.3.2 date显示非当前时间1）基本语法： （1）date -d ‘1 days ago’ （功能描述：显示前一天日期） （2）date -d yesterday +%Y%m%d （同上） （3）date -d next-day +%Y%m%d （功能描述：显示明天日期） （4）date -d ‘next monday’ （功能描述：显示下周一时间） 2）案例： [root@hadoop101 /]# date -d ‘1 days ago’ 2017年 06月 18日 星期日 21:07:22 CST [root@hadoop101 /]# date -d next-day +%Y%m%d 20170620 [root@hadoop101 /]# date -d ‘next monday’ 2017年 06月 26日 星期一 00:00:00 CST 1.3.3 date设置系统时间1）基本语法： ​ date -s 字符串时间 2）案例 ​ [root@hadoop106 /]# date -s “2017-06-19 20:52:18” 1.3.4 cal查看日历1）基本语法： cal [选项] （功能描述：不加选项，显示本月日历） 选项： -3 ，显示系统前一个月，当前月，下一个月的日历 具体某一年，显示这一年的日历。 2）案例： [root@hadoop101 /]# cal [root@hadoop101 /]# cal -3 ​ [root@hadoop101 /]# cal 2016 1.4 用户管理命令1.4.1 useradd 添加新用户1）基本语法： ​ useradd 用户名 （功能描述：添加新用户） 2）案例： ​ [root@hadoop101 opt]# user kingge 1.4.2 passwd 设置用户密码1）基本语法： ​ passwd 用户名 （功能描述：设置用户密码） 2）案例 ​ [root@hadoop101 opt]# passwd kingge 1.4.3 id 判断用户是否存在1）基本语法： ​ id 用户名 2）案例： ​ [root@hadoop101 opt]#id kingge 1.4.4 su 切换用户1）基本语法： su 用户名称 （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量） su - 用户名称 （功能描述：切换到用户并获得该用户的环境变量及执行权限） 2）案例 [root@hadoop101 opt]#su kingge [root@hadoop101 opt]#su - kingge 1.4.5 userdel 删除用户1）基本语法： ​ （1）userdel 用户名 （功能描述：删除用户但保存用户主目录） （2）userdel -r 用户名 （功能描述：用户和用户主目录，都删除） 2）案例： （1）删除用户但保存用户主目录 ​ [root@hadoop101 opt]#userdel kingge （2）删除用户和用户主目录，都删除 ​ [root@hadoop101 opt]#userdel -r kingge 1.4.6 who 查看登录用户信息1）基本语法 ​ （1）whoami （功能描述：显示自身用户名称） （2）who am i （功能描述：显示登录用户的用户名） （3）who （功能描述：看当前有哪些用户登录到了本台机器上） 2）案例 [root@hadoop101 opt]# whoami [root@hadoop101 opt]# who am i ​ [root@hadoop101 opt]# who 1.4.7 设置kingge普通用户具有root权限1）修改配置文件 修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) ALL或者配置成采用sudo命令时，不需要输入密码 Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) NOPASSWD:ALL修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。 2）案例 [kingge@hadoop101 opt]$ sudo mkdir module [root@hadoop101 opt]# chown kingge:kingge module/ 1.4.8 cat /etc/passwd 查看创建了哪些用户cat /etc/passwd 1.4.9 usermod修改用户1）基本语法： usermod -g 用户组 用户名 2）案例： 将用户kingge加入dev用户组 [root@hadoop101 opt]#usermod -g dev kingge 1.5 用户组管理命令每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同， 如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 1.5.1 groupadd 新增组1）基本语法 groupadd 组名 2）案例： ​ 添加一个kingge组 [root@hadoop101 opt]#groupadd kingge 1.5.2 groupdel删除组1）基本语法： groupdel 组名 2）案例 [root@hadoop101 opt]# groupdel kingge 1.5.3 groupmod修改组1）基本语法： groupmod -n 新组名 老组名 2）案例 ​ 修改kingge组名称为kingge1 [root@hadoop101 kingge]# groupmod -n kingge1 kingge 1.5.4 cat /etc/group 查看创建了哪些组cat /etc/group 1.5.5 综合案例[root@hadoop101 kingge]# groupadd dev [root@hadoop101 kingge]# groupmod -n device dev [root@hadoop101 kingge]# usermod -g device kingge [root@hadoop101 kingge]# su kingge [kingge@hadoop101 ~]$ mkdir kingge [kingge@hadoop101 ~]$ ls -l drwxr-xr-x. 2 kingge device 4096 5月 27 16:31 kingge [root@hadoop101 kingge]# usermod -g kingge kingge 1.6 文件权限类1.6.1 文件属性Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls –l命令来显示一个文件的属性以及文件所属的用户和组。 1）从左到右的10个字符表示： 如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示: （1）0首位表示类型 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等 - 代表文件 d 代表目录 c 字符流，装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置) s socket p 管道 l 链接文档(link file)； b 设备文件，装置文件里面的可供储存的接口设备(可随机存取装置) （2）第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User （3）第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group （4）第7-9位确定其他用户拥有该文件的权限 —Other 文件类型 属主权限 属组权限 其他用户权限 0 1 2 3 4 5 6 7 8 9 d R w x R - x R - x 目录文件 读 写 执行 读 写 执行 读 写 执行 2）rxw作用文件和目录的不同解释 （1）作用到文件： [ r ]代表可读(read): 可以读取，查看 [ w ]代表可写(write): 可以修改，但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件. [ x ]代表可执行(execute):可以被系统执行 （2）作用到目录： [ r ]代表可读(read): 可以读取，ls查看目录内容 [ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录 [ x ]代表可执行(execute):可以进入该目录 3）案例 [kingge@hadoop101 ~]$ ls -l 总用量 8 drwxrwxr-x. 2 kingge kingge 4096 5月 27 14:14 hello -rw-rw-r–. 1 kingge kingge 34 5月 27 14:20 test.txt （1）如果查看到是文件：链接数指的是硬链接个数。创建硬链接方法 ln [原文件] [目标文件] [root@hadoop101 xiyou]# ln sunhouzi/shz.txt ./shz.txt （2）如果查看的是文件夹：链接数指的是子文件夹个数。 [root@hadoop101 xiyou]# ls -al kingge/ 总用量 8 drwxr-xr-x. 2 root root 4096 9月 3 19:02 . drwxr-xr-x. 5 root root 4096 9月 3 21:21 .. 1.6.2 chmod改变权限1）基本语法： ​ chmod [{ugoa}{+-=}{rwx}] [文件或目录] [mode=421 ] [文件或目录] 2）功能描述 改变文件或者目录权限 文件: r-查看；w-修改；x-执行文件 目录: r-列出目录内容；w-在目录中创建和删除；x-进入目录 删除一个文件的前提条件:该文件所在的目录有写权限，你才能删除该文件。 3）案例 [root@hadoop101 test1]# chmod u+x test1.java [root@hadoop101 test1]# chmod g+x test1.java [root@hadoop101 test1]# chmod o+x test1.java [root@hadoop101 test1]# chmod 777 test1.java [root@hadoop101 test1]# chmod -R 777 testdir 1.6.3 chown改变所有者1）基本语法： chown [最终用户] [文件或目录] （功能描述：改变文件或者目录的所有者） 2）案例 [root@hadoop101 test1]# chown kingge test1.java [root@hadoop101 test1]# ls -al -rwxr-xr-x. 1 kingge kingge 551 5月 23 13:02 test1.java 修改前： [root@hadoop101 xiyou]# ll drwxrwxrwx. 2 root root 4096 9月 3 21:20 sunhouzi 修改后 [root@hadoop101 xiyou]# chown -R kingge:kingge sunhouzi/ [root@hadoop101 xiyou]# ll drwxrwxrwx. 2 kingge kingge 4096 9月 3 21:20 sunhouzi 1.6.4 chgrp改变所属组1）基本语法： ​ chgrp [最终用户组] [文件或目录] （功能描述：改变文件或者目录的所属组） 2）案例 [root@hadoop101 test1]# chgrp kingge test1.java [root@hadoop101 test1]# ls -al -rwxr-xr-x. 1 root kingge 551 5月 23 13:02 test1.java 1.6.5 su 切换用户1）基本语法： su -username （功能描述：切换用户） 2）案例 [root@hadoop101 kingge]# su kingge [kingge@hadoop101 ~]$ [kingge@hadoop101 ~]$ su root 密码： [root@hadoop101 kingge]# 1.7 磁盘分区类1.7.1 fdisk查看分区1）基本语法： ​ fdisk -l （功能描述：查看磁盘分区详情） ​ 注意：在root用户下才能使用 2）功能说明： ​ （1）Linux分区 这个硬盘是20G的，有255个磁面；63个扇区；2610个磁柱；每个 cylinder（磁柱）的容量是 8225280 bytes=8225.280 K（约为）=8.225280M（约为）； Device Boot Start End Blocks Id System 分区序列 引导 从X磁柱开始 到Y磁柱结束 容量 分区类型ID 分区类型 （2）Win7分区 3）案例 [root@hadoop101 /]# fdisk -l Disk /dev/sda: 21.5 GB, 21474836480 bytes 255 heads, 63 sectors/track, 2610 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x0005e654 Device Boot Start End Blocks Id System /dev/sda1 * 1 26 204800 83 Linux Partition 1 does not end on cylinder boundary. /dev/sda2 26 1332 10485760 83 Linux /dev/sda3 1332 1593 2097152 82 Linux swap / Solaris 1.7.2 df查看硬盘1）基本语法： ​ df 参数 （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况） 参数： -a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统； -k ：以 KBytes 的容量显示各文件系统； -m ：以 MBytes 的容量显示各文件系统； -h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示； -H ：以 M=1000K 取代 M=1024K 的进位方式； -T ：显示文件系统类型，连同该 partition 的 filesystem 名称 (例如 ext3) 也列出； -i ：不用硬盘容量，而以 inode 的数量来显示 2）案例 [root@hadoop101 ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda2 15G 3.5G 11G 26% / tmpfs 939M 224K 939M 1% /dev/shm /dev/sda1 190M 39M 142M 22% /boot 1.7.3 mount/umount挂载/卸载对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构 Linux中每个分区都是用来组成整个文件系统的一部分，她在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。 0**）挂载前准备（必须要有光盘或者已经连接镜像文件）** 1**）挂载光盘语法：** mount [-t vfstype] [-o options] device dir （1）-t vfstype 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。 常用类型有： 光盘或光盘镜像：iso9660 DOS fat16文件系统：msdos Windows 9x fat32文件系统：vfat Windows NT ntfs文件系统：ntfs Mount Windows文件网络共享：smbfs UNIX(LINUX) 文件网络共享：nfs （2）-o options 主要用来描述设备或档案的挂接方式。常用的参数有： loop：用来把一个文件当成硬盘分区挂接上系统 ro：采用只读方式挂接设备 rw：采用读写方式挂接设备 iocharset：指定访问文件系统所用字符集 （3）device 要挂接(mount)的设备 （4）dir设备在系统上的挂接点(mount point) 2**）案例** （1）光盘镜像文件的挂载 [root@hadoop101 ~]# mkdir /mnt/cdrom/ 建立挂载点 [root@hadoop101 ~]# mount -t iso9660 /dev/cdrom /mnt/cdrom/ 设备/dev/cdrom挂载到 挂载点 ： /mnt/cdrom中 [root@hadoop101 ~]# ll /mnt/cdrom/ 3**）卸载光盘语法：** [root@hadoop101 ~]# umount 设备文件名或挂载点 4**）案例** [root@hadoop101 ~]# umount /mnt/cdrom 5**）开机自动挂载语法：** [root@hadoop101 ~]# vi /etc/fstab 添加红框中内容，保存退出。 1.8 搜索查找类1.8.1 find 查找文件或者目录1）基本语法： ​ find [搜索范围] [匹配条件] 2）案例 （1）按文件名：根据名称查找/目录下的filename.txt文件。 [root@hadoop101 ~]# find /opt/ -name *.txt （2）按拥有者：查找/opt目录下，用户名称为-user的文件 [root@hadoop101 ~]# find /opt/ -user kingge ​ （3）按文件大小：在/home目录下查找大于200m的文件（+n 大于 -n小于 n等于） [root@hadoop101 ~]find /home -size +204800 1.8.2 grep 过滤查找及“|”管道符0）管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理 1）基本语法 grep+参数+查找内容+源文件 参数： -c：只输出匹配行的计数。 -I：不区分大小写(只适用于单字符)。 -h：查询多文件时不显示文件名。 -l：查询多文件时只输出包含匹配字符的文件名。 -n：显示匹配行及行号。 -s：不显示不存在或无匹配文本的错误信息。 -v：显示不包含匹配文本的所有行。 2）案例 [root@hadoop101 opt]# ls | grep -n test 4:test1 5:test2 1.8.3 which 文件搜索命令1）基本语法： ​ which 命令 （功能描述：搜索命令所在目录及别名信息） 2）案例 ​ [root@hadoop101 opt]# which ls ​ /bin/ls 1.9 进程线程类进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。 1.9.1 ps查看系统中所有进程1）基本语法： ​ ps -aux （功能描述：查看系统中所有进程） 2）功能说明 ​ USER：该进程是由哪个用户产生的 ​ PID：进程的ID号 %CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源； %MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源； VSZ：该进程占用虚拟内存的大小，单位KB； RSS：该进程占用实际物理内存的大小，单位KB； TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。 STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台 START：该进程的启动时间 TIME：该进程占用CPU的运算时间，注意不是系统时间 COMMAND：产生此进程的命令名 3）案例 ​ [root@hadoop101 datas]# ps -aux 1.9.2 top查看系统健康状态1）基本命令 ​ top [选项] ​ （1）选项： ​ -d 秒数：指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令： -i：使top不显示任何闲置或者僵死进程。 -p：通过指定监控进程ID来仅仅监控某个进程的状态。 ​ （2）操作选项： P： 以CPU使用率排序，默认就是此项 M： 以内存的使用率排序 N： 以PID排序 q： 退出top ​ （3）查询结果字段解释 第一行信息为任务队列信息 内容 说明 12:26:46 系统当前时间 up 1 day, 13:32 系统的运行时间，本机已经运行1天 13小时32分钟 2 users 当前登录了两个用户 load average: 0.00, 0.00, 0.00 系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。 第二行为进程信息 Tasks: 95 total 系统中的进程总数 1 running 正在运行的进程数 94 sleeping 睡眠的进程 0 stopped 正在停止的进程 0 zombie 僵尸进程。如果不是0，需要手工检 查僵尸进程 第三行为CPU信息 Cpu(s): 0.1%us 用户模式占用的CPU百分比 0.1%sy 系统模式占用的CPU百分比 0.0%ni 改变过优先级的用户进程占用的CPU百分比 99.7%id 空闲CPU的CPU百分比 0.1%wa 等待输入/输出的进程的占用CPU百分比 0.0%hi 硬中断请求服务占用的CPU百分比 0.1%si 软中断请求服务占用的CPU百分比 0.0%st st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。 第四行为物理内存信息 Mem: 625344k total 物理内存的总量，单位KB 571504k used 已经使用的物理内存数量 53840k free 空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了 65800k buffers 作为缓冲的内存数量 第五行为交换分区（swap）信息 Swap: 524280k total 交换分区（虚拟内存）的总大小 0k used 已经使用的交互分区的大小 524280k free 空闲交换分区的大小 409280k cached 作为缓存的交互分区的大小 2）案例 ​ [root@hadoop101 kingge]# top -d 1 [root@hadoop101 kingge]# top -i [root@hadoop101 kingge]# top -p 2575 执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。 1.9.3 pstree查看进程树1）基本语法： ​ pstree [选项] ​ 选项 -p： 显示进程的PID -u： 显示进程的所属用户 2）案例： ​ [root@hadoop101 datas]# pstree -u [root@hadoop101 datas]# pstree -p 1.9.4 kill终止进程1）基本语法： ​ kill -9 pid进程号 ​ 选项 -9 表示强迫进程立即停止 2）案例： ​ 启动mysql程序 ​ 切换到root用户执行 ​ [root@hadoop101 桌面]# kill -9 5102 1.9.5 netstat显示网络统计信息1）基本语法： ​ netstat -anp （功能描述：此命令用来显示整个系统目前的网络情况。例如目前的连接、数据包传递数据、或是路由表内容） ​ 选项： ​ -an 按一定顺序排列输出 ​ -p 表示显示哪个进程在调用 ​ -nltp 查看tcp协议进程端口号 2）案例 查看端口50070的使用情况 [root@hadoop101 hadoop-2.7.2]# netstat -anp | grep 50070 tcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 6816/java ​ 端口号 进程号 1.9.6 前后台进程切换1）基本语法： fg %1 （功能描述：把后台进程转换成前台进程） ctrl+z bg %1 （功能描述：把前台进程发到后台） 1.10 压缩和解压类1.10.1 gzip/gunzip压缩1）基本语法： gzip+文件 （功能描述：压缩文件，只能将文件压缩为*.gz文件） gunzip+文件.gz （功能描述：解压缩文件命令） 2）特点： （1）只能压缩文件不能压缩目录 （2）不保留原来的文件 3）案例 （1）gzip压缩 [root@hadoop101 opt]# ls test.java [root@hadoop101 opt]# gzip test.java [root@hadoop101 opt]# ls test.java.gz （2）gunzip解压缩文件 [root@hadoop101 opt]# gunzip test.java.gz [root@hadoop101 opt]# ls test.java 1.10.2 zip/unzip压缩1）基本语法： zip + 参数 + XXX.zip + 将要压缩的内容 （功能描述：压缩文件和目录的命令，window/linux通用且可以压缩目录且保留源文件） 参数： -r 压缩目录 2）案例： （1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip [root@hadoop101 opt]# zip test.zip test1.java test.java adding: test1.java (stored 0%) adding: test.java (stored 0%) [root@hadoop101 opt]# ls test1.java test.java test.zip （2）解压 mypackage.zip [root@hadoop101 opt]# unzip test.zip Archive: test.zip extracting: test1.java extracting: test.java ​ [root@hadoop101 opt]# ls test1.java test.java test.zip 1.10.3 tar打包1）基本语法： tar + 参数 + XXX.tar.gz + 将要打包进去的内容 （功能描述：打包目录，压缩后的文件格式.tar.gz） 参数： -c 产生.tar打包文件 -v 显示详细信息 -f 指定压缩后的文件名 -z 打包同时压缩 -x 解包.tar文件 2）案例 （1）压缩：tar -zcvf XXX.tar.gz n1.txt n2.txt ​ 压缩多个文件 [root@hadoop101 opt]# tar -zcvf test.tar.gz test1.java test.java test1.java test.java [root@hadoop101 opt]# ls test1.java test.java test.tar.gz 压缩目录 [root@hadoop101 opt]# tar -zcvf test.java.tar.gz test1 test1/ test1/hello test1/test1.java test1/test/ test1/test/test.java [root@hadoop106 opt]# ls test1 test.java.tar.gz （2）解压：tar -zxvf XXX.tar.gz ​ 解压到当前目录 [root@hadoop101 opt]# tar -zxvf test.tar.gz 解压到/opt目录 [root@hadoop101 opt]# tar -zxvf test.tar.gz -C /opt .11 后台服务管理类.11.1 service后台服务管理1）service network status 查看指定服务的状态 2）service network stop 停止指定服务 3）service network start 启动指定服务 4）service network restart 重启指定服务 5）service –status-all 查看系统中所有的后台服务 .11.2 chkconfig设置后台服务的自启配置1）chkconfig 查看所有服务器自启配置 2）chkconfig iptables off 关掉指定服务的自动启动 3）chkconfig iptables on 开启指定服务的自动启动 1.12 crond系统定时任务1.12.1 crond服务管理[root@hadoop101 ~]# service crond restart （重新启动服务） 1.12.2 crontab定时任务设置1）基本语法 crontab [选项] 选项： -e： 编辑crontab定时任务 -l： 查询crontab任务 -r： 删除当前用户所有的crontab任务 2）参数说明 ​ [root@hadoop101 ~]# crontab -e （1）进入crontab编辑界面。会打开vim编辑你的工作。 * 执行的任务 项目 含义 范围 第一个“*” 一小时当中的第几分钟 0-59 第二个“*” 一天当中的第几小时 0-23 第三个“*” 一个月当中的第几天 1-31 第四个“*” 一年当中的第几月 1-12 第五个“*” 一周当中的星期几 0-7（0和7都代表星期日） （2）特殊符号 特殊符号 含义 * 代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。 ， 代表不连续的时间。比如“0 8,12,16 * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 - 代表连续的时间范围。比如“0 5 1-6命令”，代表在周一到周六的凌晨5点0分执行命令 */n 代表每隔多久执行一次。比如“/10 * 命令”，代表每隔10分钟就执行一遍命令 （3）特定时间执行命令 时间 含义 45 22 * 命令 在22点45分执行命令 0 17 1 命令 每周1 的17点0分执行命令 0 5 1,15 命令 每月1号和15号的凌晨5点0分执行命令 40 4 1-5 命令 每周一到周五的凌晨4点40分执行命令 /10 4 命令 每天的凌晨4点，每隔10分钟执行一次命令 0 0 1,15 * 1 命令 每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。 3）案例： /5 * /bin/echo ”11” &gt;&gt; /tmp/test","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"linux基础","slug":"linux基础","date":"2017-06-12T01:17:03.000Z","updated":"2019-06-02T04:09:46.634Z","comments":true,"path":"2017/06/12/linux基础/","link":"","permalink":"http://kingge.top/2017/06/12/linux基础/","excerpt":"","text":"一 、Linux入门概述linux 系统也是接触了许久，不过一直没有机会总结一下，所以决定出个linux相关安装和配置以及常用指令的专栏。 1.1 概述​ Linux内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。 ​ Linux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。Linux能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 ​ 目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debain、Fedora、SuSE、OpenSUSE ​ 下面的操作，我使用的是Centos 1.2 下载地址centos下载地址： 网易镜像：http://mirrors.163.com/centos/6/isos/ 1.3 Linux特点 Linux里面一切皆是文件 Linux里面没有后缀名这一说 1.4 Linux和Windows区别目前国内Linux更多的是应用在服务器上，而桌面操作系统更多使用的是window。主要区别如下。 二 、VM安装相关（运行环境）2.1 安装VMWare虚拟机 详情这里就不说了，自行百度。 2.2 安装CentOS ​ 需要注意的是：下面的步骤，按需省略。 1 检查BIOS虚拟化支持 2 新建虚拟机 3 新建虚拟机向导 4 创建虚拟空白光盘 5 安装Linux系统对应的CentOS版 6 虚拟机命名和定位磁盘位置 7 处理器配置，看自己是否是双核、多核 8 设置内存为2GB 9 网络设置NAT 10 选择IO控制器类型 11 选择磁盘类型 12 新建虚拟磁盘 13 设置磁盘容量 14 你在哪里存储这个磁盘文件 15 新建虚拟机向导配置完成 16 VM设置 17 加载ISO 18 加电并安装配置CentOS 19 加电后初始化欢迎进入页面 回车选择第一个开始安装配置，此外，在Ctrl+Alt可以实现Windows主机和VM之间窗口的切换 20 是否对CD媒体进行测试，直接跳过**Skip** 21 CentOS欢迎页面，直接点击Next 22 选择简体中文进行安装 23 选择语言键盘 23 选择存储设备 24 给计算机起名 25 设置网络环境 安装成功后再设置。 26 选择时区 27 设置root密码 （一定记住） 28 硬盘分区-1 29 根分区新建 l Boot l swap分区设置 l 分区完成 30 程序引导，直接下一步 31 现在定制系统软件 32 Web环境 33 可扩展文件系统支持 34 基本系统 35 应用程序 36 开发、弹性存储、数据库、服务器 可以都不勾，有需要，以后使用中有需要再手动安装 37 桌面 除了KDE，其他都选就可以了。 38 语言支持 39 系统管理、虚拟化、负载平衡器、高可用性可以都不选 40 完成配置，开始安装CentOS 41 等待安装完成，等待等待等待等待……20分钟左右 42 安装完成，重新引导 43 欢迎引导页面 44 许可证 45 创建用户，可以先不创建，用root账户登录就行 46 时间和日期 47 Kdump,去掉 48 重启后用root登录 2.3 安装VMTools工具1）什么是VMtools VM tools顾名思义就是Vmware的一组工具。主要用于虚拟主机显示优化与调整，另外还可以方便虚拟主机与本机的交互，如允许共享文件夹，甚至可以直接从本机向虚拟主机拖放文件、鼠标无缝切换、显示分辨率调整等，十分实用。 安装过程自行百度 三 、Linux目录结构3.1 概览 3.2 树状目录结构 /bin：是Binary的缩写，这个目录存放着系统必备执行命令/boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件，自己的安装别放这里/dev：Device(设备)的缩写，该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。/etc：所有的系统管理所需要的配置文件和子目录。/home：存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。/lib：系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。/lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。/media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。/misc: 该目录可以用来存放杂项文件或目录，即那些用途或含义不明确的文件或目录可以存放在该目录下。/mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。/net 存放着和网络相关的一些文件./opt：这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。/proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。/root：该目录为系统管理员，也称作超级权限者的用户主目录。/sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。/selinux：这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙/srv：service缩写，该目录存放一些服务启动之后需要提取的数据。/sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。/tmp：这个目录是用来存放一些临时文件的。/usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。/var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 四 VI/VIM编辑器4.1 概述所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。但是目前我们使用比较多的是 vim 编辑器。 Vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 简单的来说vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。vim 则可以说是程序开发者的一项很好用的工具。连vim 的官方网站 (http://www.vim.org) 自己也说 vim 是一个程序开发工具而不是文字处理软件。 4.2 一般模式以 vi 打开一个档案就直接进入一般模式了(这是默认的模式)。在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、粘贴』来处理你的文件数据。 常用语法 1）yy （功能描述：复制光标当前一行） y数字y （功能描述：复制一段(从第几行到第几行)） 2）p （功能描述：箭头移动到目的行粘贴） 3）u （功能描述：撤销上一步） 4）dd （功能描述：删除光标当前行） d数字d （功能描述：删除光标(含)后多少行） 5）x （功能描述：删除一个字母，相当于del） X （功能描述：删除一个字母，相当于Backspace） 6）yw （功能描述：复制一个词） 7）dw （功能描述：删除一个词） 8）shift+^ （功能描述：移动到行头） 9）shift+$ （功能描述：移动到行尾） 10）1+shift+g （功能描述：移动到页头，数字） 11）shift+g （功能描述：移动到页尾） 12）数字N+shift+g （功能描述：移动到目标行） 4.3 编辑模式在一般模式中可以进行删除、复制、贴上等等的动作，但是却无法编辑文件内容的！ 要等到你按下『i, I, o, O, a, A, r, R』等任何一个字母之后才会进入编辑模式。 注意了！通常在 Linux 中，按下这些按键时，在画面的左下方会出现『INSERT 或 REPLACE 』的字样，此时才可以进行编辑。而如果要回到一般模式时， 则必须要按下『Esc』这个按键即可退出编辑模式。 常用语法 1）进入编辑模式 （1）i 当前光标前 （2）a 当前光标后 （3）o 当前光标行的下一行 2）退出编辑模式 按『Esc』键 4.4 指令模式在一般模式当中，输入『 : / ?』3个中的任何一个按钮，就可以将光标移动到最底下那一行。 在这个模式当中， 可以提供你『搜寻资料』的动作，而读取、存盘、大量取代字符、离开 vi 、显示行号等动作是在此模式中达成的！ 常用语法 1）基本语法 （1）: 选项 ​ 选项： w 保存 q 退出 ！ 感叹号强制执行 （2）/ 查找，/被查找词，n是查找下一个，shift+n是往上查找 （3）? 查找，?被查找词，n是查找上一个，shift+n是往下查找 2）案例 :wq! 强制保存退出 五 系统管理操作5.1 查看网络IP和网关1）查看虚拟网络编辑器 2）修改ip地址 3）查看网关 5.2 配置网络ip地址 0）查看当前ip基本语法： &gt; &gt; &gt; [root@hadoop101 /]# ifconfig&gt; &gt; 1）在终端命令窗口中输入（如果不是克隆的虚拟机可以跳过这一步）*******&gt; &gt; [root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules&gt; &gt; 进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址&gt; 2）修改IP地址 [root@hadoop101 /]#vim /etc/sysconfig/network-scripts/ifcfg-eth0需要修改的内容有5项：IPADDR=192.168.1.101GATEWAY=192.168.1.2ONBOOT=yesBOOTPROTO=staticDNS1=192.168.1.2 （1）修改前 ​ （2）修改后 ：wq 保存退出 3）执行service network restart 3）执行service network restart 4）如果报错，reboot，重启虚拟机 5.3 配置主机名0）查看主机名基本语法： [root@hadoop101 /]#hostname 1）修改linux的主机映射文件（hosts文件） （1）进入Linux系统查看本机的主机名。通过hostname命令查看[root@hadoop101 ~]# hostnamehadoop100（2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件[root@hadoop101 /]# vi /etc/sysconfig/network文件中内容NETWORKING=yesNETWORKING_IPV6=noHOSTNAME= hadoop101注意：主机名称不要有“_”下划线（3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。（4）保存退出。（5）打开/etc/hosts[root@hadoop101 /]# vim /etc/hosts添加如下内容192.168.1.101 hadoop101（6）并重启设备，重启后，查看主机名，已经修改成功 2）修改window7的主机映射文件（hosts文件）–方面在电脑使用域名进行访问hadoop相关的组件-例如hdfs，mapreduce等等。 ​ （1）进入C:\\Windows\\System32\\drivers\\etc路径 （2）打开hosts文件并添加如下内容192.168.1.101 hadoop101192.168.1.102 hadoop102192.168.1.103 hadoop103192.168.1.104 hadoop104192.168.1.105 hadoop105192.168.1.106 hadoop106192.168.1.107 hadoop107192.168.1.108 hadoop108 5.4 防火墙1）基本语法： service iptables status （功能描述：查看防火墙状态）chkconfig iptables -list （功能描述：查看防火墙开机启动状态）service iptables stop （功能描述：临时关闭防火墙）chkconfig iptables off （功能描述：关闭防火墙开机启动）chkconfig iptables on （功能描述：开启防火墙开机启动） 2）扩展 Linux系统有7个运行级别(runlevel)运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆运行级别2：多用户状态(没有NFS)运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式运行级别4：系统未使用，保留运行级别5：X11控制台，登陆后进入图形GUI模式运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 5.5 关机重启在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机 。 正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt 1）基本语法： ​ （1）sync （功能描述：将数据由内存同步到硬盘中）​ （2）shutdown [选项] 时间 ​ 选项：​ -h：关机​ -r：重启（3）halt （功能描述：关闭系统，等同于shutdown -h now 和 poweroff）（4）reboot （功能描述：就是重启，等同于 shutdown -r now） 2）案例 （1）将数据由内存同步到硬盘中[root@hadoop101 /]#sync （2）计算机将在10分钟后关机，并且会显示在登录用户的当前屏幕中[root@hadoop101 /]#shutdown -h 10 ‘This server will shutdown after 10 mins’（3）立马关机[root@hadoop101 /]# shutdown -h now （4）系统立马重启[root@hadoop101 /]# shutdown -r now（5）重启（等同于 shutdown -r now）[root@hadoop101 /]# reboot （6）关机（等同于shutdown -h now 和 poweroff）[root@hadoop101 /]#halt 注意：不管是重启系统还是关闭系统，首先要运行sync命令，把内存中的数据写到磁盘中。 5.6 找回root密码重新安装系统吗？当然不用！进入单用户模式更改一下root密码即可。 1）重启Linux，见到下图，在3秒钟之内按下回车 2）三秒之内要按一下回车，出现如下界面 3）按下e键就可以进入下图 4）移动到下一行，再次按e键 5）移动到下一行，进行修改 修改完成后回车键，然后按b键进行重新启动进入系统 6）移动到下一行，进行修改 最终修改完密码，reboot一下即可。 6.1 安装远程连接linux服务器工具Linux一般作为服务器使用，而服务器一般放在机房，你不可能在机房操作你的Linux服务器。这时我们就需要远程登录到Linux服务器来管理维护系统。 Linux系统中是通过SSH服务实现的远程登录功能，默认ssh服务端口号为 22。Window系统上 Linux 远程登录客户端有SecureCRT, Putty, SSH Secure Shell,XShell等 我这里安装的是 xshell。安装流程自行百度，比较简单。 # 好了到此，linux 相关的环境安装就已经结束了，linux相关的指令操作，可以参考下一章节。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kingge.top/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://kingge.top/tags/linux/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kingge.top/tags/hadoop/"}]},{"title":"Mysql索引详解","slug":"Mysql索引详解","date":"2016-08-01T02:37:15.000Z","updated":"2017-08-17T09:35:34.792Z","comments":true,"path":"2016/08/01/Mysql索引详解/","link":"","permalink":"http://kingge.top/2016/08/01/Mysql索引详解/","excerpt":"","text":"前言 索引对查询的速度有着至关重要的影响，理解索引也是进行数据库性能调优的起点。考虑如下情况，假设数据库中一个表有10^6条记录，DBMS的页面大小为4K，并存储100条记录。如果没有索引，查询将对整个表进行扫描，最坏的情况下，如果所有数据页都不在内存，需要读取10^4个页面，如果这10^4个页面在磁盘上随机分布，需要进行10^4次I/O，假设磁盘每次I/O时间为10ms(忽略数据传输时间)，则总共需要100s(但实际上要好很多很多)。如果对之建立B-Tree索引，则只需要进行log100(10^6)=3次页面读取，最坏情况下耗时30ms。这就是索引带来的效果，很多时候，当你的应用程序进行SQL查询速度很慢时，应该想想是否可以建索引。进入正题： 有些硬啃的干货还是得了解的，下面先了解索引的基本知识 索引分类 单列索引 主键索引 唯一索引 普通索引 组合索引用到的表CREATE TABLE `award` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '用户id', `aty_id` varchar(100) NOT NULL DEFAULT '' COMMENT '活动场景id', `nickname` varchar(12) NOT NULL DEFAULT '' COMMENT '用户昵称', `is_awarded` tinyint(1) NOT NULL DEFAULT 0 COMMENT '用户是否领奖', `award_time` int(11) NOT NULL DEFAULT 0 COMMENT '领奖时间', `account` varchar(12) NOT NULL DEFAULT '' COMMENT '帐号', `password` char(32) NOT NULL DEFAULT '' COMMENT '密码', `message` varchar(255) NOT NULL DEFAULT '' COMMENT '获奖信息', `created_time` int(11) NOT NULL DEFAULT 0 COMMENT '创建时间', `updated_time` int(11) NOT NULL DEFAULT 0 COMMENT '更新时间', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='获奖信息表'; 单列索引普通索引 这个是最基本的索引 创建语法：其sql格式是： 第一种方式 : CREATE INDEX IndexName ON `TableName`(`字段名`(length)) 第二种方式 : ALTER TABLE TableName ADD INDEX IndexName(`字段名`(length)) 创建例子：第一种方式 : CREATE INDEX account_Index ON `award`(`account`);第二种方式: ALTER TABLE award ADD INDEX account_Index(`account`) 唯一索引 与普通索引类似,但是不同的是唯一索引要求所有的类的值是唯一的,这一点和主键索引一样.但是他允许有空值 创建语法：其sql格式是： 第一种方式 : CREATE UNIQUE INDEX IndexName ON `TableName`(`字段名`(length)); 第二种方式 : ALTER TABLE TableName ADD UNIQUE (column_list) 创建例子：CREATE UNIQUE INDEX account_UNIQUE_Index ON `award`(`account`); 主键索引 他与唯一索引的不同在于不允许有空值(在B+TREE中的InnoDB引擎中,主键索引起到了至关重要的地位) 创建语法：其sql格式是： 第一种方式 : CREATE UNIQUE INDEX IndexName ON `TableName`(`字段名`(length)); 第二种方式 : ALTER TABLE TableName ADD UNIQUE (column_list) 创建例子：CREATE UNIQUE INDEX account_UNIQUE_Index ON `award`(`account`); 单列索引的总结mysql&gt;SELECT ｀uid｀ FROM people WHERE lname｀='Liu' AND ｀fname｀='Zhiqun' AND ｀age｀=26因为我们不想扫描整表，故考虑用索引。单列索引：ALTER TABLE people ADD INDEX lname (lname);将lname列建索引，这样就把范围限制在lname='Liu'的结果集1上，之后扫描结果集1，产生满足fname='Zhiqun'的结果集2，再扫描结果集2，找到 age=26的结果集3，即最终结果。由 于建立了lname列的索引，与执行表的完全扫描相比，效率提高了很多，但我们要求扫描的记录数量仍旧远远超过了实际所需 要的。虽然我们可以删除lname列上的索引，再创建fname或者age 列的索引，但是，不论在哪个列上创建索引搜索效率仍旧相似。&gt; 所以就需要组合索引 组合索引 一个表中含有多个单列索引不代表是组合索引,通俗一点讲 组合索引是:包含多个字段但是只有索引名称 创建语法：其sql格式是： CREATE INDEX IndexName On `TableName`(`字段名`(length),`字段名`(length),...); 创建例子：CREATE INDEX nickname_account_createdTime_Index ON `award`(`nickname`, `account`, `created_time`); 如果你建立了 组合索引(nickname_account_createdTime_Index) 那么他实际包含的是3个索引 (nickname) (nickname,account)(nickname,account,created_time) 组合索引的最左前缀 上面的例子中给nickname,account,created_time 这三个字段建立索引他会去创建三个索引，但是在执行查询的时候只会用其中一个索引去查询，mysql会选择一个最严格(获得结果集记录数最少)的索引，所以where子句中使用最频繁的一列放在最左边。所谓最左前缀原则就是先要看第一列，在第一列满足的条件下再看左边第二列 全文索引 文本字段上(text)如果建立的是普通索引,那么只有对文本的字段内容前面的字符进行索引,其字符大小根据索引建立索引时申明的大小来规定.如果文本中出现多个一样的字符,而且需要查找的话,那么其条件只能是 where column lick &apos;%xxxx%&apos; 这样做会让索引失效.这个时候全文索引就祈祷了作用了ALTER TABLE tablename ADD FULLTEXT(column1, column2)有了全文索引，就可以用SELECT查询命令去检索那些包含着一个或多个给定单词的数据记录了。ELECT * FROM tablenameWHERE MATCH(column1, column2) AGAINST(‘xxx′, ‘sss′, ‘ddd′)这条命令将把column1和column2字段里有xxx、sss和ddd的数据记录全部查询出来。 总结使用索引的优点 可以通过建立唯一索引或者主键索引,保证数据库表中每一行数据的唯一性. 建立索引可以大大提高检索的数据,以及减少表的检索行数 在表连接的连接条件 可以加速表与表直接的相连 在分组和排序字句进行数据检索,可以减少查询时间中 分组 和 排序时所消耗的时间(数据库的记录会重新排序) 建立索引,在查询中使用索引 可以提高性能 使用索引的缺点 在创建索引和维护索引 会耗费时间,随着数据量的增加而增加 索引文件会占用物理空间,除了数据表需要占用物理空间之外,每一个索引还会占用一定的物理空间 当对表的数据进行 INSERT,UPDATE,DELETE 的时候,索引也要动态的维护,这样就会降低数据的维护速度,(建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快)。 使用索引需要注意的地方 在经常需要搜索的列上,可以加快索引的速度 主键列上可以确保列的唯一性 在表与表的而连接条件上加上索引,可以加快连接查询的速度 在经常需要排序(order by),分组(group by)和的distinct 列上加索引 可以加快排序查询的时间, (单独order by 用不了索引，索引考虑加where 或加limit) 在一些where 之后的 &lt; &lt;= &gt; &gt;= BETWEEN IN 以及某个情况下的like 建立字段的索引(B-TREE) like语句的 如果你对nickname字段建立了一个索引.当查询的时候的语句是 nickname lick ‘%ABC%’ 那么这个索引讲不会起到作用.而nickname lick ‘ABC%’ 那么将可以用到索引 索引不会包含NULL列,如果列中包含NULL值都将不会被包含在索引中,复合索引中如果有一列含有NULL值那么这个组合索引都将失效,一般需要给默认值0或者 ‘ ‘字符串 使用短索引,如果你的一个字段是Char(32)或者int(32),在创建索引的时候指定前缀长度 比如前10个字符 (前提是多数值是唯一的..)那么短索引可以提高查询速度,并且可以减少磁盘的空间,也可以减少I/0操作. 不要在列上进行运算,这样会使得mysql索引失效,也会进行全表扫描 选择越小的数据类型越好,因为通常越小的数据类型通常在磁盘,内存,cpu,缓存中 占用的空间很少,处理起来更快 什么情况下不建立索引 查询中很少使用到的列 不应该创建索引,如果建立了索引然而还会降低mysql的性能和增大了空间需求. 很少数据的列也不应该建立索引,比如 一个性别字段 0或者1,在查询中,结果集的数据占了表中数据行的比例比较大,mysql需要扫描的行数很多,增加索引,并不能提高效率 定义为text和image和bit数据类型的列不应该增加索引 当表的修改(UPDATE,INSERT,DELETE)操作远远大于检索(SELECT)操作时不应该创建索引,这两个操作是互斥的关系 好的文章转：SQL优化转：MySQL索引原理及慢查询优化","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://kingge.top/tags/Mysql/"},{"name":"索引","slug":"索引","permalink":"http://kingge.top/tags/索引/"}]},{"title":"C++文件流操作的读与写","slug":"C-文件流操作的读与写","date":"2014-11-08T13:04:00.000Z","updated":"2017-08-17T06:33:56.710Z","comments":true,"path":"2014/11/08/C-文件流操作的读与写/","link":"","permalink":"http://kingge.top/2014/11/08/C-文件流操作的读与写/","excerpt":"","text":"对文件的写入put和&lt;&lt; 写入方式 put的操作：是对文件进行写入的操作，写入一个字符（可以使字母也可以是asci码值） file.put(' A');file.put('\\n');file &lt;&lt; \"xiezejing1994\"; 输出： &nbsp;&nbsp;&nbsp;&nbsp;A// 注意到A这里有几个空格 但是不影响左对齐xiezejing1994// 也就是说A的前面不会有空格 ##操作和&lt;&lt; 读写方式区别 put操作和 file &lt;&lt;‘A’这个基本上是一样的，但是有个区别就是他不可以这样file &lt;&lt;’ A’;（A的前面有空格）因为他是格式化输入 所以中间不能有”空格“但是这样file &lt;&lt;”‘ A”;（也就是以字符串的格式输入则会有空格） 文件的读操作1.getline（） getline（ cin ，string类型 ） getline( cin, z ); file1 &lt;&lt; z; （file1 为文件流对象） 例子： char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ); cout &lt;&lt; c; &#125; 假设文件1.txt内有' A xiezejing1994 这样文本它的输出：' Axiezejing1994 也就是说他没有读到换行的功能 不会输出' A xiezejing1994（原因就是getlibe其实里面有三个参数，第三个参数默认为'\\n'） 2.getline（ fstream，string ）while ( getline( file,z ) )&#123; cout &lt;&lt; z;&#125; 3.get（） char c[100]; while ( !file.eof() ) &#123; //file.getline( c,100 ,'\\0'); file.get( c,100 ,'\\0'); cout &lt;&lt; c; &#125;输出同getline一样----必须要写三个参数 否则只会输出一行（第三个参数为'\\n'也是只会输出一行）。非常严格的输出。 4.get操作 char c; file.get(c); while ( !file.eof() ) &#123; cout &lt;&lt; c; file.get(c); &#125;-----和getline的区别在于 他是读取单个字符的，所以会读取到结束符号故会输出' Axiezejing1994 对文件是否读到末尾的判断1.feof（） 该函数只有“已经读取了”结束标志时 feof（）才会返回非零值 也就是说当文件读取到文件结束标志位时他的返回值不是非零还是零 故还要在进行一次读. 例子 假设在1.txt中只有abc三个字符在进行 while（！feof(fp)） &#123; ch = getc(fp); putchar(ch); &#125;//实际上输出的是四个字符改为ch = getc（fp）；while （ ！feof（fp））&#123; putchar（ch）； ch = getc（fp）；&#125;// 这样就可以正常运行3. 可以不调用函数eof 直接就是 while （ file ） // file 就是文件流的对象&#123; 。。。。操作&#125;4.char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ,'\\0'); cout &lt;&lt; c; &#125;这个 和char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ,'\\n'); cout &lt;&lt; c; &#125;假设文本为上面的。输出分别为' A xiezejing1994' Axiezejing1994 读写1.read( 数组名，接收的个数 )2.write( 数组名，gcount函数 )#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main()&#123; ifstream file( \"D:\\\\jjj.txt\"); ofstream file1( \"D:\\\\j.txt\" , ios::app); string z; if ( !file ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125; char c[100]; while ( !file.eof() ) &#123; file.read( c,100 ); file1.write( c, file.gcount() ); &#125; file.close(); file.close(); return 0;&#125; **判断打开是否正确** 1. if( !file )2.if ( !file.good() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;3. if ( !file.is_open() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;4. if ( file.fail() ) &#123; cout &lt;&lt; \" 无法打开\\n \"; return 1; &#125;","categories":[{"name":"c++","slug":"c","permalink":"http://kingge.top/categories/c/"}],"tags":[{"name":"文件","slug":"文件","permalink":"http://kingge.top/tags/文件/"},{"name":"C++","slug":"C","permalink":"http://kingge.top/tags/C/"},{"name":"文件读写","slug":"文件读写","permalink":"http://kingge.top/tags/文件读写/"}]}]}