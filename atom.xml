<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>King哥</title>
  <subtitle>To know everything, no words don&#39;t talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kingge.top/"/>
  <updated>2017-08-31T09:44:24.038Z</updated>
  <id>http://kingge.top/</id>
  
  <author>
    <name>Jeremy Kinge</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hessian 多系统访问</title>
    <link href="http://kingge.top/2019/06/01/Hessian%20%E5%A4%9A%E7%B3%BB%E7%BB%9F%E8%AE%BF%E9%97%AE/"/>
    <id>http://kingge.top/2019/06/01/Hessian 多系统访问/</id>
    <published>2019-06-01T04:58:04.630Z</published>
    <updated>2017-08-31T09:44:24.038Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>hadoop在使用中的常用优化手段</title>
    <link href="http://kingge.top/2019/03/14/hadoop%E5%9C%A8%E4%BD%BF%E7%94%A8%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E6%89%8B%E6%AE%B5/"/>
    <id>http://kingge.top/2019/03/14/hadoop在使用中的常用优化手段/</id>
    <published>2019-03-14T13:59:59.000Z</published>
    <updated>2019-08-01T13:54:38.121Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h1><p>我们知道影响MapReduce运算的因素很多，主要是机器性能、网络、磁盘读写速度、I/O 操作等等有关。</p>
<p>机器的问题属于外部因素，那么下面主要是介绍关于IO操作引发的性能问题：</p>
<p>主要是有几个以下方面</p>
<blockquote>
<p>（1）数据倾斜 - <strong>重点</strong></p>
<p>（2）map和reduce数设置不合理</p>
<p>（3）map运行时间太长，导致reduce等待过久</p>
<p>（4）小文件过多 - <strong>重点</strong></p>
<p>（5）大量的不可分块的超大文件</p>
<p>（6）spill次数过多</p>
<p>（7）merge次数过多。</p>
</blockquote>
<p>​    MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。</p>
<p>  下面想讲解小文件的处理方式：</p>
<h2 id="1-1-HDFS小文件优化"><a href="#1-1-HDFS小文件优化" class="headerlink" title="1.1 HDFS小文件优化"></a>1.1 HDFS小文件优化</h2><p>​    <strong>HDFS上每个文件都要在namenode上建立一个索引</strong>，这个索引的大小约为<strong>150byte</strong>，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大是的索引速度变慢。</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p><strong>1）Hadoop Archive:</strong></p>
<p> 是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了namenode的内存使用。</p>
<p><img src="/2019/03/14/hadoop在使用中的常用优化手段/1564667225107.png" alt="1564667225107"></p>
<p><strong>2）Sequence file：</strong></p>
<p> sequence file由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。</p>
<p><strong>3）CombineFileInputFormat：</strong></p>
<p>  CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。（之前hadoop相关的章节讲解道，可以翻翻看看）</p>
<p><strong>4）开启JVM重用</strong></p>
<p>对于大量小文件Job，可以开启JVM重用会减少45%运行时间。</p>
<p>JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他map。</p>
<p>具体设置：mapreduce.job.jvm.numtasks值在10-20之间。</p>
<h2 id="1-2-分阶段优化"><a href="#1-2-分阶段优化" class="headerlink" title="1.2 分阶段优化"></a>1.2 分阶段优化</h2><h3 id="数据输入阶段"><a href="#数据输入阶段" class="headerlink" title="数据输入阶段"></a>数据输入阶段</h3><blockquote>
<p>（1）合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致mr运行较慢。</p>
<p>（2）采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景。</p>
</blockquote>
<h3 id="数据传输阶段"><a href="#数据传输阶段" class="headerlink" title="数据传输阶段"></a>数据传输阶段</h3><p><strong>1）采用数据压缩的方式</strong>，减少网络IO的的时间。安装Snappy和LZO压缩编码器。</p>
<p><strong>2）使用SequenceFile二进制文件。</strong></p>
<h3 id="进入Map阶段"><a href="#进入Map阶段" class="headerlink" title="进入Map阶段"></a>进入Map阶段</h3><blockquote>
<p><strong>1）减少溢写（spill）次数：</strong>通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘IO。</p>
<p><strong>2）减少合并（merge）次数：</strong>通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。</p>
<p>3）在map之后，<strong>不影响业务逻辑前提下，先进行combine处理</strong>，减少 I/O。</p>
</blockquote>
<h3 id="进入Reduce阶段"><a href="#进入Reduce阶段" class="headerlink" title="进入Reduce阶段"></a>进入Reduce阶段</h3><blockquote>
<p>暂无</p>
</blockquote>
<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><blockquote>
<p>暂无总结</p>
</blockquote>
<h3 id="常用参数哟花"><a href="#常用参数哟花" class="headerlink" title="常用参数哟花"></a>常用参数哟花</h3><blockquote>
<p>暂无</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h1&gt;&lt;p&gt;我们知道影响MapReduce运算的因素很多，主要是机器性能、网络、磁盘读写速度、I/O 操作等等有关。&lt;/p&gt;
&lt;p&gt;机器
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop优化" scheme="http://kingge.top/tags/hadoop%E4%BC%98%E5%8C%96/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(十二)-数据压缩</title>
    <link href="http://kingge.top/2018/03/20/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E5%8D%81%E4%BA%8C-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/"/>
    <id>http://kingge.top/2018/03/20/hadoop大数据-十二-数据压缩/</id>
    <published>2018-03-20T14:59:59.000Z</published>
    <updated>2019-08-01T13:41:44.559Z</updated>
    
    <content type="html"><![CDATA[<h2 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h2><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在Hadoop下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下，I/O操作和网络数据传输要花大量的时间。还有，Shuffle与Merge过程同样也面临着巨大的I/O压力。</p>
<p>​         鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。</p>
<p>​         如果磁盘I/O和网络带宽影响了MapReduce作业性能，在任意MapReduce阶段启用压缩都可以改善端到端处理时间并减少I/O和网络流量。</p>
<p>压缩<strong>Mapreduce的一种优化策略：通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。</strong></p>
<p>注意：压缩特性运用得当能提高性能，但运用不当也可能降低性能。</p>
<p>基本原则：</p>
<p>（1）运算密集型的job，少用压缩</p>
<p>（2）IO密集型的job，多用压缩</p>
<h2 id="4-2-MR支持的压缩编码"><a href="#4-2-MR支持的压缩编码" class="headerlink" title="4.2 MR支持的压缩编码"></a>4.2 MR支持的压缩编码</h2><table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody>
<tr>
<td>DEFAULT</td>
<td>是，直接使用</td>
<td>DEFAULT</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFAULT</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否，需要安装</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody>
</table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody>
<tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody>
</table>
<p>压缩性能的比较</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody>
<tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody>
</table>
<p><a href="http://google.github.io/snappy/" target="_blank" rel="external">http://google.github.io/snappy/</a></p>
<p>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p>
<h2 id="4-3-压缩方式选择"><a href="#4-3-压缩方式选择" class="headerlink" title="4.3 压缩方式选择"></a>4.3 压缩方式选择</h2><h3 id="4-3-1-Gzip压缩"><a href="#4-3-1-Gzip压缩" class="headerlink" title="4.3.1 Gzip压缩"></a>4.3.1 Gzip压缩</h3><p>优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。</p>
<p>缺点：不支持split。</p>
<p>应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。</p>
<h3 id="4-3-2-Bzip2压缩"><a href="#4-3-2-Bzip2压缩" class="headerlink" title="4.3.2 Bzip2压缩"></a>4.3.2 Bzip2压缩</h3><p>优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。</p>
<p>缺点：压缩/解压速度慢；不支持native。</p>
<p>应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。</p>
<h3 id="4-3-3-Lzo压缩"><a href="#4-3-3-Lzo压缩" class="headerlink" title="4.3.3 Lzo压缩"></a>4.3.3 Lzo压缩</h3><p>优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。</p>
<p>缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。</p>
<p>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。</p>
<h3 id="4-3-4-Snappy压缩"><a href="#4-3-4-Snappy压缩" class="headerlink" title="4.3.4 Snappy压缩"></a>4.3.4 Snappy压缩</h3><p>优点：高速压缩速度和合理的压缩率。</p>
<p>缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装； </p>
<p>应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。</p>
<h2 id="4-4-压缩位置选择"><a href="#4-4-压缩位置选择" class="headerlink" title="4.4 压缩位置选择"></a>4.4 压缩位置选择</h2><p>​         压缩可以在MapReduce作用的任意阶段启用。</p>
<p><img src="/2018/03/20/hadoop大数据-十二-数据压缩/64664559133.png" alt="1564664559133"></p>
<h2 id="4-5-压缩配置参数"><a href="#4-5-压缩配置参数" class="headerlink" title="4.5 压缩配置参数"></a>4.5 压缩配置参数</h2><p>要在Hadoop中启用压缩，可以配置如下参数：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td>io.compression.codecs      （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec,   org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.   DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody>
</table>
<h2 id="4-6-压缩实战"><a href="#4-6-压缩实战" class="headerlink" title="4.6 压缩实战"></a>4.6 压缩实战</h2><h3 id="4-6-1-数据流的压缩和解压缩"><a href="#4-6-1-数据流的压缩和解压缩" class="headerlink" title="4.6.1 数据流的压缩和解压缩"></a>4.6.1 数据流的压缩和解压缩</h3><p>​    CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据。要想对正在被写入一个输出流的数据进行压缩，<strong>我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream</strong>，将其以压缩格式写入底层的流。相反，要想对从输入流读取而来的数据进行解压缩，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据。</p>
<p>测试一下如下压缩方式：</p>
<table>
<thead>
<tr>
<th>DEFLATE</th>
<th>org.apache.hadoop.io.compress.DefaultCodec</th>
</tr>
</thead>
<tbody>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="code"><pre><div class="line"><span class="keyword">package</span> com.kingge.mapreduce.compress;</div><div class="line"><span class="keyword">import</span> java.io.File;</div><div class="line"><span class="keyword">import</span> java.io.FileInputStream;</div><div class="line"><span class="keyword">import</span> java.io.FileNotFoundException;</div><div class="line"><span class="keyword">import</span> java.io.FileOutputStream;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodecFactory;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionInputStream;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionOutputStream;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.ReflectionUtils;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestCompress</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		compress(<span class="string">"e:/hello.txt"</span>,<span class="string">"org.apache.hadoop.io.compress.BZip2Codec"</span>);</div><div class="line"><span class="comment">//		decompress("e:/hello.txt.bz2");</span></div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="comment">// 压缩</span></div><div class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String filename, String method)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		</div><div class="line">		<span class="comment">// 1 获取输入流</span></div><div class="line">		FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filename));</div><div class="line">		</div><div class="line">		Class codecClass = Class.forName(method);</div><div class="line">		</div><div class="line">		CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, <span class="keyword">new</span> Configuration());</div><div class="line">		</div><div class="line">		<span class="comment">// 2 获取输出流</span></div><div class="line">		FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filename +codec.getDefaultExtension()));</div><div class="line">		CompressionOutputStream cos = codec.createOutputStream(fos);</div><div class="line">		</div><div class="line">		<span class="comment">// 3 流的对拷</span></div><div class="line">		IOUtils.copyBytes(fis, cos, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>, <span class="keyword">false</span>);</div><div class="line">		</div><div class="line">		<span class="comment">// 4 关闭资源</span></div><div class="line">		fis.close();</div><div class="line">		cos.close();</div><div class="line">		fos.close();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="comment">// 解压缩</span></div><div class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String filename)</span> <span class="keyword">throws</span> FileNotFoundException, IOException </span>&#123;</div><div class="line">		</div><div class="line">		<span class="comment">// 0 校验是否能解压缩</span></div><div class="line">		CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</div><div class="line">		CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(filename));</div><div class="line">		</div><div class="line">		<span class="keyword">if</span> (codec == <span class="keyword">null</span>) &#123;</div><div class="line">			System.out.println(<span class="string">"cannot find codec for file "</span> + filename);</div><div class="line">			<span class="keyword">return</span>;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		<span class="comment">// 1 获取输入流</span></div><div class="line">		CompressionInputStream cis = codec.createInputStream(<span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filename)));</div><div class="line">		</div><div class="line">		<span class="comment">// 2 获取输出流</span></div><div class="line">		FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filename + <span class="string">".decoded"</span>));</div><div class="line">		</div><div class="line">		<span class="comment">// 3 流的对拷</span></div><div class="line">		IOUtils.copyBytes(cis, fos, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>, <span class="keyword">false</span>);</div><div class="line">		</div><div class="line">		<span class="comment">// 4 关闭资源</span></div><div class="line">		cis.close();</div><div class="line">		fos.close();</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="4-6-2-Map输出端采用压缩"><a href="#4-6-2-Map输出端采用压缩" class="headerlink" title="4.6.2 Map输出端采用压缩"></a>4.6.2 Map输出端采用压缩</h3><p>​    即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置：</p>
<p>1）给大家提供的hadoop源码支持的压缩格式有：<strong>BZip2Codec 、DefaultCodec</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.compress;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.io.compress.BZip2Codec;	</div><div class="line">import org.apache.hadoop.io.compress.CompressionCodec;</div><div class="line">import org.apache.hadoop.io.compress.GzipCodec;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class WordCountDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;</div><div class="line"></div><div class="line">		Configuration configuration = new Configuration();</div><div class="line"></div><div class="line">		// 开启map端输出压缩</div><div class="line">		configuration.setBoolean(&quot;mapreduce.map.output.compress&quot;, true);</div><div class="line">		// 设置map端输出压缩方式</div><div class="line">		configuration.setClass(&quot;mapreduce.map.output.compress.codec&quot;, BZip2Codec.class, CompressionCodec.class);</div><div class="line"></div><div class="line">		Job job = Job.getInstance(configuration);</div><div class="line"></div><div class="line">		job.setJarByClass(WordCountDriver.class);</div><div class="line"></div><div class="line">		job.setMapperClass(WordCountMapper.class);</div><div class="line">		job.setReducerClass(WordCountReducer.class);</div><div class="line"></div><div class="line">		job.setMapOutputKeyClass(Text.class);</div><div class="line">		job.setMapOutputValueClass(IntWritable.class);</div><div class="line"></div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(IntWritable.class);</div><div class="line"></div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line"></div><div class="line">		System.exit(result ? 1 : 0);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>2）Mapper保持不变</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.compress;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		// 1 获取一行</div><div class="line">		String line = value.toString();</div><div class="line">		// 2 切割</div><div class="line">		String[] words = line.split(&quot; &quot;);</div><div class="line">		// 3 循环写出</div><div class="line">		for(String word:words)&#123;</div><div class="line">			context.write(new Text(word), new IntWritable(1));</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>3）Reducer保持不变</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.compress;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,</div><div class="line">			Context context) throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		int count = 0;</div><div class="line">		// 1 汇总</div><div class="line">		for(IntWritable value:values)&#123;</div><div class="line">			count += value.get();</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">        // 2 输出</div><div class="line">		context.write(key, new IntWritable(count));</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="7-10-3-Reduce输出端采用压缩"><a href="#7-10-3-Reduce输出端采用压缩" class="headerlink" title="7.10.3 Reduce输出端采用压缩"></a>7.10.3 Reduce输出端采用压缩</h3><p>基于wordcount案例处理</p>
<p>1）修改驱动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.compress;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.io.compress.BZip2Codec;</div><div class="line">import org.apache.hadoop.io.compress.DefaultCodec;</div><div class="line">import org.apache.hadoop.io.compress.GzipCodec;</div><div class="line">import org.apache.hadoop.io.compress.Lz4Codec;</div><div class="line">import org.apache.hadoop.io.compress.SnappyCodec;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class WordCountDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		</div><div class="line">		Job job = Job.getInstance(configuration);</div><div class="line">		</div><div class="line">		job.setJarByClass(WordCountDriver.class);</div><div class="line">		</div><div class="line">		job.setMapperClass(WordCountMapper.class);</div><div class="line">		job.setReducerClass(WordCountReducer.class);</div><div class="line">		</div><div class="line">		job.setMapOutputKeyClass(Text.class);</div><div class="line">		job.setMapOutputValueClass(IntWritable.class);</div><div class="line">		</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(IntWritable.class);</div><div class="line">		</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line">		</div><div class="line">		// 设置reduce端输出压缩开启</div><div class="line">		FileOutputFormat.setCompressOutput(job, true);</div><div class="line">		</div><div class="line">		// 设置压缩的方式</div><div class="line">	    FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); </div><div class="line">//	    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); </div><div class="line">//	    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); </div><div class="line">	    </div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line">		</div><div class="line">		System.exit(result?1:0);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>2）Mapper和Reducer保持不变（详见4.6.2）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;4-1-概述&quot;&gt;&lt;a href=&quot;#4-1-概述&quot; class=&quot;headerlink&quot; title=&quot;4.1 概述&quot;&gt;&lt;/a&gt;4.1 概述&lt;/h2&gt;&lt;p&gt;压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在Hadoop下
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="MapReduce" scheme="http://kingge.top/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(十一)-Mapreduce框架原理</title>
    <link href="http://kingge.top/2018/03/18/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E5%8D%81%E4%B8%80-Mapreduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/"/>
    <id>http://kingge.top/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/</id>
    <published>2018-03-18T10:59:59.000Z</published>
    <updated>2019-06-17T13:45:43.508Z</updated>
    
    <content type="html"><![CDATA[<h1 id="三-MapReduce框架原理"><a href="#三-MapReduce框架原理" class="headerlink" title="三 MapReduce框架原理"></a>三 MapReduce框架原理</h1><h2 id="3-1-MapReduce工作流程"><a href="#3-1-MapReduce工作流程" class="headerlink" title="3.1 MapReduce工作流程"></a>3.1 MapReduce工作流程</h2><p>1）流程示意图</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/C1560701152675.png" alt="1560701152675"></p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/01246317.png" alt="1560701246317"></p>
<p>2.Submit()方法包含在这里面–<img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/clip_image005.png" alt="img"></p>
<p>然后接着是切片处理数据（128M为一片）。很明显图例200M的文件需要切成两片处理。分配两个map进行计算操作</p>
<p>3.正式提交任务到yarn上，包含一些job的相关信息。</p>
<p>4．MrAppMaster进行资源调度。根据片块数分配相应数量的MapTask（这里分配两个MapTask）</p>
<p>5.然后MapTask根据InputFormat去读取文本数据。一行一行的经过Mapper程序的map()方法进行计算操作，最后输出到分区中，并有序的存储。</p>
<p>6.等到所有MapTask计算完毕后。启动MrAppMaster启动相对应分区数量的reduce数量进行统计操作。最后生成多个分区对应的统计文件。输出。</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/560701367268.png" alt="1560701367268"></p>
<p>2）流程详解</p>
<p>上面的流程是整个mapreduce最全工作流程，但是shuffle过程只是从第7步开始到第16步结束，具体shuffle过程详解，如下：</p>
<p>1）maptask收集我们的map()方法输出的kv对，放到内存缓冲区中</p>
<p>2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p>
<p>3）多个溢出文件会被合并成大的溢出文件</p>
<p>4）在溢出过程中，及合并的过程中，都要调用partitioner进行分区和针对key进行排序</p>
<p>5）reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据</p>
<p>6）reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序）</p>
<p>7）合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法）</p>
<p>3）注意</p>
<p>Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</p>
<p>缓冲区的大小可以通过参数调整，参数：io.sort.mb  默认100M。</p>
<h2 id="3-2-InputFormat数据输入"><a href="#3-2-InputFormat数据输入" class="headerlink" title="3.2 InputFormat数据输入"></a>3.2 InputFormat数据输入</h2><h3 id="3-2-1-Job提交流程和切片源码详解"><a href="#3-2-1-Job提交流程和切片源码详解" class="headerlink" title="3.2.1 Job提交流程和切片源码详解"></a>3.2.1 Job提交流程和切片源码详解</h3><p>1）job提交流程源码详解</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">waitForCompletion()</div><div class="line">submit();</div><div class="line">// 1建立连接-主要的工作是建立集群环境，以便运行Job任务。同时会根据Configuration配置信息来辨别当前job是需要在本地LocalRunner上运行还是在真实的yarn上运行。</div><div class="line">	connect();	</div><div class="line">		// 1）创建提交job的代理</div><div class="line">		new Cluster(getConfiguration());</div><div class="line">			// （1）判断是本地yarn还是远程</div><div class="line">			initialize(jobTrackAddr, conf); </div><div class="line">	// 2 提交job</div><div class="line">submitter.submitJobInternal(Job.this, cluster)</div><div class="line">	// 1）创建给集群提交数据的Stag路径</div><div class="line">	Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</div><div class="line">	// 2）获取jobid ，并创建job路径</div><div class="line">	JobID jobId = submitClient.getNewJobID();</div><div class="line">	// 3）拷贝jar包到集群 – 如果是在本地运行那么就不需要提交jar包，但是如果是在远程服务器上运行，那么就需要提交jar包，防止找不到</div><div class="line">copyAndConfigureFiles(job, submitJobDir);	</div><div class="line">	rUploader.uploadFiles(job, jobSubmitDir);</div><div class="line">// 4）计算切片，生成切片规划文件-默认是切一片，会去读取配置文件，获取自定义的最小切片数。切片数最大值也是有一个默认值，最大值是Long.MAX_VALUE</div><div class="line">writeSplits(job, submitJobDir);</div><div class="line">	maps = writeNewSplits(job, jobSubmitDir);</div><div class="line">		input.getSplits(job);</div><div class="line">// 5）向Stag路径写xml配置文件</div><div class="line">writeConf(conf, submitJobFile);</div><div class="line">	conf.writeXml(out);</div><div class="line">// 6）提交job,返回提交状态</div><div class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</div></pre></td></tr></table></figure>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/1.png" alt="1560701501633"></p>
<p>2）FileInputFormat源码解析(input.getSplits(job))</p>
<p>（1）找到你数据存储的目录。</p>
<p>​         （2）开始遍历处理（规划切片）目录下的每一个文件</p>
<p>​         （3）遍历第一个文件ss.txt</p>
<p>​                 a）获取文件大小fs.sizeOf(ss.txt);</p>
<p>​                 b）计算切片大小computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M</p>
<p>​        c）<strong>默认情况下，切片大小=blocksize</strong></p>
<p>​                 d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）</p>
<p>​                 e）将切片信息写到一个切片规划文件中</p>
<p>​                 f）整个切片的核心过程在getSplit()方法中完成。</p>
<p>​        g）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。</p>
<p>​        h）注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分。</p>
<p>​         （4）<strong>提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。</strong></p>
<h3 id="23-2-2-FileInputFormat切片机制"><a href="#23-2-2-FileInputFormat切片机制" class="headerlink" title="23.2.2 FileInputFormat切片机制"></a>23.2.2 FileInputFormat切片机制</h3><p>1）FileInputFormat中默认的切片机制：</p>
<p>（1）简单地按照文件的内容长度进行切片</p>
<p>（2）切片大小，默认等于block大小</p>
<p>（3）<strong>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片(他会遍历输入目录里面的文件，一个一个处理，debug查看FileInputFormat的getSplits方法可知)</strong></p>
<p>比如待处理数据有两个文件：</p>
<p>   file1.txt    320M   file2.txt    10M   </p>
<p>经过FileInputFormat的切片机制运算后，形成的切片信息如下：  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">file1.txt.split1--  0~128</div><div class="line">file1.txt.split2--  128~256</div><div class="line">file1.txt.split3--  256~320</div><div class="line">file2.txt.split1--  0~10M</div></pre></td></tr></table></figure>
<p>2）FileInputFormat切片大小的参数配置</p>
<p>通过分析源码，在FileInputFormat中，计算切片大小的逻辑：<strong>Math.max(minSize, Math.min(maxSize, blockSize));</strong> </p>
<p>切片主要由这几个值来运算决定</p>
<p>mapreduce.input.fileinputformat.split.minsize=1 默认值为1</p>
<p>mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue</p>
<p>因此，<strong>默认情况下，切片大小=blocksize。</strong></p>
<p>maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。</p>
<p>minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。</p>
<p>3）获取切片信息API</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">// 根据文件类型获取切片信息</div><div class="line">FileSplit inputSplit = (FileSplit) context.getInputSplit();</div><div class="line">// 获取切片的文件名称</div><div class="line">String name = inputSplit.getPath().getName();</div></pre></td></tr></table></figure>
<h3 id="3-2-3-CombineTextInputFormat切片机制"><a href="#3-2-3-CombineTextInputFormat切片机制" class="headerlink" title="3.2.3 CombineTextInputFormat切片机制"></a>3.2.3 CombineTextInputFormat切片机制</h3><h4 id="1）关于大量小文件的优化策略"><a href="#1）关于大量小文件的优化策略" class="headerlink" title="1）关于大量小文件的优化策略"></a><strong>1）关于大量小文件的优化策略</strong></h4><p>1）默认情况下<strong>TextInputformat对任务的切片机制是按文件规划切片，不管文件多小</strong>，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。</p>
<h4 id="2）优化策略"><a href="#2）优化策略" class="headerlink" title="2）优化策略"></a>2）优化策略</h4><p>​         （1）最好的办法，在数据处理系统的最前端（<strong>预处理/采集</strong>），将小文件先合并成大文件，<strong>再上传到HDFS</strong>做后续分析。</p>
<p>​         （2）补救措施：如果已经是大量小文件在HDFS中了，可以使用另一种InputFormat来做切片（CombineTextInputFormat），它的切片逻辑跟TextFileInputFormat不同：它可以将多个小文件<strong>从逻辑上规划到一个切片</strong>中，这样，多个小文件就可以交给一个maptask。</p>
<p>​         （3）优先满足最小切片大小，不超过最大切片大小</p>
<p>​                 CombineTextInputFormat.<em>setMaxInputSplitSize</em>(job, 4194304);// 4m</p>
<p>​                 CombineTextInputFormat.<em>setMinInputSplitSize</em>(job, 2097152);// 2m</p>
<p>​         举例：0.5m+1m+0.3m+5m=2m + 4.8m=2m + 4m + 0.8m</p>
<p>​        <strong>0.5+1+0.3 = 1.8<em>没有满足最小切片大小，所以向5借0.2M,最后合并成2+4.8</em>，但是4.8大于最大切片数，所以拆成4+0.8</strong> <strong>，所以这个四个小文件最后合并成三个文件</strong></p>
<h4 id="3）具体实现步骤"><a href="#3）具体实现步骤" class="headerlink" title="3）具体实现步骤"></a>3）具体实现步骤</h4><p>注意CombineTextInputFormat的jar包是：<img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/60701587833.png" alt="1560701587833"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//  如果不设置InputFormat,它默认用的是TextInputFormat.class</div><div class="line">job.setInputFormatClass(CombineTextInputFormat.class)</div><div class="line">CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m</div><div class="line">CombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m</div></pre></td></tr></table></figure>
<h4 id="4）案例"><a href="#4）案例" class="headerlink" title="4）案例"></a>4）案例</h4><p>​         大量小文件的切片优化（CombineTextInputFormat）。</p>
<h5 id="4-1-数据准备"><a href="#4-1-数据准备" class="headerlink" title="4.1 数据准备"></a>4.1 数据准备</h5><p>准备5个小文件（这里准备五个txt文本）</p>
<h5 id="4-2-我们依旧使用我们上一个章节使用的统计文本中单词出现个数的代码"><a href="#4-2-我们依旧使用我们上一个章节使用的统计文本中单词出现个数的代码" class="headerlink" title="4.2 我们依旧使用我们上一个章节使用的统计文本中单词出现个数的代码"></a>4.2 我们依旧使用我们上一个章节使用的统计文本中单词出现个数的代码</h5><p><strong>代码详见 《hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节案例》</strong></p>
<p> 先不进行任何的改造操作，直接用着五个小文件当做输入，运行后查看日志。</p>
<p>（1）不做任何处理，运行需求1中的wordcount程序，观察切片个数为5</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/774204813.png" alt="1560774204813"></p>
<p>（2）在WordcountDriver中增加如下代码，运行程序，并观察运行的切片个数为1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</div><div class="line">job.setInputFormatClass(CombineTextInputFormat.class);</div><div class="line">CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m</div><div class="line">CombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m</div></pre></td></tr></table></figure>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/0774257244.png" alt="1560774257244"></p>
<h3 id="3-2-4-InputFormat接口实现类"><a href="#3-2-4-InputFormat接口实现类" class="headerlink" title="3.2.4 InputFormat接口实现类"></a>3.2.4 InputFormat接口实现类</h3><p>MapReduce任务的输入文件一般是存储在HDFS里面。输入的文件格式包括：基于行的日志文件、二进制格式文件等。这些文件一般会很大，达到数十GB，甚至更大。那么MapReduce是如何读取这些数据的呢？下面我们首先学习InputFormat接口。</p>
<p>InputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。</p>
<p>1）TextInputFormat</p>
<p>TextInputFormat是默认的InputFormat。每条记录是一行输入。<strong>键是LongWritable类型，存储该行在整个文件中的字节偏移量。值是这行的内容，不包括任何行终止符（换行符和回车符）</strong>。</p>
<p>以下是一个示例，比如，一个分片包含了如下4条文本记录。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Rich learning form</div><div class="line">Intelligent learning engine</div><div class="line">Learning more convenient</div><div class="line">From the real demand for more close to the enterprise</div></pre></td></tr></table></figure>
<p>每条记录表示为以下键/值对：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">(0,Rich learning form)</div><div class="line">(19,Intelligent learning engine)</div><div class="line">(47,Learning more convenient)</div><div class="line">(72,From the real demand for more close to the enterprise)</div></pre></td></tr></table></figure>
<p>很明显，键并不是行号。一般情况下，很难取得行号，因为文件按字节而不是按行切分为分片。</p>
<p>2）KeyValueTextInputFormat</p>
<p>每一行均为一条记录，被分隔符分割为key，value。<strong>可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “ “);来设定分隔符。默认分隔符是tab（\t）</strong>。</p>
<p>以下是一个示例，输入是一个包含4条记录的分片。其中——&gt;表示一个（水平方向的）制表符。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">line1 ——&gt;Rich learning form</div><div class="line">line2 ——&gt;Intelligent learning engine</div><div class="line">line3 ——&gt;Learning more convenient</div><div class="line">line4 ——&gt;From the real demand for more close to the enterprise</div></pre></td></tr></table></figure>
<p>每条记录表示为以下键/值对：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">(line1,Rich learning form)</div><div class="line">(line2,Intelligent learning engine)</div><div class="line">(line3,Learning more convenient)</div><div class="line">(line4,From the real demand for more close to the enterprise)</div></pre></td></tr></table></figure>
<p> 此时的键是每行排在制表符之前的Text序列。</p>
<p> 3）NLineInputFormat</p>
<p>如果使用NlineInputFormat，<strong>代表每个map进程处理的InputSplit不再按block块去划分，而是按NlineInputFormat指定的行数N来划分</strong>。即输入文件的总行数/N=切片数，如果不整除，切片数=商+1。</p>
<p>以下是一个示例，仍然以上面的4行输入为例。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Rich learning form</div><div class="line">Intelligent learning engine</div><div class="line">Learning more convenient</div><div class="line">From the real demand for more close to the enterprise</div></pre></td></tr></table></figure>
<p> 例如，如果N是2，则每个输入分片包含两行。开启2个maptask。</p>
<p>   (0,Rich learning form)  </p>
<p>   (19,Intelligent learning   engine)   </p>
<p>另一个 mapper 则收到后两行：</p>
<p>   (47,Learning more   convenient)  </p>
<p>   (72,From the real demand   for more close to the enterprise)   </p>
<p>​        这里的键和值与TextInputFormat生成的一样。</p>
<h3 id="3-2-5-自定义InputFormat"><a href="#3-2-5-自定义InputFormat" class="headerlink" title="3.2.5 自定义InputFormat"></a>3.2.5 自定义InputFormat</h3><h4 id="1）概述"><a href="#1）概述" class="headerlink" title="1）概述"></a>1）概述</h4><p>（1）自定义一个类继承FileInputFormat。</p>
<p>（2）改写RecordReader，实现一次读取一个完整文件封装为KV。</p>
<p>（3）在输出时使用SequenceFileOutPutFormat输出合并文件。</p>
<h4 id="2）案例"><a href="#2）案例" class="headerlink" title="2）案例"></a>2）案例</h4><p>​      无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。将多个小文件合并成一个文件SequenceFile，SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。</p>
<p>   <strong>小文件的优化无非以下几种方式：</strong></p>
<blockquote>
<p>（1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS</p>
<p>（2）在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并</p>
<p>（3）在mapreduce处理时，可采用CombineTextInputFormat提高效率</p>
</blockquote>
<h5 id="2-1-数据准备"><a href="#2-1-数据准备" class="headerlink" title="2.1 数据准备"></a>2.1 数据准备</h5><p>准备三个文本文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">aa.txt:包含以下内容</div><div class="line">yongpeng weidong weinan</div><div class="line">sanfeng luozong xiaoming</div><div class="line"></div><div class="line">bb.txt:包含以下内容</div><div class="line">longlong fanfan</div><div class="line">mazong kailun yuhang yixin</div><div class="line">longlong fanfan</div><div class="line">mazong kailun yuhang yixin</div><div class="line"></div><div class="line">cc.txt:包含以下内容</div><div class="line">shuaige changmo zhenqiang </div><div class="line">dongli lingu xuanxuan</div></pre></td></tr></table></figure>
<p>最终预期文件格式：</p>
<p>part-r-00000</p>
<h5 id="2-2-代码实现"><a href="#2-2-代码实现" class="headerlink" title="2.2 代码实现"></a>2.2 代码实现</h5><p>使用自定义InputFormat的方式，处理输入小文件的问题。</p>
<p>（1）自定义一个类继承FileInputFormat</p>
<p>（2）改写RecordReader，实现一次读取一个完整文件封装为KV</p>
<p>（3）在输出时使用SequenceFileOutPutFormat输出合并文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">（1）自定义InputFromat</div><div class="line">package com.kingge.mapreduce.inputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.BytesWritable;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.mapreduce.InputSplit;</div><div class="line">import org.apache.hadoop.mapreduce.JobContext;</div><div class="line">import org.apache.hadoop.mapreduce.RecordReader;</div><div class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"></div><div class="line">// 定义类继承FileInputFormat</div><div class="line">public class WholeFileInputformat extends FileInputFormat&lt;NullWritable, BytesWritable&gt;&#123;</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected boolean isSplitable(JobContext context, Path filename) &#123;</div><div class="line">		return false;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		WholeRecordReader recordReader = new WholeRecordReader();</div><div class="line">		recordReader.initialize(split, context);</div><div class="line">		</div><div class="line">		return recordReader;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）自定义RecordReader</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.inputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.FSDataInputStream;</div><div class="line">import org.apache.hadoop.fs.FileSystem;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.BytesWritable;</div><div class="line">import org.apache.hadoop.io.IOUtils;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.mapreduce.InputSplit;</div><div class="line">import org.apache.hadoop.mapreduce.RecordReader;</div><div class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileSplit;</div><div class="line"></div><div class="line">public class WholeRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt;&#123;</div><div class="line"></div><div class="line">	private Configuration configuration;</div><div class="line">	private FileSplit split;</div><div class="line">	</div><div class="line">	private boolean processed = false;</div><div class="line">	private BytesWritable value = new BytesWritable();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		this.split = (FileSplit)split;</div><div class="line">		configuration = context.getConfiguration();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public boolean nextKeyValue() throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		if (!processed) &#123;</div><div class="line">			// 1 定义缓存区</div><div class="line">			byte[] contents = new byte[(int)split.getLength()];</div><div class="line">			</div><div class="line">			FileSystem fs = null;</div><div class="line">			FSDataInputStream fis = null;</div><div class="line">			</div><div class="line">			try &#123;</div><div class="line">				// 2 获取文件系统</div><div class="line">				Path path = split.getPath();</div><div class="line">				fs = path.getFileSystem(configuration);</div><div class="line">				</div><div class="line">				// 3 读取数据</div><div class="line">				fis = fs.open(path);</div><div class="line">				</div><div class="line">				// 4 读取文件内容</div><div class="line">				IOUtils.readFully(fis, contents, 0, contents.length);</div><div class="line">				</div><div class="line">				// 5 输出文件内容</div><div class="line">				value.set(contents, 0, contents.length);</div><div class="line">			&#125; catch (Exception e) &#123;</div><div class="line">				</div><div class="line">			&#125;finally &#123;</div><div class="line">				IOUtils.closeStream(fis);</div><div class="line">			&#125;</div><div class="line">			</div><div class="line">			processed = true;</div><div class="line">			</div><div class="line">			return true;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		return false;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public NullWritable getCurrentKey() throws IOException, InterruptedException &#123;</div><div class="line">		return NullWritable.get();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123;</div><div class="line">		return value;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public float getProgress() throws IOException, InterruptedException &#123;</div><div class="line">		return processed? 1:0;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public void close() throws IOException &#123;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（3）SequenceFileMapper处理流程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.inputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.BytesWritable;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileSplit;</div><div class="line"></div><div class="line">public class SequenceFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;&#123;</div><div class="line">	</div><div class="line">	Text k = new Text();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void setup(Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;.Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		// 1 获取文件切片信息</div><div class="line">		FileSplit inputSplit = (FileSplit) context.getInputSplit();</div><div class="line">		// 2 获取切片名称</div><div class="line">		String name = inputSplit.getPath().toString();</div><div class="line">		// 3 设置key的输出</div><div class="line">		k.set(name);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(NullWritable key, BytesWritable value,</div><div class="line">			Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		context.write(k, value);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（4）SequenceFileReducer处理流程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.inputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.BytesWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		context.write(key, values.iterator().next());</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（5）SequenceFileDriver处理流程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.inputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.BytesWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</div><div class="line"></div><div class="line">public class SequenceFileDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		args = new String[] &#123; &quot;e:/input/inputinputformat&quot;, &quot;e:/output1&quot; &#125;;</div><div class="line">		Configuration conf = new Configuration();</div><div class="line"></div><div class="line">		Job job = Job.getInstance(conf);</div><div class="line">		job.setJarByClass(SequenceFileDriver.class);</div><div class="line">		job.setMapperClass(SequenceFileMapper.class);</div><div class="line">		job.setReducerClass(SequenceFileReducer.class);</div><div class="line"></div><div class="line">        // 设置输入的inputFormat</div><div class="line">		job.setInputFormatClass(WholeFileInputformat.class);</div><div class="line">        // 设置输出的outputFormat</div><div class="line">		job.setOutputFormatClass(SequenceFileOutputFormat.class);</div><div class="line"></div><div class="line">		job.setMapOutputKeyClass(Text.class);</div><div class="line">		job.setMapOutputValueClass(BytesWritable.class);</div><div class="line">		</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(BytesWritable.class);</div><div class="line"></div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line"></div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="3-3-MapTask工作机制"><a href="#3-3-MapTask工作机制" class="headerlink" title="3.3 MapTask工作机制"></a>3.3 MapTask工作机制</h2><h3 id="3-3-1-并行度决定机制"><a href="#3-3-1-并行度决定机制" class="headerlink" title="3.3.1 并行度决定机制"></a>3.3.1 并行度决定机制</h3><p>1）问题引出</p>
<p>maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度。那么，mapTask并行任务是否越多越好呢？</p>
<p>2）MapTask并行度决定机制</p>
<p>​         <strong>一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。</strong></p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/2.png" alt="1560701631551"></p>
<h3 id="3-3-2-MapTask工作机制"><a href="#3-3-2-MapTask工作机制" class="headerlink" title="3.3.2 MapTask工作机制"></a>3.3.2 MapTask工作机制</h3><p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/3.png" alt="1560701684388"></p>
<p>​         （1）Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p>
<p>​         （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p>
<p>​         （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner—调用用户自定义getPartition方法），并写入一个环形内存缓冲区中。</p>
<p>​         （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>​         溢写阶段详情：</p>
<p>​         步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p>
<p>​         步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>​         步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p>
<p>​         （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>​         当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。</p>
<p>​         在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>​         让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<p><a href="https://blog.csdn.net/qq_41455420/article/details/79288764" target="_blank" rel="external">https://blog.csdn.net/qq_41455420/article/details/79288764</a></p>
<p><strong>好的总结：</strong></p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/5.png" alt=""></p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/6.png" alt=""></p>
<h2 id="3-4-Shuffle机制"><a href="#3-4-Shuffle机制" class="headerlink" title="3.4 Shuffle机制"></a>3.4 Shuffle机制</h2><h3 id="3-4-1-Shuffle机制"><a href="#3-4-1-Shuffle机制" class="headerlink" title="3.4.1 Shuffle机制"></a>3.4.1 Shuffle机制</h3><p>Mapreduce确保每个reducer的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reducer）称为shuffle。</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/7.png" alt=""></p>
<p> <img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/8.png" alt="1560701949794"></p>
<p> <img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/9.png" alt=""></p>
<h3 id="3-4-2-Partition分区"><a href="#3-4-2-Partition分区" class="headerlink" title="3.4.2 Partition分区"></a>3.4.2 Partition分区</h3><p>  <strong>分区的行为在每一次的map操作都会调用一或者多次</strong></p>
<p>0）问题引出：要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<p><strong>默认只输出到一个分区，也就是结果输出到一个文件</strong></p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/11.png" alt=""></p>
<p>1）默认partition分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123;</div><div class="line">  public int getPartition(K key, V value, int numReduceTasks) &#123;</div><div class="line">    return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>​         默认分区是根据key的hashCode对reduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。（<strong>numReduceTasks默认是1，也就是说，默认返回0，也就是只创建一个分区，所以是part-r-00000</strong>）</p>
<p>2）自定义Partitioner步骤</p>
<p>​         （1）自定义类继承Partitioner，重写getPartition()方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">	public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public int getPartition(Text key, FlowBean value, int numPartitions) &#123;</div><div class="line"></div><div class="line">// 1 获取电话号码的前三位</div><div class="line">		String preNum = key.toString().substring(0, 3);</div><div class="line">		</div><div class="line">		int partition = 4;</div><div class="line">		</div><div class="line">		// 2 判断是哪个省</div><div class="line">		if (&quot;136&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 0;</div><div class="line">		&#125;else if (&quot;137&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 1;</div><div class="line">		&#125;else if (&quot;138&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 2;</div><div class="line">		&#125;else if (&quot;139&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 3;</div><div class="line">		&#125;</div><div class="line">		return partition;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>​         （2）在job驱动中，设置自定义partitioner： </p>
<p>​                    job.setPartitionerClass(CustomPartitioner.class);   </p>
<p>​         （3）自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce task</p>
<p>​                      job.setNumReduceTasks(5);   </p>
<p>3）注意：</p>
<p><strong>如果reduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；</strong></p>
<p><strong>如果1&lt;reduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；</strong></p>
<p><strong>如果reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000；</strong></p>
<p>​         例如：假设自定义分区数为5，则</p>
<p><strong>（1）job.setNumReduceTasks(1);会正常运行，只不过会产生一个输出文件</strong></p>
<p><strong>（2）job.setNumReduceTasks(2);会报错</strong></p>
<p><strong>（3）job.setNumReduceTasks(6);大于5，程序会正常运行，会产生空文件</strong></p>
<h4 id="4）案例-1"><a href="#4）案例-1" class="headerlink" title="4）案例"></a>4）案例</h4><h5 id="4-1-案例1"><a href="#4-1-案例1" class="headerlink" title="4.1 案例1"></a>4.1 案例1</h5><p>​        将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<p>1）数据准备</p>
<p>phone.txt</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1363157985066 	13726230503	00-FD-07-A4-72-B8:CMCC	120.196.100.82	i02.c.aliimg.com		24	27	2481	24681	200</div><div class="line">1363157995052 	13826544101	5C-0E-8B-C7-F1-E0:CMCC	120.197.40.4			4	0	264	0	200</div><div class="line">1363157991076 	13926435656	20-10-7A-28-CC-0A:CMCC	120.196.100.99			2	4	132	1512	200</div><div class="line">1363154400022 	13926251106	5C-0E-8B-8B-B1-50:CMCC	120.197.40.4			4	0	240	0	200</div><div class="line">1363157993044 	18211575961	94-71-AC-CD-E6-18:CMCC-EASY	120.196.100.99	iface.qiyi.com	视频网站	15	12	1527	2106	200</div><div class="line">1363157995074 	84138413	5C-0E-8B-8C-E8-20:7DaysInn	120.197.40.4	122.72.52.12		20	16	4116	1432	200</div><div class="line">1363157993055 	13560439658	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			18	15	1116	954	200</div><div class="line">1363157995033 	15920133257	5C-0E-8B-C7-BA-20:CMCC	120.197.40.4	sug.so.360.cn	信息安全	20	20	3156	2936	200</div><div class="line">1363157983019 	13719199419	68-A1-B7-03-07-B1:CMCC-EASY	120.196.100.82			4	0	240	0	200</div><div class="line">1363157984041 	13660577991	5C-0E-8B-92-5C-20:CMCC-EASY	120.197.40.4	s19.cnzz.com	站点统计	24	9	6960	690	200</div><div class="line">1363157973098 	15013685858	5C-0E-8B-C7-F7-90:CMCC	120.197.40.4	rank.ie.sogou.com	搜索引擎	28	27	3659	3538	200</div><div class="line">1363157986029 	15989002119	E8-99-C4-4E-93-E0:CMCC-EASY	120.196.100.99	www.umeng.com	站点统计	3	3	1938	180	200</div><div class="line">1363157992093 	13560439658	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			15	9	918	4938	200</div><div class="line">1363157986041 	13480253104	5C-0E-8B-C7-FC-80:CMCC-EASY	120.197.40.4			3	3	180	180	200</div><div class="line">1363157984040 	13602846565	5C-0E-8B-8B-B6-00:CMCC	120.197.40.4	2052.flash2-http.qq.com	综合门户	15	12	1938	2910	200</div><div class="line">1363157995093 	13922314466	00-FD-07-A2-EC-BA:CMCC	120.196.100.82	img.qfc.cn		12	12	3008	3720	200</div><div class="line">1363157982040 	13502468823	5C-0A-5B-6A-0B-D4:CMCC-EASY	120.196.100.99	y0.ifengimg.com	综合门户	57	102	7335	110349	200</div><div class="line">1363157986072 	18320173382	84-25-DB-4F-10-1A:CMCC-EASY	120.196.100.99	input.shouji.sogou.com	搜索引擎	21	18	9531	2412	200</div><div class="line">1363157990043 	13925057413	00-1F-64-E1-E6-9A:CMCC	120.196.100.55	t3.baidu.com	搜索引擎	69	63	11058	48243	200</div><div class="line">1363157988072 	13760778710	00-FD-07-A4-7B-08:CMCC	120.196.100.82			2	2	120	120	200</div><div class="line">1363157985066 	13726238888	00-FD-07-A4-72-B8:CMCC	120.196.100.82	i02.c.aliimg.com		24	27	2481	24681	200</div><div class="line">1363157993055 	13560436666	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			18	15	1116	954	200</div></pre></td></tr></table></figure>
<p>2）分析</p>
<p>（1）Mapreduce中会将map输出的kv对，按照相同key分组，然后分发给不同的reducetask。默认的分发规则为：根据key的hashcode%reducetask数来分发</p>
<p>（2）如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner</p>
<p>自定义一个CustomPartitioner继承抽象类：Partitioner</p>
<p>（3）在job驱动中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class)</p>
<p>3）在&lt;<strong>hadoop大数据(十)-Mapreduce基础 章节的2.6.2 案例&gt;</strong>的基础上，增加一个分区类</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.flowsum;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Partitioner;</div><div class="line">//他的key和value就是map输出的kv</div><div class="line">public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public int getPartition(Text key, FlowBean value, int numPartitions) &#123;</div><div class="line">		// 1 获取电话号码的前三位</div><div class="line">		String preNum = key.toString().substring(0, 3);</div><div class="line">		</div><div class="line">		int partition = 4;</div><div class="line">		</div><div class="line">		// 2 判断是哪个省</div><div class="line">		if (&quot;136&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 0;</div><div class="line">		&#125;else if (&quot;137&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 1;</div><div class="line">		&#125;else if (&quot;138&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 2;</div><div class="line">		&#125;else if (&quot;139&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 3;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		return partition;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p> 在驱动函数中增加自定义数据分区设置和reduce task设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.flowsum;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class FlowsumDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 获取配置信息，或者job对象实例</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		Job job = Job.getInstance(configuration);</div><div class="line"></div><div class="line">		// 6 指定本程序的jar包所在的本地路径</div><div class="line">		job.setJarByClass(FlowsumDriver.class);</div><div class="line"></div><div class="line">		// 2 指定本业务job要使用的mapper/Reducer业务类</div><div class="line">		job.setMapperClass(FlowCountMapper.class);</div><div class="line">		job.setReducerClass(FlowCountReducer.class);</div><div class="line"></div><div class="line">		// 3 指定mapper输出数据的kv类型</div><div class="line">		job.setMapOutputKeyClass(Text.class);</div><div class="line">		job.setMapOutputValueClass(FlowBean.class);</div><div class="line"></div><div class="line">		// 4 指定最终输出的数据的kv类型</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(FlowBean.class);</div><div class="line"></div><div class="line">		// 8 指定自定义数据分区</div><div class="line">		job.setPartitionerClass(ProvincePartitioner.class);</div><div class="line">		// 9 同时指定相应数量的reduce task</div><div class="line">		job.setNumReduceTasks(5);</div><div class="line">		</div><div class="line">		// 5 指定job的输入原始文件所在目录</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="4-2-案例2"><a href="#4-2-案例2" class="headerlink" title="4.2 案例2"></a>4.2 案例2</h5><p>​    把单词按照ASCII码奇偶分区（Partitioner），结合&lt;<strong>hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节–统计一堆文件中单词出现的个数</strong>&gt;</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/5967813.png" alt=""></p>
<p>只需要在此代码的基础上，添加自定义分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.wordcount;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Partitioner;</div><div class="line"></div><div class="line">public class WordCountPartitioner extends Partitioner&lt;Text, IntWritable&gt;&#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public int getPartition(Text key, IntWritable value, int numPartitions) &#123;</div><div class="line">		</div><div class="line">		// 1 获取单词key  </div><div class="line">		String firWord = key.toString().substring(0, 1);</div><div class="line">		char[] charArray = firWord.toCharArray();</div><div class="line">		int result = charArray[0];</div><div class="line">		// int result  = key.toString().charAt(0);</div><div class="line"></div><div class="line">		// 2 根据奇数偶数分区</div><div class="line">		if (result % 2 == 0) &#123;</div><div class="line">			return 0;</div><div class="line">		&#125;else &#123;</div><div class="line">			return 1;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在驱动类中配置加载分区，设置reducetask个数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">job.setPartitionerClass(WordCountPartitioner.class);</div><div class="line">job.setNumReduceTasks(2);//想分多少个区，这里必须开多少个reduce，否则默认只会生成一个分区，那么自定义分区失效</div></pre></td></tr></table></figure>
<h4 id="5）总结"><a href="#5）总结" class="headerlink" title="5）总结"></a>5）总结</h4><p>l  <strong>结果输出文件，跟分区数量和reduce数量有关系</strong></p>
<p>l  getPartition方法是在<strong>map</strong>调用之后才会进入<strong>，而且是每一次map可能会调用多次getPartition。</strong>为什么说是多次调用分区方法呢？我们知道每一次进入map方法都是一行数据（例如<strong> hello.txt的第一行hello kingge</strong>），那么经过分割后生成两个单词，调用两次**context.write（）所以为了确定这两个单词所属那个分区，那么就需要调用两次getPartition。也就说在这个例子中，一次map调用处理完后需要调用两次getPartition。（即：context.write（）内部会进行分区）</p>
<p>l  如果job.setNumReduceTasks(1)（也就是保持默认值），那么就是生成一个分区，不会进入自定义的分区方法。Redeucetask必须大于1，自定义分区方法才会生效。</p>
<h3 id="3-4-3-WritableComparable排序"><a href="#3-4-3-WritableComparable排序" class="headerlink" title="3.4.3 WritableComparable排序"></a>3.4.3 WritableComparable排序</h3><p>排序是MapReduce框架中最重要的操作之一。Map Task和Reduce Task均会对数据（按照key）进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。<strong>默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。</strong></p>
<p>​         对于Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行一次合并，以将这些文件合并成一个大的有序文件。</p>
<p>​         对于Reduce Task，它从每个Map Task上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上，否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task统一对内存和磁盘上的所有数据进行一次合并。</p>
<p><strong>每个阶段的默认排序</strong></p>
<h4 id="1）排序的分类："><a href="#1）排序的分类：" class="headerlink" title="1）排序的分类："></a>1）排序的分类：</h4><p>​         （1）部分排序：</p>
<p>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。<strong>例如输出文件到五个分区，那么部分排序能够保证各个五个分区的数据都是有序的。</strong></p>
<p>​         （2）全排序：</p>
<p>如何用Hadoop产生一个全局排序的文件？<strong>最简单的方法是使用一个分区，那么这个分区里面的数据全局都是排序的</strong>。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。</p>
<p>​         <strong>替代方案</strong>：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为上述文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。<strong>这种方式可以达到全排序的功能</strong></p>
<p>（3）辅助排序：（GroupingComparator分组）</p>
<p>​         Mapreduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。</p>
<p>​         （4）二次排序：</p>
<p>​         在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p>
<h4 id="2）自定义排序WritableComparable"><a href="#2）自定义排序WritableComparable" class="headerlink" title="2）自定义排序WritableComparable"></a>2）自定义排序WritableComparable</h4><p>（1）原理分析</p>
<p><strong>bean对象实现WritableComparable接口重写compareTo方法，就可以实现排序</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Override</div><div class="line">public int compareTo(FlowBean o) &#123;</div><div class="line">	// 倒序排列，从大到小</div><div class="line">	return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="3）案例"><a href="#3）案例" class="headerlink" title="3）案例"></a>3）案例</h4><h5 id="3-1-案例1"><a href="#3-1-案例1" class="headerlink" title="3.1 案例1"></a>3.1 案例1</h5><p>在&lt;<strong>hadoop大数据(十)-Mapreduce基础 章节的2.6.2 案例&gt;</strong>输出结果的基础上增加一个新的需求</p>
<p>根据2.6.2 案例输出的结果：再次对总流量进行排序</p>
<p>1）数据准备 phone.txt</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1363157985066 	13726230503	00-FD-07-A4-72-B8:CMCC	120.196.100.82	i02.c.aliimg.com		24	27	2481	24681	200</div><div class="line">1363157995052 	13826544101	5C-0E-8B-C7-F1-E0:CMCC	120.197.40.4			4	0	264	0	200</div><div class="line">1363157991076 	13926435656	20-10-7A-28-CC-0A:CMCC	120.196.100.99			2	4	132	1512	200</div><div class="line">1363154400022 	13926251106	5C-0E-8B-8B-B1-50:CMCC	120.197.40.4			4	0	240	0	200</div><div class="line">1363157993044 	18211575961	94-71-AC-CD-E6-18:CMCC-EASY	120.196.100.99	iface.qiyi.com	视频网站	15	12	1527	2106	200</div><div class="line">1363157995074 	84138413	5C-0E-8B-8C-E8-20:7DaysInn	120.197.40.4	122.72.52.12		20	16	4116	1432	200</div><div class="line">1363157993055 	13560439658	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			18	15	1116	954	200</div><div class="line">1363157995033 	15920133257	5C-0E-8B-C7-BA-20:CMCC	120.197.40.4	sug.so.360.cn	信息安全	20	20	3156	2936	200</div><div class="line">1363157983019 	13719199419	68-A1-B7-03-07-B1:CMCC-EASY	120.196.100.82			4	0	240	0	200</div><div class="line">1363157984041 	13660577991	5C-0E-8B-92-5C-20:CMCC-EASY	120.197.40.4	s19.cnzz.com	站点统计	24	9	6960	690	200</div><div class="line">1363157973098 	15013685858	5C-0E-8B-C7-F7-90:CMCC	120.197.40.4	rank.ie.sogou.com	搜索引擎	28	27	3659	3538	200</div><div class="line">1363157986029 	15989002119	E8-99-C4-4E-93-E0:CMCC-EASY	120.196.100.99	www.umeng.com	站点统计	3	3	1938	180	200</div><div class="line">1363157992093 	13560439658	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			15	9	918	4938	200</div><div class="line">1363157986041 	13480253104	5C-0E-8B-C7-FC-80:CMCC-EASY	120.197.40.4			3	3	180	180	200</div><div class="line">1363157984040 	13602846565	5C-0E-8B-8B-B6-00:CMCC	120.197.40.4	2052.flash2-http.qq.com	综合门户	15	12	1938	2910	200</div><div class="line">1363157995093 	13922314466	00-FD-07-A2-EC-BA:CMCC	120.196.100.82	img.qfc.cn		12	12	3008	3720	200</div><div class="line">1363157982040 	13502468823	5C-0A-5B-6A-0B-D4:CMCC-EASY	120.196.100.99	y0.ifengimg.com	综合门户	57	102	7335	110349	200</div><div class="line">1363157986072 	18320173382	84-25-DB-4F-10-1A:CMCC-EASY	120.196.100.99	input.shouji.sogou.com	搜索引擎	21	18	9531	2412	200</div><div class="line">1363157990043 	13925057413	00-1F-64-E1-E6-9A:CMCC	120.196.100.55	t3.baidu.com	搜索引擎	69	63	11058	48243	200</div><div class="line">1363157988072 	13760778710	00-FD-07-A4-7B-08:CMCC	120.196.100.82			2	2	120	120	200</div><div class="line">1363157985066 	13726238888	00-FD-07-A4-72-B8:CMCC	120.196.100.82	i02.c.aliimg.com		24	27	2481	24681	200</div><div class="line">1363157993055 	13560436666	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			18	15	1116	954	200</div></pre></td></tr></table></figure>
<p>2）分析</p>
<p>​         （1）把程序分两步走，第一步正常统计总流量，第二步再把结果进行排序</p>
<p>​         （2）context.write(总流量，手机号)</p>
<p>​         （3）FlowBean实现WritableComparable接口重写compareTo方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Override</div><div class="line">public int compareTo(FlowBean o) &#123;</div><div class="line">	// 倒序排列，从大到小</div><div class="line">	return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>3）代码实现</p>
<p>（1）FlowBean对象在在需求2.6.2基础上增加了比较功能（compareTo）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.sort;</div><div class="line">import java.io.DataInput;</div><div class="line">import java.io.DataOutput;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.WritableComparable;</div><div class="line"></div><div class="line">public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123;</div><div class="line"></div><div class="line">	private long upFlow;</div><div class="line">	private long downFlow;</div><div class="line">	private long sumFlow;</div><div class="line"></div><div class="line">	// 反序列化时，需要反射调用空参构造函数，所以必须有</div><div class="line">	public FlowBean() &#123;</div><div class="line">		super();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public FlowBean(long upFlow, long downFlow) &#123;</div><div class="line">		super();</div><div class="line">		this.upFlow = upFlow;</div><div class="line">		this.downFlow = downFlow;</div><div class="line">		this.sumFlow = upFlow + downFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void set(long upFlow, long downFlow) &#123;</div><div class="line">		this.upFlow = upFlow;</div><div class="line">		this.downFlow = downFlow;</div><div class="line">		this.sumFlow = upFlow + downFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public long getSumFlow() &#123;</div><div class="line">		return sumFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setSumFlow(long sumFlow) &#123;</div><div class="line">		this.sumFlow = sumFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public long getUpFlow() &#123;</div><div class="line">		return upFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setUpFlow(long upFlow) &#123;</div><div class="line">		this.upFlow = upFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public long getDownFlow() &#123;</div><div class="line">		return downFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setDownFlow(long downFlow) &#123;</div><div class="line">		this.downFlow = downFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	/**</div><div class="line">	 * 序列化方法</div><div class="line">	 * @param out</div><div class="line">	 * @throws IOException</div><div class="line">	 */</div><div class="line">	@Override</div><div class="line">	public void write(DataOutput out) throws IOException &#123;</div><div class="line">		out.writeLong(upFlow);</div><div class="line">		out.writeLong(downFlow);</div><div class="line">		out.writeLong(sumFlow);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	/**</div><div class="line">	 * 反序列化方法 注意反序列化的顺序和序列化的顺序完全一致</div><div class="line">	 * @param in</div><div class="line">	 * @throws IOException</div><div class="line">	 */</div><div class="line">	@Override</div><div class="line">	public void readFields(DataInput in) throws IOException &#123;</div><div class="line">		upFlow = in.readLong();</div><div class="line">		downFlow = in.readLong();</div><div class="line">		sumFlow = in.readLong();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public String toString() &#123;</div><div class="line">		return upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public int compareTo(FlowBean o) &#123;</div><div class="line">		// 倒序排列，从大到小</div><div class="line">		return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）编写mapper</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.sort;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;&#123;</div><div class="line">	FlowBean bean = new FlowBean();</div><div class="line">	Text v = new Text();</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		// 1 获取一行</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		// 2 截取</div><div class="line">		String[] fields = line.split(&quot;\t&quot;);</div><div class="line">		</div><div class="line">		// 3 封装对象</div><div class="line">		String phoneNbr = fields[0];</div><div class="line">		long upFlow = Long.parseLong(fields[1]);</div><div class="line">		long downFlow = Long.parseLong(fields[2]);</div><div class="line">		</div><div class="line">		bean.set(upFlow, downFlow);</div><div class="line">		v.set(phoneNbr);</div><div class="line">		</div><div class="line">		// 4 输出</div><div class="line">		context.write(bean, v);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（3）编写reducer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.sort;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt;&#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 循环输出，避免总流量相同情况</div><div class="line">		for (Text text : values) &#123;</div><div class="line">			context.write(text, key);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（4）编写driver</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.sort;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class FlowCountSortDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws ClassNotFoundException, IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 获取配置信息，或者job对象实例</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		Job job = Job.getInstance(configuration);</div><div class="line"></div><div class="line">		// 6 指定本程序的jar包所在的本地路径</div><div class="line">		job.setJarByClass(FlowCountSortDriver.class);</div><div class="line"></div><div class="line">		// 2 指定本业务job要使用的mapper/Reducer业务类</div><div class="line">		job.setMapperClass(FlowCountSortMapper.class);</div><div class="line">		job.setReducerClass(FlowCountSortReducer.class);</div><div class="line"></div><div class="line">		// 3 指定mapper输出数据的kv类型</div><div class="line">		job.setMapOutputKeyClass(FlowBean.class);</div><div class="line">		job.setMapOutputValueClass(Text.class);</div><div class="line"></div><div class="line">		// 4 指定最终输出的数据的kv类型</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(FlowBean.class);</div><div class="line"></div><div class="line">		// 5 指定job的输入原始文件所在目录</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line">		</div><div class="line">		// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="3-2-案例2"><a href="#3-2-案例2" class="headerlink" title="3.2 案例2"></a>3.2 案例2</h5><p>改造案例1的需求</p>
<p>​      <strong>要求每个省份手机号输出的文件中按照总流量内部排序。</strong>（部分排序）</p>
<p>2）做法</p>
<p>在案例1的基础上增加自定义分区类即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.sort;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Partitioner;</div><div class="line"></div><div class="line">public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public int getPartition(FlowBean key, Text value, int numPartitions) &#123;</div><div class="line">		</div><div class="line">		// 1 获取手机号码前三位</div><div class="line">		String preNum = value.toString().substring(0, 3);</div><div class="line">		</div><div class="line">		int partition = 4;</div><div class="line">		</div><div class="line">		// 2 根据手机号归属地设置分区</div><div class="line">		if (&quot;136&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 0;</div><div class="line">		&#125;else if (&quot;137&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 1;</div><div class="line">		&#125;else if (&quot;138&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 2;</div><div class="line">		&#125;else if (&quot;139&quot;.equals(preNum)) &#123;</div><div class="line">			partition = 3;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		return partition;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）在驱动类中添加分区类</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">	// 加载自定义分区类</div><div class="line">job.setPartitionerClass(FlowSortPartitioner.class);</div><div class="line">// 设置Reducetask个数</div><div class="line">	job.setNumReduceTasks(5);</div></pre></td></tr></table></figure>
<h3 id="3-4-4-GroupingComparator分组（辅助排序）"><a href="#3-4-4-GroupingComparator分组（辅助排序）" class="headerlink" title="3.4.4 GroupingComparator分组（辅助排序）"></a>3.4.4 GroupingComparator分组（辅助排序）</h3><p>1）对reduce阶段的数据根据某一个或几个字段进行分组。</p>
<p>2）案例</p>
<p>​    求出每一个订单中最贵的商品（GroupingComparator）</p>
<p>1）需求</p>
<p>有如下订单数据</p>
<table>
<thead>
<tr>
<th>订单id</th>
<th>商品id</th>
<th>成交金额</th>
</tr>
</thead>
<tbody>
<tr>
<td>0000001</td>
<td>Pdt_01</td>
<td>222.8</td>
</tr>
<tr>
<td>0000001</td>
<td>Pdt_06</td>
<td>25.8</td>
</tr>
<tr>
<td>0000002</td>
<td>Pdt_03</td>
<td>522.8</td>
</tr>
<tr>
<td>0000002</td>
<td>Pdt_04</td>
<td>122.4</td>
</tr>
<tr>
<td>0000002</td>
<td>Pdt_05</td>
<td>722.4</td>
</tr>
<tr>
<td>0000003</td>
<td>Pdt_01</td>
<td>222.8</td>
</tr>
<tr>
<td>0000003</td>
<td>Pdt_02</td>
<td>33.8</td>
</tr>
</tbody>
</table>
<p>现在需要求出每一个订单中最贵的商品。</p>
<p>2）输入数据</p>
<p>goods.txt</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">0000001	Pdt_01	222.8</div><div class="line">0000002	Pdt_06	722.4</div><div class="line">0000001	Pdt_05	25.8</div><div class="line">0000003	Pdt_01	222.8</div><div class="line">0000003	Pdt_01	33.8</div><div class="line">0000002	Pdt_03	522.8</div><div class="line">0000002	Pdt_04	122.4</div></pre></td></tr></table></figure>
<p>输出数据预期：</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/clip_image004.png" alt="img">  <img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/clip_image006.png" alt="img">  <img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/clip_image008.png" alt="img"></p>
<p>​      3        222.8                2    722.4                 1        222.8</p>
<p>3）分析</p>
<p>（1）利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce。</p>
<p>（2）在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值。</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/15.png" alt="1560776918628"></p>
<p>4）代码实现</p>
<p>（1）定义订单信息OrderBean</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.order;</div><div class="line">import java.io.DataInput;</div><div class="line">import java.io.DataOutput;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.WritableComparable;</div><div class="line"></div><div class="line">public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123;</div><div class="line"></div><div class="line">	private int order_id; // 订单id号</div><div class="line">	private double price; // 价格</div><div class="line"></div><div class="line">	public OrderBean() &#123;</div><div class="line">		super();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public OrderBean(int order_id, double price) &#123;</div><div class="line">		super();</div><div class="line">		this.order_id = order_id;</div><div class="line">		this.price = price;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public void write(DataOutput out) throws IOException &#123;</div><div class="line">		out.writeInt(order_id);</div><div class="line">		out.writeDouble(price);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public void readFields(DataInput in) throws IOException &#123;</div><div class="line">		order_id = in.readInt();</div><div class="line">		price = in.readDouble();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public String toString() &#123;</div><div class="line">		return order_id + &quot;\t&quot; + price;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public int getOrder_id() &#123;</div><div class="line">		return order_id;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setOrder_id(int order_id) &#123;</div><div class="line">		this.order_id = order_id;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public double getPrice() &#123;</div><div class="line">		return price;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setPrice(double price) &#123;</div><div class="line">		this.price = price;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// 二次排序</div><div class="line">	@Override</div><div class="line">	public int compareTo(OrderBean o) &#123;</div><div class="line"></div><div class="line">		int result;</div><div class="line"></div><div class="line">		if (order_id &gt; o.getOrder_id()) &#123;</div><div class="line">			result = 1;</div><div class="line">		&#125; else if (order_id &lt; o.getOrder_id()) &#123;</div><div class="line">			result = -1;</div><div class="line">		&#125; else &#123;</div><div class="line">			// 价格倒序排序</div><div class="line">			result = price &gt; o.getPrice() ? -1 : 1;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		return result;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）编写OrderSortMapper</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.order;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123;</div><div class="line">	OrderBean k = new OrderBean();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 获取一行</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		// 2 截取</div><div class="line">		String[] fields = line.split(&quot;\t&quot;);</div><div class="line">		</div><div class="line">		// 3 封装对象</div><div class="line">		k.setOrder_id(Integer.parseInt(fields[0]));</div><div class="line">		k.setPrice(Double.parseDouble(fields[2]));</div><div class="line">		</div><div class="line">		// 4 写出</div><div class="line">		context.write(k, NullWritable.get());</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（3）编写OrderSortPartitioner</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.order;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.mapreduce.Partitioner;</div><div class="line"></div><div class="line">public class OrderPartitioner extends Partitioner&lt;OrderBean, NullWritable&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public int getPartition(OrderBean key, NullWritable value, int numReduceTasks) &#123;</div><div class="line">		</div><div class="line">		return (key.getOrder_id() &amp; Integer.MAX_VALUE) % numReduceTasks;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（4）编写OrderSortGroupingComparator</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.order;</div><div class="line">import org.apache.hadoop.io.WritableComparable;</div><div class="line">import org.apache.hadoop.io.WritableComparator;</div><div class="line"></div><div class="line">public class OrderGroupingComparator extends WritableComparator &#123;</div><div class="line"></div><div class="line">	protected OrderGroupingComparator() &#123; //可以查看super的源代码，true是必须要传的，否则汇报空指针，因为我们在下面的compare方法中使用了强转的操作，那么如果不注明比较的bean的类型，那么就会有问题。</div><div class="line">		super(OrderBean.class, true);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@SuppressWarnings(&quot;rawtypes&quot;)</div><div class="line">	@Override</div><div class="line">	public int compare(WritableComparable a, WritableComparable b) &#123;</div><div class="line"></div><div class="line">		OrderBean aBean = (OrderBean) a;</div><div class="line">		OrderBean bBean = (OrderBean) b;</div><div class="line"></div><div class="line">		int result;</div><div class="line">		if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123;</div><div class="line">			result = 1;</div><div class="line">		&#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123;</div><div class="line">			result = -1;</div><div class="line">		&#125; else &#123;</div><div class="line">			result = 0;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		return result;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（5）编写OrderSortReducer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.order;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		context.write(key, NullWritable.get());</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（6）编写OrderSortDriver</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.order;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class OrderDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws Exception, IOException &#123;</div><div class="line"></div><div class="line">		// 1 获取配置信息</div><div class="line">		Configuration conf = new Configuration();</div><div class="line">		Job job = Job.getInstance(conf);</div><div class="line"></div><div class="line">		// 2 设置jar包加载路径</div><div class="line">		job.setJarByClass(OrderDriver.class);</div><div class="line"></div><div class="line">		// 3 加载map/reduce类</div><div class="line">		job.setMapperClass(OrderMapper.class);</div><div class="line">		job.setReducerClass(OrderReducer.class);</div><div class="line"></div><div class="line">		// 4 设置map输出数据key和value类型</div><div class="line">		job.setMapOutputKeyClass(OrderBean.class);</div><div class="line">		job.setMapOutputValueClass(NullWritable.class);</div><div class="line"></div><div class="line">		// 5 设置最终输出数据的key和value类型</div><div class="line">		job.setOutputKeyClass(OrderBean.class);</div><div class="line">		job.setOutputValueClass(NullWritable.class);</div><div class="line"></div><div class="line">		// 6 设置输入数据和输出数据路径</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		// 10 设置reduce端的分组</div><div class="line">		job.setGroupingComparatorClass(OrderGroupingComparator.class);</div><div class="line"></div><div class="line">		// 7 设置分区</div><div class="line">		job.setPartitionerClass(OrderPartitioner.class);</div><div class="line"></div><div class="line">		// 8 设置reduce个数</div><div class="line">		job.setNumReduceTasks(3);</div><div class="line"></div><div class="line">		// 9 提交</div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">//如果不使用GroupingComparator方法，那么就无法实现功能，因为我们知道进入reduce的数据，他们key一定是一样的。那么上面的OrderBean作为key很明显是不一样的，就算order_id相同，但是他们的price不相同。那么GroupingComparator就可以帮我们做到，假设某个值是相同的，那么他就认为整个key是相同的。那么OrderBean作为key就可以分组处理</div><div class="line"></div><div class="line">也就是说，我们通过在GroupingComparator方法中指明了，相同key的规则，那么就可以实现进入reduce的数据的分组情况</div><div class="line"></div><div class="line">尖叫提示：</div><div class="line">   Map阶段结束后，马上进入GroupingComparator方法，进行判断key的逻辑。每判断一次完后，就调用reduce一次。循环此操作直到数据统计结束。</div><div class="line">   在进入GroupingComparator之前，map阶段输出的数据，已经按照订单分区，分区内的价格也已经按照大到小排序。</div></pre></td></tr></table></figure>
<h3 id="3-4-5-Combiner合并"><a href="#3-4-5-Combiner合并" class="headerlink" title="3.4.5 Combiner合并"></a>3.4.5 Combiner合并</h3><p>1）combiner是MR程序中Mapper和Reducer之外的一种组件。</p>
<p>2）combiner组件的父类就是Reducer。</p>
<p>3）combiner和reducer的区别在于运行的位置：</p>
<p>Combiner是在每一个maptask所在的节点运行;</p>
<p>Reducer是接收全局所有Mapper的输出结果；</p>
<p>4）combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。</p>
<p>5）<strong>combiner能够应用的前提是不能影响最终的业务逻辑</strong>，而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Mapper</div><div class="line">3 5 7 -&gt;(3+5+7)/3=5 </div><div class="line">2 6 -&gt;(2+6)/2=4</div><div class="line">Reducer</div><div class="line">(3+5+7+2+6)/5=23/5    不等于    (5+4)/2=9/2</div></pre></td></tr></table></figure>
<p>很明显，combiner不适合做求平均值这样的操作。他适合做汇总这样的业务场景。</p>
<p>6）自定义Combiner实现步骤：</p>
<p>（1）自定义一个combiner继承Reducer，重写reduce方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</div><div class="line">	@Override</div><div class="line">	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,</div><div class="line">			Context context) throws IOException, InterruptedException &#123;</div><div class="line">        // 1 汇总操作</div><div class="line">		int count = 0;</div><div class="line">		for(IntWritable v :values)&#123;</div><div class="line">			count = v.get();</div><div class="line">		&#125;</div><div class="line">        // 2 写出</div><div class="line">		context.write(key, new IntWritable(count));</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）在job驱动类中设置：  </p>
<p>   job.setCombinerClass(WordcountCombiner.class);   </p>
<p>7）案例</p>
<p>​       前提：结合&lt;<strong>hadoop大数据(十)-Mapreduce基础 的 1.5 4） 章节–统计一堆文件中单词出现的个数</strong>&gt; 代码</p>
<p>数据输入也是同上</p>
<p>​       需求：统计过程中对每一个maptask的输出进行局部汇总，以减小网络传输量即采用Combiner功能。</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/560777243680.png" alt="1560777243680"></p>
<p><strong>方案一</strong></p>
<p>1）增加一个WordcountCombiner类继承Reducer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mr.combiner;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,</div><div class="line">			Context context) throws IOException, InterruptedException &#123;</div><div class="line">        // 1 汇总</div><div class="line">		int count = 0;</div><div class="line">		for(IntWritable v :values)&#123;</div><div class="line">			count += v.get();</div><div class="line">		&#125;</div><div class="line">		// 2 写出</div><div class="line">		context.write(key, new IntWritable(count));</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>   // 9 指定需要使用combiner，以及用哪个类作为combiner的逻辑   job.setCombinerClass(WordcountCombiner.class);   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">// 9 指定需要使用combiner，以及用哪个类作为combiner的逻辑</div><div class="line">job.setCombinerClass(WordcountCombiner.class);</div></pre></td></tr></table></figure>
<p><strong>方案二</strong></p>
<p>1）将WordcountReducer作为combiner在WordcountDriver驱动类中指定</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">// 指定需要使用combiner，以及用哪个类作为combiner的逻辑</div><div class="line">job.setCombinerClass(WordcountReducer.class);</div></pre></td></tr></table></figure>
<p>运行程序</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/8787878.png" alt="1560777579011"></p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/777616071.png" alt="1560777616071"></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>自定义Combiner的调用时机：是在MapTask阶段的split溢写阶段，需要写入到磁盘的之前进行。<strong>将有相同</strong> <strong>key</strong> <strong>的</strong> <strong>key/value</strong> <strong>对的</strong> <strong>value</strong> <strong>加起来，减少溢写到磁盘的数据量。调用完后进入**</strong>reduce<strong>**方法</strong></p>
<p>​     </p>
<h2 id="3-5-ReduceTask工作机制"><a href="#3-5-ReduceTask工作机制" class="headerlink" title="3.5 ReduceTask工作机制"></a>3.5 ReduceTask工作机制</h2><p>1）设置ReduceTask并行度（个数）</p>
<p>reducetask的并行度同样影响整个job的执行并发度和执行效率，<strong>但与maptask的并发数由切片数决定不同</strong>，Reducetask数量的决定是可以直接手动设置：</p>
<p>   //默认值是1，手动设置为4   job.setNumReduceTasks(4);   </p>
<p>2）注意</p>
<p>（1）reducetask=0 ，表示没有reduce阶段，输出文件个数和map个数一致。</p>
<p>​     例子7.1.1    job.setNumReduceTasks(0); 输出</p>
<p>  <img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/10.png" alt=""></p>
<p>​      生成一个分区，但是分区内的单词没有汇总</p>
<p>​         （2）reducetask默认值就是1，所以输出文件个数为一个。</p>
<p>（3）如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜（<strong>也就是说，相同key被partition分配到一个分区里,造成了’一个人累死,其他人闲死’的情况</strong>）</p>
<p>（4）reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask。</p>
<p>（5）具体多少个reducetask，需要根据集群性能而定。</p>
<p>（6）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在maptask的源码中，执行分区的前提是先判断reduceNum个数是否大于1。不大于1肯定不执行。</p>
<p>3）实验：测试reducetask多少合适。</p>
<p>（1）实验环境：1个master节点，16个slave节点：CPU:8GHZ，内存: 2G</p>
<p>（2）实验结论：</p>
<p>​                            表1 改变reduce task （数据量为1GB）</p>
<table>
<thead>
<tr>
<th>Map task =16</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Reduce task</td>
<td>1</td>
<td>5</td>
<td>10</td>
<td>15</td>
<td>16</td>
<td>20</td>
<td>25</td>
<td>30</td>
<td>45</td>
<td>60</td>
</tr>
<tr>
<td>总时间</td>
<td>892</td>
<td>146</td>
<td>110</td>
<td>92</td>
<td>88</td>
<td>100</td>
<td>128</td>
<td>101</td>
<td>145</td>
<td>104</td>
</tr>
</tbody>
</table>
<p>4）ReduceTask工作机制</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/12.png" alt="1560702180711"></p>
<p>​         （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>​         （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p>
<p>​         （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p>
<p>​         （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/13.png" alt=""></p>
<h2 id="3-6-OutputFormat数据输出"><a href="#3-6-OutputFormat数据输出" class="headerlink" title="3.6 OutputFormat数据输出"></a>3.6 OutputFormat数据输出</h2><h3 id="3-6-1-OutputFormat接口实现类"><a href="#3-6-1-OutputFormat接口实现类" class="headerlink" title="3.6.1 OutputFormat接口实现类"></a>3.6.1 OutputFormat接口实现类</h3><p> OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。</p>
<p>1）文本输出TextOutputFormat</p>
<p>​        默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串。</p>
<p>2）SequenceFileOutputFormat</p>
<p> SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p>
<p>3）自定义OutputFormat</p>
<p>​         根据用户需求，自定义实现输出。</p>
<h3 id="3-6-2-自定义OutputFormat"><a href="#3-6-2-自定义OutputFormat" class="headerlink" title="3.6.2 自定义OutputFormat"></a>3.6.2 自定义OutputFormat</h3><p>为了实现控制最终文件的输出路径，可以自定义OutputFormat。</p>
<p>要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现。</p>
<h4 id="1）自定义OutputFormat步骤"><a href="#1）自定义OutputFormat步骤" class="headerlink" title="1）自定义OutputFormat步骤"></a>1）自定义OutputFormat步骤</h4><p>（1）自定义一个类继承FileOutputFormat。</p>
<p>（2）改写recordwriter，具体改写输出数据的方法write()。</p>
<h4 id="2）案例-1"><a href="#2）案例-1" class="headerlink" title="2）案例"></a>2）案例</h4><p>​       修改日志内容及自定义日志输出路径（自定义OutputFormat）。</p>
<p>1）需求</p>
<p>​         过滤输入的log日志中是否包含kingge</p>
<p>​         （1）包含kingge的网站输出到e:/kingge.log</p>
<p>​         （2）不包含kingge的网站输出到e:/other.log</p>
<p>2）输入数据（pp.txt）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">http://www.baidu.com</div><div class="line">http://www.google.com</div><div class="line">http://cn.bing.com</div><div class="line">http://www.kingge.com</div><div class="line">http://www.sohu.com</div><div class="line">http://www.sina.com</div><div class="line">http://www.sin2a.com</div><div class="line">http://www.sin2desa.com</div><div class="line">http://www.sindsafa.com</div></pre></td></tr></table></figure>
<p>输出预期：</p>
<blockquote>
<p>kingge.log文件包含： <strong><a href="http://www.kingge.com" target="_blank" rel="external">http://www.kingge.com</a></strong> </p>
<p>other.log文件包含：</p>
<p><a href="http://cn.bing.com" target="_blank" rel="external">http://cn.bing.com</a><br><a href="http://www.baidu.com" target="_blank" rel="external">http://www.baidu.com</a><br><a href="http://www.google.com" target="_blank" rel="external">http://www.google.com</a><br><a href="http://www.sin2a.com" target="_blank" rel="external">http://www.sin2a.com</a><br><a href="http://www.sin2desa.com" target="_blank" rel="external">http://www.sin2desa.com</a><br><a href="http://www.sina.com" target="_blank" rel="external">http://www.sina.com</a><br><a href="http://www.sindsafa.com" target="_blank" rel="external">http://www.sindsafa.com</a><br><a href="http://www.sohu.com" target="_blank" rel="external">http://www.sohu.com</a></p>
</blockquote>
<p>3）代码实现：</p>
<p>（1）自定义一个outputformat</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.outputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.RecordWriter;</div><div class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		// 创建一个RecordWriter</div><div class="line">		return new FilterRecordWriter(job);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）具体的写数据RecordWriter</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.outputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.fs.FSDataOutputStream;</div><div class="line">import org.apache.hadoop.fs.FileSystem;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.RecordWriter;</div><div class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</div><div class="line"></div><div class="line">public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123;</div><div class="line">	FSDataOutputStream kinggeOut = null;</div><div class="line">	FSDataOutputStream otherOut = null;</div><div class="line"></div><div class="line">	public FilterRecordWriter(TaskAttemptContext job) &#123;</div><div class="line">		// 1 获取文件系统</div><div class="line">		FileSystem fs;</div><div class="line"></div><div class="line">		try &#123;</div><div class="line">			fs = FileSystem.get(job.getConfiguration());</div><div class="line"></div><div class="line">			// 2 创建输出文件路径</div><div class="line">			Path kinggePath = new Path(&quot;e:/kingge.log&quot;);</div><div class="line">			Path otherPath = new Path(&quot;e:/other.log&quot;);</div><div class="line"></div><div class="line">			// 3 创建输出流</div><div class="line">			kinggeOut = fs.create(kinggePath);</div><div class="line">			otherOut = fs.create(otherPath);</div><div class="line">		&#125; catch (IOException e) &#123;</div><div class="line">			e.printStackTrace();</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		// 判断是否包含“kingge”输出到不同文件</div><div class="line">		if (key.toString().contains(&quot;kingge&quot;)) &#123;</div><div class="line">			kinggeOut.write(key.toString().getBytes());</div><div class="line">		&#125; else &#123;</div><div class="line">			otherOut.write(key.toString().getBytes());</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123;</div><div class="line">		// 关闭资源</div><div class="line">		if (kinggeOut != null) &#123;</div><div class="line">			kinggeOut.close();</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		if (otherOut != null) &#123;</div><div class="line">			otherOut.close();</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（3）编写FilterMapper</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.outputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class FilterMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123;</div><div class="line">	</div><div class="line">	Text k = new Text();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		// 1 获取一行</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		k.set(line);</div><div class="line">		</div><div class="line">		// 3 写出</div><div class="line">		context.write(k, NullWritable.get());</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（4）编写FilterReducer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.outputformat;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class FilterReducer extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		String k = key.toString();</div><div class="line">		k = k + &quot;\r\n&quot;;</div><div class="line"></div><div class="line">		context.write(new Text(k), NullWritable.get());</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（5）编写FilterDriver</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.outputformat;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class FilterDriver &#123;</div><div class="line">	public static void main(String[] args) throws Exception &#123;</div><div class="line"></div><div class="line">args = new String[] &#123; &quot;e:/input/inputoutputformat&quot;, &quot;e:/output2&quot; &#125;;</div><div class="line"></div><div class="line">		Configuration conf = new Configuration();</div><div class="line"></div><div class="line">		Job job = Job.getInstance(conf);</div><div class="line"></div><div class="line">		job.setJarByClass(FilterDriver.class);</div><div class="line">		job.setMapperClass(FilterMapper.class);</div><div class="line">		job.setReducerClass(FilterReducer.class);</div><div class="line"></div><div class="line">		job.setMapOutputKeyClass(Text.class);</div><div class="line">		job.setMapOutputValueClass(NullWritable.class);</div><div class="line">		</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(NullWritable.class);</div><div class="line"></div><div class="line">		// 要将自定义的输出格式组件设置到job中</div><div class="line">		job.setOutputFormatClass(FilterOutputFormat.class);</div><div class="line"></div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line"></div><div class="line">		// 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat</div><div class="line">		// 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="3-7-Join多种应用"><a href="#3-7-Join多种应用" class="headerlink" title="3.7 Join多种应用"></a>3.7 Join多种应用</h2><h3 id="3-7-1-Reduce-join"><a href="#3-7-1-Reduce-join" class="headerlink" title="3.7.1 Reduce join"></a>3.7.1 Reduce join</h3><p><strong>1）原理：</strong></p>
<p>Map端的主要工作：为来自不同表(文件)的key/value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。</p>
<p>Reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行合并就ok了。</p>
<p><strong>2）该方法的缺点</strong></p>
<p><strong>这种方式的缺点很明显就是会造成map和reduce端也就是shuffle阶段出现大量的数据传输，效率很低。</strong></p>
<h4 id="3）案例-1"><a href="#3）案例-1" class="headerlink" title="3）案例"></a><strong>3）案例</strong></h4><p>​      reduce端表合并（数据倾斜）</p>
<p>通过将关联条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce<br>task，在reduce中进行数据的串联。</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/16.png" alt="1560778194940"></p>
<p>1）代码实现</p>
<p>​    1.1 创建商品和订合并后的bean类</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.table;</div><div class="line">import java.io.DataInput;</div><div class="line">import java.io.DataOutput;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.Writable;</div><div class="line"></div><div class="line">public class TableBean implements Writable &#123;</div><div class="line">	private String order_id; // 订单id</div><div class="line">	private String p_id; // 产品id</div><div class="line">	private int amount; // 产品数量</div><div class="line">	private String pname; // 产品名称</div><div class="line">	private String flag;// 表的标记</div><div class="line"></div><div class="line">	public TableBean() &#123;</div><div class="line">		super();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public TableBean(String order_id, String p_id, int amount, String pname, String flag) &#123;</div><div class="line">		super();</div><div class="line">		this.order_id = order_id;</div><div class="line">		this.p_id = p_id;</div><div class="line">		this.amount = amount;</div><div class="line">		this.pname = pname;</div><div class="line">		this.flag = flag;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getFlag() &#123;</div><div class="line">		return flag;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setFlag(String flag) &#123;</div><div class="line">		this.flag = flag;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getOrder_id() &#123;</div><div class="line">		return order_id;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setOrder_id(String order_id) &#123;</div><div class="line">		this.order_id = order_id;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getP_id() &#123;</div><div class="line">		return p_id;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setP_id(String p_id) &#123;</div><div class="line">		this.p_id = p_id;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public int getAmount() &#123;</div><div class="line">		return amount;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setAmount(int amount) &#123;</div><div class="line">		this.amount = amount;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getPname() &#123;</div><div class="line">		return pname;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setPname(String pname) &#123;</div><div class="line">		this.pname = pname;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public void write(DataOutput out) throws IOException &#123;</div><div class="line">		out.writeUTF(order_id);</div><div class="line">		out.writeUTF(p_id);</div><div class="line">		out.writeInt(amount);</div><div class="line">		out.writeUTF(pname);</div><div class="line">		out.writeUTF(flag);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public void readFields(DataInput in) throws IOException &#123;</div><div class="line">		this.order_id = in.readUTF();</div><div class="line">		this.p_id = in.readUTF();</div><div class="line">		this.amount = in.readInt();</div><div class="line">		this.pname = in.readUTF();</div><div class="line">		this.flag = in.readUTF();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public String toString() &#123;</div><div class="line">		return order_id + &quot;\t&quot; + pname + &quot;\t&quot; + amount + &quot;\t&quot; ;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>2）编写TableMapper程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.table;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileSplit;</div><div class="line"></div><div class="line">public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt;&#123;</div><div class="line">	TableBean bean = new TableBean();</div><div class="line">	Text k = new Text();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 获取输入文件类型</div><div class="line">		FileSplit split = (FileSplit) context.getInputSplit();</div><div class="line">		String name = split.getPath().getName();</div><div class="line">		</div><div class="line">		// 2 获取输入数据</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		// 3 不同文件分别处理</div><div class="line">		if (name.startsWith(&quot;order&quot;)) &#123;// 订单表处理</div><div class="line">			// 3.1 切割</div><div class="line">			String[] fields = line.split(&quot;\t&quot;);</div><div class="line">			</div><div class="line">			// 3.2 封装bean对象</div><div class="line">			bean.setOrder_id(fields[0]);</div><div class="line">			bean.setP_id(fields[1]);</div><div class="line">			bean.setAmount(Integer.parseInt(fields[2]));</div><div class="line">			bean.setPname(&quot;&quot;);</div><div class="line">			bean.setFlag(&quot;0&quot;);</div><div class="line">			</div><div class="line">			k.set(fields[1]);</div><div class="line">		&#125;else &#123;// 产品表处理</div><div class="line">			// 3.3 切割</div><div class="line">			String[] fields = line.split(&quot;\t&quot;);</div><div class="line">			</div><div class="line">			// 3.4 封装bean对象</div><div class="line">			bean.setP_id(fields[0]);</div><div class="line">			bean.setPname(fields[1]);</div><div class="line">			bean.setFlag(&quot;1&quot;);</div><div class="line">			bean.setAmount(0);</div><div class="line">			bean.setOrder_id(&quot;&quot;);</div><div class="line">			</div><div class="line">			k.set(fields[0]);</div><div class="line">		&#125;</div><div class="line">		// 4 写出</div><div class="line">		context.write(k, bean);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>3）编写TableReducer程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.table;</div><div class="line">import java.io.IOException;</div><div class="line">import java.util.ArrayList;</div><div class="line">import org.apache.commons.beanutils.BeanUtils;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		// 1准备存储订单的集合</div><div class="line">		ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;();</div><div class="line">		// 2 准备bean对象</div><div class="line">		TableBean pdBean = new TableBean();</div><div class="line"></div><div class="line">		for (TableBean bean : values) &#123;</div><div class="line"></div><div class="line">			if (&quot;0&quot;.equals(bean.getFlag())) &#123;// 订单表</div><div class="line">				// 拷贝传递过来的每条订单数据到集合中</div><div class="line">				TableBean orderBean = new TableBean();</div><div class="line">				try &#123;</div><div class="line">					BeanUtils.copyProperties(orderBean, bean);</div><div class="line">				&#125; catch (Exception e) &#123;</div><div class="line">					e.printStackTrace();</div><div class="line">				&#125;</div><div class="line"></div><div class="line">				orderBeans.add(orderBean);</div><div class="line">			&#125; else &#123;// 产品表</div><div class="line">				try &#123;</div><div class="line">					// 拷贝传递过来的产品表到内存中</div><div class="line">					BeanUtils.copyProperties(pdBean, bean);</div><div class="line">				&#125; catch (Exception e) &#123;</div><div class="line">					e.printStackTrace();</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		// 3 表的拼接</div><div class="line">		for(TableBean bean:orderBeans)&#123;</div><div class="line">			bean.setPname (pdBean.getPname());</div><div class="line">			</div><div class="line">			// 4 数据写出去</div><div class="line">			context.write(bean, NullWritable.get());</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>4）编写TableDriver程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.table;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class TableDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws Exception &#123;</div><div class="line">		// 1 获取配置信息，或者job对象实例</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		Job job = Job.getInstance(configuration);</div><div class="line"></div><div class="line">		// 2 指定本程序的jar包所在的本地路径</div><div class="line">		job.setJarByClass(TableDriver.class);</div><div class="line"></div><div class="line">		// 3 指定本业务job要使用的mapper/Reducer业务类</div><div class="line">		job.setMapperClass(TableMapper.class);</div><div class="line">		job.setReducerClass(TableReducer.class);</div><div class="line"></div><div class="line">		// 4 指定mapper输出数据的kv类型</div><div class="line">		job.setMapOutputKeyClass(Text.class);</div><div class="line">		job.setMapOutputValueClass(TableBean.class);</div><div class="line"></div><div class="line">		// 5 指定最终输出的数据的kv类型</div><div class="line">		job.setOutputKeyClass(TableBean.class);</div><div class="line">		job.setOutputValueClass(NullWritable.class);</div><div class="line"></div><div class="line">		// 6 指定job的输入原始文件所在目录</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>3）运行程序查看结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1001	小米	1	</div><div class="line">1001	小米	1	</div><div class="line">1002	华为	2	</div><div class="line">1002	华为	2	</div><div class="line">1003	格力	3	</div><div class="line">1003	格力	3</div></pre></td></tr></table></figure>
<p><strong>缺点：这种方式中，合并的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜</strong></p>
<p><strong>解决方案： map端实现数据合并</strong></p>
<h3 id="3-7-2-Map-join（Distributedcache分布式缓存）"><a href="#3-7-2-Map-join（Distributedcache分布式缓存）" class="headerlink" title="3.7.2 Map join（Distributedcache分布式缓存）"></a>3.7.2 Map join（Distributedcache分布式缓存）</h3><p>1）使用场景：一张表十分小、一张表很大。</p>
<p>2）解决方案</p>
<p>在map端缓存多张表，提前处理业务逻辑，这样增加map端业务，减少reduce端数据的压力，尽可能的减少数据倾斜。</p>
<p>3）具体办法：采用distributedcache</p>
<p>​         （1）在mapper的setup阶段，将文件读取到缓存集合中。</p>
<p>​         （2）在驱动函数中加载缓存。</p>
<p>job.addCacheFile(new URI(“file:/e:/mapjoincache/pd.txt”));// 缓存普通文件到task运行节点</p>
<h4 id="4）案例："><a href="#4）案例：" class="headerlink" title="4）案例："></a>4）案例：</h4><p>​            map端表合并（Distributedcache）  - 结合上个案例代码（3.7.1 3 案例）</p>
<p>1）分析</p>
<p>适用于关联表中有小表的情形；</p>
<p>可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行合并并输出最终结果，可以大大提高合并操作的并发度，加快处理速度。</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/17.png" alt="1560778526618"></p>
<p>2）实操案例</p>
<p>（1）先在驱动模块中添加缓存文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package test;</div><div class="line">import java.net.URI;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class DistributedCacheDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws Exception &#123;</div><div class="line">		// 1 获取job信息</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		Job job = Job.getInstance(configuration);</div><div class="line"></div><div class="line">		// 2 设置加载jar包路径</div><div class="line">		job.setJarByClass(DistributedCacheDriver.class);</div><div class="line"></div><div class="line">		// 3 关联map</div><div class="line">		job.setMapperClass(DistributedCacheMapper.class);</div><div class="line">		</div><div class="line">		// 4 设置最终输出数据类型</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(NullWritable.class);</div><div class="line"></div><div class="line">		// 5 设置输入输出路径</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		// 6 加载缓存数据</div><div class="line">		job.addCacheFile(new URI(&quot;file:///e:/inputcache/pd.txt&quot;));</div><div class="line">		</div><div class="line">		// 7 map端join的逻辑不需要reduce阶段，设置reducetask数量为0</div><div class="line">		job.setNumReduceTasks(0);</div><div class="line"></div><div class="line">		// 8 提交</div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）读取缓存的文件数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package test;</div><div class="line">import java.io.BufferedReader;</div><div class="line">import java.io.FileInputStream;</div><div class="line">import java.io.IOException;</div><div class="line">import java.io.InputStreamReader;</div><div class="line">import java.util.HashMap;</div><div class="line">import java.util.Map;</div><div class="line">import org.apache.commons.lang.StringUtils;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123;</div><div class="line"></div><div class="line">	Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		// 1 获取缓存的文件</div><div class="line">		BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;pd.txt&quot;),&quot;UTF-8&quot;));</div><div class="line">		</div><div class="line">		String line;</div><div class="line">		while(StringUtils.isNotEmpty(line = reader.readLine()))&#123;</div><div class="line">			// 2 切割</div><div class="line">			String[] fields = line.split(&quot;\t&quot;);</div><div class="line">			</div><div class="line">			// 3 缓存数据到集合</div><div class="line">			pdMap.put(fields[0], fields[1]);</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		// 4 关流</div><div class="line">		reader.close();</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	Text k = new Text();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		// 1 获取一行</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		// 2 截取</div><div class="line">		String[] fields = line.split(&quot;\t&quot;);</div><div class="line">		</div><div class="line">		// 3 获取产品id</div><div class="line">		String pId = fields[1];</div><div class="line">		</div><div class="line">		// 4 获取商品名称</div><div class="line">		String pdName = pdMap.get(pId);</div><div class="line">		</div><div class="line">		// 5 拼接</div><div class="line">		k.set(line + &quot;\t&quot;+ pdName);</div><div class="line">		</div><div class="line">		// 6 写出</div><div class="line">		context.write(k, NullWritable.get());</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="3-8-数据清洗（ETL）"><a href="#3-8-数据清洗（ETL）" class="headerlink" title="3.8 数据清洗（ETL）"></a>3.8 数据清洗（ETL）</h2><p>1）概述</p>
<p>在运行核心业务Mapreduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行mapper程序，不需要运行reduce程序。</p>
<h4 id="2）案例-2"><a href="#2）案例-2" class="headerlink" title="2）案例"></a>2）案例</h4><p>日志清洗（数据清洗）。</p>
<h5 id="简单解析版"><a href="#简单解析版" class="headerlink" title="简单解析版"></a>简单解析版</h5><p>1）需求：</p>
<p>去除日志中字段长度小于等于11的日志。</p>
<p>2）输入数据</p>
<p><img src="/2018/03/18/hadoop大数据-十一-Mapreduce框架原理/clip_image00772.png" alt="img"></p>
<p>里面的内容就是我们平时网站输出的日志。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;</div><div class="line">183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;</div><div class="line">163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</div><div class="line">163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</div><div class="line">101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</div><div class="line">101.226.68.137 - - [18/Sep/2013:06:49:45 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</div><div class="line">60.208.6.156 - - [18/Sep/2013:06:49:48 +0000] &quot;GET /wp-content/uploads/2013/07/rcassandra.png HTTP/1.0&quot; 200 185524 &quot;http://cos.name/category/software/packages/&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;</div><div class="line">222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] &quot;GET /images/my.jpg HTTP/1.1&quot; 200 19939 &quot;http://www.angularjs.cn/A00n&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;</div><div class="line">222.68.172.190 - - [18/Sep/2013:06:50:08 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;</div><div class="line">183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</div><div class="line">183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</div></pre></td></tr></table></figure>
<p>3）实现代码：</p>
<p>（1）编写LogMapper </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.weblog;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123;</div><div class="line">	</div><div class="line">	Text k = new Text();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 获取1行数据</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		// 2 解析日志</div><div class="line">		boolean result = parseLog(line,context);</div><div class="line">		</div><div class="line">		// 3 日志不合法退出</div><div class="line">		if (!result) &#123;</div><div class="line">			return;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		// 4 设置key</div><div class="line">		k.set(line);</div><div class="line">		</div><div class="line">		// 5 写出数据</div><div class="line">		context.write(k, NullWritable.get());</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// 2 解析日志</div><div class="line">	private boolean parseLog(String line, Context context) &#123;</div><div class="line">		// 1 截取</div><div class="line">		String[] fields = line.split(&quot; &quot;);</div><div class="line">		</div><div class="line">		// 2 日志长度大于11的为合法</div><div class="line">		if (fields.length &gt; 11) &#123;</div><div class="line">			// 系统计数器</div><div class="line">			context.getCounter(&quot;map&quot;, &quot;true&quot;).increment(1);</div><div class="line">			return true;</div><div class="line">		&#125;else &#123;</div><div class="line">			context.getCounter(&quot;map&quot;, &quot;false&quot;).increment(1);</div><div class="line">			return false;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）编写LogDriver</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.weblog;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class LogDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws Exception &#123;</div><div class="line"></div><div class="line">        args = new String[] &#123; &quot;e:/input/inputlog&quot;, &quot;e:/output1&quot; &#125;;</div><div class="line"></div><div class="line">		// 1 获取job信息</div><div class="line">		Configuration conf = new Configuration();</div><div class="line">		Job job = Job.getInstance(conf);</div><div class="line"></div><div class="line">		// 2 加载jar包</div><div class="line">		job.setJarByClass(LogDriver.class);</div><div class="line"></div><div class="line">		// 3 关联map</div><div class="line">		job.setMapperClass(LogMapper.class);</div><div class="line"></div><div class="line">		// 4 设置最终输出类型</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(NullWritable.class);</div><div class="line"></div><div class="line">		// 设置reducetask个数为0</div><div class="line">		job.setNumReduceTasks(0);</div><div class="line"></div><div class="line">		// 5 设置输入和输出路径</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		// 6 提交</div><div class="line">		job.waitForCompletion(true);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="复杂解析版"><a href="#复杂解析版" class="headerlink" title="复杂解析版"></a>复杂解析版</h5><p>1）需求：</p>
<p>对web访问日志中的各字段识别切分</p>
<p>去除日志中不合法的记录</p>
<p>根据统计需求，生成各类访问请求过滤数据</p>
<p>2）输入数据</p>
<p>   输入同上一个案例</p>
<p>3）实现代码：</p>
<p>（1）定义一个bean，用来记录日志数据中的各数据字段</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.log;</div><div class="line"></div><div class="line">public class LogBean &#123;</div><div class="line">	private String remote_addr;// 记录客户端的ip地址</div><div class="line">	private String remote_user;// 记录客户端用户名称,忽略属性&quot;-&quot;</div><div class="line">	private String time_local;// 记录访问时间与时区</div><div class="line">	private String request;// 记录请求的url与http协议</div><div class="line">	private String status;// 记录请求状态；成功是200</div><div class="line">	private String body_bytes_sent;// 记录发送给客户端文件主体内容大小</div><div class="line">	private String http_referer;// 用来记录从那个页面链接访问过来的</div><div class="line">	private String http_user_agent;// 记录客户浏览器的相关信息</div><div class="line"></div><div class="line">	private boolean valid = true;// 判断数据是否合法</div><div class="line"></div><div class="line">	public String getRemote_addr() &#123;</div><div class="line">		return remote_addr;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setRemote_addr(String remote_addr) &#123;</div><div class="line">		this.remote_addr = remote_addr;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getRemote_user() &#123;</div><div class="line">		return remote_user;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setRemote_user(String remote_user) &#123;</div><div class="line">		this.remote_user = remote_user;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getTime_local() &#123;</div><div class="line">		return time_local;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setTime_local(String time_local) &#123;</div><div class="line">		this.time_local = time_local;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getRequest() &#123;</div><div class="line">		return request;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setRequest(String request) &#123;</div><div class="line">		this.request = request;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getStatus() &#123;</div><div class="line">		return status;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setStatus(String status) &#123;</div><div class="line">		this.status = status;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getBody_bytes_sent() &#123;</div><div class="line">		return body_bytes_sent;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setBody_bytes_sent(String body_bytes_sent) &#123;</div><div class="line">		this.body_bytes_sent = body_bytes_sent;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getHttp_referer() &#123;</div><div class="line">		return http_referer;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setHttp_referer(String http_referer) &#123;</div><div class="line">		this.http_referer = http_referer;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public String getHttp_user_agent() &#123;</div><div class="line">		return http_user_agent;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setHttp_user_agent(String http_user_agent) &#123;</div><div class="line">		this.http_user_agent = http_user_agent;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public boolean isValid() &#123;</div><div class="line">		return valid;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setValid(boolean valid) &#123;</div><div class="line">		this.valid = valid;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	public String toString() &#123;</div><div class="line">		StringBuilder sb = new StringBuilder();</div><div class="line">		sb.append(this.valid);</div><div class="line">		sb.append(&quot;\001&quot;).append(this.remote_addr);</div><div class="line">		sb.append(&quot;\001&quot;).append(this.remote_user);</div><div class="line">		sb.append(&quot;\001&quot;).append(this.time_local);</div><div class="line">		sb.append(&quot;\001&quot;).append(this.request);</div><div class="line">		sb.append(&quot;\001&quot;).append(this.status);</div><div class="line">		sb.append(&quot;\001&quot;).append(this.body_bytes_sent);</div><div class="line">		sb.append(&quot;\001&quot;).append(this.http_referer);</div><div class="line">		sb.append(&quot;\001&quot;).append(this.http_user_agent);</div><div class="line">		</div><div class="line">		return sb.toString();</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）编写LogMapper程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.log;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123;</div><div class="line">	Text k = new Text();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		// 1 获取1行</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		// 2 解析日志是否合法</div><div class="line">		LogBean bean = pressLog(line);</div><div class="line">		</div><div class="line">		if (!bean.isValid()) &#123;</div><div class="line">			return;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		k.set(bean.toString());</div><div class="line">		</div><div class="line">		// 3 输出</div><div class="line">		context.write(k, NullWritable.get());</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// 解析日志</div><div class="line">	private LogBean pressLog(String line) &#123;</div><div class="line">		LogBean logBean = new LogBean();</div><div class="line">		</div><div class="line">		// 1 截取</div><div class="line">		String[] fields = line.split(&quot; &quot;);</div><div class="line">		</div><div class="line">		if (fields.length &gt; 11) &#123;</div><div class="line">			// 2封装数据</div><div class="line">			logBean.setRemote_addr(fields[0]);</div><div class="line">			logBean.setRemote_user(fields[1]);</div><div class="line">			logBean.setTime_local(fields[3].substring(1));</div><div class="line">			logBean.setRequest(fields[6]);</div><div class="line">			logBean.setStatus(fields[8]);</div><div class="line">			logBean.setBody_bytes_sent(fields[9]);</div><div class="line">			logBean.setHttp_referer(fields[10]);</div><div class="line">			</div><div class="line">			if (fields.length &gt; 12) &#123;</div><div class="line">				logBean.setHttp_user_agent(fields[11] + &quot; &quot;+ fields[12]);</div><div class="line">			&#125;else &#123;</div><div class="line">				logBean.setHttp_user_agent(fields[11]);</div><div class="line">			&#125;</div><div class="line">			</div><div class="line">			// 大于400，HTTP错误</div><div class="line">			if (Integer.parseInt(logBean.getStatus()) &gt;= 400) &#123;</div><div class="line">				logBean.setValid(false);</div><div class="line">			&#125;</div><div class="line">		&#125;else &#123;</div><div class="line">			logBean.setValid(false);</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		return logBean;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（3）编写LogDriver程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.log;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class LogDriver &#123;</div><div class="line">	public static void main(String[] args) throws Exception &#123;</div><div class="line">		// 1 获取job信息</div><div class="line">		Configuration conf = new Configuration();</div><div class="line">		Job job = Job.getInstance(conf);</div><div class="line"></div><div class="line">		// 2 加载jar包</div><div class="line">		job.setJarByClass(LogDriver.class);</div><div class="line"></div><div class="line">		// 3 关联map</div><div class="line">		job.setMapperClass(LogMapper.class);</div><div class="line"></div><div class="line">		// 4 设置最终输出类型</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(NullWritable.class);</div><div class="line"></div><div class="line">		// 5 设置输入和输出路径</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		// 6 提交</div><div class="line">		job.waitForCompletion(true);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="3-9-计数器应用"><a href="#3-9-计数器应用" class="headerlink" title="3.9 计数器应用"></a>3.9 计数器应用</h2><p>​         Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。</p>
<p>1）API</p>
<p>​         （1）采用枚举的方式统计计数</p>
<p>enum MyCounter{MALFORORMED,NORMAL}</p>
<p>//对枚举定义的自定义计数器加1</p>
<p>context.getCounter(MyCounter.MALFORORMED).increment(1);</p>
<p>（2）采用计数器组、计数器名称的方式统计</p>
<p>context.getCounter(“counterGroup”, “countera”).increment(1);</p>
<p>​                 组名和计数器名称随便起，但最好有意义。</p>
<p>​         （3）计数结果在程序运行后的控制台上查看。</p>
<p>2）案例</p>
<p>​      数据清洗的两个案例</p>
<h2 id="3-10-MapReduce开发总结"><a href="#3-10-MapReduce开发总结" class="headerlink" title="3.10 MapReduce开发总结"></a>3.10 MapReduce开发总结</h2><p>在编写mapreduce程序时，需要考虑的几个方面：</p>
<p>1）输入数据接口：InputFormat </p>
<p>   默认使用的实现类是：TextInputFormat </p>
<p>   TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。</p>
<p>KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\t）。</p>
<p>NlineInputFormat按照指定的行数N来划分切片。</p>
<p>CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率。</p>
<p>用户还可以自定义InputFormat。</p>
<p>2）逻辑处理接口：Mapper  </p>
<p>   用户根据业务需求实现其中三个方法：map()   setup()   cleanup () </p>
<p>3）Partitioner分区</p>
<p>​         有默认实现 HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</p>
<p>​         如果业务上有特别的需求，可以自定义分区。</p>
<p>4）Comparable排序</p>
<p>​         当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法。</p>
<p>​         部分排序：对最终输出的每一个文件进行内部排序。</p>
<p>​         全排序：对所有数据进行排序，通常只有一个Reduce。</p>
<p>​         二次排序：排序的条件有两个。</p>
<p>5）Combiner合并</p>
<p>Combiner合并可以提高程序执行效率，减少io传输。但是使用时必须不能影响原有的业务处理结果。</p>
<p>6）reduce端分组：Groupingcomparator</p>
<p>​         reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数。</p>
<p>​         利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑。</p>
<p>​         自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果。然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）。这样，我们要取的最大值就是reduce()方法中传进来key。</p>
<p>7）逻辑处理接口：Reducer</p>
<p>​         用户根据业务需求实现其中三个方法：reduce()   setup()   cleanup () </p>
<p>8）输出数据接口：OutputFormat</p>
<p>​         默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对向目标文本文件中输出为一行。</p>
<p> SequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p>
<p>用户还可以自定义OutputFormat。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;三-MapReduce框架原理&quot;&gt;&lt;a href=&quot;#三-MapReduce框架原理&quot; class=&quot;headerlink&quot; title=&quot;三 MapReduce框架原理&quot;&gt;&lt;/a&gt;三 MapReduce框架原理&lt;/h1&gt;&lt;h2 id=&quot;3-1-MapReduce
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="MapReduce" scheme="http://kingge.top/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(十)-Mapreduce基础</title>
    <link href="http://kingge.top/2018/03/16/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E5%8D%81-Mapreduce%E5%9F%BA%E7%A1%80/"/>
    <id>http://kingge.top/2018/03/16/hadoop大数据-十-Mapreduce基础/</id>
    <published>2018-03-16T11:59:59.000Z</published>
    <updated>2019-06-17T12:46:35.921Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-MapReduce入门"><a href="#一-MapReduce入门" class="headerlink" title="一 MapReduce入门"></a>一 MapReduce入门</h1><h2 id="1-1-MapReduce定义"><a href="#1-1-MapReduce定义" class="headerlink" title="1.1 MapReduce定义"></a>1.1 MapReduce定义</h2><p>Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。</p>
<p>Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。</p>
<h2 id="1-2-MapReduce优缺点"><a href="#1-2-MapReduce优缺点" class="headerlink" title="1.2 MapReduce优缺点"></a>1.2 MapReduce优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><p><strong>1**</strong>）MapReduce<strong> </strong>易于编程。**它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。</p>
<p><strong>2**</strong>）良好的扩展性。**当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。</p>
<p><strong>3**</strong>）高容错性。**MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。</p>
<p><strong>4**</strong>）适合PB<strong>**级以上海量数据的</strong>离线处理<strong>（他跟其他的分布式运行框架不同，例如spark等等）。</strong>这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。</p>
<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><p><strong>MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。</strong></p>
<p><strong>1）实时计算。</strong>MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。</p>
<p><strong>2）流式计算。</strong>流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了<strong>数据源必须是静态的</strong>。</p>
<p><strong>3）DAG</strong>（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。</p>
<h2 id="1-3-MapReduce核心思想"><a href="#1-3-MapReduce核心思想" class="headerlink" title="1.3 MapReduce核心思想"></a>1.3 MapReduce核心思想</h2><p>  下面根据一个小小的案例来体现 mapreduce的运转流程。</p>
<p><img src="/2018/03/16/hadoop大数据-十-Mapreduce基础/5C1560695415804.png" alt=""></p>
<p>根据块大小（128M）进行分片运算，每个maptask负责处理自己所属的块数据，把每个单词出现个数计算统计然后放到hashmap（实际上是放到磁盘上）中，key是单词，value是单词出现次数。</p>
<p>1）分布式的运算程序往往需要分成至少2个阶段。（<strong>map阶段和reduce阶段</strong>）</p>
<p>2）第一个阶段的maptask并发实例，完全并行运行，互不相干。</p>
<p>3）第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。</p>
<p>4）MapReduce编程模型只能包含一个map阶段和一个reduce阶段，<strong>如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行</strong>。</p>
<h2 id="1-4-MapReduce进程"><a href="#1-4-MapReduce进程" class="headerlink" title="1.4 MapReduce进程"></a>1.4 <em>MapReduce进程</em></h2><p>一个完整的mapreduce程序在分布式运行时有三类实例进程：</p>
<p>1）MrAppMaster：负责整个程序的过程调度及状态协调。</p>
<p>2）MapTask：负责map阶段的整个数据处理流程。</p>
<p>3）ReduceTask：负责reduce阶段的整个数据处理流程。</p>
<h2 id="1-5-MapReduce编程规范"><a href="#1-5-MapReduce编程规范" class="headerlink" title="1.5 MapReduce编程规范"></a>1.5 MapReduce编程规范</h2><p>用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端)</p>
<h3 id="1）Mapper阶段"><a href="#1）Mapper阶段" class="headerlink" title="1）Mapper阶段"></a>1）Mapper阶段</h3><p>​         （1）用户自定义的Mapper要继承自己的父类</p>
<p>​         （2）Mapper的输入数据是KV对的形式（KV的类型可自定义）</p>
<p>​         （3）Mapper中的业务逻辑写在map()方法中</p>
<p>​         （4）Mapper的输出数据是KV对的形式（KV的类型可自定义）</p>
<p>​         （5）map()方法（maptask进程）对每一个<k,v>调用一次</k,v></p>
<h3 id="2）Reducer阶段"><a href="#2）Reducer阶段" class="headerlink" title="2）Reducer阶段"></a>2）Reducer阶段</h3><p>​         （1）用户自定义的Reducer要继承自己的父类</p>
<p>​         （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</p>
<p>​         （3）Reducer的业务逻辑写在reduce()方法中</p>
<p>​         （4）Reducetask进程对每一组相同k的<k,v>组调用一次reduce()方法</k,v></p>
<h3 id="3）Driver阶段"><a href="#3）Driver阶段" class="headerlink" title="3）Driver阶段"></a>3）Driver阶段</h3><p>整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象</p>
<h3 id="4）案例"><a href="#4）案例" class="headerlink" title="4）案例"></a>4）案例</h3><p>​         统计一堆文件中单词出现的个数（WordCount案例）。</p>
<p>在一堆给定的文本文件中统计输出每一个单词出现的总次数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1.数据准备 anly.text 包涵一下数据。</div><div class="line"></div><div class="line">hello world</div><div class="line">kingge kingge</div><div class="line">hadoop </div><div class="line">spark</div><div class="line">hello world</div><div class="line">kingge	kingge</div><div class="line">hadoop </div><div class="line">spark</div><div class="line">hello world</div><div class="line">hadoop </div><div class="line">spark</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">2.按照mapreduce编程规范，分别编写Mapper，Reducer，Driver。</div></pre></td></tr></table></figure>
<p><img src="/2018/03/16/hadoop大数据-十-Mapreduce基础/1560696740319.png" alt=""></p>
<p>简单案例分析</p>
<p><img src="/2018/03/16/hadoop大数据-十-Mapreduce基础/1560696778909.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">3.书写java代码</div><div class="line">（1）编写mapper类</div><div class="line"></div><div class="line">package com.kingge.mapreduce;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line">//四个参数：前两个是map的输入参数类型，后两个数输出参数类型</div><div class="line">//很明显，执行一个map，数据的key值是long类型代表着数据所属的行号，那么value值就是string类型，对应Hadoop的序列化类型是text.</div><div class="line">//输出的结果是，每个单词对应的个数。那么输出的key应该是Text,代表单词,value应该是Int类型，代表这个单词的个数</div><div class="line">public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</div><div class="line">	</div><div class="line">	Text k = new Text();</div><div class="line">	IntWritable v = new IntWritable(1);</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 获取一行-因为map是一行一行进行处理的</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		// 2 切割</div><div class="line">		String[] words = line.split(&quot; &quot;);</div><div class="line">		</div><div class="line">		// 3 输出</div><div class="line">		for (String word : words) &#123;</div><div class="line">			</div><div class="line">			k.set(word);</div><div class="line">			context.write(k, v);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">（2）编写reducer类</div><div class="line"></div><div class="line">package com.kingge.mapreduce.wordcount;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void reduce(Text key, Iterable&lt;IntWritable&gt; value,</div><div class="line">			Context context) throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 累加求和</div><div class="line">		int sum = 0;</div><div class="line">		for (IntWritable count : value) &#123;</div><div class="line">			sum += count.get();</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		// 2 输出</div><div class="line">		context.write(key, new IntWritable(sum));</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">执行到reduce阶段，那么经过map的计算和排序，最终会形成了一组一组的相同key的KV键值对（key group）。然后相同组的会进行reduce统计。一组接着一组进行计算。并不是所有组都通过reduce。</div><div class="line">//例如假设最终返回的KV值是：</div><div class="line">//hello 1</div><div class="line">//hello 1</div><div class="line">//word 1</div><div class="line">//word 1</div><div class="line">   那么 前两个hello为一组，经过reduce运算，然后返回，同时word为一组也经过统计返回。这两组并不会都由同一个reduce处理</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">（3）编写驱动类</div><div class="line">package com.kingge.mapreduce.wordcount;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.IntWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class WordcountDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;</div><div class="line"></div><div class="line">		// 1 获取配置信息</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		Job job = Job.getInstance(configuration);</div><div class="line"></div><div class="line">		// 2 设置jar加载路径</div><div class="line">		job.setJarByClass(WordcountDriver.class);</div><div class="line"></div><div class="line">		// 3 设置map和Reduce类</div><div class="line">		job.setMapperClass(WordcountMapper.class);</div><div class="line">		job.setReducerClass(WordcountReducer.class);</div><div class="line"></div><div class="line">		// 4 设置map输出</div><div class="line">		job.setMapOutputKeyClass(Text.class);</div><div class="line">		job.setMapOutputValueClass(IntWritable.class);</div><div class="line"></div><div class="line">		// 5 设置Reduce输出</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(IntWritable.class);</div><div class="line">		</div><div class="line">		// 6 设置输入和输出路径</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		// 7 提交</div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line"></div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>4）集群上测试</p>
<p>（1）将程序打成jar包，然后拷贝到hadoop集群中。</p>
<p>（2）启动hadoop集群</p>
<p>（3）执行wordcount程序</p>
<p>[kingge@hadoop102 software]$ hadoop jar  wc.jar com.kingge.wordcount.WordcountDriver /user/kingge/input /user/kingge/output1</p>
<p>5）本地测试</p>
<p>（1）在windows环境上配置HADOOP_HOME环境变量。</p>
<p>（2）在eclipse上运行程序</p>
<p><strong>（3）注意：如果eclipse打印不出日志，在控制台上只显示</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).  </div><div class="line">2.log4j:WARN Please initialize the log4j system properly.  </div><div class="line">3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</div></pre></td></tr></table></figure>
<p>需要在项目的src目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">log4j.rootLogger=INFO, stdout  </div><div class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender  </div><div class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  </div><div class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  </div><div class="line">log4j.appender.logfile=org.apache.log4j.FileAppender  </div><div class="line">log4j.appender.logfile.File=target/spring.log  </div><div class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  </div><div class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n  </div><div class="line"></div><div class="line">经过debug发现，只有当map处理完所有数据，才会进入reduce，map处理数据是一行一行进行处理的，每一行数据的处理都会经过一次map方法，直到所有数据处理完毕。Map处理完所有数据后，会排序所有的key，进行分组。然后一组一组的经过reduce，进行统计操作。直到所有组统计完毕，然后输出数据。</div></pre></td></tr></table></figure>
<h1 id="二-Hadoop序列化"><a href="#二-Hadoop序列化" class="headerlink" title="二 Hadoop序列化"></a>二 Hadoop序列化</h1><h2 id="2-1-为什么要序列化？"><a href="#2-1-为什么要序列化？" class="headerlink" title="2.1 为什么要序列化？"></a>2.1 为什么要序列化？</h2><p>​        一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。</p>
<h2 id="2-2-什么是序列化？"><a href="#2-2-什么是序列化？" class="headerlink" title="2.2 什么是序列化？"></a>2.2 什么是序列化？</h2><p>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。 </p>
<p>反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。</p>
<h2 id="2-3-为什么不用Java的序列化？"><a href="#2-3-为什么不用Java的序列化？" class="headerlink" title="2.3 为什么不用Java的序列化？"></a>2.3 为什么不用Java的序列化？</h2><p>​        Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效。</p>
<h2 id="2-4-为什么序列化对Hadoop很重要？"><a href="#2-4-为什么序列化对Hadoop很重要？" class="headerlink" title="2.4 为什么序列化对Hadoop很重要？"></a>2.4 为什么序列化对Hadoop很重要？</h2><p>​         <strong>因为Hadoop在集群之间进行通讯或者RPC调用的时候</strong>，需要序列化，而且要求序列化要快，且体积要小，占用带宽要小。所以必须理解Hadoop的序列化机制。</p>
<p>​        序列化和反序列化在分布式数据处理领域经常出现：进程通信和永久存储。然而Hadoop中各个节点的通信是通过远程调用（RPC）实现的，那么RPC序列化要求具有以下特点：</p>
<p>1）紧凑：紧凑的格式能让我们充分利用网络带宽，而带宽是数据中心最稀缺的资源</p>
<p>2）快速：进程通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本的；</p>
<p>3）可扩展：协议为了满足新的需求变化，所以控制客户端和服务器过程中，需要直接引进相应的协议，这些是新协议，原序列化方式能支持新的协议报文；</p>
<p>4）互操作：能支持不同语言写的客户端和服务端进行交互； </p>
<h2 id="2-5-常用数据序列化类型"><a href="#2-5-常用数据序列化类型" class="headerlink" title="2.5 常用数据序列化类型"></a>2.5 常用数据序列化类型</h2><p>常用的数据类型对应的hadoop数据序列化类型</p>
<table>
<thead>
<tr>
<th><strong>Java**</strong>类型**</th>
<th><strong>Hadoop Writable**</strong>类型**</th>
</tr>
</thead>
<tbody>
<tr>
<td>boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>string</td>
<td>Text</td>
</tr>
<tr>
<td>map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>array</td>
<td>ArrayWritable</td>
</tr>
</tbody>
</table>
<h2 id="2-6-自定义bean对象实现序列化接口（Writable）"><a href="#2-6-自定义bean对象实现序列化接口（Writable）" class="headerlink" title="2.6 自定义bean对象实现序列化接口（Writable）"></a>2.6 自定义bean对象实现序列化接口（Writable）</h2><p>1）自定义bean对象要想序列化传输，必须实现序列化接口，需要注意以下7项。</p>
<p>（1）必须实现Writable接口</p>
<p> <img src="/2018/03/16/hadoop大数据-十-Mapreduce基础/clip_image001.png" alt=""></p>
<p>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p>
<p>​            <strong>public</strong>   FlowBean() {                    <strong>super</strong>();            }   </p>
<p>（3）重写序列化方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Override</div><div class="line">	public void write(DataOutput out) throws IOException &#123;</div><div class="line">		out.writeLong(upFlow);</div><div class="line">		out.writeLong(downFlow);</div><div class="line">		out.writeLong(sumFlow);</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>（4）重写反序列化方法</p>
<p>​            </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Override</div><div class="line">public void readFields(DataInput in) throws IOException &#123;</div><div class="line">	upFlow = in.readLong();</div><div class="line">	downFlow = in.readLong();</div><div class="line">	sumFlow = in.readLong();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（5）<strong>注意反序列化的顺序和序列化的顺序完全一致</strong></p>
<p>（6）要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。</p>
<p>（7）如果需要将<strong>自定义的bean放在key中传输</strong>，则还需要实现WritableComparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序。</p>
<p>​     <strong>《自定义的bean放在key中传输》是什么意思呢？因为我们知道map操作中输入数据的存储结构是-key-value的形式.</strong>上面的例子中统计文本单词数，那么文本文件中每一行的文本的序号就是key（0,1,2,3）每一行的文本，就是value<strong><strong>的值。Map</strong></strong>操作完后输出的数据结构也是key-value<strong><strong>的形式。</strong></strong>而且输出的数据会根据key<strong><strong>排序</strong></strong>，以便reduce<strong><strong>处理。那么怎么排序在hadoop</strong></strong>中有一个默认规则（如果key<strong><strong>是2.5</strong></strong>中的常用数据类型），如果使我们自定义的序列化数据类型作为key<strong><strong>。那么默认排序规则就会失效，那么就需要我们制定一个排序规则就需要覆盖compareTo</strong></strong>方法。**</p>
<p>​           </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Override</div><div class="line">public int compareTo(FlowBean o) &#123;</div><div class="line">	// 倒序排列，从大到小</div><div class="line">	return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="2）案例"><a href="#2）案例" class="headerlink" title="2）案例"></a>2）案例</h3><p>​         每一个手机号耗费的总上行流量、下行流量、总流量（序列化）。</p>
<h4 id="2-1-数据准备"><a href="#2-1-数据准备" class="headerlink" title="2.1 数据准备"></a>2.1 数据准备</h4><p>pd.txt</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1363157985066 	13726230503	00-FD-07-A4-72-B8:CMCC	120.196.100.82	i02.c.aliimg.com		24	27	2481	24681	200</div><div class="line">1363157995052 	13826544101	5C-0E-8B-C7-F1-E0:CMCC	120.197.40.4			4	0	264	0	200</div><div class="line">1363157991076 	13926435656	20-10-7A-28-CC-0A:CMCC	120.196.100.99			2	4	132	1512	200</div><div class="line">1363154400022 	13926251106	5C-0E-8B-8B-B1-50:CMCC	120.197.40.4			4	0	240	0	200</div><div class="line">1363157993044 	18211575961	94-71-AC-CD-E6-18:CMCC-EASY	120.196.100.99	iface.qiyi.com	视频网站	15	12	1527	2106	200</div><div class="line">1363157995074 	84138413	5C-0E-8B-8C-E8-20:7DaysInn	120.197.40.4	122.72.52.12		20	16	4116	1432	200</div><div class="line">1363157993055 	13560439658	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			18	15	1116	954	200</div><div class="line">1363157995033 	15920133257	5C-0E-8B-C7-BA-20:CMCC	120.197.40.4	sug.so.360.cn	信息安全	20	20	3156	2936	200</div><div class="line">1363157983019 	13719199419	68-A1-B7-03-07-B1:CMCC-EASY	120.196.100.82			4	0	240	0	200</div><div class="line">1363157984041 	13660577991	5C-0E-8B-92-5C-20:CMCC-EASY	120.197.40.4	s19.cnzz.com	站点统计	24	9	6960	690	200</div><div class="line">1363157973098 	15013685858	5C-0E-8B-C7-F7-90:CMCC	120.197.40.4	rank.ie.sogou.com	搜索引擎	28	27	3659	3538	200</div><div class="line">1363157986029 	15989002119	E8-99-C4-4E-93-E0:CMCC-EASY	120.196.100.99	www.umeng.com	站点统计	3	3	1938	180	200</div><div class="line">1363157992093 	13560439658	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			15	9	918	4938	200</div><div class="line">1363157986041 	13480253104	5C-0E-8B-C7-FC-80:CMCC-EASY	120.197.40.4			3	3	180	180	200</div><div class="line">1363157984040 	13602846565	5C-0E-8B-8B-B6-00:CMCC	120.197.40.4	2052.flash2-http.qq.com	综合门户	15	12	1938	2910	200</div><div class="line">1363157995093 	13922314466	00-FD-07-A2-EC-BA:CMCC	120.196.100.82	img.qfc.cn		12	12	3008	3720	200</div><div class="line">1363157982040 	13502468823	5C-0A-5B-6A-0B-D4:CMCC-EASY	120.196.100.99	y0.ifengimg.com	综合门户	57	102	7335	110349	200</div><div class="line">1363157986072 	18320173382	84-25-DB-4F-10-1A:CMCC-EASY	120.196.100.99	input.shouji.sogou.com	搜索引擎	21	18	9531	2412	200</div><div class="line">1363157990043 	13925057413	00-1F-64-E1-E6-9A:CMCC	120.196.100.55	t3.baidu.com	搜索引擎	69	63	11058	48243	200</div><div class="line">1363157988072 	13760778710	00-FD-07-A4-7B-08:CMCC	120.196.100.82			2	2	120	120	200</div><div class="line">1363157985066 	13560436666	00-FD-07-A4-72-B8:CMCC	120.196.100.82	i02.c.aliimg.com		24	27	2481	24681	200</div><div class="line">1363157993055 	13560436666	C4-17-FE-BA-DE-D9:CMCC	120.196.100.99			18	15	1116	954	200</div></pre></td></tr></table></figure>
<p>输入数据格式： </p>
<p><img src="/2018/03/16/hadoop大数据-十-Mapreduce基础/1560697913154.png" alt=""></p>
<p>输出数据格式</p>
<p><img src="/2018/03/16/hadoop大数据-十-Mapreduce基础/0697933916.png" alt=""></p>
<h4 id="2-2-分析"><a href="#2-2-分析" class="headerlink" title="2.2 分析"></a>2.2 分析</h4><p>基本思路：</p>
<p>Map阶段：</p>
<p>（1）读取一行数据，切分字段</p>
<p>（2）抽取手机号、上行流量、下行流量</p>
<p>（3）以手机号为key，bean对象为value输出，即context.write(手机号,bean);</p>
<p>Reduce阶段：</p>
<p>（1）累加上行流量和下行流量得到总流量。</p>
<p>（2）实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输</p>
<p>（3）MR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key</p>
<p><strong>所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable。然后重写key的compareTo方法。</strong></p>
<h4 id="2-3-编写mapreduce程序"><a href="#2-3-编写mapreduce程序" class="headerlink" title="2.3 编写mapreduce程序"></a>2.3 编写mapreduce程序</h4><p>（1）编写流量统计的bean对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.flowsum;</div><div class="line">import java.io.DataInput;</div><div class="line">import java.io.DataOutput;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.Writable;</div><div class="line"></div><div class="line">// 1 实现writable接口</div><div class="line">public class FlowBean implements Writable&#123;</div><div class="line"></div><div class="line">	private long upFlow ;</div><div class="line">	private long downFlow;</div><div class="line">	private long sumFlow;</div><div class="line">	</div><div class="line">	//2  反序列化时，需要反射调用空参构造函数，所以必须有</div><div class="line">	public FlowBean() &#123;</div><div class="line">		super();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public FlowBean(long upFlow, long downFlow) &#123;</div><div class="line">		super();</div><div class="line">		this.upFlow = upFlow;</div><div class="line">		this.downFlow = downFlow;</div><div class="line">		this.sumFlow = upFlow + downFlow;</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	//3  写序列化方法</div><div class="line">	@Override</div><div class="line">	public void write(DataOutput out) throws IOException &#123;</div><div class="line">		out.writeLong(upFlow);</div><div class="line">		out.writeLong(downFlow);</div><div class="line">		out.writeLong(sumFlow);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	//4 反序列化方法</div><div class="line">	//5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致</div><div class="line">	@Override</div><div class="line">	public void readFields(DataInput in) throws IOException &#123;</div><div class="line">		this.upFlow  = in.readLong();</div><div class="line">		this.downFlow = in.readLong();</div><div class="line">		this.sumFlow = in.readLong();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// 6 编写toString方法，方便后续打印到文本</div><div class="line">	@Override</div><div class="line">	public String toString() &#123;</div><div class="line">		return upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public long getUpFlow() &#123;</div><div class="line">		return upFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setUpFlow(long upFlow) &#123;</div><div class="line">		this.upFlow = upFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public long getDownFlow() &#123;</div><div class="line">		return downFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setDownFlow(long downFlow) &#123;</div><div class="line">		this.downFlow = downFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public long getSumFlow() &#123;</div><div class="line">		return sumFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public void setSumFlow(long sumFlow) &#123;</div><div class="line">		this.sumFlow = sumFlow;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）编写mapper</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.flowsum;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123;</div><div class="line">	</div><div class="line">	FlowBean v = new FlowBean();</div><div class="line">	Text k = new Text();</div><div class="line">	</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 获取一行</div><div class="line">		String line = value.toString();</div><div class="line">		</div><div class="line">		// 2 切割字段</div><div class="line">		String[] fields = line.split(&quot;\t&quot;);</div><div class="line">		</div><div class="line">		// 3 封装对象</div><div class="line">		// 取出手机号码</div><div class="line">		String phoneNum = fields[1];</div><div class="line">		// 取出上行流量和下行流量</div><div class="line">		long upFlow = Long.parseLong(fields[fields.length - 3]);</div><div class="line">		long downFlow = Long.parseLong(fields[fields.length - 2]);</div><div class="line">		</div><div class="line">		v.set(downFlow, upFlow);</div><div class="line">		</div><div class="line">		// 4 写出</div><div class="line">		context.write(new Text(phoneNum), new FlowBean(upFlow, downFlow));</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（3）编写reducer</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.flowsum;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Reducer;</div><div class="line"></div><div class="line">public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context)</div><div class="line">			throws IOException, InterruptedException &#123;</div><div class="line"></div><div class="line">		long sum_upFlow = 0;</div><div class="line">		long sum_downFlow = 0;</div><div class="line"></div><div class="line">		// 1 遍历所用bean，将其中的上行流量，下行流量分别累加</div><div class="line">		for (FlowBean flowBean : values) &#123;</div><div class="line">			sum_upFlow += flowBean.getSumFlow();</div><div class="line">			sum_downFlow += flowBean.getDownFlow();</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		// 2 封装对象</div><div class="line">		FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow);</div><div class="line">		</div><div class="line">		// 3 写出</div><div class="line">		context.write(key, resultBean);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（4）编写驱动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.mapreduce.flowsum;</div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Job;</div><div class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"></div><div class="line">public class FlowsumDriver &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123;</div><div class="line">		</div><div class="line">		// 1 获取配置信息，或者job对象实例</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		Job job = Job.getInstance(configuration);</div><div class="line"></div><div class="line">		// 6 指定本程序的jar包所在的本地路径</div><div class="line">		job.setJarByClass(FlowsumDriver.class);</div><div class="line"></div><div class="line">		// 2 指定本业务job要使用的mapper/Reducer业务类</div><div class="line">		job.setMapperClass(FlowCountMapper.class);</div><div class="line">		job.setReducerClass(FlowCountReducer.class);</div><div class="line"></div><div class="line">		// 3 指定mapper输出数据的kv类型</div><div class="line">		job.setMapOutputKeyClass(Text.class);</div><div class="line">		job.setMapOutputValueClass(FlowBean.class);</div><div class="line"></div><div class="line">		// 4 指定最终输出的数据的kv类型</div><div class="line">		job.setOutputKeyClass(Text.class);</div><div class="line">		job.setOutputValueClass(FlowBean.class);</div><div class="line">		</div><div class="line">		// 5 指定job的输入原始文件所在目录</div><div class="line">		FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class="line">		FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class="line"></div><div class="line">		// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</div><div class="line">		boolean result = job.waitForCompletion(true);</div><div class="line">		System.exit(result ? 0 : 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一-MapReduce入门&quot;&gt;&lt;a href=&quot;#一-MapReduce入门&quot; class=&quot;headerlink&quot; title=&quot;一 MapReduce入门&quot;&gt;&lt;/a&gt;一 MapReduce入门&lt;/h1&gt;&lt;h2 id=&quot;1-1-MapReduce定义&quot;&gt;&lt;a h
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="MapReduce" scheme="http://kingge.top/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(九)-yarn</title>
    <link href="http://kingge.top/2018/03/14/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%B9%9D-yarn/"/>
    <id>http://kingge.top/2018/03/14/hadoop大数据-九-yarn/</id>
    <published>2018-03-14T14:59:59.000Z</published>
    <updated>2019-08-01T13:33:46.678Z</updated>
    
    <content type="html"><![CDATA[<h2 id="5-1-Hadoop1-x和Hadoop2-x架构区别"><a href="#5-1-Hadoop1-x和Hadoop2-x架构区别" class="headerlink" title="5.1 Hadoop1.x和Hadoop2.x架构区别"></a>5.1 Hadoop1.x和Hadoop2.x架构区别</h2><p>在Hadoop1.x时代，Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。</p>
<blockquote>
<ol>
<li>ResourceManagement 资源管理</li>
<li>JobScheduling/JobMonitoring 任务调度监控</li>
</ol>
</blockquote>
<p>在Hadoop2.x时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。这样就能够各司其职</p>
<blockquote>
<ol>
<li>ResourceManger</li>
<li>ApplicationMaster</li>
</ol>
</blockquote>
<p>​    需要注意的是，在Yarn中我们把job的概念换成了<code>application</code>，因为在新的Hadoop2.x中，运行的应用不只是MapReduce了，还有可能是其它应用如一个DAG（有向无环图Directed Acyclic Graph，例如storm应用）。Yarn的另一个目标就是拓展Hadoop，使得它不仅仅可以支持MapReduce计算，还能很方便的管理诸如Hive、Hbase、Pig、Spark/Shark等应用。这种新的架构设计能够使得各种类型的应用运行在Hadoop上面，并通过Yarn从系统层面进行统一的管理，也就是说，有了Yarn，各种应用就可以互不干扰的运行在同一个Hadoop系统中，共享整个集群资源。</p>
<h2 id="5-2-Yarn概述"><a href="#5-2-Yarn概述" class="headerlink" title="5.2 Yarn概述"></a>5.2 Yarn概述</h2><p><strong>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源</strong>，相当于一个分布式的操作系统平台，而<strong>MapReduce等运算程序则相当于运行于操作系统之上的应用程序</strong>。</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/5399709.png" alt="1564665399709"></p>
<h2 id="5-3-Yarn基本架构"><a href="#5-3-Yarn基本架构" class="headerlink" title="5.3 Yarn基本架构"></a>5.3 Yarn基本架构</h2><p>​         YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/4665463356.png" alt="1564665463356"></p>
<h2 id="5-4-Yarn工作机制"><a href="#5-4-Yarn工作机制" class="headerlink" title="5.4 Yarn工作机制"></a>5.4 Yarn工作机制</h2><p>1）Yarn运行机制</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/665729629.png" alt="1564665729629"></p>
<p>2）工作机制详解</p>
<blockquote>
<p>​         （0）Mr程序提交到客户端所在的节点。</p>
<p>​         （1）Yarnrunner向Resourcemanager申请一个Application。</p>
<p>​         （2）rm将该应用程序的资源路径返回给yarnrunner。</p>
<p>​         （3）该程序将运行所需资源提交到HDFS上。</p>
<p>​         （4）程序资源提交完毕后，申请运行mrAppMaster。</p>
<p>​         （5）RM将用户的请求初始化成一个task。</p>
<p>​         （6）其中一个NodeManager领取到task任务。</p>
<p>​         （7）该NodeManager创建容器Container，并产生MRAppmaster。</p>
<p>​         （8）Container从HDFS上拷贝资源到本地。</p>
<p>​         （9）MRAppmaster向RM 申请运行maptask资源。</p>
<p>​         （10）RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<p>​         （11）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。</p>
<p>（12）MrAppMaster等待所有maptask运行完毕后，向RM申请容器，运行reduce task。</p>
<p>​         （13）reduce task向maptask获取相应分区的数据。</p>
<p>​         （14）程序运行完毕后，MR会向RM申请注销自己。</p>
</blockquote>
<h2 id="5-5-作业提交全过程"><a href="#5-5-作业提交全过程" class="headerlink" title="5.5 作业提交全过程"></a>5.5 作业提交全过程</h2><p>1）作业提交过程之YARN</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/665811570.png" alt="1564665811570"></p>
<p>作业提交全过程详解</p>
<h3 id="（1）作业提交"><a href="#（1）作业提交" class="headerlink" title="（1）作业提交"></a>（1）作业提交</h3><p>第0步：client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。</p>
<p>第1步：client向RM申请一个作业id。</p>
<p>第2步：RM给client返回该job资源的提交路径和作业id。</p>
<p>第3步：client提交jar包、切片信息和配置文件到指定的资源提交路径。</p>
<p>第4步：client提交完资源后，向RM申请运行MrAppMaster。</p>
<h3 id="（2）作业初始化"><a href="#（2）作业初始化" class="headerlink" title="（2）作业初始化"></a>（2）作业初始化</h3><p>第5步：当RM收到client的请求后，将该job添加到容量调度器中。</p>
<p>第6步：某一个空闲的NM领取到该job。</p>
<p>第7步：该NM创建Container，并产生MRAppmaster。</p>
<p>第8步：下载client提交的资源到本地。</p>
<h3 id="（3）任务分配"><a href="#（3）任务分配" class="headerlink" title="（3）任务分配"></a>（3）任务分配</h3><p>第9步：MrAppMaster向RM申请运行多个maptask任务资源。</p>
<p>第10步：RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<h3 id="（4）任务运行"><a href="#（4）任务运行" class="headerlink" title="（4）任务运行"></a>（4）任务运行</h3><p>第11步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。</p>
<p>第12步：MrAppMaster等待所有maptask运行完毕后，向RM申请容器，运行reduce task。</p>
<p>第13步：reduce task向maptask获取相应分区的数据。</p>
<p>第14步：程序运行完毕后，MR会向RM申请注销自己。</p>
<h3 id="（5）进度和状态更新"><a href="#（5）进度和状态更新" class="headerlink" title="（5）进度和状态更新"></a>（5）进度和状态更新</h3><p>YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。</p>
<h3 id="（6）作业完成"><a href="#（6）作业完成" class="headerlink" title="（6）作业完成"></a>（6）作业完成</h3><p>除了向应用管理器请求作业进度外, 客户端每5分钟都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p>
<p>2）作业提交过程之MapReduce</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/65974601.png" alt="1564665974601"></p>
<p>3）作业提交过程之读数据</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/4666063752.png" alt="1564666063752"></p>
<p>4）作业提交过程之写数据</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/64666122043.png" alt="1564666122043"></p>
<h2 id="5-6-资源调度器"><a href="#5-6-资源调度器" class="headerlink" title="5.6 资源调度器"></a>5.6 资源调度器</h2><p>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2默认的资源调度器是Capacity Scheduler。</p>
<p>具体设置详见：yarn-default.xml文件</p>
   <property>       <description>The class to use as   the resource scheduler.</description>         <name>yarn.resourcemanager.scheduler.class</name>   <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>   </property>   

<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt;</div><div class="line">    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</div><div class="line">&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>　　1）先进先出调度器（FIFO）</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/66221031.png" alt="1564666221031">      </p>
<p>   2）容量调度器（Capacity Scheduler）</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/4666258448.png" alt="1564666258448">        </p>
<p> 3）公平调度器（Fair Scheduler）</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/564666314115.png" alt="1564666314115"></p>
<h2 id="5-7-任务的推测执行"><a href="#5-7-任务的推测执行" class="headerlink" title="5.7 任务的推测执行"></a>5.7 任务的推测执行</h2><p>1）作业完成时间取决于最慢的任务完成时间</p>
<p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p>
<p>典型案例：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p>
<p>2）推测执行机制：</p>
<p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</p>
<p>3）执行推测任务的前提条件</p>
<p>（1）每个task只能有一个备份任务；</p>
<p>（2）当前job已完成的task必须不小于0.05（5%）</p>
<p>（3）开启推测执行参数设置。Hadoop2.7.2 mapred-site.xml文件中默认是打开的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;mapreduce.map.speculative&lt;/name&gt;</div><div class="line">  &lt;value&gt;true&lt;/value&gt;</div><div class="line">  &lt;description&gt;If true, then multiple instances of some map tasks                may be executed in parallel.&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt;</div><div class="line">  &lt;value&gt;true&lt;/value&gt;</div><div class="line">  &lt;description&gt;If true, then multiple instances of some reduce tasks </div><div class="line">               may be executed in parallel.&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
   <property>       <name>mapreduce.map.speculative</name>       <value>true</value>     <description>If   true, then multiple instances of some map tasks                may be executed in   parallel.</description>   </property>       <property>       <name>mapreduce.reduce.speculative</name>       <value>true</value>       <description>If true, then multiple instances of some reduce   tasks                   may be executed in   parallel.</description>   </property>   

<p>4）不能启用推测执行机制情况</p>
<p>   （1）任务间存在严重的负载倾斜；</p>
<p>   （2）特殊任务，比如任务向数据库中写数据。</p>
<p>5）算法原理：</p>
<p><img src="/2018/03/14/hadoop大数据-九-yarn/64666366883.png" alt="1564666366883"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;5-1-Hadoop1-x和Hadoop2-x架构区别&quot;&gt;&lt;a href=&quot;#5-1-Hadoop1-x和Hadoop2-x架构区别&quot; class=&quot;headerlink&quot; title=&quot;5.1 Hadoop1.x和Hadoop2.x架构区别&quot;&gt;&lt;/a&gt;5.1 H
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="yarn" scheme="http://kingge.top/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(八)-namenode和resourcemanager高可用</title>
    <link href="http://kingge.top/2018/03/12/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E5%85%AB-namenode%E5%92%8Cresourcemanager%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    <id>http://kingge.top/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/</id>
    <published>2018-03-12T12:59:59.000Z</published>
    <updated>2019-06-10T13:20:52.956Z</updated>
    
    <content type="html"><![CDATA[<h1 id="HDFS-高可用"><a href="#HDFS-高可用" class="headerlink" title="HDFS 高可用"></a>HDFS 高可用</h1><h2 id="高可用概述"><a href="#高可用概述" class="headerlink" title="高可用概述"></a>高可用概述</h2><p>1）所谓HA（high available），即高可用（7*24小时不中断服务）。</p>
<p>2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。</p>
<p>3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。</p>
<p>4）NameNode主要在以下两个方面影响HDFS集群</p>
<p>​         NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启</p>
<p>​         NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用</p>
<p>HDFS HA功能通过配置Active/Standby两个nameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。</p>
<h2 id="HDFS-HA工作机制"><a href="#HDFS-HA工作机制" class="headerlink" title="HDFS-HA工作机制"></a>HDFS-HA工作机制</h2><p>1）通过双namenode消除单点故障</p>
<h3 id="HDFS-HA工作要点"><a href="#HDFS-HA工作要点" class="headerlink" title="HDFS-HA工作要点"></a>HDFS-HA工作要点</h3><p>1）元数据管理方式需要改变：</p>
<p>内存中各自保存一份元数据；</p>
<p>Edits日志只有Active状态的namenode节点可以做写操作；</p>
<p>两个namenode都可以读取edits；</p>
<p>共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；</p>
<p>2）需要一个状态管理功能模块</p>
<p>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止<strong>brain split（脑裂）</strong>现象的发生。</p>
<p>脑裂：集群中存在两台active状态的namenode。</p>
<p>3）必须保证两个NameNode之间能够ssh无密码登录。</p>
<p>4）隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务</p>
<p>怎么能够保证两台namenode，有一台是active另一台是standby，而不出现脑裂现象呢？</p>
<p>  假想一：两台namenode进行通信，周期请求对面，告知自己状态。在一定的条件下可以实现高可用，但是存在如下问题：1.两台namenode直接通信，如果namenode1（active）处理client请求时，没空响应namenode2那么nn2等待了一段时间，就认为nn1已经碟机，那么nn2启动（切换为active）。这个时候集群出现脑裂现象。2.nn1和nn2 因为网络问题，可能存在一定的延迟，无法实时的切换（nn1碟机，切换到nn2的时候，可能会等待一两分钟）。</p>
<p>  假想二：使用zookeeper记录namenode 的状态。也会出现上面的问题。如果网络出现问题，nn1（active）无法正确汇报自己的状态到zookeeper，那么nn2启动，也会出现脑裂问题。</p>
<p>   假想三：zkfailover，一个namenode的内部进程（解决网络交互问题）</p>
<h3 id="HDFS-HA自动故障转移工作机制"><a href="#HDFS-HA自动故障转移工作机制" class="headerlink" title="HDFS-HA自动故障转移工作机制"></a>HDFS-HA自动故障转移工作机制</h3><p>前面学习了使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能：</p>
<p><strong>1）故障检测：</strong>集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</p>
<p><strong>2）现役NameNode选择：</strong>ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</p>
<p>ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：</p>
<p><strong>1）健康监测：</strong>ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</p>
<p><strong>2）ZooKeeper会话管理：</strong>当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</p>
<p><strong>3）基于ZooKeeper的选择：</strong>如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为active状态。</p>
<p>​                                                                 zookeeper服务端</p>
<p><img src="/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/560172363333.png" alt="1560172363333"></p>
<h2 id="HDFS-HA集群配置"><a href="#HDFS-HA集群配置" class="headerlink" title="HDFS-HA集群配置"></a>HDFS-HA集群配置</h2><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>1）修改IP</p>
<p>2）修改主机名及主机名和IP地址的映射</p>
<p>3）关闭防火墙</p>
<p>4）ssh免密登录</p>
<p>5）安装JDK，配置环境变量等</p>
<h3 id="规划集群"><a href="#规划集群" class="headerlink" title="规划集群"></a>规划集群</h3><p>hadoop102                               hadoop103                              hadoop104               </p>
<p>NameNode                                 NameNode</p>
<p>JournalNode                              JournalNode                             JournalNode            </p>
<p>DataNode                                   DataNode                                  DataNode                 </p>
<p>ZK                                              ZK                                              ZK</p>
<p>ResourceManager</p>
<p>NodeManager                           NodeManager                           NodeManager         </p>
<h3 id="配置Zookeeper集群"><a href="#配置Zookeeper集群" class="headerlink" title="配置Zookeeper集群"></a>配置Zookeeper集群</h3><p>0）集群规划</p>
<p>在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。</p>
<p>1）解压安装</p>
<p>（1）解压zookeeper安装包到/opt/module/目录下</p>
<p> [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</p>
<p>（2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData</p>
<p>​         mkdir -p zkData</p>
<p>（3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg</p>
<p>​         mv zoo_sample.cfg zoo.cfg</p>
<p>2）配置zoo.cfg文件</p>
<p>​         （1）具体配置</p>
<p>​         dataDir=/opt/module/zookeeper-3.4.10/zkData</p>
<p>​         增加如下配置</p>
<p>​         #######################cluster##########################</p>
<p>server.2=hadoop102:2888:3888</p>
<p>server.3=hadoop103:2888:3888</p>
<p>server.4=hadoop104:2888:3888</p>
<p>（2）配置参数解读</p>
<p>Server.A=B:C:D。</p>
<p>A是一个数字，表示这个是第几号服务器；</p>
<p>B是这个服务器的ip地址；</p>
<p>C是这个服务器与集群中的Leader服务器交换信息的端口；</p>
<p>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p>
<p>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p>
<p>3）集群操作</p>
<p>（1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件</p>
<p>​         touch myid</p>
<p><strong>添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码</strong></p>
<p>（2）编辑myid文件</p>
<p>​         vi myid</p>
<p>​         在文件中添加与server对应的编号：如2</p>
<p>（3）拷贝配置好的zookeeper到其他机器上</p>
<p>​         scp -r zookeeper-3.4.10/ <a href="mailto:root@hadoop103.kingge.com:/opt/app/" target="_blank" rel="external">root@hadoop103.kingge.com:/opt/app/</a></p>
<p>​         scp -r zookeeper-3.4.10/ <a href="mailto:root@hadoop104.kingge.com:/opt/app/" target="_blank" rel="external">root@hadoop104.kingge.com:/opt/app/</a></p>
<p>​         并分别修改myid文件中内容为3、4</p>
<p>（4）分别启动zookeeper</p>
<p>​         [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start</p>
<p>[root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start</p>
<p>[root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start</p>
<p>（5）查看状态</p>
<p>[root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status</p>
<p>JMX enabled by default</p>
<p>Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</p>
<p>Mode: follower</p>
<p>[root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status</p>
<p>JMX enabled by default</p>
<p>Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</p>
<p>Mode: leader</p>
<p>[root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status</p>
<p>JMX enabled by default</p>
<p>Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</p>
<p>Mode: follower</p>
<h3 id="配置HDFS-HA集群（手动故障转移配置）"><a href="#配置HDFS-HA集群（手动故障转移配置）" class="headerlink" title="配置HDFS-HA集群（手动故障转移配置）"></a>配置HDFS-HA集群（手动故障转移配置）</h3><p>  <strong>为什么说是手动，因为假设某一台namenode</strong> <strong>出现了问题，并不会自动的切换另一台namenode为active状态，需要我们手动切换</strong></p>
<p>1）官方地址：<a href="http://hadoop.apache.org/" target="_blank" rel="external">http://hadoop.apache.org/</a></p>
<p>2）在opt目录下创建一个ha文件夹</p>
<p>mkdir ha</p>
<p>3）将/opt/app/下的 hadoop-2.7.2拷贝到/opt/ha目录下</p>
<p>cp -r hadoop-2.7.2/ /opt/ha/</p>
<p>4）配置hadoop-env.sh</p>
<p>   export JAVA_HOME=/opt/module/jdk1.8.0_144   </p>
<p>5）配置core-site.xml</p>
   <configuration>   <!-- 把两个NameNode）的地址组装成一个集群mycluster -->                    <property>                             <name>fs.defaultFS</name>                    <value>hdfs://mycluster</value> //任意名字，代表着整个namenode集群，至于调用那个namenode，他会自己分配                    </property>                        <!-- 指定hadoop运行时产生文件的存储目录 -->                    <property>                             <name>hadoop.tmp.dir</name>                             <value>/opt/ha/hadoop-2.7.2/data/tmp</value>                    </property>   </configuration>   

<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt;</div><div class="line">		&lt;property&gt;</div><div class="line">			&lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">        	&lt;value&gt;hdfs://mycluster&lt;/value&gt; //任意名字，代表着整个namenode集群，至于调用那个namenode，他会自己分配</div><div class="line">		&lt;/property&gt;</div><div class="line"></div><div class="line">		&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</div><div class="line">		&lt;property&gt;</div><div class="line">			&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</div><div class="line">			&lt;value&gt;/opt/ha/hadoop-2.7.2/data/tmp&lt;/value&gt;</div><div class="line">		&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>6）配置hdfs-site.xml（<strong>因为有了热备namenode，那么就可以把secondary-namenode关闭，功能重复</strong>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;!—可以配置文件块备份数--&gt;</div><div class="line">	&lt;!-- 完全分布式集群名称 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.nameservices&lt;/name&gt;</div><div class="line">		&lt;value&gt;mycluster&lt;/value&gt; //这个名字必须与上面的mycluster一致</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 集群中NameNode节点都有哪些 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</div><div class="line">		&lt;value&gt;nn1,nn2&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- nn1的RPC通信地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</div><div class="line">		&lt;value&gt;hadoop102:9000&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- nn2的RPC通信地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</div><div class="line">		&lt;value&gt;hadoop103:9000&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- nn1的http通信地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</div><div class="line">		&lt;value&gt;hadoop102:50070&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- nn2的http通信地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</div><div class="line">		&lt;value&gt;hadoop103:50070&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 指定NameNode元数据（edit.log）在JournalNode上的存放位置 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</div><div class="line">	&lt;value&gt;qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 防止脑裂--&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</div><div class="line">		&lt;value&gt;sshfence&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</div><div class="line">		&lt;value&gt;/home/kingge/.ssh/id_rsa&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 声明journalnode服务器存储目录--&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</div><div class="line">		&lt;value&gt;/opt/ha/hadoop-2.7.2/data/jn&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 关闭权限检查--&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.permissions.enable&lt;/name&gt;</div><div class="line">		&lt;value&gt;false&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">  		&lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</div><div class="line">	&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
   <configuration>       &lt;!—可以配置文件块备份数–&gt;            <!-- 完全分布式集群名称 -->            <property>                    <name>dfs.nameservices</name>                    <value>mycluster</value>   //这个名字必须与上面的mycluster一致            </property>                <!-- 集群中NameNode节点都有哪些   -->            <property>                    <name>dfs.ha.namenodes.mycluster</name>                    <value>nn1,nn2</value>            </property>                <!-- nn1的RPC通信地址 -->            <property>                    <name>dfs.namenode.rpc-address.mycluster.nn1</name>                    <value>hadoop102:9000</value>            </property>                <!-- nn2的RPC通信地址 -->            <property>                    <name>dfs.namenode.rpc-address.mycluster.nn2</name>                    <value>hadoop103:9000</value>            </property>                <!-- nn1的http通信地址 -->            <property>                    <name>dfs.namenode.http-address.mycluster.nn1</name>                    <value>hadoop102:50070</value>            </property>                <!-- nn2的http通信地址 -->            <property>                    <name>dfs.namenode.http-address.mycluster.nn2</name>                    <value>hadoop103:50070</value>            </property>                <!-- 指定NameNode元数据（edit.log）在JournalNode上的存放位置   -->            <property>                    <name>dfs.namenode.shared.edits.dir</name>            <value>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster</value>            </property>                <!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 **防止脑裂**-->            <property>                    <name>dfs.ha.fencing.methods</name>                    <value>sshfence</value>            </property>                <!-- 使用隔离机制时需要ssh无秘钥登录-->            <property>                    <name>dfs.ha.fencing.ssh.private-key-files</name>                    <value>/home/atguigu/.ssh/id_rsa</value>            </property>                <!-- 声明journalnode服务器存储目录-->            <property>                    <name>dfs.journalnode.edits.dir</name>                    <value>/opt/ha/hadoop-2.7.2/data/jn</value>            </property>                <!-- 关闭权限检查-->            <property>                    <name>dfs.permissions.enable</name>                    <value>false</value>            </property>                <!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式-->            <property>                  <name>dfs.client.failover.proxy.provider.mycluster</name>            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>            </property>   </configuration>   

<p>7）可以关闭secondary namenode和关闭原先的namenode 的http访问模式</p>
<p><img src="/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/560172564080.png" alt="1560172564080"></p>
<p>8）拷贝配置好的hadoop环境到其他节点</p>
<h3 id="启动HDFS-HA集群"><a href="#启动HDFS-HA集群" class="headerlink" title="启动HDFS-HA集群"></a>启动HDFS-HA集群</h3><p>1）在各个JournalNode节点上，输入以下命令启动journalnode服务：（在这里是hadoop102、hadoop103.、hadoop104 三台服务器都需要执行下面命令）</p>
<p>​         sbin/hadoop-daemon.sh start journalnode</p>
<p>2）在[nn1]上，对其进行格式化，并启动：</p>
<p>​         bin/hdfs namenode –format  // </p>
<p>​         sbin/hadoop-daemon.sh start namenode //开启active namenode</p>
<p>3）在[nn2]上，同步nn1的元数据信息：</p>
<p>​         bin/hdfs namenode -bootstrapStandby</p>
<p>4）启动[nn2]：启动备用namenode</p>
<p>​         sbin/hadoop-daemon.sh start namenode</p>
<p>5）查看web页面显示</p>
<p><img src="/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/5C1560172587527.png" alt="1560172587527"></p>
<p>6）在[nn1]上，启动所有datanode</p>
<p>​         sbin/hadoop-daemons.sh start datanode</p>
<p>7）将[nn1]切换为Active</p>
<p>​         bin/hdfs haadmin -transitionToActive nn1</p>
<p>8）查看是否Active</p>
<p>​         bin/hdfs haadmin -getServiceState nn1</p>
<p>9）尝试kill 掉nn1的namenode</p>
<p>  Kill 7575</p>
<p>查看nn1和nn2的namenode 状态， 你会发现nn1挂掉后，nn2不还是standby状态，没有自动切换为active，需要手动切换为Active</p>
<h3 id="配置HDFS-HA自动故障转移（上面的是手动故障转移，就是需要手动启动某个namenode为active）"><a href="#配置HDFS-HA自动故障转移（上面的是手动故障转移，就是需要手动启动某个namenode为active）" class="headerlink" title="配置HDFS-HA自动故障转移（上面的是手动故障转移，就是需要手动启动某个namenode为active）"></a>配置HDFS-HA自动故障转移（上面的是手动故障转移，就是需要手动启动某个namenode为active）</h3><p>1）具体配置</p>
<p>​         （1）在hdfs-site.xml中增加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</div><div class="line">	&lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
   <property>            <name>dfs.ha.automatic-failover.enabled</name>            <value>true</value>   </property>   

<p>​         （2）在core-site.xml文件中增加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</div><div class="line">	&lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
   <property>            <name>ha.zookeeper.quorum</name>            <value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>   </property>   

<p>2）启动</p>
<p>​         （1）关闭所有HDFS服务：</p>
<p>​                 sbin/stop-dfs.sh</p>
<p>​         （2）启动Zookeeper集群：</p>
<p>​                 bin/zkServer.sh start</p>
<p>​         （3）初始化HA在Zookeeper中状态：</p>
<p>​                 bin/hdfs zkfc -formatZK</p>
<p>​         （4）启动HDFS服务：</p>
<p>​                 sbin/start-dfs.sh</p>
<p>​         （5）在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode</p>
<p>​                 sbin/hadoop-daemin.sh start zkfc</p>
<p>3）验证</p>
<p>​         （1）将Active NameNode进程kill</p>
<p>​                 kill -9 namenode的进程id</p>
<p>​         （2）将Active NameNode机器断开网络</p>
<p>​                 service network stop</p>
<h2 id="YARN-高可用配置"><a href="#YARN-高可用配置" class="headerlink" title="YARN-高可用配置"></a>YARN-高可用配置</h2><h3 id="YARN-HA工作机制"><a href="#YARN-HA工作机制" class="headerlink" title="YARN-HA工作机制"></a>YARN-HA工作机制</h3><p>1）官方文档：</p>
<p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html</a></p>
<p>2）YARN-HA工作机制</p>
<p><img src="/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/C1560172645503.png" alt="1560172645503"></p>
<h3 id="配置YARN-HA集群（也就是开两台resourcemanager）"><a href="#配置YARN-HA集群（也就是开两台resourcemanager）" class="headerlink" title="配置YARN-HA集群（也就是开两台resourcemanager）"></a>配置YARN-HA集群（也就是开两台resourcemanager）</h3><p>0）环境准备</p>
<p>（1）修改IP</p>
<p>（2）修改主机名及主机名和IP地址的映射</p>
<p>（3）关闭防火墙</p>
<p>（4）ssh免密登录</p>
<p>（5）安装JDK，配置环境变量等</p>
<p>​         （6）配置Zookeeper集群</p>
<p>1）规划集群</p>
<p>hadoop102                              hadoop103                             hadoop104               </p>
<p>NameNode                                NameNode</p>
<p>JournalNode                             JournalNode                             JournalNode            </p>
<p>DataNode                                  DataNode                                  DataNode                 </p>
<p>ZK                                              ZK                                              ZK</p>
<p>ResourceManager                    ResourceManager</p>
<p>NodeManager                           NodeManager                          NodeManager         </p>
<p>2）具体配置</p>
<p>（1）yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line"></div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!--启用resourcemanager ha--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</div><div class="line">        &lt;value&gt;true&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"> </div><div class="line">    &lt;!--声明两台resourcemanager的地址--&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</div><div class="line">        &lt;value&gt;cluster-yarn1&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</div><div class="line">        &lt;value&gt;rm1,rm2&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</div><div class="line">        &lt;value&gt;hadoop102&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</div><div class="line">        &lt;value&gt;hadoop103&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"> </div><div class="line">    &lt;!--指定zookeeper集群的地址--&gt; </div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</div><div class="line">        &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"></div><div class="line">    &lt;!--启用自动恢复 – 当resourcemanager碟机后自动重启--&gt; </div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</div><div class="line">        &lt;value&gt;true&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line"> </div><div class="line">    &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; </div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;     &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
   <configuration>           <property>             <name>yarn.nodemanager.aux-services</name>           <value>mapreduce_shuffle</value>       </property>           <!--启用resourcemanager   ha-->       <property>             <name>yarn.resourcemanager.ha.enabled</name>           <value>true</value>       </property>           <!--声明两台resourcemanager的地址-->       <property>           <name>yarn.resourcemanager.cluster-id</name>             <value>cluster-yarn1</value>       </property>           <property>             <name>yarn.resourcemanager.ha.rm-ids</name>           <value>rm1,rm2</value>       </property>           <property>           <name>yarn.resourcemanager.hostname.rm1</name>           <value>hadoop102</value>       </property>           <property>             <name>yarn.resourcemanager.hostname.rm2</name>           <value>hadoop103</value>       </property>           <!--指定zookeeper集群的地址-->        <property>           <name>yarn.resourcemanager.zk-address</name>             <value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>       </property>           <!--启用自动恢复 – 当resourcemanager碟机后自动重启-->        <property>             <name>yarn.resourcemanager.recovery.enabled</name>           <value>true</value>       </property>           <!--指定resourcemanager的状态信息存储在zookeeper集群-->        <property>             <name>yarn.resourcemanager.store.class</name>       <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>   </property>       </configuration>   

<p>​         （2）同步更新其他节点的配置信息</p>
<p>3）启动hdfs </p>
<p>（1）在各个JournalNode节点上，输入以下命令启动journalnode服务：</p>
<p>​         sbin/hadoop-daemon.sh start journalnode</p>
<p>（2）在[nn1]上，对其进行格式化，并启动：</p>
<p>​         bin/hdfs namenode -format</p>
<p>​         sbin/hadoop-daemon.sh start namenode</p>
<p>（3）在[nn2]上，同步nn1的元数据信息：</p>
<p>​         bin/hdfs namenode -bootstrapStandby</p>
<p>（4）启动[nn2]：</p>
<p>​         sbin/hadoop-daemon.sh start namenode</p>
<p>（5）启动所有datanode</p>
<p>​         sbin/hadoop-daemons.sh start datanode</p>
<p>（6）将[nn1]切换为Active</p>
<p>​         bin/hdfs haadmin -transitionToActive nn1</p>
<p>4）启动yarn </p>
<p>（1）在hadoop102中执行：</p>
<p>sbin/start-yarn.sh</p>
<p>（2）在hadoop103中执行：</p>
<p>sbin/yarn-daemon.sh start resourcemanager</p>
<p>（3）查看服务状态</p>
<p>bin/yarn rmadmin -getServiceState rm1</p>
<p><img src="/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/5C1560172700630.png" alt="1560172700630"></p>
<h2 id="HDFS-Federation架构设计"><a href="#HDFS-Federation架构设计" class="headerlink" title="HDFS Federation架构设计"></a>HDFS Federation架构设计</h2><p>1）  NameNode架构的局限性</p>
<p>（1）Namespace（命名空间）的限制</p>
<p>由于NameNode在内存中存储所有的元数据（metadata），因此单个namenode所能存储的对象（文件+块）数目受到namenode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个datanode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个datanode从4T增长到36T，集群的尺寸增长到8000个datanode。存储的需求从12PB增长到大于100PB。</p>
<p>（2）隔离问题</p>
<p>由于HDFS仅有一个namenode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。</p>
<p>​         （3）性能的瓶颈</p>
<p>​         由于是单个namenode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个namenode的吞吐量。</p>
<p>2）HDFS Federation架构设计</p>
<p>能不能有多个NameNode</p>
<p>NameNode                                         NameNode                                         NameNode</p>
<p>元数据                                                元数据                                                元数据</p>
<p>Log                                                     machine                                             电商数据/话单数据</p>
<p><img src="/2018/03/12/hadoop大数据-八-namenode和resourcemanager高可用/5C1560172721474.png" alt="1560172721474"></p>
<p>3）HDFS Federation应用思考</p>
<p>不同应用可以使用不同NameNode进行数据管理</p>
<p>​          图片业务、爬虫业务、日志审计业务</p>
<p>Hadoop生态系统中，不同的框架使用不同的namenode进行管理namespace。（隔离性）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;HDFS-高可用&quot;&gt;&lt;a href=&quot;#HDFS-高可用&quot; class=&quot;headerlink&quot; title=&quot;HDFS 高可用&quot;&gt;&lt;/a&gt;HDFS 高可用&lt;/h1&gt;&lt;h2 id=&quot;高可用概述&quot;&gt;&lt;a href=&quot;#高可用概述&quot; class=&quot;headerlink
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="http://kingge.top/tags/HDFS/"/>
    
      <category term="hadoop高可用" scheme="http://kingge.top/tags/hadoop%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(七)-HDFS的Namenode和Datanode</title>
    <link href="http://kingge.top/2018/03/10/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%B8%83-HDFS%E7%9A%84Namenode%E5%92%8CDatanode/"/>
    <id>http://kingge.top/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/</id>
    <published>2018-03-10T07:38:59.000Z</published>
    <updated>2019-06-10T12:56:33.609Z</updated>
    
    <content type="html"><![CDATA[<h1 id="五-NameNode工作机制"><a href="#五-NameNode工作机制" class="headerlink" title="五 NameNode工作机制"></a>五 NameNode工作机制</h1><h2 id="5-1-NameNode-amp-Secondary-NameNode工作机制"><a href="#5-1-NameNode-amp-Secondary-NameNode工作机制" class="headerlink" title="5.1 NameNode&amp;Secondary NameNode工作机制"></a>5.1 NameNode&amp;Secondary NameNode工作机制</h2><p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/clip_image002.png" alt="img"></p>
<h3 id="1）第一阶段：namenode启动"><a href="#1）第一阶段：namenode启动" class="headerlink" title="1）第一阶段：namenode启动"></a>1）第一阶段：namenode启动</h3><p>（1）第一次启动namenode格式化后，创建fsimage和edits文件<strong>。如果不是第一次启动，直接加载编辑日志和镜像文件到内存</strong>（初始化系统为上一次退出时的最新状态）。</p>
<p>（2）客户端对元数据进行增删改的请求</p>
<p>（3）namenode记录操作日志，更新滚动日志。</p>
<p>（4）namenode在内存中对数据进行增删改查</p>
<p><strong><em>对于namenode而言最新的操作日志是 edits.in.progress(正在执行的日志)</em></strong></p>
<h3 id="2）第二阶段：Secondary-NameNode工作"><a href="#2）第二阶段：Secondary-NameNode工作" class="headerlink" title="2）第二阶段：Secondary NameNode工作"></a><strong>2）第二阶段：Secondary NameNode工作</strong></h3><blockquote>
<p>   <strong>核心工作：检查是否需要合并namenode的编辑日志和镜像文件（checkpoint）</strong></p>
</blockquote>
<p>​         （1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。<strong>定时时间默认1小时，edits默认一百万次</strong></p>
<p>​         （2）Secondary NameNode请求执行checkpoint。（是否需要合并两个文件）</p>
<p>​         （3）namenode滚动正在写的edits日志（<strong>edits.in.progress</strong>）</p>
<p>​         （4）将<strong>滚动前的编辑日志和镜像文件拷贝</strong>到Secondary NameNode</p>
<p>​         （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p>
<p>​         （6）生成新的镜像文件fsimage.chkpoint</p>
<p>​         （7）拷贝fsimage.chkpoint到namenode</p>
<p>​         （8）namenode将fsimage.chkpoint重新命名成fsimage</p>
<blockquote>
<p><strong>也就是说，secondarynamenode的主要作用是帮助namenode分担他的压力，主要是帮助namenode合并镜像和操作日志，合并后，推给namenode。</strong></p>
</blockquote>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">正如上面所分析的，Hadoop文件系统会出现编辑日志（edits）不断增长的情况，尽管在NameNode运行期间不会对文件系统造成影响，但是如果NameNode重新启动，它将会花费大量的时间运行编辑日志中的每个操作，在此期间也就是我们前面所说的安全模式下，文件系统是不可用的。</div><div class="line"></div><div class="line">为了解决上述问题，Hadoop会运行一个Secondary NameNode进程，它的任务就是为原NameNode内存中的文件系统元数据产生检查点。其实说白了，就是辅助NameNode来处理fsimage文件与edits文件的一个进程。它从NameNode中复制fsimage与edits到临时目录并定期合并成一个新的fsimage并且删除原来的编辑日志edits。具体 步骤如下：</div><div class="line"></div><div class="line">（1）Secondary NameNode首先请求原NameNode进行edits的滚动，这样会产生一个新的编辑日志文件edits来保存对文件系统的操作（例如：上传新文件，删除文件，修改文件）。</div><div class="line"></div><div class="line">（2）Secondary NameNode通过Http方式读取原NameNode中的fsimage及edits。</div><div class="line"></div><div class="line">（3）Secondary NameNode将fsimage及edits进行合并产生新的fsimage</div><div class="line"></div><div class="line">（4）Secondary NameNode通过Http方式将新生成的fsimage发送到原来的NameNode中</div><div class="line"></div><div class="line">（5）原NameNode用新生成的fsimage替换掉旧的fsimage文件，新生成的edits文件也就是（1）生成的滚动编辑日志文件替换掉之前的edits文件</div></pre></td></tr></table></figure>
<h3 id="3）web端访问SecondaryNameNode"><a href="#3）web端访问SecondaryNameNode" class="headerlink" title="3）web端访问SecondaryNameNode"></a><strong>3）web端访问SecondaryNameNode</strong></h3><p>​         （1）启动集群</p>
<p>​         （2）浏览器中输入：<a href="http://hadoop102:50090/status.html" target="_blank" rel="external">http://hadoop102:50090/status.html</a></p>
<p>​         （3）查看SecondaryNameNode信息</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/clip_image0024.jpg" alt="img"></p>
<h3 id="4）chkpoint检查时间参数设置"><a href="#4）chkpoint检查时间参数设置" class="headerlink" title="4）chkpoint检查时间参数设置"></a><strong>4）chkpoint检查时间参数设置</strong></h3><p>（1）通常情况下，SecondaryNameNode每隔一小时执行一次。</p>
<p>​         [hdfs-default.xml]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</div><div class="line">  &lt;value&gt;3600&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>（2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt;</div><div class="line">  &lt;value&gt;1000000&lt;/value&gt;</div><div class="line">&lt;description&gt;操作动作次数&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt;</div><div class="line">  &lt;value&gt;60&lt;/value&gt;</div><div class="line">&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<h2 id="5-2-镜像文件和编辑日志文件"><a href="#5-2-镜像文件和编辑日志文件" class="headerlink" title="5.2 镜像文件和编辑日志文件"></a>5.2 镜像文件和编辑日志文件</h2><h3 id="1）概念"><a href="#1）概念" class="headerlink" title="1）概念"></a>1）概念</h3><p>​         namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件</p>
<p>   edits_0000000000000000000   fsimage_0000000000000000000.md5   seen_txid   VERSION   </p>
<p>（1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。 </p>
<p>（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。 </p>
<p>（3）seen<em>txid文件保存的是一个数字，就是最后一个edits</em>的数字（最后一次操作的序号）</p>
<p>（4）每次Namenode启动的时候都会将fsimage文件读入内存，并从00001开始到seen_txid中记录的数字依次执行每个edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，<strong>可以看成Namenode启动的时候就将fsimage和edits文件进行了合并。</strong></p>
<h3 id="2）oiv查看fsimage文件"><a href="#2）oiv查看fsimage文件" class="headerlink" title="2）oiv查看fsimage文件"></a>2）oiv查看fsimage文件</h3><p>（1）查看oiv和oev命令</p>
<p>[kingge@hadoop102 current]$ hdfs</p>
<p>oiv                  apply the offline fsimage viewer to an fsimage</p>
<p>oev                  apply the offline edits viewer to an edits file</p>
<p>（2）基本语法</p>
<p>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</p>
<p>（3）案例实操</p>
<p>[kingge@hadoop102 current]$ pwd</p>
<p>/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current</p>
<p>[kingge@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml</p>
<p>[kingge@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml</p>
<p>将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/clip_imag4e002.jpg" alt="img"></p>
<p>​        <strong>总结</strong></p>
<p>查看XML你会发现，里面存储了HDFS中文件或者文件夹的创建日期，权限，名称等等元数据信息。<strong>但是并没有存储文件保存的位置，也就是：并没有发现文件存储的DataNode节点信息</strong>信息那么当客户端请求读数据的时候，namenode是怎么返回数据所在块信息呢？<strong>原来他会一直跟datanode进行交互，获取数据所在块信息。</strong></p>
<h3 id="3）oev查看edits文件"><a href="#3）oev查看edits文件" class="headerlink" title="3）oev查看edits文件"></a>3）oev查看edits文件</h3><p>edits包括两类，edits_XXX,edits_inprogress_XXX</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">edits_XXX：保存文件系统的操作，查看方式见下面语法</div><div class="line"></div><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">-&lt;EDITS&gt;</div><div class="line">&lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt;</div><div class="line">-&lt;RECORD&gt;</div><div class="line">&lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt;</div><div class="line">-&lt;DATA&gt;</div><div class="line">&lt;TXID&gt;7170&lt;/TXID&gt;</div><div class="line">&lt;/DATA&gt;</div><div class="line">&lt;/RECORD&gt;</div><div class="line">-&lt;RECORD&gt;</div><div class="line">&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;</div><div class="line">-&lt;DATA&gt;</div><div class="line">&lt;TXID&gt;7171&lt;/TXID&gt;</div><div class="line">&lt;LENGTH&gt;0&lt;/LENGTH&gt;</div><div class="line">&lt;INODEID&gt;17787&lt;/INODEID&gt;</div><div class="line">&lt;PATH&gt;/user/zpx/a.txt&lt;/PATH&gt;</div><div class="line">&lt;REPLICATION&gt;3&lt;/REPLICATION&gt;</div><div class="line">&lt;MTIME&gt;1489118864779&lt;/MTIME&gt;</div><div class="line">&lt;ATIME&gt;1489118864779&lt;/ATIME&gt;</div><div class="line">&lt;BLOCKSIZE&gt;数据的大小&lt;/BLOCKSIZE&gt;</div><div class="line">&lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_1295720148_1&lt;/CLIENT_NAME&gt;</div><div class="line">&lt;CLIENT_MACHINE&gt;192.168.231.1&lt;/CLIENT_MACHINE&gt;</div><div class="line">&lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt;</div><div class="line">-&lt;PERMISSION_STATUS&gt;</div><div class="line">&lt;USERNAME&gt;Administrator&lt;/USERNAME&gt;</div><div class="line">&lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt;</div><div class="line">&lt;MODE&gt;420&lt;/MODE&gt;</div><div class="line">&lt;/PERMISSION_STATUS&gt;</div><div class="line">&lt;RPC_CLIENTID&gt;0c9a5af9-26a8-45d9-8754-cd0e7e47f65b&lt;/RPC_CLIENTID&gt;</div><div class="line">&lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt;</div><div class="line">&lt;/DATA&gt;</div><div class="line">&lt;/RECORD&gt;</div><div class="line">&lt;/EDITS&gt;</div><div class="line"></div><div class="line">对Hdfs文件系统的每一个操作都保存在了edits文件中，每一个操作都是事务，有事务id——&lt;TXID&gt;7171&lt;/TXID&gt;，还有当前操作做了什么&lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt;，副本数，以及大小</div><div class="line"></div><div class="line">edits_inprogress_XXX：正在使用的过程，当前正在向前滚动。查看方式见下面语法</div></pre></td></tr></table></figure>
<p>（1）基本语法</p>
<p>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</p>
<p>（2）案例实操</p>
<p>[kingge@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</p>
<p>[kingge@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml</p>
<p>将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。</p>
<p>总结</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/clip_imag9e002.jpg" alt="img"></p>
<p>你会发现，edits_inprogress，记录的是当前客户端请求执行的操作（增量记录当前操作）</p>
<h2 id="5-3-滚动编辑日志"><a href="#5-3-滚动编辑日志" class="headerlink" title="5.3 滚动编辑日志"></a>5.3 滚动编辑日志</h2><p>正常情况HDFS文件系统有更新操作时，就会滚动编辑日志。也可以用命令强制滚动编辑日志。</p>
<p>1）滚动编辑日志（前提必须启动集群）</p>
<p>[kingge@hadoop102 current]$ hdfs dfsadmin -rollEdits</p>
<p>2）镜像文件什么时候产生</p>
<p>Namenode启动时加载镜像文件和编辑日志</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/169952390.png" alt="1560169952390"></p>
<h2 id="5-4-Namenode版本号"><a href="#5-4-Namenode版本号" class="headerlink" title="5.4 Namenode版本号"></a>5.4 Namenode版本号</h2><p>1）查看namenode版本号</p>
<p>在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current这个目录下查看VERSION</p>
<p>namespaceID=1933630176</p>
<p>clusterID=CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175</p>
<p>cTime=0</p>
<p>storageType=NAME_NODE</p>
<p>blockpoolID=BP-97847618-192.168.10.102-1493726072779</p>
<p>layoutVersion=-63</p>
<p>2）namenode版本号具体解释</p>
<p>（1） namespaceID在HDFS上，会有多个Namenode，所以不同Namenode的namespaceID是不同的，分别管理一组blockpoolID。</p>
<p>（2）clusterID集群id，全局唯一</p>
<p>（3）cTime属性标记了namenode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。</p>
<p>（4）storageType属性说明该存储目录包含的是namenode的数据结构。</p>
<p>（5）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。</p>
<p>（6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。</p>
<h2 id="5-5-SecondaryNameNode目录结构"><a href="#5-5-SecondaryNameNode目录结构" class="headerlink" title="5.5 SecondaryNameNode目录结构"></a>5.5 SecondaryNameNode目录结构</h2><p>Secondary NameNode用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/0170177897.png" alt="1560170177897"></p>
<blockquote>
<p><strong>也即是说存在两种情况secondarynamenode会向namenode请求合并镜像文件和日志文件。（1）当上次请求时间已经间隔了一个小时后，会去请求（2）当操作数（edits，操作日志数）到达一百万次时，会去请求</strong><br><strong>那么他怎么知道操作次数到达一百万次呢？答案是，一分钟请求namenode一次，查询操作次数是否到达一百万次。注意，这个检查操作数的时间设置最好不要跟</strong><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/clip_image001.png" alt="img"></p>
</blockquote>
<p>一致，不然他会默认执行第一种场景（间隔一个小时）</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/70270685.png" alt="1560170270685"></p>
<p>在/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/current这个目录中查看SecondaryNameNode目录结构。</p>
<p>   edits_0000000000000000001-0000000000000000002   fsimage_0000000000000000002   fsimage_0000000000000000002.md5   VERSION   </p>
<p>SecondaryNameNode的namesecondary/current目录和主namenode的current目录的布局相同。</p>
<p><strong>好处：在主namenode**</strong>发生故障时（假设没有及时备份数据），可以从SecondaryNameNode<strong>**恢复数据。</strong></p>
<h3 id="根据secondarynamenode恢复namenode"><a href="#根据secondarynamenode恢复namenode" class="headerlink" title="根据secondarynamenode恢复namenode"></a>根据secondarynamenode恢复namenode</h3><p>方法一：将SecondaryNameNode中数据拷贝到namenode存储数据的目录；</p>
<p>方法二：使用-importCheckpoint选项启动namenode守护进程，从而将SecondaryNameNode中数据拷贝到namenode目录中。</p>
<p>1）案例实操（一）：</p>
<p>模拟namenode故障，并采用方法一，恢复namenode数据</p>
<p>（1）kill -9 namenode进程</p>
<p>（2）删除namenode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</p>
<p>（3）拷贝SecondaryNameNode中数据到原namenode存储数据目录</p>
<p>​           [kingge@hadoop102 hadoop-2.7.2]$ scp -R /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* /opt/module/hadoop-2.7.2/data/tmp/dfs/name/</p>
<p>（4）重新启动namenode</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</p>
<p>2）案例实操（二）：</p>
<p>模拟namenode故障，并采用方法二，恢复namenode数据</p>
<p>（0）修改hdfs-site.xml中的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</div><div class="line">  &lt;value&gt;120&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">#  120秒checkpoint一次</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</div><div class="line">  &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"># namenode镜像文件和操作日志存放目录</div></pre></td></tr></table></figure>
<p>（1）kill -9 namenode进程</p>
<p>（2）删除namenode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</p>
<p>（3）如果SecondaryNameNode不和Namenode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到Namenode存储数据的平级目录。Scp命令拷贝过来</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/70458431.png" alt="1560170458431"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">  </div><div class="line">[kingge@hadoop102 dfs]$ pwd</div><div class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</div><div class="line">[kingge@hadoop102 dfs]$ ls</div><div class="line">data  name  namesecondary</div></pre></td></tr></table></figure>
<p>（4）导入检查点数据（等待一会ctrl+c结束掉）</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</p>
<p>（5）启动namenode</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</p>
<p>（6）如果提示文件锁了，可以删除in_use.lock </p>
<p>​                 [kingge@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/in_use.lock</p>
<h2 id="5-5-5-设置checkpoint检查时间"><a href="#5-5-5-设置checkpoint检查时间" class="headerlink" title="5.5.5 设置checkpoint检查时间"></a>5.5.5 设置checkpoint检查时间</h2><p>默认的checkpoint period是1个小时。可以去hdfs-site.xml中修改</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/0170515392.png" alt="1560170515392"></p>
<h2 id="5-6-集群安全模式操作"><a href="#5-6-集群安全模式操作" class="headerlink" title="5.6 集群安全模式操作"></a>5.6 集群安全模式操作</h2><p>1）概述</p>
<p>Namenode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的fsimage文件和一个空的编辑日志。此时，namenode开始监听datanode请求。但是此刻，namenode运行在安全模式，即namenode的文件系统对于客户端来说是只读的。（<strong>可以解释为什么在namenode启动的时候，我们put数据到hdfs会提示，安全模式错误</strong>）因为这个时候namenode和datanode还没有联通对方，需要等待连通后，安全模式自动关闭，然后就可以上传文件了</p>
<p>系统中的数据块的位置并不是由namenode维护的，而是以块列表的形式存储在datanode中。在系统的正常操作期间，namenode会在内存中保留所有块位置的映射信息。在安全模式下，各个datanode会向namenode发送最新的块列表信息，namenode了解到足够多的块位置信息之后，即可高效运行文件系统。</p>
<p>如果满足“最小副本条件”，namenode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以namenode不会进入安全模式。</p>
<p>2）基本语法</p>
<p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p>
<p>（1）bin/hdfs dfsadmin -safemode get          （功能描述：查看安全模式状态）</p>
<p>（2）bin/hdfs dfsadmin -safemode enter     （功能描述：进入安全模式状态）</p>
<p>（3）bin/hdfs dfsadmin -safemode leave      （功能描述：离开安全模式状态）</p>
<p>（4）bin/hdfs dfsadmin -safemode wait        （功能描述：等待安全模式状态）</p>
<p>3）案例</p>
<p>​         模拟等待安全模式</p>
<p>​         1）先进入安全模式</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter</p>
<p>​         2）执行下面的脚本</p>
<p>编辑一个脚本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line">bin/hdfs dfsadmin -safemode wait</div><div class="line">bin/hdfs dfs -put ~/hello.txt /root/hello.txt</div></pre></td></tr></table></figure>
<p>​         3）再打开一个窗口，执行</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave</p>
<h2 id="5-7-Namenode多目录配置"><a href="#5-7-Namenode多目录配置" class="headerlink" title="5.7 Namenode多目录配置"></a>5.7 Namenode多目录配置</h2><p>1）namenode的本地目录可以配置成多个，<strong>且每个目录存放内容相同</strong>，增加了可靠性。</p>
<p>2）具体配置如下：</p>
<p>​         hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</div><div class="line">&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line"></div><div class="line"> 2.停止集群，删除数据文件 --  rm -rf data/logs (集群里有多少台服务器就删除多少台)</div><div class="line"></div><div class="line">3.格式化namenode</div><div class="line"></div><div class="line">4.调用xsync 分发脚本到各个集群 </div><div class="line"></div><div class="line">5.启动集群</div><div class="line"></div><div class="line">6.查看设置的本地目录name1、name2 你会发现里面的数据一模一样</div></pre></td></tr></table></figure>
<h2 id="测试NameNode"><a href="#测试NameNode" class="headerlink" title="测试NameNode"></a>测试NameNode</h2><p>场景：关闭namenode（stop-dfs.sh），关闭yarn（stop-yarn.sh），删除hadoop目录下的data目录和log目录。</p>
<p>1.格式化namenode  – bin/hdfs namenode -format</p>
<p>2.启动hdfs和yarn</p>
<h1 id="六-DataNode工作机制"><a href="#六-DataNode工作机制" class="headerlink" title="六 DataNode工作机制"></a>六 DataNode工作机制</h1><h2 id="6-1-DataNode工作机制"><a href="#6-1-DataNode工作机制" class="headerlink" title="6.1 DataNode工作机制"></a>6.1 DataNode工作机制</h2><p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/170744123.png" alt="1560170744123"></p>
<p>1）一个数据块在datanode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p>
<p>2）DataNode启动后向namenode注册，通过后，周期性（1小时）的向namenode上报所有的块信息。</p>
<p>3）心跳是每3秒一次，心跳返回结果带有namenode给该datanode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个datanode的心跳，则认为该节点不可用。</p>
<p>4）<strong>集群运行中可以安全加入和退出一些机器</strong>（在不关闭集群的情况下服役和退役服务器）</p>
<h2 id="6-2-数据完整性"><a href="#6-2-数据完整性" class="headerlink" title="6.2 数据完整性"></a>6.2 数据完整性</h2><p>1）当DataNode读取block的时候，它会计算checksum</p>
<p>2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。</p>
<p>3）client读取其他DataNode上的block。</p>
<p>4）datanode在其文件创建后周期验证checksum</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/0170775293.png" alt="1560170775293"></p>
<h2 id="6-3-掉线时限参数设置"><a href="#6-3-掉线时限参数设置" class="headerlink" title="6.3 掉线时限参数设置"></a>6.3 掉线时限参数设置</h2><p>datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：</p>
<p>​         timeout  = 2 <em> dfs.namenode.heartbeat.recheck-interval + 10 </em> dfs.heartbeat.interval。</p>
<p>​         而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p>
<p>​         需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt;</div><div class="line">    &lt;value&gt;300000&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt; dfs.heartbeat.interval &lt;/name&gt;</div><div class="line">    &lt;value&gt;3&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<h2 id="6-4-DataNode的目录结构"><a href="#6-4-DataNode的目录结构" class="headerlink" title="6.4 DataNode的目录结构"></a>6.4 DataNode的目录结构</h2><p>和namenode不同的是，datanode的存储目录是初始阶段自动创建的，不需要额外格式化。</p>
<p>1）在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current这个目录下查看版本号</p>
<p>[kingge@hadoop102 current]$ cat VERSION </p>
<p>storageID=DS-1b998a1d-71a3-43d5-82dc-c0ff3294921b</p>
<p>clusterID=CID-1f2bf8d1-5ad2-4202-af1c-6713ab381175</p>
<p>cTime=0</p>
<p>datanodeUuid=970b2daf-63b8-4e17-a514-d81741392165</p>
<p>storageType=DATA_NODE</p>
<p>layoutVersion=-56</p>
<p>2）具体解释</p>
<p>​         （1）storageID：存储id号</p>
<p>​         （2）clusterID集群id，全局唯一</p>
<p>​         （3）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。</p>
<p>​         （4）datanodeUuid：datanode的唯一识别码</p>
<p>​         （5）storageType：存储类型</p>
<p>​         （6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。</p>
<p>3）在/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-97847618-192.168.10.102-1493726072779/current这个目录下查看该数据块的版本号</p>
<p>[kingge@hadoop102 current]$ cat VERSION </p>
<p>#Mon May 08 16:30:19 CST 2017</p>
<p>namespaceID=1933630176</p>
<p>cTime=0</p>
<p>blockpoolID=BP-97847618-192.168.10.102-1493726072779</p>
<p>layoutVersion=-56</p>
<p>4）具体解释</p>
<p>（1）namespaceID：是datanode首次访问namenode的时候从namenode处获取的storageID对每个datanode来说是唯一的（但对于单个datanode中所有存储目录来说则是相同的），namenode可用这个属性来区分不同datanode。</p>
<p>（2）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。</p>
<p>（3）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。</p>
<p>（4）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。</p>
<h2 id="6-5-服役新数据节点"><a href="#6-5-服役新数据节点" class="headerlink" title="6.5 服役新数据节点"></a>6.5 服役新数据节点</h2><p>0）需求：</p>
<p>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。</p>
<p>1）环境准备</p>
<p>​         （1）克隆一台虚拟机</p>
<p>​         （2）修改ip地址和主机名称</p>
<p>​         （3）修改xcall和xsync文件，增加新`增节点的同步ssh</p>
<p>​         （4）删除原来HDFS文件系统留存的文件</p>
<p>​                 /opt/module/hadoop-2.7.2/data 和 /opt/module/hadoop-2.7.2/log目录</p>
<p>2）服役新节点具体步骤（<strong>下面的操作建议在namenode所在节点进行操作</strong>）</p>
<p>​         （1）在<strong>namenode</strong>的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件</p>
<p>[kingge@hadoop105 hadoop]$ pwd</p>
<p>/opt/module/hadoop-2.7.2/etc/hadoop</p>
<p>[kingge@hadoop105 hadoop]$ touch dfs.hosts （<strong>名字任意</strong>）</p>
<p>[kingge@hadoop105 hadoop]$ vi dfs.hosts</p>
<p>添加如下主机名称（包含新服役的节点）</p>
<p>hadoop102</p>
<p>hadoop103</p>
<p>hadoop104</p>
<p>hadoop105</p>
<p>​         （2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;dfs.hosts&lt;/name&gt;</div><div class="line">      &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>​         （3）刷新namenode </p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</p>
<p>Refresh nodes successful</p>
<p>​         （4）更新resourcemanager节点</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</p>
<p>17/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033</p>
<p><strong>操作完后，打开**</strong>hdfs<strong><strong>文件系统，发现已经服役了一个新的</strong></strong>data<strong>**节点</strong></p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/560171043334.png" alt="1560171043334"></p>
<p>​         （5）在namenode的slaves文件中增加新主机名称</p>
<p>​                 增加105  不需要分发</p>
<p>hadoop102</p>
<p>hadoop103</p>
<p>hadoop104</p>
<p>hadoop105</p>
<p>​         （6）单独命令启动新的数据节点和节点管理器</p>
<p>[kingge@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</p>
<p>starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-datanode-hadoop105.out</p>
<p>[kingge@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager</p>
<p>starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-kingge-nodemanager-hadoop105.out</p>
<p>​         （7）在web浏览器上检查是否ok</p>
<p>3）如果数据不均衡，可以用命令实现集群的再平衡</p>
<p>​         [kingge@hadoop102 sbin]$ ./start-balancer.sh</p>
<p>starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-balancer-hadoop102.out</p>
<p>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</p>
<h2 id="6-6-退役旧数据节点"><a href="#6-6-退役旧数据节点" class="headerlink" title="6.6 退役旧数据节点"></a>6.6 退役旧数据节点</h2><p>1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件</p>
<p>​         [kingge@hadoop102 hadoop]$ pwd</p>
<p>/opt/module/hadoop-2.7.2/etc/hadoop</p>
<p>[kingge@hadoop102 hadoop]$ touch dfs.hosts.exclude</p>
<p>[kingge@hadoop102 hadoop]$ vi dfs.hosts.exclude</p>
<p>添加如下主机名称（要退役的节点）</p>
<p>hadoop105</p>
<p>2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</div><div class="line">      &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>3）刷新namenode、刷新resourcemanager</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</p>
<p>Refresh nodes successful</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</p>
<p>17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033</p>
<p>4）检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点。</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/1560171084624.png" alt="1560171084624"></p>
<p>5）等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役。·</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/560171100904.png" alt="1560171100904"></p>
<p>[kingge@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode</p>
<p>stopping datanode</p>
<p>[kingge@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager</p>
<p>stopping nodemanager</p>
<p>6）从include文件中删除退役节点，再运行刷新节点的命令</p>
<p>​         （1）从namenode的dfs.hosts文件中删除退役节点hadoop105</p>
<p>hadoop102</p>
<p>hadoop103</p>
<p>hadoop104</p>
<p>​         （2）刷新namenode，刷新resourcemanager</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</p>
<p>Refresh nodes successful</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</p>
<p>17/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033</p>
<p>7）从namenode的slave文件中删除退役节点hadoop105</p>
<p>hadoop102</p>
<p>hadoop103</p>
<p>hadoop104</p>
<p>8）如果数据不均衡，可以用命令实现集群的再平衡</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh </p>
<p>starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-kingge-balancer-hadoop102.out</p>
<p>Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</p>
<h2 id="6-7-Datanode多目录配置"><a href="#6-7-Datanode多目录配置" class="headerlink" title="6.7 Datanode多目录配置"></a>6.7 Datanode多目录配置</h2><p>1）datanode也可以配置成多个目录，<strong>每个目录存储的数据不一样，即是上传一个文本，那么文本存储在data，但是data2什么都没有（跟namenode多目录区别）</strong>。即：数据不是副本。</p>
<p>2）具体配置如下：</p>
<p>​         hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</div><div class="line">        &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<h1 id="七-HDFS其他功能"><a href="#七-HDFS其他功能" class="headerlink" title="七 HDFS其他功能"></a>七 HDFS其他功能</h1><h2 id="7-1-集群间数据拷贝"><a href="#7-1-集群间数据拷贝" class="headerlink" title="7.1 集群间数据拷贝"></a>7.1 集群间数据拷贝</h2><p>1）scp实现两个远程主机之间的文件复制</p>
<p>​         scp -r hello.txt <a href="mailto:root@hadoop103:/user/kingge/hello.txt" target="_blank" rel="external">root@hadoop103:/user/kingge/hello.txt</a>                 // 推 push</p>
<p>​         scp -r <a href="mailto:root@hadoop103:/user/kingge/hello.txt%20%20hello.txt" target="_blank" rel="external">root@hadoop103:/user/kingge/hello.txt  hello.txt</a>             // 拉 pull</p>
<p>​         scp -r <a href="mailto:root@hadoop103:/user/kingge/hello.txt" target="_blank" rel="external">root@hadoop103:/user/kingge/hello.txt</a> root@hadoop104:/user/kingge   //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。</p>
<p>2）采用discp命令实现两个hadoop集群之间的递归数据复制</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$  bin/hadoop distcp hdfs://haoop102:9000/user/kingge/hello.txt hdfs://hadoop103:9000/user/kingge/hello.txt</p>
<h2 id="7-2-Hadoop存档"><a href="#7-2-Hadoop存档" class="headerlink" title="7.2 Hadoop存档"></a>7.2 Hadoop存档</h2><p>1）理论概述</p>
<p>每个文件均按块存储，每个块的元数据存储在namenode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽namenode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。</p>
<p>Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少namenode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件可以用作MapReduce的输入。</p>
<p>2）案例实操</p>
<p>（1）需要启动yarn进程</p>
<p>​         [kingge@hadoop102 hadoop-2.7.2]$ start-yarn.sh</p>
<p>（2）归档文件</p>
<p>​         归档成一个叫做xxx.har的文件夹，该文件夹下有相应的数据文件。Xx.har目录是一个整体，该目录看成是一个归档文件即可。</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p /user/kingge   /user/my</p>
<p>（3）查看归档</p>
<p>​         [kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///myhar.har</p>
<p>（4）解归档文件</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har /* /user/kingge</p>
<h2 id="7-3-快照管理"><a href="#7-3-快照管理" class="headerlink" title="7.3 快照管理"></a>7.3 快照管理</h2><p>快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。</p>
<p>1）基本语法</p>
<p>​         （1）hdfs dfsadmin -allowSnapshot 路径   （功能描述：开启指定目录的快照功能）</p>
<p>​         （2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）</p>
<p>​         （3）hdfs dfs -createSnapshot 路径        （功能描述：对目录创建快照）</p>
<p>​         （4）hdfs dfs -createSnapshot 路径 名称   （功能描述：指定名称创建快照）</p>
<p>​         （5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）</p>
<p>​         （6）hdfs lsSnapshottableDir         （功能描述：列出当前用户所有可快照目录）</p>
<p>​         （7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处）</p>
<p>​         （8）hdfs dfs -deleteSnapshot <path></path> <snapshotname>  （功能描述：删除快照）</snapshotname></p>
<p>2）案例实操</p>
<p>​         （1）开启/禁用指定目录的快照功能</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/kingge/data</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/kingge/data</p>
<p>​         （2）对目录创建快照</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/kingge/data      </p>
<p>通过web访问hdfs://hadoop102:9000/user/kingge/data/.snapshot/s…..// 快照和源文件使用相同数据块</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/kingge/data/.snapshot/</p>
<p>​         （3）指定名称创建快照</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/kingge/data miao170508</p>
<p>​         （4）重命名快照</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/kingge/data/ miao170508 kingge170508</p>
<p>​         （5）列出当前用户所有可快照目录</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir</p>
<p>​         （6）比较两个快照目录的不同之处</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/kingge/data/  .  .snapshot/kingge170508 </p>
<p>​         （7）恢复快照</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/kingge/input/.snapshot/s20170708-134303.027 /user</p>
<h2 id="7-4-回收站"><a href="#7-4-回收站" class="headerlink" title="7.4 回收站"></a>7.4 回收站</h2><p>1）默认回收站</p>
<p>默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。</p>
<p>默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。</p>
<p>要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。</p>
<p><img src="/2018/03/10/hadoop大数据-七-HDFS的Namenode和Datanode/60171183235.png" alt="1560171183235"></p>
<p>2）启用回收站</p>
<p>修改core-site.xml，配置垃圾回收时间为1分钟。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</div><div class="line">    &lt;value&gt;1&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>3）查看回收站</p>
<p>回收站在集群中的；路径：/user/kingge/.Trash/….</p>
<p>4）修改访问垃圾回收站用户名称(<strong>如果不修改为想要查看该回收站的用户的名称，那么该用户试图进入回收站时会提示权限问题</strong>)</p>
<p>​         进入垃圾回收站用户名称，默认是dr.who，修改为kingge用户</p>
<p>​         [core-site.xml]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</div><div class="line">  &lt;value&gt;kingge&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>5）通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站</p>
<p>Trash trash = New Trash(conf);</p>
<p>trash.moveToTrash(path);</p>
<p>6）恢复回收站数据</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/kingge/.Trash/Current/user/kingge/input    /user/kingge/input</p>
<p>7）清空回收站（<strong>他并不是真正删除文件，而是生成一个当前时间戳的文件夹然后把回收站里面的文件都放到这个文件夹里面</strong>）</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hdfs dfs -expunge</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;五-NameNode工作机制&quot;&gt;&lt;a href=&quot;#五-NameNode工作机制&quot; class=&quot;headerlink&quot; title=&quot;五 NameNode工作机制&quot;&gt;&lt;/a&gt;五 NameNode工作机制&lt;/h1&gt;&lt;h2 id=&quot;5-1-NameNode-amp-
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="http://kingge.top/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(六)-HDFS的读写数据流程</title>
    <link href="http://kingge.top/2018/03/08/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E5%85%AD-HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B/"/>
    <id>http://kingge.top/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/</id>
    <published>2018-03-08T14:38:59.000Z</published>
    <updated>2019-06-09T04:48:50.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="四-HDFS的数据流"><a href="#四-HDFS的数据流" class="headerlink" title="四 HDFS的数据流"></a>四 HDFS的数据流</h1><h2 id="4-1-HDFS写数据流程"><a href="#4-1-HDFS写数据流程" class="headerlink" title="4.1 HDFS写数据流程"></a>4.1 HDFS写数据流程</h2><h3 id="4-1-1-剖析文件写入"><a href="#4-1-1-剖析文件写入" class="headerlink" title="4.1.1 剖析文件写入"></a>4.1.1 剖析文件写入</h3><p><img src="/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/60054959268.png" alt="1560054959268"></p>
<p>1）客户端通过Distributed FileSystem模块向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。（<strong>存在覆盖，不存在创建</strong>）</p>
<p>2）namenode返回是否可以上传。</p>
<p>3）客户端请求第一个 block上传到哪几个datanode服务器上。</p>
<p>4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。（根据配置文件中指定的<strong>备份数量及机架感知原理</strong>进行文件分配）</p>
<p>5）客户端通过FSDataOutputStream模块请求dn1上传数据（<strong>建立RPC请求</strong>），dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</p>
<p>6）dn1、dn2、dn3逐级应答客户端。-应答成功，开始传输数据</p>
<p>7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet（默认 64K）为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</p>
<p>  <strong>128M他并不是一次性写入dn1</strong>，而是分包的形式，<em>dn1接收到一个包，然后保存在自己所在服务器的本地缓存中</em>。<strong>然后再写入自己磁盘（7_blk_1）的同时，传输给dn2，以此类推。直到传输完整个128M。第一块传输完毕。</strong></p>
<p>8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器。（重复执行3-7步）。</p>
<h3 id="4-1-2-网络拓扑概念"><a href="#4-1-2-网络拓扑概念" class="headerlink" title="4.1.2 网络拓扑概念"></a>4.1.2 网络拓扑概念</h3><p>​         在本地网络中，两个节点被称为“彼此近邻”是什么意思？在海量数据处理中，其主要限制因素是节点之间数据的传输速率——带宽很稀缺。这里的想法是将两个节点间的带宽作为距离的衡量标准。</p>
<p>​         节点距离：<strong>两个节点到达最近的共同祖先的距离总和</strong>。</p>
<p>例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。</p>
<p>Distance(/d1/r1/n1, /d1/r1/n1)=0（同一节点上的进程）</p>
<p>Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点）</p>
<p>Distance(/d1/r1/n1, /d1/r3/n2)=4（同一数据中心不同机架上的节点）</p>
<p>Distance(/d1/r1/n1, /d2/r4/n2)=6（不同数据中心的节点）</p>
<p><img src="/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/055275110.png" alt="1560055275110"></p>
<p>大家算一算每两个节点之间的距离。</p>
<p><img src="/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/clip_image006.jpg" alt="img"></p>
<h3 id="4-1-3-机架感知（副本节点选择）"><a href="#4-1-3-机架感知（副本节点选择）" class="headerlink" title="4.1.3 机架感知（副本节点选择）"></a>4.1.3 机架感知（副本节点选择）</h3><p>  一份数据如果存在三个副本，那么副本存放服务器的选择，应该采取怎么样的策略</p>
<p>1）官方ip地址：</p>
<p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html</a></p>
<p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication</a></p>
<p>2）低版本Hadoop副本节点选择</p>
<p>第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。</p>
<p>第二个副本和第一个副本位于不相同机架的随机节点上。</p>
<p>第三个副本和第二个副本位于相同机架，节点随机。</p>
<p><img src="/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/clip_image008.png" alt="img"></p>
<p>3）Hadoop2.7.2副本节点选择</p>
<p>​         第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。</p>
<p>​         第二个副本和第一个副本位于相同机架，随机节点。</p>
<p>​         第三个副本位于不同机架，随机节点。</p>
<p><img src="/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/clip_image010.png" alt="img"></p>
<p><strong>为什么第一个副本选择在客户端所在的节点，因为这样client**</strong>请求数据的时候，可以做到更快的响应，优先读取当前节点副本信息（设计网络拓扑概念），距离客户端越近的节点，数据传输速率越快**</p>
<p><img src="/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/clip_image011.png" alt="img"></p>
<h2 id="4-2-HDFS读数据流程"><a href="#4-2-HDFS读数据流程" class="headerlink" title="4.2 HDFS读数据流程"></a>4.2 HDFS读数据流程</h2><p><img src="/2018/03/08/hadoop大数据-六-HDFS的读写数据流程/055352768.png" alt="1560055352768"></p>
<p>1）客户端通过Distributed FileSystem向namenode请求下载文件，namenode通过查询元数据，找到文件块所在的datanode地址。</p>
<p>2）挑选一台datanode（就近原则，然后随机）服务器，请求读取数据。</p>
<p>3）datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验）。</p>
<p>4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。</p>
<h2 id="4-3-一致性模型"><a href="#4-3-一致性模型" class="headerlink" title="4.3 一致性模型"></a>4.3 一致性模型</h2><p>1）debug调试如下代码</p>
<p>   @Test            public void writeFile() throws   Exception{                    // 1 创建配置信息对象                    Configuration configuration =   new Configuration();                    fs = FileSystem.get(configuration);                                        // 2 创建文件输出流                    Path path = new   Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”);                    FSDataOutputStream fos =   fs.create(path);                                        // 3 写数据                    fos.write(“hello”.getBytes());           // 4 一致性刷新                    fos.hflush();                                        fos.close();            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">	public void writeFile() throws Exception&#123;</div><div class="line">		// 1 创建配置信息对象</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		fs = FileSystem.get(configuration);</div><div class="line">		</div><div class="line">		// 2 创建文件输出流</div><div class="line">		Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;);</div><div class="line">		FSDataOutputStream fos = fs.create(path);</div><div class="line">		</div><div class="line">		// 3 写数据</div><div class="line">		fos.write(&quot;hello&quot;.getBytes());</div><div class="line">        // 4 一致性刷新</div><div class="line">		fos.hflush();</div><div class="line">		</div><div class="line">		fos.close();</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>2）总结</p>
<p>写入数据时，如果希望数据被其他client立即可见，调用如下方法</p>
<p>FsDataOutputStream. hflush ();              //清理客户端缓冲区数据，被其他client立即可见</p>
<p><strong>因为传统的流操作，只有在关闭流的时候，才会去执行flush**</strong>操作，那么可能在关闭流之前发生错误，导致数据没有存储到对应的节点。为了避免这种问题，可以手动显式的执行flush<strong>**写入磁盘操作。（IOUtils. copyBytes</strong> <strong>内部已经实现刷新机制，不需要手动刷新）</strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;四-HDFS的数据流&quot;&gt;&lt;a href=&quot;#四-HDFS的数据流&quot; class=&quot;headerlink&quot; title=&quot;四 HDFS的数据流&quot;&gt;&lt;/a&gt;四 HDFS的数据流&lt;/h1&gt;&lt;h2 id=&quot;4-1-HDFS写数据流程&quot;&gt;&lt;a href=&quot;#4-1-HDFS
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="http://kingge.top/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(五)-HDFS概念和基本操作</title>
    <link href="http://kingge.top/2018/03/06/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%BA%94-HDFS%E6%A6%82%E5%BF%B5%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://kingge.top/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/</id>
    <published>2018-03-06T12:31:59.000Z</published>
    <updated>2019-06-09T04:01:11.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-HDFS简单介绍"><a href="#一-HDFS简单介绍" class="headerlink" title="一 HDFS简单介绍"></a>一 HDFS简单介绍</h1><h2 id="1-1-HDFS产生背景"><a href="#1-1-HDFS产生背景" class="headerlink" title="1.1 HDFS产生背景"></a>1.1 HDFS产生背景</h2><p>随着数据量越来越大，在一个操作系统管辖的范围内存不下了，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。</p>
<h2 id="1-2-HDFS概念"><a href="#1-2-HDFS概念" class="headerlink" title="1.2 HDFS概念"></a>1.2 HDFS概念</h2><p><strong>HDFS**</strong>，它是一个文件系统<strong>，用于存储文件，通过目录树来定位文件；</strong>其次，它是分布式的**，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>HDFS的设计适合<strong>一次写入，多次读出</strong>的场景，且<strong>不支持文件的修改</strong>（也是可以修改的，但是不建议）。适合用来做数据分析，并不适合用来做网盘应用。</p>
<h2 id="1-3-HDFS-优缺点"><a href="#1-3-HDFS-优缺点" class="headerlink" title="1.3 HDFS 优缺点"></a>1.3 HDFS 优缺点</h2><h3 id="1-3-1-优点"><a href="#1-3-1-优点" class="headerlink" title="1.3.1 优点"></a>1.3.1 优点</h3><p>1）高容错性</p>
<p>​        （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。</p>
<p>​        （2）某一个副本丢失以后，它可以自动恢复，这是由 HDFS 内部机制实现的，我们不必关心。</p>
<p>2）适合大数据处理</p>
<p>​       （1）数据规模：能够处理数据规模达到 GB、TB、甚至PB级别的数据。（<strong>因为他会切块存储，所以可以存储大文件-**</strong>参见HDFS<strong>**写数据流程</strong>）</p>
<p>​        （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。</p>
<p>3）<strong>流式数据访问**</strong>（一点一点的处理数据，而不是一次性读取整个数据，这样会极大消耗内存）**</p>
<p>​        （1）一次写入，多次读取，不能修改，只能追加。</p>
<p>​        （2）它能保证数据的一致性。</p>
<p>4）可构建在廉价机器上</p>
<p>​        （1）它通过多副本机制，提高可靠性。</p>
<p>​        （2）它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。</p>
<h3 id="1-3-2-缺点"><a href="#1-3-2-缺点" class="headerlink" title="1.3.2 缺点"></a>1.3.2 缺点</h3><p>1）不适合低延时数据访问</p>
<p>​       （1）比如毫秒级的来存储数据，这是不行的，它做不到。</p>
<p>​        （2）它适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况下是不行的，比如毫秒级以内读取数据，这样它是很难做到的。</p>
<p>2）无法高效的对大量小文件进行存储（HDFS默认的最基本的存储单位是128M的数据块。）</p>
<p>​        （1）存储大量<a href="http://hadoop2.dajiangtai.com/javascript:void(0" target="_blank" rel="external">小文件</a>;)的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。</p>
<p>​        （2）<strong>小文件存储的寻道时间会超过读取时间</strong>，它违反了HDFS的设计目标。</p>
<p>3）并发写入、文件随机修改</p>
<p>​        （1）<strong>一个文件只能有一个写，不允许多个线程同时写</strong>。</p>
<p>​        （2）仅支持数据 append（追加），不支持文件的随机修改。</p>
<p><img src="/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/clip_image002.jpg" alt="img"></p>
<h2 id="1-4-HDFS架构"><a href="#1-4-HDFS架构" class="headerlink" title="1.4 HDFS架构"></a>1.4 HDFS架构</h2><p>HDFS的架构图</p>
<p>​        这种架构主要由四个部分组成，分别为<strong>HDFS Client、NameNode、DataNode和Secondary NameNode</strong>。下面我们分别介绍这四个组成部分。</p>
<p>1）Client：就是客户端。</p>
<p>​        （1）文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的Block，然后进行存储。</p>
<p>​        （2）与NameNode交互，获取文件的位置信息。</p>
<p>​        （3）与DataNode交互，读取或者写入数据。</p>
<p>​       （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS。</p>
<p>​        （5）Client可以通过一些命令来访问HDFS。</p>
<p>2）NameNode：就是master，它是一个主管、管理者。</p>
<p>​        （1）管理HDFS的名称空间。</p>
<p>​       （2）管理<a href="http://hadoop2.dajiangtai.com/javascript:void(0" target="_blank" rel="external">数据块（Block）</a>;)映射信息</p>
<p>​        （3）配置<a href="http://hadoop2.dajiangtai.com/javascript:void(0" target="_blank" rel="external">副本策略</a>;)</p>
<p>​       （4）处理客户端读写请求。</p>
<p>3） DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。</p>
<p>​        （1）存储实际的数据块。</p>
<p>​        （2）执行数据块的读/写操作。</p>
<p>4） Secondary NameNode：<strong>并非NameNode的热备</strong>。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</p>
<p>​        （1）辅助NameNode，分担其工作量。</p>
<p>​        （2）定期合并<a href="http://hadoop2.dajiangtai.com/javascript:void(0" target="_blank" rel="external">fsimage和fsedits</a>;)，并推送给NameNode。</p>
<p>​        （3）在紧急情况下，可辅助恢复NameNode。</p>
<h2 id="1-5-HDFS-文件块大小"><a href="#1-5-HDFS-文件块大小" class="headerlink" title="1.5 HDFS 文件块大小"></a>1.5 HDFS 文件块大小</h2><p><strong>HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。</strong></p>
<p>HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于<strong>磁盘传输速率</strong>。</p>
<p>如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。</p>
<p>块的大小：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">10ms*100*100M/s = 100M</div></pre></td></tr></table></figure>
<p><img src="/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/15614.png" alt="1560051715614"></p>
<p><img src="/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/clip_image006.jpg" alt="img"></p>
<p><img src="/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/clip_image3002.jpg" alt="img"></p>
<p><strong>块大小取决于磁盘传输速率</strong></p>
<h1 id="二-HFDS命令行操作"><a href="#二-HFDS命令行操作" class="headerlink" title="二 HFDS命令行操作"></a>二 HFDS命令行操作</h1><p>操作HDFS的命令其实有三个：</p>
<p>​    Hadoop fs：使用面最广，可以操作任何文件系统。</p>
<p>​    hadoop dfs与hdfs dfs：只能操作HDFS文件系统相关（包括与Local FS间的操作），前者已经Deprecated，一般使用后者。 </p>
<p>推荐使用：hadoop fs</p>
<h2 id="1）基本语法"><a href="#1）基本语法" class="headerlink" title="1）基本语法"></a>1）基本语法</h2><p>bin/hadoop fs 具体命令</p>
<h2 id="2）参数大全"><a href="#2）参数大全" class="headerlink" title="2）参数大全"></a>2）参数大全</h2><p>​         [kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs</p>
<p>   [-appendToFile <localsrc>   … <dst>]           [-cat [-ignoreCrc] <src> …]           [-checksum <src> …]           [-chgrp [-R] GROUP PATH…]           [-chmod [-R] <mode[,mode]... |="" octalmode=""> PATH…]           [-chown [-R] [OWNER][:[GROUP]]   PATH…]           [-copyFromLocal [-f] [-p]   <localsrc> … <dst>]           [-copyToLocal [-p] [-ignoreCrc]   [-crc] <src> … <localdst>]           [-count [-q] <path></path> …]           [-cp [-f] [-p] <src> …   <dst>]           [-createSnapshot <snapshotdir>   [<snapshotname>]]           [-deleteSnapshot <snapshotdir>   <snapshotname>]           [-df [-h] [<path></path> …]]           [-du [-s] [-h] <path></path> …]           [-expunge]           [-get [-p] [-ignoreCrc] [-crc]   <src> … <localdst>]           [-getfacl [-R] <path></path>]           [-getmerge [-nl] <src>   <localdst>]           [-help [cmd …]]           [-ls [-d] [-h] [-R] [<path></path>   …]]           [-mkdir [-p] <path></path> …]           [-moveFromLocal <localsrc> …   <dst>]           [-moveToLocal <src>   <localdst>]           [-mv <src> … <dst>]           [-put [-f] [-p] <localsrc> …   <dst>]           [-renameSnapshot <snapshotdir>   <oldname> <newname>]           [-rm [-f] [-r|-R] [-skipTrash]   <src> …]           [-rmdir [–ignore-fail-on-non-empty]   <dir> …]           [-setfacl [-R] [{-b|-k} {-m|-x   <acl_spec>} <path></path>]|[–set <acl_spec> <path></path>]]           [-setrep [-R] [-w] <rep>   <path></path> …]           [-stat [format] <path></path> …]           [-tail [-f] <file>]           [-test -[defsz] <path></path>]           [-text [-ignoreCrc] <src> …]           [-touchz <path></path> …]           [-usage [cmd …]]   </src></file></rep></acl_spec></acl_spec></dir></src></newname></oldname></snapshotdir></dst></localsrc></dst></src></localdst></src></dst></localsrc></localdst></src></localdst></src></snapshotname></snapshotdir></snapshotname></snapshotdir></dst></src></localdst></src></dst></localsrc></mode[,mode]...></src></src></dst></localsrc></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line">        [-cat [-ignoreCrc] &lt;src&gt; ...]</div><div class="line">        [-checksum &lt;src&gt; ...]</div><div class="line">        [-chgrp [-R] GROUP PATH...]</div><div class="line">        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</div><div class="line">        [-chown [-R] [OWNER][:[GROUP]] PATH...]</div><div class="line">        [-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line">        [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</div><div class="line">        [-count [-q] &lt;path&gt; ...]</div><div class="line">        [-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;]</div><div class="line">        [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</div><div class="line">        [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</div><div class="line">        [-df [-h] [&lt;path&gt; ...]]</div><div class="line">        [-du [-s] [-h] &lt;path&gt; ...]</div><div class="line">        [-expunge]</div><div class="line">        [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</div><div class="line">        [-getfacl [-R] &lt;path&gt;]</div><div class="line">        [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</div><div class="line">        [-help [cmd ...]]</div><div class="line">        [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</div><div class="line">        [-mkdir [-p] &lt;path&gt; ...]</div><div class="line">        [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line">        [-moveToLocal &lt;src&gt; &lt;localdst&gt;]</div><div class="line">        [-mv &lt;src&gt; ... &lt;dst&gt;]</div><div class="line">        [-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;]</div><div class="line">        [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</div><div class="line">        [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</div><div class="line">        [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</div><div class="line">        [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</div><div class="line">        [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</div><div class="line">        [-stat [format] &lt;path&gt; ...]</div><div class="line">        [-tail [-f] &lt;file&gt;]</div><div class="line">        [-test -[defsz] &lt;path&gt;]</div><div class="line">        [-text [-ignoreCrc] &lt;src&gt; ...]</div><div class="line">        [-touchz &lt;path&gt; ...]</div><div class="line">        [-usage [cmd ...]]</div></pre></td></tr></table></figure>
<h2 id="3）常用命令"><a href="#3）常用命令" class="headerlink" title="3）常用命令"></a>3）常用命令</h2><p> （0）启动Hadoop集群（方便后续的测试）</p>
<p>​                 [kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</p>
<p>[kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</p>
<p>（1）-help：输出这个命令参数</p>
<p>​                 [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -help rm</p>
<p>​         （2）-ls: 显示目录信息</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /</p>
<p>（3）-mkdir：在hdfs上创建目录</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /user/kingge/test</p>
<p>（4）-moveFromLocal从本地剪切粘贴到hdfs</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ touch jinlian.txt</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop  fs  -moveFromLocal  ./jinlian.txt  /user/kingge/test</p>
<p>（5）–appendToFile  ：追加一个文件到已经存在的文件末尾</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ touch ximen.txt</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ vi ximen.txt</p>
<p>输入</p>
<p>wo ai  jinlian</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile ximen.txt /user/kingge/test/jinlian.txt</p>
<p>（6）-cat ：显示文件内容</p>
<p>（7）-tail：显示一个文件的末尾</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /user/kingge/test/jinlian.txt</p>
<p>（8）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop  fs  -chmod  666  /hello.txt</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop  fs  -chown  someuser:somegrp   /hello.txt</p>
<p>（9）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /user/kingge/test</p>
<p>（10）-copyToLocal：从hdfs拷贝到本地</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /user/kingge/test/jinlian.txt ./jinlian.txt</p>
<p>（11）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /user/kingge/test/jinlian.txt /jinlian2.txt</p>
<p>（12）-mv：在hdfs目录中移动文件</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /jinlian2.txt /user/kingge/test/</p>
<p>（13）-get：等同于copyToLocal，就是从hdfs下载文件到本地</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -get /user/kingge/test/jinlian2.txt ./</p>
<p>（14）-getmerge  ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, <em>log.2,log.3,…</em></p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/kingge/test/* ./zaiyiqi.txt</p>
<p>（15）-put：等同于copyFromLocal</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/kingge/test/</p>
<p>（16）-rm：删除文件或文件夹</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/kingge/test/jinlian2.txt</p>
<p>（17）-rmdir：删除空目录</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test</p>
<p>（18）-df ：统计文件系统的可用空间信息</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -df -h /</p>
<p>（19）-du统计文件夹的大小信息</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/kingge/test</p>
<p>2.7 K  /user/kingge/test      ###查看文件夹下文件总大小</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -du  -h /user/kingge/test </p>
<p>1.3 K  /user/kingge/test/README.txt    </p>
<p>15     /user/kingge/test/jinlian.txt</p>
<p>1.4 K  /user/kingge/test/zaiyiqi.txt               </p>
<p>##查看文件夹下各个文件大小</p>
<p>（20）-setrep：设置hdfs中文件的副本数量</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 2 /user/kingge/test/jinlian.txt</p>
<p><img src="/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/clip_image0062.jpg" alt="img"></p>
<p><strong>这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</strong></p>
<h1 id="三-HDFS客户端操作-eclipse"><a href="#三-HDFS客户端操作-eclipse" class="headerlink" title="三 HDFS客户端操作-eclipse"></a>三 HDFS客户端操作-eclipse</h1><h2 id="3-1-Eclipse环境准备"><a href="#3-1-Eclipse环境准备" class="headerlink" title="3.1 Eclipse环境准备"></a>3.1 Eclipse环境准备</h2><p><strong>注意：以下操作，是在hadoop集群中中进行的，也就是说，需要先启动linux的hadoop集群</strong></p>
<h3 id="3-1-1-jar包准备"><a href="#3-1-1-jar包准备" class="headerlink" title="3.1.1 jar包准备"></a>3.1.1 jar包准备</h3><p>1）解压hadoop-2.7.2.tar.gz到非中文目录</p>
<p>2）进入share文件夹，查找所有jar包，并把jar包拷贝到_lib文件夹下</p>
<p>3）在全部jar包中查找sources.jar，并剪切到_source文件夹。</p>
<p>4）在全部jar包中查找tests.jar，并剪切到_test文件夹。</p>
<h3 id="3-1-2-Eclipse准备"><a href="#3-1-2-Eclipse准备" class="headerlink" title="3.1.2 Eclipse准备"></a>3.1.2 Eclipse准备</h3><p>1）配置HADOOP_HOME环境变量</p>
<p>2）采用Hadoop编译后的bin 、lib两个文件夹（如果不生效，重新启动eclipse）</p>
<p>3）创建第一个java工程</p>
<p>4）导入 编译后目录的jar包（<strong>可以在文章下方回复我，我私信给你们</strong>）</p>
<p>   <strong>public</strong> <strong>class</strong>   HdfsClientDemo1 {            <strong>public</strong> <strong>static</strong>   <strong>void</strong> main(String[] args) <strong>throws</strong> Exception {                    // 1 获取文件系统                    Configuration   configuration = <strong>new</strong> Configuration();                    // 配置在集群上运行                    configuration.set(“fs.defaultFS”,   “hdfs://hadoop102:9000”);                    FileSystem   fileSystem = FileSystem.<em>get</em>(configuration);                                        // 直接配置访问集群的路径和访问集群的用户名称   //               FileSystem   fileSystem = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration,   “atguigu”);                                        // 2 把本地文件上传到文件系统中                    fileSystem.copyFromLocalFile(<strong>new</strong>   Path(“f:/hello.txt”), <strong>new</strong>   Path(“/hello1.copy.txt”));                                        // 3 关闭资源                    fileSystem.close();                    System.<em>out</em>.println(“over”);            }   }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public class HdfsClientDemo1 &#123;</div><div class="line">	public static void main(String[] args) throws Exception &#123;</div><div class="line">		// 1 获取文件系统</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line">		// 配置在集群上运行 - 如果不配置，默认使用的是本地的hadoop运行环境。</div><div class="line">		configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;);</div><div class="line">		FileSystem fileSystem = FileSystem.get(configuration);</div><div class="line">		</div><div class="line">		// 直接配置访问集群的路径和访问集群的用户名称</div><div class="line">//		FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);</div><div class="line">		</div><div class="line">		// 2 把本地文件上传到文件系统中</div><div class="line">		fileSystem.copyFromLocalFile(new Path(&quot;f:/hello.txt&quot;), new Path(&quot;/hello1.copy.txt&quot;));</div><div class="line">		</div><div class="line">		// 3 关闭资源</div><div class="line">		fileSystem.close();</div><div class="line">		System.out.println(&quot;over&quot;);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>4）执行程序</p>
<p>运行时需要配置用户名称</p>
<p><img src="/2018/03/06/hadoop大数据-五-HDFS概念和基本操作/clip_image07062.jpg" alt="img"></p>
<p>客户端去操作hdfs时，是有一个用户身份的。默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=kingge，kingge为用户名称。</p>
<h2 id="3-2-通过API操作HDFS"><a href="#3-2-通过API操作HDFS" class="headerlink" title="3.2 通过API操作HDFS"></a>3.2 通过API操作HDFS</h2><h3 id="3-2-1-HDFS获取文件系统"><a href="#3-2-1-HDFS获取文件系统" class="headerlink" title="3.2.1 HDFS获取文件系统"></a>3.2.1 HDFS获取文件系统</h3><p>1）详细代码</p>
<p>​            @Test            <strong>public</strong> <strong>void</strong> initHDFS() <strong>throws</strong>   Exception{                     //   1 创建配置信息对象                    Configuration configuration =   <strong>new</strong> Configuration();                                        // 2 获取文件系统                    FileSystem fs =   FileSystem.<em>get</em>(configuration);                                        // 3 打印文件系统                    System.<em>out</em>.println(fs.toString());            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void initHDFS() throws Exception&#123;</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">	</div><div class="line">	// 2 获取文件系统</div><div class="line">	FileSystem fs = FileSystem.get(configuration);</div><div class="line">	</div><div class="line">	// 3 打印文件系统</div><div class="line">	System.out.println(fs.toString());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-2-2-HDFS文件上传"><a href="#3-2-2-HDFS文件上传" class="headerlink" title="3.2.2 HDFS文件上传"></a>3.2.2 HDFS文件上传</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">	@Test</div><div class="line">	public void putFileToHDFS() throws Exception&#123;</div><div class="line">		// 1 创建配置信息对象</div><div class="line">		// new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml</div><div class="line">		// 然后再加载classpath下的hdfs-site.xml</div><div class="line">		Configuration configuration = new Configuration();</div><div class="line"></div><div class="line">		// 2 设置参数 </div><div class="line">		// 参数优先级： 1、客户端代码中设置的值  2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置</div><div class="line">		configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;);</div><div class="line"></div><div class="line">		FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);</div><div class="line">		</div><div class="line">		// 3 创建要上传文件所在的本地路径</div><div class="line">		Path src = new Path(&quot;e:/hello.txt&quot;);</div><div class="line">		</div><div class="line">		// 4 创建要上传到hdfs的目标路径</div><div class="line">		Path dst = new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;);</div><div class="line">		</div><div class="line">		// 5 拷贝文件</div><div class="line">		fs.copyFromLocalFile(src, dst);</div><div class="line">		fs.close();	</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>​            @Test            <strong>public</strong> <strong>void</strong>   putFileToHDFS() <strong>throws</strong> Exception{                    // 1 创建配置信息对象                    // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml                    // 然后再加载classpath下的hdfs-site.xml                    Configuration configuration =   <strong>new</strong> Configuration();                        // 2 设置参数                     // 参数优先级： 1、客户端代码中设置的值  2、classpath下的用户自定义配置文件   3、然后是服务器的默认配置                    configuration.set(“dfs.replication”,   “2”);                        FileSystem fs = FileSystem.<em>get</em>(<strong>new</strong>   URI(“hdfs://hadoop102:9000”),configuration, “atguigu”);                                        // 3 创建要上传文件所在的本地路径                    Path src = <strong>new</strong>   Path(“e:/hello.txt”);                                        // 4 创建要上传到hdfs的目标路径                    Path dst = <strong>new</strong>   Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”);                                        // 5 拷贝文件                    fs.copyFromLocalFile(src,   dst);                    fs.close();    }   </p>
<p>2）将core-site.xml拷贝到项目的根目录下</p>
<p>   &lt;?xml   version=”1.0” encoding=”UTF-8”?&gt;   &lt;?xml-stylesheet   type=”text/xsl” href=”configuration.xsl”?&gt;   <configuration>   <!-- 指定HDFS中NameNode的地址 -->            <property>                    <name>fs.defaultFS</name>             <value>hdfs://hadoop102:9000</value>            </property>   </configuration>   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line">&lt;configuration&gt;</div><div class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">        &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>3）将hdfs-site.xml拷贝到项目的根目录下</p>
<p>   &lt;?xml   version=”1.0” encoding=”UTF-8”?&gt;   &lt;?xml-stylesheet   type=”text/xsl” href=”configuration.xsl”?&gt;       <configuration>            <property>                    <name>dfs.replication</name>           <value>1</value>            </property>    </configuration>   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line"></div><div class="line">&lt;configuration&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">        &lt;value&gt;1&lt;/value&gt;</div><div class="line">	&lt;/property&gt; </div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>4）测试参数优先级</p>
<p>参数优先级： <strong>1**</strong>、客户端代码中设置的值  2<strong><strong>、classpath</strong></strong>下的用户自定义配置文件 3<strong>**、然后是服务器的默认配置</strong></p>
<h3 id="3-2-3-HDFS文件下载"><a href="#3-2-3-HDFS文件下载" class="headerlink" title="3.2.3 HDFS文件下载"></a>3.2.3 HDFS文件下载</h3><p>   @Test   <strong>public</strong> <strong>void</strong> getFileFromHDFS() <strong>throws</strong>   Exception{                                //   1 创建配置信息对象            Configuration   configuration = <strong>new</strong> Configuration();                                FileSystem   fs = FileSystem.<em>get</em>(<strong>new</strong> URI(“hdfs://hadoop102:9000”),configuration,   “atguigu”);                     //      fs.copyToLocalFile(new   Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”),   new Path(“d:/hello.txt”));            //   boolean delSrc 指是否将原文件删除            //   Path src 指要下载的文件路径            //   Path dst 指将文件下载到的路径            //   boolean useRawLocalFileSystem 是否开启文件效验             // 2 下载文件            fs.copyToLocalFile(<strong>false</strong>,   <strong>new</strong> Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), <strong>new</strong>   Path(“e:/hellocopy.txt”), <strong>true</strong>);            fs.close();            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void getFileFromHDFS() throws Exception&#123;</div><div class="line">		</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">		</div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);	</div><div class="line">	</div><div class="line">//	fs.copyToLocalFile(new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;d:/hello.txt&quot;));</div><div class="line">	// boolean delSrc 指是否将原文件删除</div><div class="line">	// Path src 指要下载的文件路径</div><div class="line">	// Path dst 指将文件下载到的路径</div><div class="line">	// boolean useRawLocalFileSystem 是否开启文件效验</div><div class="line"></div><div class="line">    // 2 下载文件</div><div class="line">	fs.copyToLocalFile(false, new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;e:/hellocopy.txt&quot;), true);</div><div class="line">	fs.close();</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<h3 id="3-2-4-HDFS目录创建"><a href="#3-2-4-HDFS目录创建" class="headerlink" title="3.2.4 HDFS目录创建"></a>3.2.4 HDFS目录创建</h3><p>​            @Test            <strong>public</strong> <strong>void</strong>   mkdirAtHDFS() <strong>throws</strong> Exception{                    // 1 创建配置信息对象                    Configuration configuration =   <strong>new</strong> Configuration();                                        FileSystem fs = FileSystem.<em>get</em>(<strong>new</strong>   URI(“hdfs://hadoop102:9000”),configuration, “atguigu”);                                              //2 创建目录                    fs.mkdirs(<strong>new</strong>   Path(“hdfs://hadoop102:9000/user/atguigu/output”));            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void mkdirAtHDFS() throws Exception&#123;</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">	</div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);	</div><div class="line">	</div><div class="line">	//2 创建目录</div><div class="line">	fs.mkdirs(new Path(&quot;hdfs://hadoop102:9000/user/kingge/output&quot;));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-2-5-HDFS文件夹删除"><a href="#3-2-5-HDFS文件夹删除" class="headerlink" title="3.2.5 HDFS文件夹删除"></a>3.2.5 HDFS文件夹删除</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void deleteAtHDFS() throws Exception&#123;</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">	</div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);	</div><div class="line">	</div><div class="line">	//2 删除文件夹 ，如果是非空文件夹，参数2是否递归删除，true递归</div><div class="line">	fs.delete(new Path(&quot;hdfs://hadoop102:9000/user/kingge/output&quot;), true);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>​            @Test            <strong>public</strong> <strong>void</strong>   deleteAtHDFS() <strong>throws</strong> Exception{                    // 1 创建配置信息对象                    Configuration configuration =   <strong>new</strong> Configuration();                                        FileSystem fs = FileSystem.<em>get</em>(<strong>new</strong>   URI(“hdfs://hadoop102:9000”),configuration, “atguigu”);                                              //2 删除文件夹   ，如果是非空文件夹，参数2是否递归删除，true递归                    fs.delete(<strong>new</strong>   Path(“hdfs://hadoop102:9000/user/atguigu/output”), <strong>true</strong>);            }   </p>
<h3 id="3-2-6-HDFS文件名更改"><a href="#3-2-6-HDFS文件名更改" class="headerlink" title="3.2.6 HDFS文件名更改"></a>3.2.6 HDFS文件名更改</h3><p>​            @Test            <strong>public</strong> <strong>void</strong>   renameAtHDFS() <strong>throws</strong> Exception{                    // 1 创建配置信息对象                    Configuration configuration =   <strong>new</strong> Configuration();                                        FileSystem fs = FileSystem.<em>get</em>(<strong>new</strong>   URI(“hdfs://hadoop102:9000”),configuration, “atguigu”);                                        //2 重命名文件或文件夹                    fs.rename(<strong>new</strong>   Path(“hdfs://hadoop102:9000/user/atguigu/hello.txt”), <strong>new</strong>   Path(“hdfs://hadoop102:9000/user/atguigu/hellonihao.txt”));            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void renameAtHDFS() throws Exception&#123;</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">	</div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);</div><div class="line">	</div><div class="line">	//2 重命名文件或文件夹</div><div class="line">	fs.rename(new Path(&quot;hdfs://hadoop102:9000/user/kingge/hello.txt&quot;), new Path(&quot;hdfs://hadoop102:9000/user/kingge/hellonihao.txt&quot;));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-2-7-HDFS文件详情查看"><a href="#3-2-7-HDFS文件详情查看" class="headerlink" title="3.2.7 HDFS文件详情查看"></a>3.2.7 HDFS文件详情查看</h3><p>查看文件名称、权限、长度、块信息-<strong>不是文件夹，是文件</strong> </p>
<p>   @Test   <strong>public</strong> <strong>void</strong> readListFiles() <strong>throws</strong> Exception   {            //   1 创建配置信息对象            Configuration   configuration = <strong>new</strong> Configuration();                                FileSystem   fs = FileSystem.<em>get</em>(<strong>new</strong> URI(“hdfs://hadoop102:9000”),configuration,   “atguigu”);                                //   思考：为什么返回迭代器，而不是List之类的容器            RemoteIterator<locatedfilestatus>   listFiles = fs.listFiles(<strong>new</strong> Path(“/“), <strong>true</strong>);                <strong>while</strong>   (listFiles.hasNext()) {                    LocatedFileStatus   fileStatus = listFiles.next();                                                 System.<strong>out</strong>.println(fileStatus.getPath().getName());                    System.<strong>out</strong>.println(fileStatus.getBlockSize());                    System.<strong>out</strong>.println(fileStatus.getPermission());                    System.<strong>out</strong>.println(fileStatus.getLen());                                                 BlockLocation[]   blockLocations = fileStatus.getBlockLocations();                                                 <strong>for</strong>   (BlockLocation bl : blockLocations) {                                                                   System.<strong>out</strong>.println(“block-offset:”   + bl.getOffset());                                                                   String[]   hosts = bl.getHosts();                                                                   <strong>for</strong>   (String host : hosts) {                                      System.<strong>out</strong>.println(host);                             }                     }                                                 System.<strong>out</strong>.println(“————–李冰冰的分割线————–”);            }            }   </locatedfilestatus></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void readListFiles() throws Exception &#123;</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">		</div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);</div><div class="line">		</div><div class="line">	// 思考：为什么返回迭代器，而不是List之类的容器</div><div class="line">	RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true);</div><div class="line"></div><div class="line">	while (listFiles.hasNext()) &#123;</div><div class="line">		LocatedFileStatus fileStatus = listFiles.next();</div><div class="line">			</div><div class="line">		System.out.println(fileStatus.getPath().getName());</div><div class="line">		System.out.println(fileStatus.getBlockSize());</div><div class="line">		System.out.println(fileStatus.getPermission());</div><div class="line">		System.out.println(fileStatus.getLen());</div><div class="line">			</div><div class="line">		BlockLocation[] blockLocations = fileStatus.getBlockLocations();</div><div class="line">			</div><div class="line">		for (BlockLocation bl : blockLocations) &#123;</div><div class="line">				</div><div class="line">			System.out.println(&quot;block-offset:&quot; + bl.getOffset());</div><div class="line">				</div><div class="line">			String[] hosts = bl.getHosts();</div><div class="line">				</div><div class="line">			for (String host : hosts) &#123;</div><div class="line">				System.out.println(host);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">			</div><div class="line">		System.out.println(&quot;--------------李冰冰的分割线--------------&quot;);</div><div class="line">	&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<h3 id="3-2-8-HDFS文件和文件夹判断"><a href="#3-2-8-HDFS文件和文件夹判断" class="headerlink" title="3.2.8 HDFS文件和文件夹判断"></a>3.2.8 HDFS文件和文件夹判断</h3><p>  <strong>不支持递归，只能查询当前某个目录下的文件或者或者文件夹，然后判断</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void findAtHDFS() throws Exception, IllegalArgumentException, IOException&#123;</div><div class="line">		</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">		</div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);</div><div class="line">		</div><div class="line">	// 2 获取查询路径下的文件状态信息</div><div class="line">	FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;));</div><div class="line"></div><div class="line">	// 3 遍历所有文件状态</div><div class="line">	for (FileStatus status : listStatus) &#123;</div><div class="line">		if (status.isFile()) &#123;</div><div class="line">			System.out.println(&quot;f--&quot; + status.getPath().getName());</div><div class="line">		&#125; else &#123;</div><div class="line">			System.out.println(&quot;d--&quot; + status.getPath().getName());</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>   @Test   public void findAtHDFS() throws   Exception, IllegalArgumentException, IOException{                                //   1 创建配置信息对象            Configuration   configuration = new Configuration();                                FileSystem   fs = FileSystem.get(new URI(“hdfs://hadoop102:9000”),configuration,   “atguigu”);                                //   2 获取查询路径下的文件状态信息            FileStatus[]   listStatus = fs.listStatus(new Path(“/“));                //   3 遍历所有文件状态            <strong>for</strong>   (FileStatus status : listStatus) {                    <strong>if</strong>   (status.isFile()) {                             System.<strong>out</strong>.println(“f–”   + status.getPath().getName());                    }   <strong>else</strong> {                             System.<strong>out</strong>.println(“d–”   + status.getPath().getName());                    }            }   }   </p>
<h2 id="3-3-通过IO流操作HDFS"><a href="#3-3-通过IO流操作HDFS" class="headerlink" title="3.3 通过IO流操作HDFS"></a>3.3 通过IO流操作HDFS</h2><h3 id="3-3-1-HDFS文件上传"><a href="#3-3-1-HDFS文件上传" class="headerlink" title="3.3.1 HDFS文件上传"></a>3.3.1 HDFS文件上传</h3><p>​            @Test            <strong>public</strong> <strong>void</strong>   putFileToHDFS() <strong>throws</strong> Exception{                    // 1 创建配置信息对象                    Configuration configuration =   <strong>new</strong> Configuration();                                        FileSystem fs = FileSystem.<em>get</em>(<strong>new</strong>   URI(“hdfs://hadoop102:9000”),configuration, “atguigu”);                                        // 2 创建输入流                    FileInputStream inStream = <strong>new</strong>   FileInputStream(<strong>new</strong> File(“e:/hello.txt”));                                        // 3 获取输出路径                    String putFileName =   “hdfs://hadoop102:9000/user/atguigu/hello1.txt”;                    Path writePath = <strong>new</strong>   Path(putFileName);                        // 4 创建输出流                    FSDataOutputStream outStream   = fs.create(writePath);                        // 5 流对接                    <strong>try</strong>{                             IOUtils.<em>copyBytes</em>(inStream,   outStream, 4096, <strong>false</strong>);                    }<strong>catch</strong>(Exception e){                             e.printStackTrace();                    }<strong>finally</strong>{                             IOUtils.<em>closeStream</em>(inStream);                             IOUtils.<em>closeStream</em>(outStream);                    }            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void putFileToHDFS() throws Exception&#123;</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">	</div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);</div><div class="line">	</div><div class="line">	// 2 创建输入流</div><div class="line">	FileInputStream inStream = new FileInputStream(new File(&quot;e:/hello.txt&quot;));</div><div class="line">	</div><div class="line">	// 3 获取输出路径</div><div class="line">	String putFileName = &quot;hdfs://hadoop102:9000/user/kingge/hello1.txt&quot;;</div><div class="line">	Path writePath = new Path(putFileName);</div><div class="line"></div><div class="line">	// 4 创建输出流</div><div class="line">	FSDataOutputStream outStream = fs.create(writePath);</div><div class="line"></div><div class="line">	// 5 流对接</div><div class="line">	try&#123;</div><div class="line">		IOUtils.copyBytes(inStream, outStream, 4096, false);</div><div class="line">	&#125;catch(Exception e)&#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;finally&#123;</div><div class="line">		IOUtils.closeStream(inStream);</div><div class="line">		IOUtils.closeStream(outStream);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-3-2-HDFS文件下载"><a href="#3-3-2-HDFS文件下载" class="headerlink" title="3.3.2 HDFS文件下载"></a>3.3.2 HDFS文件下载</h3><p>​            @Test            <strong>public</strong> <strong>void</strong>   getFileToHDFS() <strong>throws</strong> Exception{                    // 1 创建配置信息对象                    Configuration configuration =   <strong>new</strong> Configuration();                                        FileSystem fs = FileSystem.<em>get</em>(<strong>new</strong>   URI(“hdfs://hadoop102:9000”),configuration, “atguigu”);                                        // 2 获取读取文件路径                    String filename =   “hdfs://hadoop102:9000/user/atguigu/hello1.txt”;                                        // 3 创建读取path                    Path readPath = <strong>new</strong>   Path(filename);                                        // 4 创建输入流                    FSDataInputStream inStream =   fs.open(readPath);                                        // 5 流对接输出到控制台                    <strong>try</strong>{                             IOUtils.<em>copyBytes</em>(inStream,   System.<strong>out</strong>, 4096, <strong>false</strong>);                    }<strong>catch</strong>(Exception e){                             e.printStackTrace();                    }<strong>finally</strong>{                             IOUtils.<em>closeStream</em>(inStream);                    }            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">public void getFileToHDFS() throws Exception&#123;</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line">	</div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;),configuration, &quot;kingge&quot;);</div><div class="line">	</div><div class="line">	// 2 获取读取文件路径</div><div class="line">	String filename = &quot;hdfs://hadoop102:9000/user/kingge/hello1.txt&quot;;</div><div class="line">	</div><div class="line">	// 3 创建读取path</div><div class="line">	Path readPath = new Path(filename);</div><div class="line">	</div><div class="line">	// 4 创建输入流</div><div class="line">	FSDataInputStream inStream = fs.open(readPath);</div><div class="line">	</div><div class="line">	// 5 流对接输出到控制台</div><div class="line">	try&#123;</div><div class="line">		IOUtils.copyBytes(inStream, System.out, 4096, false);</div><div class="line">	&#125;catch(Exception e)&#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;finally&#123;</div><div class="line">		IOUtils.closeStream(inStream);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-3-3-定位文件读取"><a href="#3-3-3-定位文件读取" class="headerlink" title="3.3.3 定位文件读取"></a>3.3.3 定位文件读取</h3><p>1）下载第一块</p>
<p>   @Test   // 定位下载第一块内容   <strong>public</strong> <strong>void</strong> readFileSeek1() <strong>throws</strong> Exception   {                //   1 创建配置信息对象            Configuration   configuration = <strong>new</strong> Configuration();                FileSystem   fs = FileSystem.<em>get</em>(<strong>new</strong> URI(“hdfs://hadoop102:9000”),   configuration, “atguigu”);                //   2 获取输入流路径            Path   path = <strong>new</strong> Path(“hdfs://hadoop102:9000/user/atguigu/tmp/hadoop-2.7.2.tar.gz”);                //   3 打开输入流            FSDataInputStream   fis = fs.open(path);                //   4 创建输出流            FileOutputStream   fos = <strong>new</strong> FileOutputStream(“e:/hadoop-2.7.2.tar.gz.part1”);                //   5 流对接            <strong>byte</strong>[]   buf = <strong>new</strong> <strong>byte</strong>[1024];            <strong>for</strong>   (<strong>int</strong> i = 0; i &lt; 128 <em> 1024; i++) {                    fis.read(buf);                    fos.write(buf);            }                //   6 关闭流            IOUtils.</em>closeStream<em>(fis);            IOUtils.</em>closeStream*(fos);            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">// 定位下载第一块内容</div><div class="line">public void readFileSeek1() throws Exception &#123;</div><div class="line"></div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line"></div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kingge&quot;);</div><div class="line"></div><div class="line">	// 2 获取输入流路径</div><div class="line">	Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/tmp/hadoop-2.7.2.tar.gz&quot;);</div><div class="line"></div><div class="line">	// 3 打开输入流</div><div class="line">	FSDataInputStream fis = fs.open(path);</div><div class="line"></div><div class="line">	// 4 创建输出流</div><div class="line">	FileOutputStream fos = new FileOutputStream(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;);</div><div class="line"></div><div class="line">	// 5 流对接</div><div class="line">	byte[] buf = new byte[1024];</div><div class="line">	for (int i = 0; i &lt; 128 * 1024; i++) &#123;</div><div class="line">		fis.read(buf);</div><div class="line">		fos.write(buf);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// 6 关闭流</div><div class="line">	IOUtils.closeStream(fis);</div><div class="line">	IOUtils.closeStream(fos);</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>2）下载第二块</p>
<p>​            @Test            // 定位下载第二块内容            <strong>public</strong> <strong>void</strong>   readFileSeek2() <strong>throws</strong> Exception{                                        // 1 创建配置信息对象                    Configuration configuration =   <strong>new</strong> Configuration();                        FileSystem fs = FileSystem.<em>get</em>(<strong>new</strong>   URI(“hdfs://hadoop102:9000”), configuration, “atguigu”);                                        // 2 获取输入流路径                    Path path = <strong>new</strong>   Path(“hdfs://hadoop102:9000/user/atguigu/tmp/hadoop-2.7.2.tar.gz”);                                        // 3 打开输入流                    FSDataInputStream fis =   fs.open(path);                                        // 4 创建输出流                    FileOutputStream fos = <strong>new</strong>   FileOutputStream(“e:/hadoop-2.7.2.tar.gz.part2”);                                        // 5 定位偏移量（第二块的首位）                    fis.seek(1024 <em> 1024 </em> 128);                                        // 6 流对接                    IOUtils.<em>copyBytes</em>(fis,   fos, 1024);                                        // 7 关闭流                    IOUtils.<em>closeStream</em>(fis);                    IOUtils.<em>closeStream</em>(fos);            }   </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">@Test</div><div class="line">// 定位下载第二块内容</div><div class="line">public void readFileSeek2() throws Exception&#123;</div><div class="line">	</div><div class="line">	// 1 创建配置信息对象</div><div class="line">	Configuration configuration = new Configuration();</div><div class="line"></div><div class="line">	FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kingge&quot;);</div><div class="line">	</div><div class="line">	// 2 获取输入流路径</div><div class="line">	Path path = new Path(&quot;hdfs://hadoop102:9000/user/kingge/tmp/hadoop-2.7.2.tar.gz&quot;);</div><div class="line">	</div><div class="line">	// 3 打开输入流</div><div class="line">	FSDataInputStream fis = fs.open(path);</div><div class="line">	</div><div class="line">	// 4 创建输出流</div><div class="line">	FileOutputStream fos = new FileOutputStream(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;);</div><div class="line">	</div><div class="line">	// 5 定位偏移量（第二块的首位）</div><div class="line">	fis.seek(1024 * 1024 * 128);</div><div class="line">	</div><div class="line">	// 6 流对接</div><div class="line">	IOUtils.copyBytes(fis, fos, 1024);</div><div class="line">	</div><div class="line">	// 7 关闭流</div><div class="line">	IOUtils.closeStream(fis);</div><div class="line">	IOUtils.closeStream(fos);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>3）合并文件</p>
<p>在window命令窗口中执行</p>
<p>type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</p>
<p>解压，发现，就是我们上传的压缩文件。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一-HDFS简单介绍&quot;&gt;&lt;a href=&quot;#一-HDFS简单介绍&quot; class=&quot;headerlink&quot; title=&quot;一 HDFS简单介绍&quot;&gt;&lt;/a&gt;一 HDFS简单介绍&lt;/h1&gt;&lt;h2 id=&quot;1-1-HDFS产生背景&quot;&gt;&lt;a href=&quot;#1-1-HDFS产
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="HDFS" scheme="http://kingge.top/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(四)-Hadoop编译源码</title>
    <link href="http://kingge.top/2018/02/28/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E5%9B%9B-Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/"/>
    <id>http://kingge.top/2018/02/28/hadoop大数据-四-Hadoop编译源码/</id>
    <published>2018-02-28T11:31:59.000Z</published>
    <updated>2019-06-09T02:46:53.773Z</updated>
    
    <content type="html"><![CDATA[<h1 id="五-Hadoop编译源码"><a href="#五-Hadoop编译源码" class="headerlink" title="五 Hadoop编译源码"></a>五 Hadoop编译源码</h1><h2 id="5-1-前期准备工作"><a href="#5-1-前期准备工作" class="headerlink" title="5.1 前期准备工作"></a>5.1 前期准备工作</h2><p>1）CentOS联网 </p>
<p>配置CentOS能连接外网。Linux虚拟机ping <a href="http://www.baidu.com" target="_blank" rel="external">www.baidu.com</a> 是畅通的</p>
<p>注意：采用root角色编译，减少文件夹权限出现问题</p>
<p>2）jar包准备(hadoop源码、JDK7 、 maven、 ant 、protobuf)</p>
<p>（1）hadoop-2.7.2-src.tar.gz</p>
<p>（2）jdk-7u79-linux-x64.gz</p>
<p>（3）apache-ant-1.9.9-bin.tar.gz</p>
<p>（4）apache-maven-3.0.5-bin.tar.gz</p>
<p>（5）protobuf-2.5.0.tar.gz</p>
<h2 id="5-2-jar包安装"><a href="#5-2-jar包安装" class="headerlink" title="5.2 jar包安装"></a>5.2 jar包安装</h2><p>0）注意：所有操作必须在root用户下完成</p>
<p>1）JDK解压、配置环境变量 JAVA_HOME和PATH，验证<a href="http://lib.csdn.net/base/javase" target="_blank" rel="external">java</a>-version(如下都需要验证是否配置成功)</p>
<p>[root@hadoop101 software] # tar -zxf jdk-7u79-linux-x64.gz -C /opt/module/</p>
<p>[root@hadoop101 software]# vi /etc/profile</p>
<p>   #JAVA_HOME   export   JAVA_HOME=/opt/module/jdk1.8.0_144   export   PATH=$PATH:$JAVA_HOME/bin   </p>
<p>[root@hadoop101 software]#source /etc/profile</p>
<p>验证命令：java -version</p>
<p>2）Maven解压、配置  MAVEN_HOME和PATH。</p>
<p>[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 apache-maven-3.0.5]#  vi /etc/profile</p>
<p>   #MAVEN_HOME   export   MAVEN_HOME=/opt/module/apache-maven-3.0.5   export   PATH=$PATH:$MAVEN_HOME/bin   </p>
<p>[root@hadoop101 software]#source /etc/profile</p>
<p>验证命令：mvn -version</p>
<p>3）ant解压、配置  ANT _HOME和PATH。</p>
<p>[root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 apache-ant-1.9.9]# vi /etc/profile</p>
<p>   #ANT_HOME   export   ANT_HOME=/opt/module/apache-ant-1.9.9   export   PATH=$PATH:$ANT_HOME/bin   </p>
<p>[root@hadoop101 software]#source /etc/profile</p>
<p>验证命令：ant -version</p>
<p>4）安装  glibc-headers 和  g++  命令如下: </p>
<p>[root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers</p>
<p>[root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++</p>
<p>5）安装make和cmake</p>
<p>[root@hadoop101 apache-ant-1.9.9]# yum install make</p>
<p>[root@hadoop101 apache-ant-1.9.9]# yum install cmake</p>
<p>6）解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0</p>
<p>然后相继执行命令：</p>
<p>[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</p>
<p>[root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/</p>
<p>[root@hadoop101 protobuf-2.5.0]#./configure </p>
<p>[root@hadoop101 protobuf-2.5.0]# make </p>
<p>[root@hadoop101 protobuf-2.5.0]# make check </p>
<p>[root@hadoop101 protobuf-2.5.0]# make install </p>
<p>[root@hadoop101 protobuf-2.5.0]# ldconfig </p>
<p>[root@hadoop101 hadoop-dist]# vi /etc/profile</p>
<p>   #LD_LIBRARY_PATH   export   LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0   export PATH=$PATH:$LD_LIBRARY_PATH   </p>
<p>[root@hadoop101 software]#source /etc/profile</p>
<p>验证命令：protoc –version</p>
<p>7）安装openssl库</p>
<p>[root@hadoop101 software]#yum install openssl-devel</p>
<p>8）安装 ncurses-devel库：</p>
<p>[root@hadoop101 software]#yum install ncurses-devel</p>
<p>到此，编译工具安装基本完成。</p>
<h2 id="5-3-编译源码"><a href="#5-3-编译源码" class="headerlink" title="5.3 编译源码"></a>5.3 编译源码</h2><p>1）解压源码到/opt/tools目录</p>
<p>[root@hadoop101 software]# tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/</p>
<p>2）进入到hadoop源码主目录</p>
<p>[root@hadoop101 hadoop-2.7.2-src]# pwd</p>
<p>/opt/hadoop-2.7.2-src</p>
<p>3）通过maven执行编译命令</p>
<p>[root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar</p>
<p>等待时间30分钟左右，最终成功是全部SUCCESS。</p>
<p><img src="/2018/02/28/hadoop大数据-四-Hadoop编译源码/clip_image002.jpg" alt="img"></p>
<p>4）成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下。</p>
<p>[root@hadoop101 target]# pwd</p>
<p>/opt/hadoop-2.7.2-src/hadoop-dist/target</p>
<h2 id="5-4-常见的问题及解决方案"><a href="#5-4-常见的问题及解决方案" class="headerlink" title="5.4 常见的问题及解决方案"></a>5.4 常见的问题及解决方案</h2><p>1）MAVEN install时候JVM内存溢出</p>
<p>处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。（详情查阅MAVEN 编译 JVM调优问题，如：<a href="http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method）" target="_blank" rel="external">http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method）</a></p>
<p>2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）：</p>
<p>[root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar</p>
<p>3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐</p>
<p>2.7.0版本的问题汇总帖子   <a href="http://www.tuicool.com/articles/IBn63qf" target="_blank" rel="external">http://www.tuicool.com/articles/IBn63qf</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;五-Hadoop编译源码&quot;&gt;&lt;a href=&quot;#五-Hadoop编译源码&quot; class=&quot;headerlink&quot; title=&quot;五 Hadoop编译源码&quot;&gt;&lt;/a&gt;五 Hadoop编译源码&lt;/h1&gt;&lt;h2 id=&quot;5-1-前期准备工作&quot;&gt;&lt;a href=&quot;#5-1
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(三)-Hadoop三种部署和运行方式</title>
    <link href="http://kingge.top/2018/02/26/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%B8%89-Hadoop%E4%B8%89%E7%A7%8D%E9%83%A8%E7%BD%B2%E5%92%8C%E8%BF%90%E8%A1%8C%E6%96%B9%E5%BC%8F/"/>
    <id>http://kingge.top/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/</id>
    <published>2018-02-26T15:21:59.000Z</published>
    <updated>2019-06-09T02:46:10.040Z</updated>
    
    <content type="html"><![CDATA[<h1 id="四-Hadoop运行模式"><a href="#四-Hadoop运行模式" class="headerlink" title="四 Hadoop运行模式"></a>四 Hadoop运行模式</h1><p><strong>1）官方网址</strong></p>
<p>（1）官方网站：</p>
<p><a href="http://hadoop.apache.org/" target="_blank" rel="external">http://hadoop.apache.org/</a></p>
<p>（2）各个版本归档库地址   </p>
<p><a href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/" target="_blank" rel="external">https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/</a></p>
<p>（3）hadoop2.7.2版本详情介绍</p>
<p><a href="http://hadoop.apache.org/docs/r2.7.2/" target="_blank" rel="external">http://hadoop.apache.org/docs/r2.7.2/</a></p>
<p><strong>2）Hadoop运行模式</strong></p>
<p>（1）本地模式（默认模式）：</p>
<p>不需要启用单独进程，直接可以运行，测试和开发时使用。</p>
<p>（2）伪分布式模式：</p>
<p>等同于完全分布式，只有一个节点。</p>
<p>（3）完全分布式模式：</p>
<p>多个节点一起运行。</p>
<h2 id="4-1-本地运行Hadoop-案例"><a href="#4-1-本地运行Hadoop-案例" class="headerlink" title="4.1 本地运行Hadoop 案例"></a>4.1 本地运行Hadoop 案例</h2><h3 id="4-1-1-官方grep案例"><a href="#4-1-1-官方grep案例" class="headerlink" title="4.1.1 官方grep案例"></a>4.1.1 官方grep案例</h3><p>1）创建在hadoop-2.7.2文件下面创建一个input文件夹</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$mkdir input</p>
<p>2）将hadoop的xml配置文件复制到input</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$cp etc/hadoop/*.xml input</p>
<p>3）执行share目录下的mapreduce程序</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar</p>
<p>share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output ‘dfs[a-z.]+’</p>
<p>4）查看输出结果</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ cat output/*</p>
<h3 id="4-1-2-官方wordcount案例"><a href="#4-1-2-官方wordcount案例" class="headerlink" title="4.1.2 官方wordcount案例"></a>4.1.2 官方wordcount案例</h3><p>1）创建在hadoop-2.7.2文件下面创建一个wcinput文件夹</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$mkdir wcinput</p>
<p>2）在wcinput文件下创建一个wc.input文件</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$cd wcinput</p>
<p>[kingge@hadoop101 wcinput]$touch wc.input</p>
<p>3）编辑wc.input文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[kingge@hadoop101 wcinput]$vim wc.input</div><div class="line">在文件中输入如下内容</div><div class="line">hadoop yarn</div><div class="line">hadoop mapreduce </div><div class="line">kingge</div><div class="line">kingge</div><div class="line">保存退出：：wq</div></pre></td></tr></table></figure>
<p>4）回到hadoop目录/opt/module/hadoop-2.7.2</p>
<p>5）执行程序：</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput</p>
<p>6）查看结果：</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$cat wcoutput/part-r-00000</p>
<p>kingge 2</p>
<p>hadoop  2</p>
<p>mapreduce       1</p>
<p>yarn    1</p>
<h3 id="4-1-3-总结"><a href="#4-1-3-总结" class="headerlink" title="4.1.3 总结"></a>4.1.3 总结</h3><p>第一个案例：统计input文件里面的文件，文件内容包含’dfs[a-z.]+’ 规则的文字，筛选出来。</p>
<p>第二个案例：统计wc.input 文件中单词出现的个数，<strong>特别注意，第二次运行之前必须删除已有的结果输出目录（wcoutput</strong>）（rm -rf wcoutput/），否则执行wordcount指令就会报错，提示文件已经存在</p>
<p>—-  操作设计的文件都是存储在linux 的文件系统中。本地操作，不支持联网操作</p>
<h2 id="4-2-伪分布式运行Hadoop案例"><a href="#4-2-伪分布式运行Hadoop案例" class="headerlink" title="4.2 伪分布式运行Hadoop案例"></a>4.2 伪分布式运行Hadoop案例</h2><h3 id="4-2-1-启动HDFS并运行MapReduce程序"><a href="#4-2-1-启动HDFS并运行MapReduce程序" class="headerlink" title="4.2.1 启动HDFS并运行MapReduce程序"></a>4.2.1 启动HDFS并运行MapReduce程序</h3><p>1）分析：</p>
<p>​         （1）准备1台客户机</p>
<p>​         （2）安装jdk</p>
<p>​         （3）配置环境变量</p>
<p>​         （4）安装hadoop</p>
<p>​         （5）配置环境变量</p>
<p>​         （6）配置集群</p>
<p>​         （7）启动、测试集群增、删、查</p>
<p>​         （8）执行wordcount案例</p>
<p>2）执行步骤</p>
<p>需要配置hadoop文件如下</p>
<h4 id="（1）配置集群"><a href="#（1）配置集群" class="headerlink" title="（1）配置集群"></a>（1）配置集群</h4><p>  （a）配置：hadoop-env.sh</p>
<p>​                          1.Linux系统中获取jdk的安装路径：</p>
<p>[kingge@hadoop100 hadoop-2.7.2]$ cd etc/hadoop/</p>
<p>[root@ hadoop101 ~]# echo $JAVA_HOME</p>
<p>/opt/module/jdk1.8.0_144</p>
<p>​                          2.修改Jhadoop-env.sh的JAVA_HOME 路径：</p>
<p>   export   JAVA_HOME=/opt/module/jdk1.8.0_144   </p>
<p>（b）配置：core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">    &lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</div><div class="line">	&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>（c）配置：hdfs-site.xml  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;!-- 指定HDFS副本的数量 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">	&lt;value&gt;1&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<h4 id="（2）启动集群"><a href="#（2）启动集群" class="headerlink" title="（2）启动集群"></a>（2）启动集群</h4><p><strong>（a）格式化namenode</strong>（<strong>第一次启动时格式化，以后就不要总格式化</strong>）</p>
<p>​                          [kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs namenode –format</p>
<p>​                 会在上面配置配置的存储目录生成 这两个文件</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image002.jpg" alt="img"></p>
<p> <strong>（b）启动namenode</strong></p>
<p>​                          [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</p>
<p>​    <strong>c）启动datanode</strong></p>
<p>​                          [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</p>
<h4 id="（3）查看集群"><a href="#（3）查看集群" class="headerlink" title="（3）查看集群"></a>（3）查看集群</h4><p>​                 <strong>（a）查看是否启动成功</strong></p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ jps</p>
<p>13586 NameNode</p>
<p>13668 DataNode</p>
<p>13786 Jps</p>
<p>​                 <strong>（b）查看产生的log日志</strong></p>
<p>当前目录：/opt/module/hadoop-2.7.2/logs</p>
<p>[kingge@hadoop101 logs]$ ls</p>
<p>hadoop-kingge-datanode-hadoop.kingge.com.log</p>
<p>hadoop-kingge-datanode-hadoop.kingge.com.out</p>
<p>hadoop-kingge-namenode-hadoop.kingge.com.log</p>
<p>hadoop-kingge-namenode-hadoop.kingge.com.out</p>
<p>SecurityAuth-root.audit</p>
<p>[kingge@hadoop101 logs]# cat hadoop-kingge-datanode-hadoop101.log</p>
<p>​                 <strong>（c）web端查看HDFS文件系统</strong></p>
<p>​                          <a href="http://192.168.1.101:50070/dfshealth.html#tab-overview" target="_blank" rel="external">http://192.168.1.101:50070/dfshealth.html#tab-overview</a></p>
<p>​                          注意：如果不能查看，看如下帖子处理</p>
<p><a href="http://www.cnblogs.com/zlslch/p/6604189.html" target="_blank" rel="external">http://www.cnblogs.com/zlslch/p/6604189.html</a></p>
<h4 id="（4）操作集群"><a href="#（4）操作集群" class="headerlink" title="（4）操作集群"></a>（4）操作集群</h4><p>​                 （a）在hdfs文件系统上<strong>创建</strong>一个input文件夹</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/kingge/input</p>
<p>​                 （b）将测试文件内容<strong>上传</strong>到文件系统上</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -put wcinput/wc.input  /user/kingge/input/</p>
<p>​                 <strong>（c）查看上传的文件是否正确</strong></p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -ls  /user/kingge/input/</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat  /user/kingge/ input/wc.input</p>
<p>​                 （d）运行mapreduce程序(<strong>所有数据在HDFS上</strong>)</p>
<p>​                          [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input/ /user/kingge/output</p>
<p>​                 （e）查看输出结果</p>
<p>命令行查看：</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/output/*</p>
<p>浏览器查看</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image004.jpg" alt="img"></p>
<p>​                 （f）将测试文件内容<strong>下载</strong>到本地</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ hadoop fs -get /user/kingge/ output/part-r-00000 ./wcoutput/</p>
<p>（g）<strong>删除</strong>输出结果</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ hdfs dfs -rmr /user/kingge/output</p>
<h3 id="4-2-2-YARN上运行MapReduce-程序"><a href="#4-2-2-YARN上运行MapReduce-程序" class="headerlink" title="4.2.2 YARN上运行MapReduce 程序"></a>4.2.2 YARN上运行MapReduce 程序</h3><h4 id="1）分析："><a href="#1）分析：" class="headerlink" title="1）分析："></a>1）分析：</h4><p>​         （1）准备1台客户机</p>
<p>​         （2）安装jdk</p>
<p>​         （3）配置环境变量</p>
<p>​         （4）安装hadoop</p>
<p>​         （5）配置环境变量</p>
<p>​         （6）配置集群yarn上运行</p>
<p>​         （7）启动、测试集群增、删、查</p>
<p>​         （8）在yarn上执行wordcount案例</p>
<h4 id="2）执行步骤"><a href="#2）执行步骤" class="headerlink" title="2）执行步骤"></a>2）执行步骤</h4><p><strong>（1）配置集群</strong></p>
<p>​（a）配置yarn-env.sh</p>
<p>​         配置一下JAVA_HOME</p>
<p>   export JAVA_HOME=/opt/module/jdk1.8.0_144   </p>
<p>（b）配置yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;!-- reducer获取数据的方式 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line"> &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line"> &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</div><div class="line">&lt;value&gt;hadoop101&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>​                 （c）配置：mapred-env.sh</p>
<p>​         配置一下JAVA_HOME</p>
<p>   export   JAVA_HOME=/opt/module/jdk1.8.0_144   </p>
<p>​                 （d）配置： (对mapred-site.xml.template重新命名为) mapred-site.xml</p>
<p>[kingge@hadoop101 hadoop]$ mv mapred-site.xml.template mapred-site.xml</p>
<p>[kingge@hadoop101 hadoop]$ vi mapred-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;!-- 指定mr运行在yarn上 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">	&lt;value&gt;yarn&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>​         <strong>（2）启动集群</strong></p>
<p>（a）启动resourcemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager</p>
<p>（b）启动nodemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager</p>
<p>​         <strong>（3）集群操作</strong></p>
<p>（a）yarn的浏览器页面查看</p>
<p><a href="http://192.168.1.101:8088/cluster" target="_blank" rel="external">http://192.168.1.101:8088/cluster</a></p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image006.jpg" alt="img"></p>
<p>​                 （b）删除文件系统上的output文件</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/kingge/output</p>
<p>​                 （c）执行mapreduce程序</p>
<p>​                          [kingge@hadoop101 hadoop-2.7.2]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input  /user/kingge/output</p>
<p>​                 （d）查看运行结果</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -cat /user/kingge/output/*</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image008.jpg" alt="img"></p>
<h3 id="4-2-3-配置临时文件存储路径"><a href="#4-2-3-配置临时文件存储路径" class="headerlink" title="4.2.3 配置临时文件存储路径"></a>4.2.3 配置临时文件存储路径</h3><p>1）停止进程</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop namenode</p>
<p>2）修改hadoop.tmp.dir</p>
<p>​         [core-site.xml]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line">	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</div><div class="line">	&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>3）将/opt/module/hadoop-2.7.2路径中的logs文件夹删除掉</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ rm -rf logs/</p>
<p>4）进入到tmp目录将tmp目录中hadoop-kingge目录删除掉</p>
<p>[kingge@hadoop101 tmp]$ rm -rf hadoop-kingge/</p>
<p>5）格式化NameNode</p>
<p>​         [kingge@hadoop101 hadoop-2.7.2]$ hadoop namenode -format</p>
<p>6）启动所有进程</p>
<p>​         [kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager</p>
<p>​         7）查看/opt/module/hadoop-2.7.2/data/tmp这个目录下的内容。</p>
<h3 id="4-2-4-配置历史服务器"><a href="#4-2-4-配置历史服务器" class="headerlink" title="4.2.4 配置历史服务器"></a>4.2.4 配置历史服务器</h3><p>​         1）配置mapred-site.xml</p>
<p>[kingge@hadoop101 hadoop]$ vi mapred-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</div><div class="line">&lt;value&gt;hadoop101:10020&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</div><div class="line">    &lt;value&gt;hadoop101:19888&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>​         2）查看启动历史服务器文件目录：</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ ls sbin/ | grep mr</p>
<p>mr-jobhistory-daemon.sh</p>
<p>​         3）启动历史服务器</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver</p>
<p>​         4）查看历史服务器是否启动</p>
<p>​                 [kingge@hadoop101 hadoop-2.7.2]$ jps</p>
<p>​         5）查看jobhistory</p>
<p><a href="http://192.168.1.101:19888/jobhistory" target="_blank" rel="external">http://192.168.1.101:19888/jobhistory</a></p>
<h3 id="4-2-5-配置日志的聚集"><a href="#4-2-5-配置日志的聚集" class="headerlink" title="4.2.5 配置日志的聚集"></a>4.2.5 配置日志的聚集</h3><p>日志聚集概念：应用运行完成以后，将日志信息上传到HDFS系统上。</p>
<p>开启日志聚集功能步骤：</p>
<p>（1）配置yarn-site.xml</p>
<p>[kingge@hadoop101 hadoop]$ vi yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;!-- 日志聚集功能使能 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</div><div class="line">&lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;!-- 日志保留时间设置7天 --&gt;</div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</div><div class="line">&lt;value&gt;604800&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>（2）关闭nodemanager 、resourcemanager和historymanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh stop historyserver</p>
<p>（3）启动nodemanager 、resourcemanager和historymanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver</p>
<p>（4）删除hdfs上已经存在的hdfs文件</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -rm -R /user/kingge/output</p>
<p>（5）执行wordcount程序</p>
<p>[kingge@hadoop101 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/kingge/input /user/kingge/output</p>
<p>（6）查看日志</p>
<p><a href="http://192.168.1.101:19888/jobhistory" target="_blank" rel="external">http://192.168.1.101:19888/jobhistory</a></p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image0022.jpg" alt="img"></p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image012.jpg" alt="img"></p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image014.jpg" alt="img"></p>
<h3 id="4-2-6-配置文件说明"><a href="#4-2-6-配置文件说明" class="headerlink" title="4.2.6 配置文件说明"></a>4.2.6 配置文件说明</h3><p>Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
<p>（1）默认配置文件：存放在hadoop相应的jar包中</p>
<p>[core-default.xml]</p>
<p>​                          hadoop-common-2.7.2.jar/ core-default.xml</p>
<p>​                 [hdfs-default.xml]</p>
<p>hadoop-hdfs-2.7.2.jar/ hdfs-default.xml</p>
<p>​                 [yarn-default.xml]</p>
<p>hadoop-yarn-common-2.7.2.jar/ yarn-default.xml</p>
<p>​                 [core-default.xml]</p>
<p>hadoop-mapreduce-client-core-2.7.2.jar/ core-default.xml</p>
<p>​         （2）自定义配置文件：存放在$HADOOP_HOME/etc/hadoop</p>
<p>​                 core-site.xml</p>
<p>​                 hdfs-site.xml</p>
<p>​                 yarn-site.xml</p>
<p>​                 mapred-site.xml</p>
<h3 id="4-2-7-总结"><a href="#4-2-7-总结" class="headerlink" title="4.2.7 总结"></a>4.2.7 总结</h3><p>相比于本地运行模式，伪分布式模式支持互联网操作，不过集群的副本是1（不配置的话默认是3，详情查看hdfs-default.xml的dfs.replication属性）。</p>
<h2 id="4-3-完全分布式部署Hadoop"><a href="#4-3-完全分布式部署Hadoop" class="headerlink" title="4.3 完全分布式部署Hadoop"></a>4.3 完全分布式部署Hadoop</h2><p>分析：</p>
<p>​         1）准备3台客户机（关闭防火墙、静态ip、主机名称）</p>
<p>​         2）安装jdk</p>
<p>​         3）配置环境变量</p>
<p>​         4）安装hadoop</p>
<p>​         5）配置环境变量</p>
<p>​         6）安装ssh</p>
<p>​         7）配置集群</p>
<p>​         8）启动测试集群</p>
<h3 id="4-3-1-虚拟机准备"><a href="#4-3-1-虚拟机准备" class="headerlink" title="4.3.1 虚拟机准备"></a>4.3.1 虚拟机准备</h3><p>详见3.2-3.3章。</p>
<p><a href="/2018/02/24/hadoop大数据-二-运行环境搭建">Hadoop运行环境搭建3.2-3.3</a></p>
<h3 id="4-3-2-主机名设置"><a href="#4-3-2-主机名设置" class="headerlink" title="4.3.2 主机名设置"></a>4.3.2 主机名设置</h3><p><a href="/2018/02/24/hadoop大数据-二-运行环境搭建">Hadoop运行环境搭建3.4</a></p>
<h3 id="4-3-3-scp"><a href="#4-3-3-scp" class="headerlink" title="4.3.3 scp"></a>4.3.3 scp</h3><p>1）scp可以实现服务器与服务器之间的数据拷贝。</p>
<p>  操作一：hadoop101 主动推送数据到hadoop102</p>
<p>  操作二：hadoop102主动从hadoop101获取数据到本地</p>
<p>  操作三：hadoop101 控制将hadoop102的数据拷贝到hadoop103</p>
<p>​        <strong>接收方一般使用root**</strong>用户接受，因为有些文件夹的权限只有root<strong>用户才有，为保证传输成功，双方最好都切换到root</strong>用户**</p>
<p>2）案例实操</p>
<p>（1）将hadoop101中/opt/module和/opt/software文件拷贝到hadoop102、hadoop103和hadoop104上。</p>
<p>[root@hadoop101 /]# scp -r /opt/module/  root@hadoop102:/opt</p>
<p>[root@hadoop101 /]# scp -r /opt/software/  root@hadoop102:/opt</p>
<p>[root@hadoop101 /]# scp -r /opt/module/  root@hadoop103:/opt</p>
<p>[root@hadoop101 /]# scp -r /opt/software/  root@hadoop103:/opt</p>
<p>[root@hadoop101 /]# scp -r /opt/module/  root@hadoop104:/opt</p>
<p>[root@hadoop101 /]# scp -r /opt/software/  root@hadoop105:/opt</p>
<p>（2）将hadoop101服务器上的/etc/profile文件拷贝到hadoop102上。</p>
<p>[root@hadoop102 opt]# scp root@hadoop101:/etc/profile /etc/profile 例子1</p>
<p>[root@hadoop102 opt]# scp -r  root@192.168.1.101:/opt/module/ /opt/–例子二</p>
<p>​         （3）实现两台远程机器之间的文件传输（hadoop103主机文件拷贝到hadoop104主机上）</p>
<p>​                 [kingge@hadoop102 test]$ scp kingge@hadoop103:/opt/test/haha kingge@hadoop104:/opt/test/</p>
<p>注意：如果传递环境变量配置文件后需要source /etc/profile 一下，让其生效。同时可能需要修改一下文件的权限或者文件所属（chmod chown）</p>
<h3 id="4-3-4-SSH无密码登录"><a href="#4-3-4-SSH无密码登录" class="headerlink" title="4.3.4 SSH无密码登录"></a>4.3.4 SSH无密码登录</h3><p> <em>针对执行ssh<strong>命令</strong>的</em> <em>无密码操作</em></p>
<p>1）配置ssh</p>
<p>（1）基本语法</p>
<p>ssh 另一台电脑的ip地址</p>
<p>（2）ssh连接时出现Host key verification failed的解决方法</p>
<p>[root@hadoop102 opt]# ssh 192.168.1.103</p>
<p>The authenticity of host ‘192.168.1.103 (192.168.1.103)’ can’t be established.</p>
<p>RSA key fingerprint is cf:1e:de:d7:d0:4c:2d:98:60:b4:fd:ae:b1:2d:ad:06.</p>
<p>Are you sure you want to continue connecting (yes/no)? </p>
<p>Host key verification failed.</p>
<p>（3）解决方案如下：直接输入yes</p>
<p>2）无密钥配置</p>
<p>（1）进入到我的home目录</p>
<p>​                 [kingge@hadoop102 opt]$ cd ~/.ssh </p>
<p>（2）生成公钥和私钥：</p>
<p>[kingge@hadoop102 .ssh]$ ssh-keygen -t rsa </p>
<p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p>
<p>（3）将公钥拷贝到要免密登录的目标机器上</p>
<p>[kingge@hadoop102 .ssh]$ ssh-copy-id hadoop102 （<strong>给自己授权免密登录</strong>）</p>
<p>[kingge@hadoop102 .ssh]$ ssh-copy-id hadoop103</p>
<p>[kingge@hadoop102 .ssh]$ ssh-copy-id hadoop104</p>
<p>查看103或者104的~/.ssh，发现多了authorized_keys 个文件</p>
<p>需求： A服务器需要访问B服务器，不需要输入密码</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/8026452.png" alt="1560048026452"></p>
<p>3）.ssh文件夹下的文件功能解释</p>
<p>​         （1）~/.ssh/known_hosts      ：记录ssh访问过计算机的公钥(public key)</p>
<p>​         （2）id_rsa    ：生成的私钥</p>
<p>​         （3）id_rsa.pub     ：生成的公钥</p>
<p>​         （4）authorized_keys    ：存放授权过得无秘登录服务器公钥</p>
<p><strong>注意：</strong> 如果你授权的免密登录用户，被切换了，那么还是需要输入密码才能够登录。</p>
<p><strong>例子：hadoop100服务器的kingge用户生成了密匙，然后发给你了hadoop101，那么100服务器就可以免密登录101服务器，但是假设100服务器su root（切换为了root用户），那么当执行ssh hadoop101 操作时，就需要输入101服务器密码，而不能免密登录，所以要想在root用户下也能免密登录101服务器，就需要在root用户下重新走一遍免密登录流程</strong></p>
<h3 id="4-3-5-rsync"><a href="#4-3-5-rsync" class="headerlink" title="4.3.5 rsync"></a>4.3.5 rsync</h3><p>rsync远程同步工具，主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p><strong>rsync**</strong>和<strong><strong>scp</strong></strong>区别：<strong><strong>用</strong></strong>rsync<strong><strong>做文件的复制要比</strong></strong>scp<strong><strong>的速度快，</strong></strong>rsync<strong><strong>只对差异文件做更新。</strong></strong>scp<strong>**是把所有文件都复制过去。</strong></p>
<p>（1）查看rsync使用说明</p>
<p>man rsync | more</p>
<p>​         （2）基本语法</p>
<p>rsync -rvl     $pdir/$fname         $user@hadoop$host:$pdir</p>
<p>​                 命令 命令参数 要拷贝的文件路径/名称   目的用户@主机:目的路径</p>
<p>​                 选项</p>
<p>-r 递归</p>
<p>-v 显示复制过程</p>
<p>-l 拷贝符号连接</p>
<p>​         （3）案例实操</p>
<p>​                 把本机/opt/tmp目录同步到hadoop103服务器的root用户下的/opt/tmp目录</p>
<p>[kingge@hadoop102 opt]$ rsync -rvl /opt/tmp  root@hadoop103:/opt/</p>
<h3 id="4-3-6-编写集群分发脚本xsync"><a href="#4-3-6-编写集群分发脚本xsync" class="headerlink" title="4.3.6 编写集群分发脚本xsync"></a>4.3.6 编写集群分发脚本xsync</h3><p> <strong>场景：分布式系统中假设有6900**</strong>台服务器，那么假设我们需要同步配置信息，我们不可能一台台的执行scp/rsync<strong> </strong>命令，效率极低，那怎么办呢？**</p>
<p>1）需求分析：循环复制文件到所有节点的相同目录下。</p>
<p>​         （1）原始拷贝：</p>
<p>rsync  -rvl     /opt/module                root@hadoop103:/opt/</p>
<p>​         （2）期望脚本：</p>
<p>xsync 要同步的文件名称</p>
<p>​         （3）在/usr/local/bin这个目录下存放的脚本，可以在系统任何地方直接执行。（就是在执行xsync命令时可以不用输入/usr/local/bin这样前缀）</p>
<p>2）案例实操：</p>
<p>（1）在/usr/local/bin目录下创建xsync文件，文件内容如下：</p>
<p>[root@hadoop102 bin]# touch xsync</p>
<p>[root@hadoop102 bin]# vi xsync</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line">#1 获取输入参数个数，如果没有参数，直接退出</div><div class="line">pcount=$#</div><div class="line">if((pcount==0)); then</div><div class="line">echo no args;</div><div class="line">exit;</div><div class="line">fi</div><div class="line"></div><div class="line">#2 获取文件名称</div><div class="line">p1=$1</div><div class="line">fname=`basename $p1`</div><div class="line">echo fname=$fname</div><div class="line"></div><div class="line">#3 获取上级目录到绝对路径</div><div class="line">pdir=`cd -P $(dirname $p1); pwd`</div><div class="line">echo pdir=$pdir</div><div class="line"></div><div class="line">#4 获取当前用户名称</div><div class="line">user=`whoami`</div><div class="line"></div><div class="line">#5 循环</div><div class="line">for((host=103; host&lt;105; host++)); do</div><div class="line">        #echo $pdir/$fname $user@hadoop$host:$pdir</div><div class="line">        echo --------------- hadoop$host ----------------</div><div class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</div><div class="line">done</div></pre></td></tr></table></figure>
<p>（2）修改脚本 xsync 具有执行权限</p>
<p>[root@hadoop102 bin]# chmod 777 xsync</p>
<p>[root@hadoop102 bin]# chown kingge:kingge -R xsync</p>
<p>（3）调用脚本形式：xsync 文件名称</p>
<p>[kingge@hadoop102 opt]$ xsync tmp/</p>
<h3 id="4-3-7-编写集群操作脚本xcall"><a href="#4-3-7-编写集群操作脚本xcall" class="headerlink" title="4.3.7 编写集群操作脚本xcall"></a>4.3.7 编写集群操作脚本xcall</h3><p>1）需求分析：在所有主机上同时执行相同的命令</p>
<p>xcall +命令</p>
<p>2）具体实现</p>
<p>（1）在/usr/local/bin目录下创建xcall文件，文件内容如下：</p>
<p>[root@hadoop102 bin]# touch xcall</p>
<p>[root@hadoop102 bin]# vi xcall</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line">pcount=$#</div><div class="line">if((pcount==0));then</div><div class="line">        echo no args;</div><div class="line">        exit;</div><div class="line">fi</div><div class="line"></div><div class="line">echo -------------localhost----------</div><div class="line">$@</div><div class="line">for((host=101; host&lt;=108; host++)); do</div><div class="line">        echo ----------hadoop$host---------</div><div class="line">        ssh hadoop$host $@</div><div class="line">done</div></pre></td></tr></table></figure>
<p>（2）修改脚本xcall具有执行权限</p>
<p>​                [root@hadoop102 bin]# chmod 777 xcall</p>
<p>[root@hadoop102 bin]# chown kingge:kingge xcall</p>
<p>（3）调用脚本形式： xcall 操作命令</p>
<p>[root@hadoop102 ~]# xcall rm -rf /opt/tmp/ </p>
<h3 id="4-3-8-配置集群"><a href="#4-3-8-配置集群" class="headerlink" title="4.3.8 配置集群"></a>4.3.8 配置集群</h3><h4 id="1）集群部署规划"><a href="#1）集群部署规划" class="headerlink" title="1）集群部署规划"></a>1）集群部署规划</h4><table>
<thead>
<tr>
<th></th>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS</td>
<td>NameNode   DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode   DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>ResourceManager   NodeManager</td>
<td>NodeManager</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>   <strong>集群规划的原则</strong>：NameNode/SecondaryNameNode/ ResourceManager必须要单独占据一个服务器(或者这三个不能在同一个节点上运行)，因为他是相当于目录，请求他的次数最大，所以不能跟其他插件部署在一起，不能跟NameNode抢占资源，因为他不能挂掉。Datanode就没有这些限制，挂掉也无所谓。</p>
<p>   但是下面的例子中NameNode和DataNode都部署在hadoop102这个节点，因为我们是属于测试搭建环境下，所以无所谓，<strong>但是生产环境下必须按照规则</strong></p>
<h4 id="2）配置文件"><a href="#2）配置文件" class="headerlink" title="2）配置文件"></a>2）配置文件</h4><p>​         （1）core-site.xml</p>
<p>[kingge@hadoop102 hadoop]$ vi core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">        &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</div><div class="line">		&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div></pre></td></tr></table></figure>
<p>​         （2）Hdfs</p>
<p>​                 <strong>2.1 hadoop-env.sh</strong></p>
<p>[kingge@hadoop102 hadoop]$ vi hadoop-env.sh</p>
<p>   export   JAVA_HOME=/opt/module/jdk1.8.0_144   </p>
<p>​                 <strong>2.2 hdfs-site.xml</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;	</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">		&lt;value&gt;3&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line"># 如果不配置。默认是跟namenode同个位置</div><div class="line">	&lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</div><div class="line">        &lt;value&gt;hadoop104:50090&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>​                 <strong>2.3 Slaves</strong></p>
<p>​              <strong>配置文件里面不能存在多余的空格或者换行</strong></p>
<p>[kingge@hadoop102 hadoop]$ vi slaves</p>
<p>   hadoop102   hadoop103   hadoop104   </p>
<p>​         （3）yarn</p>
<p>​                 yarn-env.sh</p>
<p>[kingge@hadoop102 hadoop]$ vi yarn-env.sh</p>
<p>   export   JAVA_HOME=/opt/module/jdk1.8.0_144   </p>
<p>​                 yarn-site.xml</p>
<p>​                 [kingge@hadoop102 hadoop]$ vi yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line"></div><div class="line">	&lt;!-- reducer获取数据的方式 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		 &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">		 &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</div><div class="line">		&lt;value&gt;hadoop103&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>​         （4）mapreduce</p>
<p>​                 mapred-env.sh</p>
<p>[kingge@hadoop102 hadoop]$ vi mapred-env.sh</p>
<p>   export   JAVA_HOME=/opt/module/jdk1.8.0_144   </p>
<p>​                 mapred-site.xml</p>
<p>[kingge@hadoop102 hadoop]$ vi mapred-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">	&lt;!-- 指定mr运行在yarn上 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">		&lt;value&gt;yarn&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<h4 id="3）在集群上分发以上所有文件"><a href="#3）在集群上分发以上所有文件" class="headerlink" title="3）在集群上分发以上所有文件"></a>3）在集群上分发以上所有文件</h4><p>[kingge@hadoop102 hadoop]$ pwd</p>
<p>/opt/module/hadoop-2.7.2/etc/hadoop</p>
<p>[kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml</p>
<p>[kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml</p>
<p>[kingge@hadoop102 hadoop]$ xsync /opt/module/hadoop-2.7.2/etc/hadoop/slaves</p>
<p>4）查看文件分发情况</p>
<p>[kingge@hadoop102 hadoop]$ xcall cat /opt/module/hadoop-2.7.2/etc/hadoop/slaves</p>
<h3 id="4-3-9-集群启动及测试"><a href="#4-3-9-集群启动及测试" class="headerlink" title="4.3.9 集群启动及测试"></a>4.3.9 集群启动及测试</h3><p>1）启动集群</p>
<p>​        清空之前启动namenode的数据</p>
<p>​            Rm –rf data/ log/</p>
<p>​       Data就是在core-site.xml中配置的文件存储目录，log就是hadoop的log目录。</p>
<p>​         （0）如果集群是第一次启动，需要格式化namenode</p>
<p>​                 [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -format</p>
<p>（1）启动HDFS：</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</p>
<p> –<strong>这一步跟我们之前的单个启动不一样（sbin/hadoop-daemon.sh start namenode**</strong>），这个操作是启动整个集群的namenode<strong>**和datanode</strong>—那么就需要配置ssh 无密码登录</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image00209.png" alt="img"></p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ jps</p>
<p>4166 NameNode</p>
<p>4482 Jps</p>
<p>4263 DataNode</p>
<p>[kingge@hadoop103 hadoop-2.7.2]$ jps</p>
<p>3218 DataNode</p>
<p>3288 Jps</p>
<p>[kingge@hadoop104 hadoop-2.7.2]$ jps</p>
<p>3221 DataNode</p>
<p>3283 SecondaryNameNode</p>
<p>3364 Jps</p>
<p>（2）启动yarn （启动namemanager 和 resourceManager）</p>
<p>[kingge@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</p>
<p>注意：<strong>Namenode和ResourceManger如果不是同一台机器，不能在NameNode上启动 yarn，应该在ResouceManager所在的机器上启动yarn。</strong></p>
<p> <strong>因为我们在这个集群中ResourceManager是在hadoop103上面的，所以在103上启动</strong></p>
<p>2）集群基本测试</p>
<p>（1）上传文件到集群</p>
<p>​         1.上传小文件</p>
<p>​         [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/kingge/input</p>
<p>​         [kingge@hadoop102 hadoop-2.7.2]$ bin/hdfs dfs -put etc/hadoop/wc.input /user/kingge/input</p>
<p>上传完后 hadoop103和hadoop104 上面也会同步副本（即是，也会存在上面的/user/kingge/tep/conf 和 *-site.xml  这些东西）</p>
<p> 查看HDFS系统的文件结构</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_imag3e006.jpg" alt="img"></p>
<p> 我们可以看到他是存储在了块0，而且有三个副本101/102/103</p>
<p>​    接下来我们再上传一个大文件看看</p>
<p>​         2.上传大文件</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -put /opt/software/hadoop-2.7.2.tar.gz  /user/kingge/input</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_imagfe008.jpg" alt="img"></p>
<p><strong>为什么他这里会分为两块存储呢？因为默认块大小是128，当超过这个大小时就需要分块存储</strong></p>
<p>（2）上传文件后查看文件存放在什么位置</p>
<p>​         文件存储路径</p>
<p>​         [kingge@hadoop102 subdir0]$ pwd</p>
<p>/opt/module/hadoop-2.7.2/data/tmp/dfs/data/current/BP-938951106-192.168.10.107-1495462844069/current/finalized/subdir0/subdir0</p>
<p>​         查看文件内容(<strong>wc.input</strong>)</p>
<p>[kingge@hadoop102 subdir0]$ cat blk_1073741825</p>
<p>hadoop</p>
<p>kingge</p>
<p>kingge</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_image0r09.png" alt="img"></p>
<p>然后26 27 就是那个大文件，分为了两块</p>
<p>（3）拼接</p>
<p>-rw-rw-r–. 1 kingge kingge 134217728 5月  23 16:01 blk_1073741836</p>
<p>-rw-rw-r–. 1 kingge kingge   1048583 5月  23 16:01 blk_1073741836_1012.meta</p>
<p>-rw-rw-r–. 1 kingge kingge  63439959 5月  23 16:01 blk_1073741837</p>
<p>-rw-rw-r–. 1 kingge kingge    495635 5月  23 16:01 blk_1073741837_1013.meta</p>
<p>[kingge@hadoop102 subdir0]$ cat blk_1073741836&gt;&gt;tmp.file</p>
<p>[kingge@hadoop102 subdir0]$ cat blk_1073741837&gt;&gt;tmp.file</p>
<p>[kingge@hadoop102 subdir0]$ tar -zxvf tmp.file</p>
<p>  <strong>已解压发现就是我们上传那个tar**</strong>大文件的解压版本，说明这两个文件就是存放着大文件**</p>
<p>（4）下载</p>
<p>[kingge@hadoop102 hadoop-2.7.2]$ bin/hadoop fs -get /user/kingge/input/hadoop-2.7.2.tar.gz</p>
<p>3）性能测试集群</p>
<p>​         写海量数据</p>
<p>​         读海量数据</p>
<h3 id="4-3-10-Hadoop启动停止方式"><a href="#4-3-10-Hadoop启动停止方式" class="headerlink" title="4.3.10 Hadoop启动停止方式"></a>4.3.10 Hadoop启动停止方式</h3><p>1）各个服务组件逐一启动</p>
<p>​         （1）分别启动hdfs组件</p>
<p>​                 hadoop-daemon.sh  start|stop  namenode|datanode|secondarynamenode</p>
<p>​         （2）启动yarn</p>
<p>​                 yarn-daemon.sh  start|stop  resourcemanager|nodemanager</p>
<p>2）各个模块分开启动（配置ssh是前提）常用</p>
<p>​         （1）整体启动/停止hdfs</p>
<p>​                 start-dfs.sh</p>
<p>​                 stop-dfs.sh</p>
<p>​         （2）整体启动/停止yarn</p>
<p>​                 start-yarn.sh</p>
<p>​                 stop-yarn.sh</p>
<p>3）全部启动（不建议使用）</p>
<p>​         start-all.sh</p>
<p>​         stop-all.sh</p>
<h3 id="4-3-11-集群时间同步"><a href="#4-3-11-集群时间同步" class="headerlink" title="4.3.11 集群时间同步"></a>4.3.11 集群时间同步</h3><p>时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。</p>
<p><strong>配置时间同步实操：</strong></p>
<p>1）时间服务器配置（必须root用户）</p>
<p>（1）检查ntp是否安装</p>
<p>[root@hadoop102 桌面]# rpm -qa|grep ntp</p>
<p>ntp-4.2.6p5-10.el6.centos.x86_64</p>
<p>fontpackages-filesystem-1.41-1.1.el6.noarch</p>
<p>ntpdate-4.2.6p5-10.el6.centos.x86_64</p>
<p>（2）修改ntp配置文件</p>
<p>[root@hadoop102 桌面]# vi /etc/ntp.conf</p>
<p>修改内容如下</p>
<p>a）修改1</p>
<p>#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap为</p>
<p>restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</p>
<p>​                 b）修改2 注释时间服务器</p>
<p>server 0.centos.pool.ntp.org iburst</p>
<p>server 1.centos.pool.ntp.org iburst</p>
<p>server 2.centos.pool.ntp.org iburst</p>
<p>server 3.centos.pool.ntp.org iburst为</p>
<p>#server 0.centos.pool.ntp.org iburst</p>
<p>#server 1.centos.pool.ntp.org iburst</p>
<p>#server 2.centos.pool.ntp.org iburst</p>
<p>#server 3.centos.pool.ntp.org iburst</p>
<p>​                 c）添加3 自己的时间服务器<img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/clip_imagwe010.png" alt="img"></p>
<p>server 127.127.1.0</p>
<p>fudge 127.127.1.0 stratum 10</p>
<p>（3）修改/etc/sysconfig/ntpd 文件</p>
<p>[root@hadoop102 桌面]# vim /etc/sysconfig/ntpd</p>
<p>增加内容如下</p>
<p>SYNC_HWCLOCK=yes</p>
<p>​         （4）重新启动ntpd</p>
<p>[root@hadoop102 桌面]# service ntpd status</p>
<p>ntpd 已停</p>
<p>[root@hadoop102 桌面]# service ntpd start</p>
<p>正在启动 ntpd：                                            [确定]</p>
<p>​         （5）执行：</p>
<p>​                  [root@hadoop102 桌面]# chkconfig ntpd on</p>
<p>2）其他机器配置（必须root用户）</p>
<p>​         （1）在其他机器配置10分钟与时间服务器同步一次</p>
<p>​                 [root@hadoop103 hadoop-2.7.2]# crontab -e</p>
<p>​                 编写脚本</p>
<p>​                 <em>/10 </em> <em> </em> * /usr/sbin/ntpdate hadoop102</p>
<p>​         （2）修改任意机器时间</p>
<p>​                 [root@hadoop103 hadoop]# date -s “2017-9-11 11:11:11”</p>
<p>​         （3）十分钟后查看机器是否与时间服务器同步</p>
<p>​                 [root@hadoop103 hadoop]# date</p>
<h3 id="4-3-12-配置集群常见问题"><a href="#4-3-12-配置集群常见问题" class="headerlink" title="4.3.12 配置集群常见问题"></a>4.3.12 配置集群常见问题</h3><p>1）防火墙没关闭、或者没有启动yarn</p>
<p><em>INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032</em></p>
<p>2）主机名称配置错误</p>
<p>3）ip地址配置错误</p>
<p>4）ssh没有配置好</p>
<p>5）root用户和kingge两个用户启动集群不统一</p>
<p>6）配置文件修改不细心</p>
<p>7）未编译源码</p>
<p><em>Unable to load native-hadoop library for your platform… using builtin-java classes where applicable</em></p>
<p><em>17/05/22 15:38:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032</em></p>
<p>8）datanode不被namenode识别问题</p>
<p>Namenode在format初始化的时候会形成两个标识，blockPoolId和clusterId。新的datanode加入时，会获取这两个标识作为自己工作目录中的标识。</p>
<p>一旦namenode重新format后，namenode的身份标识已变，而datanode如果依然持有原来的id，就不会被namenode识别。</p>
<p>解决办法，删除datanode节点中的数据后，再次重新格式化namenode。</p>
<p>9）不识别主机名称</p>
<p>   java.net.UnknownHostException: hadoop102: hadoop102           at   java.net.InetAddress.getLocalHost(InetAddress.java:1475)           at   org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:146)           at   org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)           at   org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)           at   java.security.AccessController.doPrivileged(Native Method)           at   javax.security.auth.Subject.doAs(Subject.java:415)   </p>
<p>解决办法：</p>
<p>（1）在/etc/hosts文件中添加192.168.1.102 hadoop102</p>
<p>​         （2）主机名称不要起hadoop  hadoop000等特殊名称</p>
<p>10）datanode和namenode进程同时只能工作一个。</p>
<p><img src="/2018/02/26/hadoop大数据-三-Hadoop三种部署和运行方式/0048286271.png" alt="1560048286271"></p>
<p>11）执行命令 不生效，粘贴word中命令时，遇到-和长–没区分开。导致命令失效</p>
<p>解决办法：尽量不要粘贴word中代码。</p>
<p>12）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。</p>
<p>13）jps不生效。</p>
<p>原因：全局变量hadoop   java没有生效，需要source /etc/profile文件。</p>
<p>14）8088端口连接不上</p>
<p>[kingge@hadoop102 桌面]$ cat /etc/hosts</p>
<p>注释掉如下代码</p>
<p>#127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</p>
<p>#::1         hadoop102</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;四-Hadoop运行模式&quot;&gt;&lt;a href=&quot;#四-Hadoop运行模式&quot; class=&quot;headerlink&quot; title=&quot;四 Hadoop运行模式&quot;&gt;&lt;/a&gt;四 Hadoop运行模式&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;1）官方网址&lt;/strong&gt;&lt;/p&gt;
&lt;p
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(二)-运行环境搭建</title>
    <link href="http://kingge.top/2018/02/24/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%BA%8C-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://kingge.top/2018/02/24/hadoop大数据-二-运行环境搭建/</id>
    <published>2018-02-24T02:21:59.000Z</published>
    <updated>2019-06-07T09:12:38.271Z</updated>
    
    <content type="html"><![CDATA[<h1 id="三-Hadoop运行环境搭建"><a href="#三-Hadoop运行环境搭建" class="headerlink" title="三 Hadoop运行环境搭建"></a>三 Hadoop运行环境搭建</h1><h2 id="3-0-前置准备"><a href="#3-0-前置准备" class="headerlink" title="3.0 前置准备"></a>3.0 前置准备</h2><p>需要linux相关的知识，和安装虚拟机。所以还不了解的请看：<a href="/2017/06/12/linux基础">Linux基础</a></p>
<h2 id="3-1-虚拟机网络模式设置为NAT"><a href="#3-1-虚拟机网络模式设置为NAT" class="headerlink" title="3.1 虚拟机网络模式设置为NAT"></a>3.1 虚拟机网络模式设置为NAT</h2><p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/9896745303.png" alt="1559896745303"></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image0072.jpg" alt="1559896834748"></p>
<p>​         最后，重新启动系统。</p>
<p>​                  [root@hadoop101 ~]# sync</p>
<p>​                  [root@hadoop101 ~]# reboot</p>
<h2 id="3-2-克隆虚拟机"><a href="#3-2-克隆虚拟机" class="headerlink" title="3.2 克隆虚拟机"></a>3.2 克隆虚拟机</h2><p>1）克隆虚拟机</p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/20366.png" alt="1559896920366"></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image008.jpg" alt="img"></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image010.jpg" alt="img"></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image012.jpg" alt="img"></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image014.jpg" alt="img"></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image016.jpg" alt="img"></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image018.jpg" alt="img"></p>
<p>2）启动虚拟机</p>
<h2 id="3-3-修改为静态ip"><a href="#3-3-修改为静态ip" class="headerlink" title="3.3 修改为静态ip"></a>3.3 修改为静态ip</h2><h3 id="1）在终端命令窗口中输入"><a href="#1）在终端命令窗口中输入" class="headerlink" title="1）在终端命令窗口中输入"></a><strong>1）在终端命令窗口中输入</strong></h3><p>[root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules</p>
<p>进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址</p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image020.jpg" alt="img"></p>
<h3 id="2）修改IP地址"><a href="#2）修改IP地址" class="headerlink" title="2）修改IP地址"></a><strong>2）修改IP地址</strong></h3><p>[root@hadoop101 /]# vim /etc/sysconfig/network-scripts/ifcfg-eth0</p>
<p>需要修改的内容有5项：</p>
<p>IPADDR=192.168.1.101</p>
<p>GATEWAY=192.168.1.2</p>
<p>ONBOOT=yes</p>
<p>BOOTPROTO=static</p>
<p>DNS1=192.168.1.2</p>
<p>​         <strong>（1）修改前</strong></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image022.jpg" alt="img"></p>
<p>​         <strong>（2）修改后</strong></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image023.png" alt="img"></p>
<p>：wq  保存退出</p>
<h3 id="3）执行"><a href="#3）执行" class="headerlink" title="3）执行"></a>3）执行</h3><p>[root@hadoop101 /]# service network restart</p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image025.jpg" alt="img"></p>
<h3 id="4）如果报错，reboot，重启虚拟机。"><a href="#4）如果报错，reboot，重启虚拟机。" class="headerlink" title="4）如果报错，reboot，重启虚拟机。"></a>4）如果报错，reboot，重启虚拟机。</h3><p>​         [root@hadoop101 /]# reboot</p>
<h2 id="3-4-修改主机名"><a href="#3-4-修改主机名" class="headerlink" title="3.4 修改主机名"></a>3.4 修改主机名</h2><h3 id="1）修改linux的hosts文件"><a href="#1）修改linux的hosts文件" class="headerlink" title="1）修改linux的hosts文件"></a>1）修改linux的hosts文件</h3><p>（1）进入Linux系统查看本机的主机名。通过hostname命令查看。</p>
<p>[root@hadoop100 /]# hostname</p>
<p>hadoop100</p>
<p>（2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件。</p>
<p>[root@hadoop100~]# vi /etc/sysconfig/network</p>
<p>修改文件中主机名称</p>
<p>NETWORKING=yes</p>
<p>NETWORKING_IPV6=no</p>
<p>HOSTNAME= hadoop101</p>
<p>注意：主机名称不要有“_”下划线</p>
<p>（3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。</p>
<p>（4）保存退出。</p>
<p>（5）打开/etc/hosts</p>
<p>[root@hadoop100 ~]# vim /etc/hosts</p>
<p>添加如下内容</p>
<p>192.168.1.100 hadoop100</p>
<p>192.168.1.101 hadoop101</p>
<p>192.168.1.102 hadoop102</p>
<p>192.168.1.103 hadoop103</p>
<p>192.168.1.104 hadoop104</p>
<p>192.168.1.105 hadoop105</p>
<p>192.168.1.106 hadoop106</p>
<p>192.168.1.107 hadoop107</p>
<p>192.168.1.108 hadoop108</p>
<p>192.168.1.109 hadoop109</p>
<p>192.168.1.110 hadoop110</p>
<p>（6）并重启设备，重启后，查看主机名，已经修改成功</p>
<h3 id="2）修改window7的hosts文件-可以不改"><a href="#2）修改window7的hosts文件-可以不改" class="headerlink" title="2）修改window7的hosts文件(可以不改)"></a>2）修改window7的hosts文件(可以不改)</h3><p>只不过是方便于在windows服务器中，使用域名的方式访问hadopp相关的组件。</p>
<p>​         （1）进入C:\Windows\System32\drivers\etc路径</p>
<p>​         （2）打开hosts文件并添加如下内容</p>
<p>192.168.1.100 hadoop100</p>
<p>192.168.1.101 hadoop101</p>
<p>192.168.1.102 hadoop102</p>
<p>192.168.1.103 hadoop103</p>
<p>192.168.1.104 hadoop104</p>
<p>192.168.1.105 hadoop105</p>
<p>192.168.1.106 hadoop106</p>
<p>192.168.1.107 hadoop107</p>
<p>192.168.1.108 hadoop108</p>
<p>192.168.1.109 hadoop109</p>
<p>192.168.1.110 hadoop110</p>
<h2 id="3-5-关闭防火墙"><a href="#3-5-关闭防火墙" class="headerlink" title="3.5 关闭防火墙"></a>3.5 关闭防火墙</h2><p>1）查看防火墙开机启动状态</p>
<p>[root@hadoop101 ~]# chkconfig iptables –list</p>
<p>2）关闭防火墙</p>
<p>[root@hadoop101 ~]# chkconfig iptables off  </p>
<h2 id="3-6-在opt目录下创建文件"><a href="#3-6-在opt目录下创建文件" class="headerlink" title="3.6 在opt目录下创建文件"></a>3.6 在opt目录下创建文件</h2><h3 id="1）创建kingge用户"><a href="#1）创建kingge用户" class="headerlink" title="1）创建kingge用户"></a>1）创建kingge用户</h3><p>​         在root用户里面执行如下操作</p>
<p>   [root@hadoop101 opt]# adduser atguigu   [root@hadoop101 opt]# passwd atguigu   更改用户 test 的密码   。   新的 密码：   无效的密码： 它没有包含足够的不同字符   无效的密码： 是回文   重新输入新的 密码：   passwd： 所有的身份验证令牌已经成功更新。   </p>
<h3 id="2）设置kingge用户具有root权限"><a href="#2）设置kingge用户具有root权限" class="headerlink" title="2）设置kingge用户具有root权限"></a>2）设置kingge用户具有root权限</h3><p>修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示：</p>
<p>[root@hadoop101 kingge]# vi /etc/sudoers</p>
<p>## Allow root to run any commands anywhere</p>
<p>root    ALL=(ALL)     ALL</p>
<p>kingge   ALL=(ALL)     ALL</p>
<p>修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。</p>
<h3 id="3）在-opt目录下创建文件夹"><a href="#3）在-opt目录下创建文件夹" class="headerlink" title="3）在/opt目录下创建文件夹"></a>3）在/opt目录下创建文件夹</h3><p>（1）在root用户下创建module、software文件夹</p>
<p>[root@hadoop101 opt]# mkdir module</p>
<p>[root@hadoop101 opt]# mkdir software</p>
<p>（2）修改module、software文件夹的所有者</p>
<p>[root@hadoop101 opt]# chown kingge:kingge module</p>
<p>[root@hadoop101 opt]# chown kingge:kingge sofrware</p>
<p>[root@hadoop101 opt]# ls -al</p>
<p>总用量 16</p>
<p>drwxr-xr-x.  6 root    root 4096 4月  24 09:07 .</p>
<p>dr-xr-xr-x. 23 root    root 4096 4月  24 08:52 ..</p>
<p>drwxr-xr-x.  4 kingge kingge 4096 4月  23 16:26 module</p>
<p>drwxr-xr-x.  2 kingge kingge 4096 4月  23 16:25 software</p>
<h2 id="3-7-安装jdk"><a href="#3-7-安装jdk" class="headerlink" title="3.7 安装jdk"></a>3.7 安装jdk</h2><h3 id="1）卸载现有jdk"><a href="#1）卸载现有jdk" class="headerlink" title="1）卸载现有jdk"></a>1）卸载现有jdk</h3><p>（1）查询是否安装java软件：</p>
<p>[root@hadoop101 opt]# rpm -qa|grep java</p>
<p>（2）如果安装的版本低于1.7，卸载该jdk：</p>
<p>[root@hadoop101 opt]# rpm -e 软件包</p>
<h3 id="2）复制文件"><a href="#2）复制文件" class="headerlink" title="2）复制文件"></a>2）复制文件</h3><p>用SecureCRT工具将jdk、Hadoop-2.7.2.tar.gz导入到opt目录下面的software文件夹下面</p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image027.jpg" alt="img"></p>
<p><img src="/2018/02/24/hadoop大数据-二-运行环境搭建/clip_image029.jpg" alt="img"></p>
<h3 id="3）在linux系统下的opt目录中查看软件包是否导入成功。"><a href="#3）在linux系统下的opt目录中查看软件包是否导入成功。" class="headerlink" title="3）在linux系统下的opt目录中查看软件包是否导入成功。"></a>3）在linux系统下的opt目录中查看软件包是否导入成功。</h3><p>[root@hadoop101opt]# cd software/</p>
<p>[root@hadoop101software]# ls</p>
<p>hadoop-2.7.2.tar.gz  jdk-8u144-linux-x64.tar.gz</p>
<h3 id="4）解压jdk到-opt-module目录下"><a href="#4）解压jdk到-opt-module目录下" class="headerlink" title="4）解压jdk到/opt/module目录下"></a>4）解压jdk到/opt/module目录下</h3><p>​         [root@hadoop101software]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/</p>
<h3 id="5）配置jdk环境变量"><a href="#5）配置jdk环境变量" class="headerlink" title="5）配置jdk环境变量"></a>5）配置jdk环境变量</h3><p>​         （1）先获取jdk路径：</p>
<p>[root@hadoop101 jdk1.8.0_144]# pwd</p>
<p>/opt/module/jdk1.8.0_144</p>
<p>​         （2）打开/etc/profile文件：</p>
<p>[root@hadoop101 jdk1.8.0_144]# vi /etc/profile</p>
<p>​                 在profie文件末尾添加jdk路径：</p>
<p>​                 ##JAVA_HOME</p>
<p>export JAVA_HOME=/opt/module/jdk1.8.0_144</p>
<p>export PATH=$PATH:$JAVA_HOME/bin</p>
<p>​         （3）保存后退出：</p>
<p>:wq</p>
<p>​         （4）让修改后的文件生效：</p>
<p>[root@hadoop101 jdk1.8.0_144]# source /etc/profile</p>
<p>​         （5）重启（如果java -version可以用就不用重启）：      </p>
<p>[root@hadoop101 jdk1.8.0_144]# sync</p>
<p>​                 [root@hadoop101 jdk1.8.0_144]# reboot</p>
<h3 id="6）测试jdk安装成功"><a href="#6）测试jdk安装成功" class="headerlink" title="6）测试jdk安装成功"></a>6）测试jdk安装成功</h3><p>[root@hadoop101 jdk1.8.0_144]# java -version</p>
<p>java version “1.8.0_144”</p>
<h2 id="3-8-安装Hadoop"><a href="#3-8-安装Hadoop" class="headerlink" title="3.8 安装Hadoop"></a>3.8 安装Hadoop</h2><p>  <strong>这里使用的是已经编译过后的hadoop</strong>源码，官网下载的是非编译过后的，需要编译。如何编译参见。</p>
<p><a href="/2018/02/28/hadoop大数据-四-Hadoop编译源码">Hadoop编译源码</a></p>
<p>1）进入到Hadoop安装包路径下：</p>
<blockquote>
<p>[root@hadoop101 ~]# cd /opt/software/</p>
</blockquote>
<p>2）解压安装文件到/opt/module下面</p>
<blockquote>
<p>[root@hadoop101 software]# tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/</p>
</blockquote>
<p>3）查看是否解压成功</p>
<blockquote>
<p>[root@hadoop101 software]# ls /opt/module/</p>
<p>hadoop-2.7.2  </p>
</blockquote>
<p>4）在/opt/module/hadoop-2.7.2/etc/hadoop路径下配置hadoop-env.sh</p>
<blockquote>
<p>   （1）Linux系统中获取jdk的安装路径：   [root@hadoop101   jdk1.8.0_144]# echo $JAVA_HOME   /opt/module/jdk1.8.0_144   （2）修改hadoop-env.sh文件中JAVA_HOME   路径：   [root@hadoop101   hadoop]# vi hadoop-env.sh       修改JAVA_HOME如下   export   JAVA_HOME=/opt/module/jdk1.8.0_144   </p>
</blockquote>
<p>5）将hadoop添加到环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"> （1）获取hadoop安装路径：</div><div class="line"></div><div class="line">[root@ hadoop101 hadoop-2.7.2]# pwd</div><div class="line"></div><div class="line">/opt/module/hadoop-2.7.2</div><div class="line"></div><div class="line"> （2）打开/etc/profile文件：</div><div class="line"></div><div class="line">[root@ hadoop101 hadoop-2.7.2]# vi /etc/profile</div><div class="line"></div><div class="line"> 在profie文件末尾添加jdk路径：（shitf+g）</div><div class="line"></div><div class="line">\##HADOOP_HOME</div><div class="line"></div><div class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</div><div class="line"></div><div class="line">export PATH=$PATH:$HADOOP_HOME/bin</div><div class="line"></div><div class="line">export PATH=$PATH:$HADOOP_HOME/sbin</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">（3）保存后退出：</div><div class="line"></div><div class="line">:wq</div><div class="line"></div><div class="line"> （4）让修改后的文件生效：</div><div class="line"></div><div class="line">[root@ hadoop101 hadoop-2.7.2]# source /etc/profile</div><div class="line"></div><div class="line">（5）重启(如果hadoop命令不能用再重启)：  </div><div class="line"></div><div class="line">[root@ hadoop101 hadoop-2.7.2]# sync</div><div class="line"></div><div class="line"> [root@ hadoop101 hadoop-2.7.2]# reboot</div></pre></td></tr></table></figure>
<p>6）修改/opt目录下的所有文件所有者为kingge</p>
<p>​         [root@hadoop101 opt]# chown kingge:kingge -R /opt/</p>
<p>7）切换到kingge用户</p>
<p>​         [root@hadoop101 opt]# su kingge</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;三-Hadoop运行环境搭建&quot;&gt;&lt;a href=&quot;#三-Hadoop运行环境搭建&quot; class=&quot;headerlink&quot; title=&quot;三 Hadoop运行环境搭建&quot;&gt;&lt;/a&gt;三 Hadoop运行环境搭建&lt;/h1&gt;&lt;h2 id=&quot;3-0-前置准备&quot;&gt;&lt;a href
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>hadoop大数据(一)-理论知识了解</title>
    <link href="http://kingge.top/2018/02/21/hadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%B8%80-%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E4%BA%86%E8%A7%A3/"/>
    <id>http://kingge.top/2018/02/21/hadoop大数据-一-理论知识了解/</id>
    <published>2018-02-20T16:31:59.000Z</published>
    <updated>2019-06-07T08:31:03.225Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>首先本人要先声明的是，这一系列的大数据总结，只是对于整个大数据生态的一个稍微深入的总结。并非是非常深入的，但是能够满足大部分人的需求。</p>
<h1 id="一-大数据概念"><a href="#一-大数据概念" class="headerlink" title="一 大数据概念"></a>一 大数据概念</h1><p>大数据的概念：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产</div></pre></td></tr></table></figure>
<p>   这个概念的解释来源于百度百科，这样的解释太过空洞，那么就用更浅显易懂的解释是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">解决海量数据的存储和海量数据的分析计算</div></pre></td></tr></table></figure>
<h2 id="1-2-大数据的特点"><a href="#1-2-大数据的特点" class="headerlink" title="1.2 大数据的特点"></a>1.2 大数据的特点</h2><p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/1.png" alt="1559894361305"></p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/2.png" alt="1559894439655"></p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/3.png" alt="1559894459441"></p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/4.png" alt="1559894528860"></p>
<p><strong>图片来源于网上</strong></p>
<h2 id="1-3-大数据应用场景"><a href="#1-3-大数据应用场景" class="headerlink" title="1.3 大数据应用场景"></a>1.3 大数据应用场景</h2><p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/5.png" alt="1559894676554"></p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/6.png" alt="1559894704915"></p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/7.png" alt="1559894726997"> </p>
<p>4.给顾客推荐访问过的商品，我们使用淘宝之类的，当我们浏览某个商品之后，下次进来时，他会默认给你推荐你上次浏览过的商品。</p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/8.png" alt="1559894817282"></p>
<p>第八：人工智能，目前最火的的风口</p>
<h1 id="二-Hadoop框架"><a href="#二-Hadoop框架" class="headerlink" title="二 Hadoop框架"></a>二 Hadoop框架</h1><h2 id="2-1-Hadoop是什么"><a href="#2-1-Hadoop是什么" class="headerlink" title="2.1 Hadoop是什么"></a>2.1 Hadoop是什么</h2><p>1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构</p>
<p>2）主要解决，<strong>海量数据的存储和海量数据的分析计算问题</strong>（很明显只是一句废话，哈哈哈）。</p>
<p>3）广义上来说，HADOOP通常是指一个更广泛的概念——<strong>HADOOP生态圈</strong></p>
<h2 id="2-2-Hadoop发展历史"><a href="#2-2-Hadoop发展历史" class="headerlink" title="2.2 Hadoop发展历史"></a>2.2 Hadoop发展历史</h2><p>1）Lucene–Doug Cutting开创的开源软件，用java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎 </p>
<p>2）2001年年底成为apache基金会的一个子项目</p>
<p>3）对于大数量的场景，Lucene面对与Google同样的困难</p>
<p>4）学习和模仿Google解决这些问题的办法 ：微型版Nutch</p>
<p>5）可以说Google是hadoop的思想之源(Google在大数据方面的三篇论文)</p>
<p>​         GFS —&gt;HDFS</p>
<p>​         Map-Reduce —&gt;MR</p>
<p>​         BigTable —&gt;Hbase</p>
<p>6）2003-2004年，Google公开了部分GFS和Mapreduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和Mapreduce机制，使Nutch性能飙升 </p>
<p>7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。2006 年 3 月份，Map-Reduce和Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中 </p>
<p>8）名字来源于Doug Cutting儿子的玩具大象</p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/clip_image002.jpg" alt="img"></p>
<p>9）Hadoop就此诞生并迅速发展，标志这云计算时代来临</p>
<h2 id="2-3-Hadoop三大发行版本"><a href="#2-3-Hadoop三大发行版本" class="headerlink" title="2.3 Hadoop三大发行版本"></a>2.3 Hadoop三大发行版本</h2><p>Hadoop 三大发行版本: <strong>Apache、Cloudera、Hortonworks。</strong></p>
<p>Apache版本最原始（最基础）的版本，对于入门学习最好。</p>
<p>Cloudera在大型互联网企业中用的较多。（<strong>因为他解决了hadoop各个版本和其他框架的兼容问题</strong>）</p>
<p>Hortonworks文档较好。</p>
<p>1）Apache Hadoop</p>
<p> 官网地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="external">http://hadoop.apache.org/releases.html</a></p>
<p>下载地址：<a href="https://archive.apache.org/dist/hadoop/common/" target="_blank" rel="external">https://archive.apache.org/dist/hadoop/common/</a></p>
<p>2）Cloudera Hadoop </p>
<p>官网地址：<a href="https://www.cloudera.com/downloads/cdh/5-10-0.html" target="_blank" rel="external">https://www.cloudera.com/downloads/cdh/5-10-0.html</a></p>
<p>下载地址：<a href="http://archive-primary.cloudera.com/cdh5/cdh/5/" target="_blank" rel="external">http://archive-primary.cloudera.com/cdh5/cdh/5/</a></p>
<p>（1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。</p>
<p>（2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support</p>
<p>（3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。</p>
<p>（4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。</p>
<p>（5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。</p>
<p>3）Hortonworks Hadoop</p>
<p>官网地址：<a href="https://hortonworks.com/products/data-center/hdp/" target="_blank" rel="external">https://hortonworks.com/products/data-center/hdp/</a></p>
<p>下载地址：<a href="https://hortonworks.com/downloads/#data-platform" target="_blank" rel="external">https://hortonworks.com/downloads/#data-platform</a></p>
<p>（1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。</p>
<p>（2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。</p>
<p>（3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。</p>
<p>（4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。</p>
<p>（5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。</p>
<p>（6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的microsoft Windows平台上本地运行。定价以集群为基础，每10个节点每年为12500美元。</p>
<h2 id="2-4-Hadoop的优势"><a href="#2-4-Hadoop的优势" class="headerlink" title="2.4 Hadoop的优势"></a>2.4 Hadoop的优势</h2><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1）高可靠性：因为Hadoop假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理。</div><div class="line"></div><div class="line">2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。</div><div class="line"></div><div class="line">3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度（根据文件快开启相应数量的map任务处理，reduce的并行数量是可控的）。</div><div class="line"></div><div class="line">4）高容错性：自动保存多份副本数据，并且能够自动将失败的任务重新分配。</div></pre></td></tr></table></figure>
<h2 id="2-5-Hadoop组成"><a href="#2-5-Hadoop组成" class="headerlink" title="2.5 Hadoop组成"></a>2.5 Hadoop组成</h2><h3 id="2-5-0-四个组成"><a href="#2-5-0-四个组成" class="headerlink" title="2.5.0 四个组成"></a>2.5.0 四个组成</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1）Hadoop HDFS：一个高可靠、高吞吐量的分布式文件系统。</div><div class="line"></div><div class="line">2）Hadoop MapReduce：一个分布式的离线并行计算框架。</div><div class="line"></div><div class="line">3）Hadoop YARN：作业调度与集群资源管理的框架。</div><div class="line"></div><div class="line">4）Hadoop Common：支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作）。</div></pre></td></tr></table></figure>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/44.png" alt="1559895142902"></p>
<h3 id="2-5-1-HDFS"><a href="#2-5-1-HDFS" class="headerlink" title="2.5.1 HDFS"></a>2.5.1 HDFS</h3><p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/clip_image0022.jpg" alt="img"></p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/clip_image00212.jpg" alt="img"></p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/9895311471.png" alt="1559895311471"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">组成：</div><div class="line"></div><div class="line">  namenode：存储文件元数据。例如文件名称，文件目录结构，文件属性（生成时间、副本数，文件权限），以及每个文件的快列表和快所在的datanode。hdfs的文件目录维护中心。</div><div class="line"></div><div class="line">datanode：真正存储文件的单位，也就是统称的块。</div><div class="line"></div><div class="line">secondary namenode：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS的元数据的快照。帮助namenode合并镜像文件和操作日志，合并完成后同步给namenode，减少namenode的处理任务的压力。他不能够替代namenode，只能够做备份（方便namenode意外挂掉后，能够恢复数据）</div></pre></td></tr></table></figure>
<h3 id="2-5-2-YARN"><a href="#2-5-2-YARN" class="headerlink" title="2.5.2 YARN"></a>2.5.2 YARN</h3><p>1）ResourceManager(rm)：处理客户端请求、启动/监控ApplicationMaster、监控NodeManager、资源分配与调度；</p>
<p>2）NodeManager(nm)：单个节点上的资源管理、处理来自ResourceManager的命令、处理来自ApplicationMaster的命令；</p>
<p>3）ApplicationMaster：数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错。</p>
<p>4）Container：对任务运行环境的抽象，封装了CPU、内存等多维资源以及环境变量、启动命令等任务运行相关的信息。</p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/5748151.png" alt="1559895748151"></p>
<h3 id="2-5-3-MapReduce"><a href="#2-5-3-MapReduce" class="headerlink" title="2.5.3 MapReduce"></a>2.5.3 MapReduce</h3><p>主要是获取HDFS存储的数据，通过拆分块的形式进行数据获取分析处理，最后输出自己想要的结果。</p>
<p>MapReduce将计算过程分为两个阶段：Map和Reduce</p>
<p>1）Map阶段并行处理输入数据</p>
<p>2）Reduce阶段对Map结果进行汇总</p>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/clip_imag1e002.jpg" alt="img"></p>
<p>上图简单的阐明了map和reduce的两个过程或者作用，虽然不够严谨，但是足以提供一个大概的认知，map过程是一个蔬菜到制成食物前的准备工作，reduce将准备好的材料合并进而制作出食物的过程。</p>
<h2 id="2-6-大数据整个结构"><a href="#2-6-大数据整个结构" class="headerlink" title="2.6 大数据整个结构"></a>2.6 大数据整个结构</h2><p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/895909084.png" alt="1559895909084"></p>
<p><strong>图片来源于网上</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">图中涉及的技术名词解释如下：</div><div class="line"></div><div class="line">1）Sqoop：sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</div><div class="line">  一般是将关系型数据库的数据导入到hive或者HDFS中，目的就是为了分析这些数据，因为我们知道关系型数据库当数据量变得很大时，通过sql去统计查询，那么就会卡死。就是因为整个架构他们本省就是不支持的，他们通常进行的是扫表操作。</div><div class="line"></div><div class="line">2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</div><div class="line"></div><div class="line">3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：</div><div class="line"></div><div class="line">（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。</div><div class="line"></div><div class="line">（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息</div><div class="line"></div><div class="line">（3）支持通过Kafka服务器和消费机集群来分区消息。</div><div class="line"></div><div class="line">（4）支持Hadoop并行数据加载。</div><div class="line"></div><div class="line">4）Storm：Storm为分布式实时计算提供了一组通用原语，可被用于“流处理”之中，实时处理消息并更新数据库。这是管理队列及工作者集群的另一种方式。 Storm也可被用于“连续计算”（continuous computation），对数据流做连续查询，在计算时就将结果以流的形式输出给用户。</div><div class="line"></div><div class="line">5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</div><div class="line"></div><div class="line">6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。Oozie协调作业就是通过时间（频率）和有效数据触发当前的Oozie工作流程。</div><div class="line"></div><div class="line">7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</div><div class="line"></div><div class="line">8）Hive：hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</div><div class="line"></div><div class="line">10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。</div><div class="line"></div><div class="line">11）Mahout:</div><div class="line"></div><div class="line">Apache Mahout是个可扩展的机器学习和数据挖掘库，当前Mahout支持主要的4个用例：</div><div class="line"></div><div class="line">推荐挖掘：搜集用户动作并以此给用户推荐可能喜欢的事物。</div><div class="line"></div><div class="line">聚集：收集文件并进行相关文件分组。</div><div class="line"></div><div class="line">分类：从现有的分类文档中学习，寻找文档中的相似特征，并为无标签的文档进行正确的归类。</div><div class="line"></div><div class="line">频繁项集挖掘：将一组项分组，并识别哪些个别项会经常一起出现。</div><div class="line"></div><div class="line">12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</div></pre></td></tr></table></figure>
<p><img src="/2018/02/21/hadoop大数据-一-理论知识了解/1559896191939.png" alt="1559896191939"></p>
<p>好了到这里我们已经了解了hadoop整个生态相关的概念和组成，以及架构。那么接下来让我们进行hadoop环境的搭建。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;首先本人要先声明的是，这一系列的大数据总结，只是对于整个大数据生态的一个稍微深入的总结。并非是非常深入的，但是能够满足大部分人的需求。&lt;/p
    
    </summary>
    
      <category term="hadoop" scheme="http://kingge.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="http://kingge.top/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://kingge.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>关于2018年的计划</title>
    <link href="http://kingge.top/2018/02/19/%E5%85%B3%E4%BA%8E2018%E5%B9%B4%E7%9A%84%E8%AE%A1%E5%88%92/"/>
    <id>http://kingge.top/2018/02/19/关于2018年的计划/</id>
    <published>2018-02-18T16:00:00.000Z</published>
    <updated>2019-06-04T16:28:14.626Z</updated>
    
    <content type="html"><![CDATA[<p>因为公司业务的转移和相关产品的开发，再加上大数据的火热，所以本人决定更新一些hadoop生态链相关的文章。所以敬请期待吧，哈哈哈哈哈。</p>
<p>Comming Soon！！！</p>
<p><img src="/2018/02/19/关于2018年的计划/111.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为公司业务的转移和相关产品的开发，再加上大数据的火热，所以本人决定更新一些hadoop生态链相关的文章。所以敬请期待吧，哈哈哈哈哈。&lt;/p&gt;
&lt;p&gt;Comming Soon！！！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2018/02/19/关于2018年的计划/111.jp
    
    </summary>
    
      <category term="新年计划" scheme="http://kingge.top/categories/%E6%96%B0%E5%B9%B4%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="新年计划" scheme="http://kingge.top/tags/%E6%96%B0%E5%B9%B4%E8%AE%A1%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper知识学习</title>
    <link href="http://kingge.top/2018/02/02/zookeeper%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/"/>
    <id>http://kingge.top/2018/02/02/zookeeper知识学习/</id>
    <published>2018-02-02T11:12:44.000Z</published>
    <updated>2019-06-02T13:24:38.033Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>   最近公司开发saas模式的企业应用软件服务，所以下面是我个人使用和总结（掺杂了大数据相关的总结）</p>
<h1 id="一、正文"><a href="#一、正文" class="headerlink" title="一、正文"></a>一、正文</h1><h2 id="0-1-下载"><a href="#0-1-下载" class="headerlink" title="0.1 下载"></a>0.1 下载</h2><p>1）官网首页：</p>
<p><a href="https://zookeeper.apache.org/" target="_blank" rel="external">https://zookeeper.apache.org/</a></p>
<p>2）下载截图</p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image002.jpg" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image004.jpg" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image006.jpg" alt="img"></p>
<h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image002.png" alt="img"></p>
<h2 id="1-2-模型构造和特点"><a href="#1-2-模型构造和特点" class="headerlink" title="1.2 模型构造和特点"></a>1.2 模型构造和特点</h2><p><img src="/2018/02/02/zookeeper知识学习/clip_image0021.png" alt="img"></p>
<p>1）Zookeeper：<strong>一个领导者（leader），多个跟随者（follower）</strong>组成的集群。</p>
<p>2）Leader负责进行投票的发起和决议，更新系统状态。</p>
<p>3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票。</p>
<p>4）<strong><em>集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。（例如现在zookeeper集群现在有四台，那么挂掉两台后就不能正常工作了。假设初始时只有三台，那么最多也是挂掉两台后就不能工作了。也就是说，部署三台和部署四台的效用其实是一样的，所以一般都是部署奇数台zookeeper，节省资源）</em></strong></p>
<p>5）<strong>全局数据一致</strong>：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。</p>
<p>6）<strong>更新请求顺序进行</strong>（全局数据一致性的提现），来自同一个client的更新请求按其发送顺序依次执行。</p>
<p>7）数据更新原子性，一次数据更新要么成功，要么失败。</p>
<p>8）实时性，在一定时间范围内（<strong>数据一致性的更新会有延迟</strong>），client能读到最新数据。</p>
<p>9）<strong>一次性监听（缺点）</strong>-这个缺点我们在后面可以用代码解决。</p>
<h2 id="1-3-数据结构"><a href="#1-3-数据结构" class="headerlink" title="1.3 数据结构"></a>1.3 数据结构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。</div><div class="line"></div><div class="line">每个节点的存储的数据量，也决定了他的应用场景并不是存储大量的数据。下面的1.4章节会阐述到他的作用-应用场景</div></pre></td></tr></table></figure>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image0024.jpg" alt="数据结构图"></p>
<h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><p><strong>如果某个需求：当某个节点发生变化，通知其他关注这个节点的其他节点。那么就可以使用**</strong>zookeeper**</p>
<p>项目中常用到分布式锁和配置管理</p>
<h3 id="1-4-1-统一命名服务"><a href="#1-4-1-统一命名服务" class="headerlink" title="1.4.1 统一命名服务"></a>1.4.1 统一命名服务</h3><p><img src="/2018/02/02/zookeeper知识学习/clip_image002a.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">意思就是：我们通常使用域名来访问某个网站，但是域名对应的ip我们是不需要关注的</div><div class="line">为了系统的容错性，我们访问Baidu的这个请求是会随机寻找一个正常运行的服务器去处理。那么我们就可以使用zookeeper来进行管理。管理可以访问到Baidu这个网址的ip列表。客户端每次请求百度时，只需要去请求这个zookeeper获取可访问的ip即可。</div><div class="line">实现动态ip的上下线管理</div></pre></td></tr></table></figure>
<h3 id="1-4-2-统一配置管理"><a href="#1-4-2-统一配置管理" class="headerlink" title="1.4.2 统一配置管理"></a>1.4.2 统一配置管理</h3><p><img src="/2018/02/02/zookeeper知识学习/clip_image004.png" alt="img"></p>
<h3 id="1-4-3-统一集群管理"><a href="#1-4-3-统一集群管理" class="headerlink" title="1.4.3 统一集群管理"></a>1.4.3 统一集群管理</h3><p>集群管理结构图如下所示。</p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image006.png" alt="img"></p>
<h3 id="1-4-4-服务器节点动态上下线"><a href="#1-4-4-服务器节点动态上下线" class="headerlink" title="1.4.4 服务器节点动态上下线"></a>1.4.4 服务器节点动态上下线</h3><p><img src="/2018/02/02/zookeeper知识学习/clip_image008.png" alt="img"></p>
<h3 id="1-4-5-软负载均衡"><a href="#1-4-5-软负载均衡" class="headerlink" title="1.4.5 软负载均衡"></a>1.4.5 软负载均衡</h3><p><img src="/2018/02/02/zookeeper知识学习/clip_image010.png" alt="img"></p>
<p>控制某个服务器的访问数，达到资源合理分配</p>
<h3 id="1-4-6-分布式锁（主要原理是同一路径下的节点名称不能重复，不能重复创建）"><a href="#1-4-6-分布式锁（主要原理是同一路径下的节点名称不能重复，不能重复创建）" class="headerlink" title="1.4.6 分布式锁（主要原理是同一路径下的节点名称不能重复，不能重复创建）"></a>1.4.6 分布式锁（<em>主要原理是同一路径下的节点名称不能重复，不能重复创建</em>）</h3><p>有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。</p>
<p>对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。</p>
<p><strong>对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。</strong>（<strong>在下面的3.2章节会讲到zookeeper顺序节点的相关内容</strong>）</p>
<p> 好的博客：</p>
<p><a href="https://my.oschina.net/aidelingyu/blog/1600979" target="_blank" rel="external">https://my.oschina.net/aidelingyu/blog/1600979</a> </p>
<p> <a href="https://www.jianshu.com/p/5d12a01018e1" target="_blank" rel="external">https://www.jianshu.com/p/5d12a01018e1</a></p>
<h3 id="1-4-7-队列管理"><a href="#1-4-7-队列管理" class="headerlink" title="1.4.7 队列管理"></a>1.4.7 队列管理</h3><p>两种类型的队列：</p>
<p>1、 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。</p>
<p>2、队列按照 FIFO 方式进行入队和出队操作。</p>
<p>第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。</p>
<p>第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。</p>
<h1 id="二-Zookeeper安装"><a href="#二-Zookeeper安装" class="headerlink" title="二 Zookeeper安装"></a>二 Zookeeper安装</h1><h2 id="2-1-本地模式安装部署"><a href="#2-1-本地模式安装部署" class="headerlink" title="2.1 本地模式安装部署"></a>2.1 本地模式安装部署</h2><p>1）安装前准备：</p>
<p>（1）安装jdk</p>
<p>（2）通过cshell工具拷贝zookeeper到linux系统下</p>
<p>（3）修改tar包权限</p>
<p> [kingge@hadoop102 software]$ chmod u+x zookeeper-3.4.10.tar.gz</p>
<p>（4）解压到指定目录</p>
<p> [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</p>
<p>2）配置修改</p>
<p> 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg；</p>
<p>​          进入zoo.cfg文件：vim zoo.cfg</p>
<p>​                  修改dataDir路径为</p>
<p>​                dataDir=/opt/module/zookeeper-3.4.10/zkData</p>
<p>​          在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹</p>
<p>​                  [kingge@hadoop102 zookeeper-3.4.10]$ mkdir zkData</p>
<p>3）操作zookeeper</p>
<p>（1）启动zookeeper</p>
<p>[kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start</p>
<p>（2）查看进程是否启动</p>
<p>​         [kingge@hadoop102 zookeeper-3.4.10]$ jps</p>
<p>4020 Jps</p>
<p>4001 QuorumPeerMain</p>
<p>（3）查看状态：</p>
<p>[kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status</p>
<p>ZooKeeper JMX enabled by default</p>
<p>Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</p>
<p>Mode: <strong>standalone</strong></p>
<p>（4）启动客户端：</p>
<p>[kingge@hadoop102 zookeeper-3.4.10]$ bin/zkCli.sh</p>
<p>（5）退出客户端：</p>
<p>[zk: localhost:2181(CONNECTED) 0] quit</p>
<p>（6）停止zookeeper</p>
<p> [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh stop</p>
<h2 id="2-2-配置参数解读"><a href="#2-2-配置参数解读" class="headerlink" title="2.2 配置参数解读"></a>2.2 配置参数解读</h2><p>解读zoo.cfg文件中参数含义</p>
<p>1）tickTime=2000：通信心跳数，Zookeeper服务器心跳时间，单位毫秒</p>
<p>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</p>
<p>它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</p>
<p>2）initLimit=10：Leader和Follower<strong>初始通信时限</strong>（<strong>10* tickTime –</strong> <strong>也就是不要超过二十秒</strong>）</p>
<p>集群中的follower跟随者服务器与leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</p>
<p>投票选举新leader的初始化时间</p>
<p>Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。</p>
<p>Leader允许Follower在initLimit时间内完成这个工作。</p>
<p>3）syncLimit=5：Leader和Follower同步通信时限（<strong>5* tickTime –</strong> <strong>也就是不要超过十秒</strong>）</p>
<p>集群中Leader与Follower之间的<strong>最大响应时间单位</strong>，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。（默认超过十秒，leader就认为follwer已经碟机）</p>
<p>在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。</p>
<p>如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。</p>
<p>4）dataDir：数据文件目录+数据持久化路径</p>
<p>保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。</p>
<p>5）clientPort=2181：客户端连接端口</p>
<p>监听客户端连接的端口</p>
<h2 id="2-3-分布式模式下的安装详见"><a href="#2-3-分布式模式下的安装详见" class="headerlink" title="2.3 分布式模式下的安装详见"></a>2.3 分布式模式下的安装详见</h2><p>第四章节</p>
<h1 id="三-Zookeeper内部原理"><a href="#三-Zookeeper内部原理" class="headerlink" title="三 Zookeeper内部原理"></a>三 Zookeeper内部原理</h1><h2 id="3-1-选举机制"><a href="#3-1-选举机制" class="headerlink" title="3.1 选举机制"></a>3.1 选举机制</h2><p>1）半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。<strong>所以zookeeper**</strong>适合装在<strong>奇数台机器</strong>上**。</p>
<p>2）Zookeeper虽然在配置文件中并没有指定master和slave。但是，zookeeper工作时，是有一个节点为leader，其他则为follower，Leader是通过内部的选举机制临时产生的。</p>
<p>3）以一个简单的例子来说明整个选举的过程。</p>
<p>假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。</p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image0a02.png" alt="img"></p>
<p>（1）服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是LOOKING状态。</p>
<p>（2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3    <strong>5/2=2.5 向上取整3</strong>)，所以服务器1、2还是继续保持LOOKING状态。</p>
<p>（3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的leader。</p>
<p>（4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。</p>
<p>（5）服务器5启动，同4一样当小弟。</p>
<h2 id="3-2-节点类型"><a href="#3-2-节点类型" class="headerlink" title="3.2 节点类型"></a>3.2 节点类型</h2><p>1）Znode有两种类型：</p>
<p>短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除</p>
<p>持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除</p>
<p>2）Znode有四种形式的目录节点（默认是persistent ）</p>
<p>（1）持久化目录节点（PERSISTENT）</p>
<p>​         客户端与zookeeper断开连接后，该节点依旧存在。</p>
<p>（2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL）</p>
<p>​         客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。（<strong>保证创建的节点名称不会重复</strong>）</p>
<p>（3）临时目录节点（EPHEMERAL）</p>
<p>客户端与zookeeper断开连接后，该节点被删除。</p>
<p>（4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL）</p>
<p>客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。</p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image00524.jpg" alt="img"></p>
<p>3）创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护</p>
<p>4）在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序（<strong>应用场景1.4.6 分布式锁 第二种方式</strong>）</p>
<h2 id="3-3-stat结构体"><a href="#3-3-stat结构体" class="headerlink" title="3.3 stat结构体"></a>3.3 stat结构体</h2><p>1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid</p>
<p>每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</p>
<p>事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</p>
<p>2）ctime - znode被创建的毫秒数(从1970年开始)</p>
<p>3）mzxid - znode最后更新的zxid</p>
<p>4）mtime - znode最后修改的毫秒数(从1970年开始)</p>
<p>5）pZxid-znode最后更新的子节点zxid</p>
<p>6）cversion - znode子节点变化号，znode子节点修改次数</p>
<p>7）dataversion - znode数据变化号</p>
<p>8）aclVersion - znode访问控制列表的变化号</p>
<p>9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</p>
<p>10）dataLength- znode的数据长度</p>
<p>11）numChildren - znode子节点数量</p>
<h2 id="3-4-监听器原理"><a href="#3-4-监听器原理" class="headerlink" title="3.4 监听器原理"></a>3.4 监听器原理</h2><p><img src="/2018/02/02/zookeeper知识学习/clip_image0069.png" alt="img"></p>
<h2 id="3-5-写数据流程"><a href="#3-5-写数据流程" class="headerlink" title="3.5 写数据流程"></a>3.5 写数据流程</h2><p><img src="/2018/02/02/zookeeper知识学习/clip_image0084.png" alt="img"></p>
<p>1.收到请求，先找到leader节点</p>
<p>2.广播请求给其他follower</p>
<p>3.哥哥follower写入数据，写入成功后，通知leader写入成功。（<strong>半数以上follower写入成功即为写入数据成功</strong>）</p>
<p>4.leader通知最初收到客户请求的server，数据写入成功，该server通知客户端写入数据成功</p>
<h1 id="四-Zookeeper实战"><a href="#四-Zookeeper实战" class="headerlink" title="四 Zookeeper实战"></a>四 Zookeeper实战</h1><h2 id="4-1-分布式安装部署"><a href="#4-1-分布式安装部署" class="headerlink" title="4.1 分布式安装部署"></a>4.1 分布式安装部署</h2><p>0）集群规划</p>
<p>在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。</p>
<p>1）解压安装</p>
<p>（1）解压zookeeper安装包到/opt/module/目录下</p>
<p> [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</p>
<p>（2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData</p>
<p>​         mkdir -p zkData</p>
<p>（3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg</p>
<p>​         mv zoo_sample.cfg zoo.cfg</p>
<p>2）配置zoo.cfg文件</p>
<p>​         （1）具体配置</p>
<p>​         dataDir=/opt/module/zookeeper-3.4.10/zkData</p>
<p>​         增加如下配置</p>
<p>​         #######################cluster##########################</p>
<p>server.2=hadoop102:2888:3888</p>
<p>server.3=hadoop103:2888:3888</p>
<p>server.4=hadoop104:2888:3888</p>
<p><strong>（2）配置参数解读</strong></p>
<p>Server.A=B:C:D。</p>
<p>A是一个数字，表示这个是第几号服务器；（<strong>必须唯一</strong>）</p>
<p>B是这个服务器的ip地址；</p>
<p>C是这个服务器与集群中的Leader服务器交换信息的端口；</p>
<p>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</div></pre></td></tr></table></figure>
<p>3）集群操作</p>
<p>（1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件</p>
<p>​         touch myid</p>
<p><strong>添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码</strong></p>
<p>（2）编辑myid文件</p>
<p>​         vi myid</p>
<p>​         在文件中添加与server对应的编号：如2</p>
<p>（3）拷贝配置好的zookeeper到其他机器上（<strong>可以用shell脚本进行分发数据</strong>）</p>
<p>​         scp -r zookeeper-3.4.10/ <a href="mailto:root@hadoop103.kingge.com:/opt/app/" target="_blank" rel="external">root@hadoop103.kingge.com:/opt/app/</a></p>
<p>​         scp -r zookeeper-3.4.10/ <a href="mailto:root@hadoop104.kingge.com:/opt/app/" target="_blank" rel="external">root@hadoop104.kingge.com:/opt/app/</a></p>
<p>​         并分别修改myid文件中内容为3、4</p>
<p>（4）分别启动zookeeper</p>
<p>​         [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start</p>
<p>[root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start</p>
<p>[root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start</p>
<p>（5）查看状态</p>
<p>[root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status</p>
<p>JMX enabled by default</p>
<p>Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</p>
<p>Mode: <strong>follower</strong></p>
<p>[root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status</p>
<p>JMX enabled by default</p>
<p>Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</p>
<p>Mode: <strong>leader</strong></p>
<p>[root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status</p>
<p>JMX enabled by default</p>
<p>Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</p>
<p>Mode: <strong>follower</strong></p>
<p><strong>分析：当第二个zookeeper启动时，因为2 &gt; 3/2=1.5，所以他被投票为了Leader，所以其他的节点就是follwer</strong></p>
<h2 id="4-2-客户端命令行操作"><a href="#4-2-客户端命令行操作" class="headerlink" title="4.2 客户端命令行操作"></a>4.2 客户端命令行操作</h2><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path   [watch]</td>
<td>使用 ls 命令来查看当前znode中所包含的内容</td>
</tr>
<tr>
<td>ls2   path [watch]</td>
<td>查看当前节点数据并能看到更新次数等数据</td>
</tr>
<tr>
<td>create</td>
<td>普通创建   -s  含有序列   -e  临时（重启或者超时消失）</td>
</tr>
<tr>
<td>get   path [watch]</td>
<td>获得节点的值</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>rmr</td>
<td>递归删除节点</td>
</tr>
</tbody>
</table>
<p>1）启动客户端（<strong>随便连接那个zookeeper都可以，因为他们内容都是一样的，下面连接的是103服务器</strong>）</p>
<p>[kingge@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh</p>
<p>2）显示所有操作命令</p>
<p>[zk: localhost:2181(CONNECTED) 1] help</p>
<p>3）查看当前znode中所包含的内容</p>
<p>[zk: localhost:2181(CONNECTED) 0] ls /</p>
<p>[zookeeper]</p>
<p>4）查看当前节点数据并能看到更新次数等数据</p>
<p>[zk: localhost:2181(CONNECTED) 1] ls2 /</p>
<p>[zookeeper]</p>
<p>cZxid = 0x0</p>
<p>ctime = Thu Jan 01 08:00:00 CST 1970</p>
<p>mZxid = 0x0</p>
<p>mtime = Thu Jan 01 08:00:00 CST 1970</p>
<p>pZxid = 0x0</p>
<p>cversion = -1</p>
<p>dataVersion = 0</p>
<p>aclVersion = 0</p>
<p>ephemeralOwner = 0x0</p>
<p>dataLength = 0</p>
<p>numChildren = 1</p>
<p>5）创建普通节点（<strong><em>注意创建节点时需要写入一些数据，否则创建不成功</em>-例如create /app1 这样的话创建是没有效果的</strong>）</p>
<p>[zk: localhost:2181(CONNECTED) 2] create /app1 “hello app1”</p>
<p>Created /app1</p>
<p>[zk: localhost:2181(CONNECTED) 4] create /app1/server101 “192.168.1.101”</p>
<p>Created /app1/server101</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1.不支持递归创建节点，比如你要创建/app1/a,如果app1不存在，你就不能创建a( KeeperException.NoNode)。</div><div class="line"></div><div class="line">2.不可以再ephemeral类型的节点下创建子节点(KeeperException.NoChildrenForEphemerals)。（因为他本身是临时节点）</div><div class="line"></div><div class="line">3.如果指定的节点已经存在，会触发KeeperException.NodeExists 异常,当然了对于sequential类型的，不会抛出这个异常。（有编号类型的节点名称会自动递增）</div><div class="line"></div><div class="line">4.数据内容不能超过1M,否则将抛出KeeperException异常。</div></pre></td></tr></table></figure>
<p>6）获得节点的值</p>
<p>[zk: localhost:2181(CONNECTED) 6] get /app1</p>
<p>hello app1</p>
<p>cZxid = 0x20000000a</p>
<p>ctime = Mon Jul 17 16:08:35 CST 2017</p>
<p>mZxid = 0x20000000a</p>
<p>mtime = Mon Jul 17 16:08:35 CST 2017</p>
<p>pZxid = 0x20000000b</p>
<p>cversion = 1</p>
<p>dataVersion = 0</p>
<p>aclVersion = 0</p>
<p>ephemeralOwner = 0x0</p>
<p>dataLength = 10</p>
<p>numChildren = 1</p>
<p>[zk: localhost:2181(CONNECTED) 8] get /app1/server101</p>
<p>192.168.1.101</p>
<p>cZxid = 0x20000000b</p>
<p>ctime = Mon Jul 17 16:11:04 CST 2017</p>
<p>mZxid = 0x20000000b</p>
<p>mtime = Mon Jul 17 16:11:04 CST 2017</p>
<p>pZxid = 0x20000000b</p>
<p>cversion = 0</p>
<p>dataVersion = 0</p>
<p>aclVersion = 0</p>
<p>ephemeralOwner = 0x0</p>
<p>dataLength = 13</p>
<p>numChildren = 0</p>
<p>7）创建短暂节点</p>
<p>[zk: localhost:2181(CONNECTED) 9] create -e /app-emphemeral 8888</p>
<p>（1）在当前客户端是能查看到的</p>
<p>[zk: localhost:2181(CONNECTED) 10] ls /</p>
<p>[app1, app-emphemeral, zookeeper]</p>
<p>（2）退出当前客户端然后再重启客户端</p>
<p>​         [zk: localhost:2181(CONNECTED) 12] quit</p>
<p>[kingge@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh</p>
<p>（3）再次查看根目录下短暂节点已经删除</p>
<p>​         [zk: localhost:2181(CONNECTED) 0] ls /</p>
<p>[app1, zookeeper]</p>
<p>8）创建带序号的节点</p>
<p>​         （1）先创建一个普通的根节点app2</p>
<p>​         [zk: localhost:2181(CONNECTED) 11] create /app2 “app2”</p>
<p>​         （2）创建带序号的节点</p>
<p>​         [zk: localhost:2181(CONNECTED) 13] create -s /app2/aa 888</p>
<p>Created /app2/aa0000000000</p>
<p>[zk: localhost:2181(CONNECTED) 14] create -s /app2/bb 888</p>
<p>Created /app2/bb0000000001</p>
<p>[zk: localhost:2181(CONNECTED) 15] create -s /app2/cc 888</p>
<p>Created /app2/cc0000000002</p>
<p>如果原节点下有1个节点，则再排序时从1开始，以此类推。</p>
<p>[zk: localhost:2181(CONNECTED) 16] create -s /app1/aa 888</p>
<p>Created /app1/aa0000000001</p>
<p>9）修改节点数据值</p>
<p>[zk: localhost:2181(CONNECTED) 2] set /app1 999</p>
<p>10）节点的值变化监听（<strong>一次性触发器</strong>）（<strong>Watch**</strong>的通知事件是从服务器发送给客户端的，是异步的**）</p>
<p>​         （1）在104主机上注册监听/app1节点数据变化（）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">需要注意的是，注册一次监听，只能够响应一次，如果/app1节点的数据修改了两次，那么只显示第一次监听的信息，第二次不会有任何响应，想要得到响应，需要再次监听</div></pre></td></tr></table></figure>
<p>[zk: localhost:2181(CONNECTED) 26] get /app1 watch</p>
<p>​         （2）在103主机上修改/app1节点的数据</p>
<p>[zk: localhost:2181(CONNECTED) 5] set /app1  777</p>
<p>​         （3）观察104主机收到数据变化的监听</p>
<p>WATCHER::</p>
<p>WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1</p>
<p>11）节点的子节点变化监听（路径变化）</p>
<p>​         （1）在104主机上注册监听/app1节点的子节点变化</p>
<p>[zk: localhost:2181(CONNECTED) 1] ls /app1 watch</p>
<p>[aa0000000001, server101]</p>
<p>​         （2）在103主机/app1节点上创建子节点</p>
<p>[zk: localhost:2181(CONNECTED) 6] create /app1/bb 666</p>
<p>Created /app1/bb</p>
<p>​         （3）观察104主机收到子节点变化的监听</p>
<p>WATCHER::</p>
<p>WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1</p>
<p>12）删除节点</p>
<p>[zk: localhost:2181(CONNECTED) 4] delete /app1/bb</p>
<p>13）递归删除节点</p>
<p>[zk: localhost:2181(CONNECTED) 7] rmr /app2</p>
<p>14）查看节点状态</p>
<p>[zk: localhost:2181(CONNECTED) 12] stat /app1</p>
<p>cZxid = 0x20000000a</p>
<p>ctime = Mon Jul 17 16:08:35 CST 2017</p>
<p>mZxid = 0x200000018</p>
<p>mtime = Mon Jul 17 16:54:38 CST 2017</p>
<p>pZxid = 0x20000001c</p>
<p>cversion = 4</p>
<p>dataVersion = 2</p>
<p>aclVersion = 0</p>
<p>ephemeralOwner = 0x0</p>
<p>dataLength = 3</p>
<p>numChildren = 2</p>
<p>15）exists 节点</p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image00222.jpg" alt="img"></p>
<p>这个函数很特殊，因为他可以监听一个尚未存在的节点，这是getData，getChildren不能做到的。exists可以监听一个节点的生命周期：从无到有，节点数据的变化，从有到无。</p>
<p>   在传递给exists的watcher里，当path指定的节点被成功创建后，watcher会收到NodeCreated事件通知。当path所指定的节点的数据内容发送了改变后，wather会受到NodeDataChanged事件通知。</p>
<p><strong>这里最需要注意的就是，exists可以监听一个未存在的节点，这是他与getData，getChildren本质的区别。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">注意看上面的代码，其实我们已经实现了多次监听，解决了zookeeper单次监听的缺点。</div><div class="line"></div><div class="line">关键代码，我们在监听器里面，又再次声明了一次监听---zkClient.exists(&quot;eclipse&quot;,true)</div></pre></td></tr></table></figure>
<p>16） getData</p>
<p><img src="/2018/02/02/zookeeper知识学习/1clip_image004.jpg" alt="img"></p>
<p>16） getChildren </p>
<p><img src="/2018/02/02/zookeeper知识学习/cl8ip_image006.jpg" alt="img"></p>
<h2 id="4-3-API应用"><a href="#4-3-API应用" class="headerlink" title="4.3 API应用"></a>4.3 API应用</h2><h3 id="4-3-1-Eclipse环境搭建"><a href="#4-3-1-Eclipse环境搭建" class="headerlink" title="4.3.1 Eclipse环境搭建"></a>4.3.1 Eclipse环境搭建</h3><p>1）创建一个工程</p>
<p>2）解压zookeeper-3.4.10.tar.gz文件</p>
<p>3）拷贝zookeeper-3.4.10.jar、jline-0.9.94.jar、log4j-1.2.16.jar、netty-3.10.5.Final.jar、slf4j-api-1.6.1.jar、slf4j-log4j12-1.6.1.jar到工程的lib目录。并build一下，导入工程。</p>
<p>4）拷贝log4j.properties文件到项目根目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">log4j.rootLogger=INFO, stdout  </div><div class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender  </div><div class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  </div><div class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  </div><div class="line">log4j.appender.logfile=org.apache.log4j.FileAppender  </div><div class="line">log4j.appender.logfile.File=target/spring.log  </div><div class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  </div><div class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</div></pre></td></tr></table></figure>
<h3 id="4-3-2-创建ZooKeeper客户端"><a href="#4-3-2-创建ZooKeeper客户端" class="headerlink" title="4.3.2 创建ZooKeeper客户端"></a>4.3.2 创建ZooKeeper客户端</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;;</div><div class="line">	private static int sessionTimeout = 2000;</div><div class="line">	private ZooKeeper zkClient = null;</div><div class="line"></div><div class="line">	@Before</div><div class="line">	public void init() throws Exception &#123;</div><div class="line">//创建zookeeper连接的时候同时注册一个全局的默认的事件监听器 – </div><div class="line">// event.getType() 永远为null默认监听到None事件</div><div class="line">//  //默认监听也可以使用register方法注册</div><div class="line">        //zkClient.register(watcherDefault);</div><div class="line"></div><div class="line">	zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; </div><div class="line">			@Override</div><div class="line">			public void process(WatchedEvent event) &#123;</div><div class="line">				// 收到事件通知后的回调函数（用户的业务逻辑）</div><div class="line">				System.out.println(event.getType() + &quot;--&quot; + event.getPath());</div><div class="line"></div><div class="line">				// 再次启动监听 - 解决zookeeper单次监听的缺点</div><div class="line">				try &#123;</div><div class="line">					zkClient.getChildren(&quot;/&quot;, true);</div><div class="line">				&#125; catch (Exception e) &#123;</div><div class="line">					e.printStackTrace();</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;);</div><div class="line">	&#125;</div><div class="line">这里的watcher是该客户端总的监听方法，任何操作都会执行，而且是可以多次执行，并非单次。</div></pre></td></tr></table></figure>
<h3 id="4-3-3-创建子节点"><a href="#4-3-3-创建子节点" class="headerlink" title="4.3.3 创建子节点"></a>4.3.3 创建子节点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">// 创建子节点</div><div class="line">@Test</div><div class="line">public void create() throws Exception &#123;</div><div class="line">	// 数据的增删改查</div><div class="line">	// 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型</div><div class="line">	String nodeCreated = zkClient.create(&quot;/eclipse&quot;, &quot;hello zk&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="4-3-4-获取子节点并监听"><a href="#4-3-4-获取子节点并监听" class="headerlink" title="4.3.4 获取子节点并监听"></a>4.3.4 获取子节点并监听</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">// 获取子节点</div><div class="line">	@Test</div><div class="line">	public void getChildren() throws Exception &#123;</div><div class="line">		List&lt;String&gt; children = zkClient.getChildren(&quot;/&quot;, true);</div><div class="line"></div><div class="line">		for (String child : children) &#123;</div><div class="line">			System.out.println(child);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		// 延时阻塞</div><div class="line">		Thread.sleep(Long.MAX_VALUE);</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<h3 id="4-3-5-判断znode是否存在"><a href="#4-3-5-判断znode是否存在" class="headerlink" title="4.3.5 判断znode是否存在"></a>4.3.5 判断znode是否存在</h3><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">// 判断znode是否存在</div><div class="line">	@Test</div><div class="line">	public void exist() throws Exception &#123;</div><div class="line">		Stat stat = zkClient.exists(&quot;/eclipse&quot;, false);</div><div class="line"></div><div class="line">		System.out.println(stat == null ? &quot;not exist&quot; : &quot;exist&quot;);</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<h3 id="4-3-6-事件类型对照表"><a href="#4-3-6-事件类型对照表" class="headerlink" title="4.3.6 事件类型对照表"></a>4.3.6 事件类型对照表</h3><p><img src="/2018/02/02/zookeeper知识学习/clip_ima99ge002.jpg" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_ima324ge002.jpg" alt="img"></p>
<p><strong>本表总结：exits和getData设置数据监视，而getChildren设置子节点监视</strong></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_ima555ge002.jpg" alt="img"></p>
<h3 id="4-3-7-实现永久监听（伪）"><a href="#4-3-7-实现永久监听（伪）" class="headerlink" title="4.3.7 实现永久监听（伪）"></a>4.3.7 实现永久监听（伪）</h3><p>我们知道zookeeper的监听是一次性监听（on-time-trriger）</p>
<p><img src="/2018/02/02/zookeeper知识学习/111.png" alt="img"></p>
<blockquote>
<p>详情可查看 4.3.2代码 和 4.2 的15）exists 节点</p>
</blockquote>
<h2 id="4-4-案例总结"><a href="#4-4-案例总结" class="headerlink" title="4.4 案例总结"></a>4.4 案例总结</h2><p>1）需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线</p>
<p>2）需求分析</p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image00rr2.png" alt="img"></p>
<p>3）具体实现：</p>
<p>（0）现在集群上创建/servers节点</p>
<p>[zk: localhost:2181(CONNECTED) 10] create /servers “servers”</p>
<p>Created /servers</p>
<p>（1）服务器端代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><div class="line"><span class="keyword">package</span> com.kingge.zkcase;</div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.CreateMode;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooDefs.Ids;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeServer</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span> String connectString = <span class="string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>;</div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</div><div class="line">	<span class="keyword">private</span> ZooKeeper zk = <span class="keyword">null</span>;</div><div class="line">	<span class="keyword">private</span> String parentNode = <span class="string">"/servers"</span>;</div><div class="line">	</div><div class="line">	<span class="comment">// 创建到zk的客户端连接</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</div><div class="line">		</div><div class="line">		zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</div><div class="line"></div><div class="line">			<span class="meta">@Override</span></div><div class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line"></div><div class="line">			&#125;</div><div class="line">		&#125;);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">// 注册服务器</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registServer</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">		String create = zk.create(parentNode + <span class="string">"/server"</span>, hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</div><div class="line">		</div><div class="line">		System.out.println(hostname +<span class="string">" is noline "</span>+ create);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">// 业务功能</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">		System.out.println(hostname+<span class="string">" is working ..."</span>);</div><div class="line">		</div><div class="line">		Thread.sleep(Long.MAX_VALUE);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		<span class="comment">// 获取zk连接</span></div><div class="line">		DistributeServer server = <span class="keyword">new</span> DistributeServer();</div><div class="line">		server.getConnect();</div><div class="line">		</div><div class="line">		<span class="comment">// 利用zk连接注册服务器信息</span></div><div class="line">		server.registServer(args[<span class="number">0</span>]);</div><div class="line">		</div><div class="line">		<span class="comment">// 启动业务功能</span></div><div class="line">		server.business(args[<span class="number">0</span>]);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>（2）客户端代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.kingge.zkcase;</div><div class="line">import java.io.IOException;</div><div class="line">import java.util.ArrayList;</div><div class="line">import java.util.List;</div><div class="line">import org.apache.zookeeper.WatchedEvent;</div><div class="line">import org.apache.zookeeper.Watcher;</div><div class="line">import org.apache.zookeeper.ZooKeeper;</div><div class="line"></div><div class="line">public class DistributeClient &#123;</div><div class="line">	private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;;</div><div class="line">	private static int sessionTimeout = 2000;</div><div class="line">	private ZooKeeper zk = null;</div><div class="line">	private String parentNode = &quot;/servers&quot;;</div><div class="line">	private volatile ArrayList&lt;String&gt; serversList = new ArrayList&lt;&gt;();</div><div class="line"></div><div class="line">	// 创建到zk的客户端连接</div><div class="line">	public void getConnect() throws IOException &#123;</div><div class="line">		zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123;</div><div class="line"></div><div class="line">			@Override</div><div class="line">			public void process(WatchedEvent event) &#123;</div><div class="line"></div><div class="line">				// 再次启动监听</div><div class="line">				try &#123;</div><div class="line">					getServerList();</div><div class="line">				&#125; catch (Exception e) &#123;</div><div class="line">					e.printStackTrace();</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	//</div><div class="line">	public void getServerList() throws Exception &#123;</div><div class="line">		</div><div class="line">		// 获取服务器子节点信息，并且对父节点进行监听</div><div class="line">		List&lt;String&gt; children = zk.getChildren(parentNode, true);</div><div class="line">		ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;();</div><div class="line">		</div><div class="line">		for (String child : children) &#123;</div><div class="line">			byte[] data = zk.getData(parentNode + &quot;/&quot; + child, false, null);</div><div class="line"></div><div class="line">			servers.add(new String(data));</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		// 把servers赋值给成员serverList，已提供给各业务线程使用</div><div class="line">		serversList = servers;</div><div class="line"></div><div class="line">		System.out.println(serversList);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// 业务功能</div><div class="line">	public void business() throws Exception &#123;</div><div class="line">		System.out.println(&quot;client is working ...&quot;);</div><div class="line">Thread.sleep(Long.MAX_VALUE);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	public static void main(String[] args) throws Exception &#123;</div><div class="line"></div><div class="line">		// 获取zk连接</div><div class="line">		DistributeClient client = new DistributeClient();</div><div class="line">		client.getConnect();</div><div class="line"></div><div class="line">		// 获取servers的子节点信息，从中获取服务器信息列表</div><div class="line">		client.getServerList();</div><div class="line"></div><div class="line">		// 业务进程启动</div><div class="line">		client.business();</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="4-5-zookeeper核心原理（事件）"><a href="#4-5-zookeeper核心原理（事件）" class="headerlink" title="4.5 zookeeper核心原理（事件）"></a>4.5 zookeeper核心原理（事件）</h1><p><img src="/2018/02/02/zookeeper知识学习/222.png" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image0304.jpg" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_imagyye006.jpg" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_imagerr008.jpg" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/333.png" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image012.jpg" alt="img"></p>
<p><img src="/2018/02/02/zookeeper知识学习/clip_image013.png" alt="img"></p>
<p><a href="https://blog.csdn.net/yinwenjie/article/details/47685077" target="_blank" rel="external">https://blog.csdn.net/yinwenjie/article/details/47685077</a>  </p>
<h1 id="五-好的总结网站"><a href="#五-好的总结网站" class="headerlink" title="五 好的总结网站"></a>五 好的总结网站</h1><p>\1.      <a href="https://blog.csdn.net/liu857279611/article/details/70495413" target="_blank" rel="external">https://blog.csdn.net/liu857279611/article/details/70495413</a> </p>
<p>\2.      <a href="https://www.jianshu.com/p/a1d7826073e6" target="_blank" rel="external">https://www.jianshu.com/p/a1d7826073e6</a> </p>
<p>\3.      <a href="https://blog.csdn.net/yinwenjie/article/details/47685077" target="_blank" rel="external">https://blog.csdn.net/yinwenjie/article/details/47685077</a> </p>
<p>\4.      <a href="https://www.jianshu.com/p/5d12a01018e1" target="_blank" rel="external">https://www.jianshu.com/p/5d12a01018e1</a> </p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;   最近公司开发saas模式的企业应用软件服务，所以下面是我个人使用和总结（掺杂了大数据相关的总结）&lt;/p&gt;
&lt;h1 id=&quot;一、正文&quot;&gt;
    
    </summary>
    
      <category term="zookeeper" scheme="http://kingge.top/categories/zookeeper/"/>
    
    
      <category term="Java" scheme="http://kingge.top/tags/Java/"/>
    
      <category term="hadoop，linux" scheme="http://kingge.top/tags/hadoop%EF%BC%8Clinux/"/>
    
  </entry>
  
  <entry>
    <title>聊聊分布式事务，再说说解决方案-cap</title>
    <link href="http://kingge.top/2017/10/18/%E8%81%8A%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%EF%BC%8C%E5%86%8D%E8%AF%B4%E8%AF%B4%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-cap/"/>
    <id>http://kingge.top/2017/10/18/聊聊分布式事务，再说说解决方案-cap/</id>
    <published>2017-10-18T09:57:58.000Z</published>
    <updated>2017-10-18T10:00:42.654Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据库事务"><a href="#数据库事务" class="headerlink" title="数据库事务"></a>数据库事务</h1><blockquote>
<p>在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会经常使用到。但是即使如此，可能对于一些细节问题，很多人仍然不清楚。比如很多人都知道数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。但是再往下比如问到隔离性指的是什么的时候可能就不知道了，或者是知道隔离性是什么但是再问到数据库实现隔离的都有哪些级别，或者是每个级别他们有什么区别的时候可能就不知道了。</p>
</blockquote>
<p>本文并不打算介绍这些数据库事务的这些东西，有兴趣可以搜索一下相关资料。不过有一个知识点我们需要了解，就是假如数据库在提交事务的时候突然断电，那么它是怎么样恢复的呢？ 为什么要提到这个知识点呢？ 因为分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，所以我们在做分布式系统的时候，最先考虑的就是这种情况。这些异常可能有 机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常等等…</p>
<p>我们接着说本地事务数据库断电的这种情况，它是怎么保证数据一致性的呢？我们使用SQL Server来举例，我们知道我们在使用 SQL Server 数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多。数据库进行任何写入操作的时候都是要先写日志的，同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。</p>
<p>接着，我们就说一下分布式事务。</p>
<h1 id="分布式理论"><a href="#分布式理论" class="headerlink" title="分布式理论"></a>分布式理论</h1><blockquote>
<p>当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况了，而在这种ACID的集群环境下，再想保证集群的ACID几乎是很难达到，或者即使能达到那么效率和性能会大幅下降，最为关键的是再很难扩展新的分区了，这个时候如果再追求集群的ACID会导致我们的系统变得很差，这时我们就需要引入一个新的理论原则来适应这种集群的情况，就是 CAP 原则或者叫CAP定理，那么CAP定理指的是什么呢？</p>
<p>CAP定理</p>
<blockquote>
<p>CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性：</p>
</blockquote>
</blockquote>
<ul>
<li>一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效)</li>
<li>可用性(Availability) ： 每个操作都必须以可预期的响应结束</li>
<li>分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成</li>
<li>具体地讲在分布式系统中，在任何数据库设计中，一个Web应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。</li>
</ul>
<blockquote>
<p>这个定理在迄今为止的分布式系统中都是适用的！ 为什么这么说呢？</p>
</blockquote>
<p>转载链接描述的很到位：<a href="http://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html" target="_blank" rel="external">http://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;数据库事务&quot;&gt;&lt;a href=&quot;#数据库事务&quot; class=&quot;headerlink&quot; title=&quot;数据库事务&quot;&gt;&lt;/a&gt;数据库事务&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会
    
    </summary>
    
      <category term="分布式" scheme="http://kingge.top/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="分布式" scheme="http://kingge.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="数据库" scheme="http://kingge.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>数据库中的undo和redo日志</title>
    <link href="http://kingge.top/2017/10/18/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84undo%E5%92%8Credo%E6%97%A5%E5%BF%97/"/>
    <id>http://kingge.top/2017/10/18/数据库中的undo和redo日志/</id>
    <published>2017-10-18T09:52:46.000Z</published>
    <updated>2017-10-18T09:55:01.852Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>转载好的博客解释1： <a href="http://blog.csdn.net/kobejayandy/article/details/50885693" target="_blank" rel="external">http://blog.csdn.net/kobejayandy/article/details/50885693</a> </p>
<p>转载好的博客解释2： <a href="http://www.cnblogs.com/Bozh/archive/2013/03/18/2966494.html" target="_blank" rel="external">http://www.cnblogs.com/Bozh/archive/2013/03/18/2966494.html</a></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;转载好的博客解释1： &lt;a href=&quot;http://blog.csdn.net/kobejayandy/article/details/50885693&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.cs
    
    </summary>
    
      <category term="Mysql" scheme="http://kingge.top/categories/Mysql/"/>
    
    
      <category term="分布式" scheme="http://kingge.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="数据库" scheme="http://kingge.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>vSphere与Workstation虚拟机交互的几种方法</title>
    <link href="http://kingge.top/2017/10/18/vSphere%E4%B8%8EWorkstation%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://kingge.top/2017/10/18/vSphere与Workstation虚拟机交互的几种方法/</id>
    <published>2017-10-18T08:11:28.000Z</published>
    <updated>2017-10-18T08:14:18.402Z</updated>
    
    <content type="html"><![CDATA[<p>参见转载链接：  <a href="http://wangchunhai.blog.51cto.com/225186/1884052" target="_blank" rel="external">http://wangchunhai.blog.51cto.com/225186/1884052</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参见转载链接：  &lt;a href=&quot;http://wangchunhai.blog.51cto.com/225186/1884052&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://wangchunhai.blog.51cto.com/2251
    
    </summary>
    
      <category term="Linux" scheme="http://kingge.top/categories/Linux/"/>
    
    
      <category term="linux" scheme="http://kingge.top/tags/linux/"/>
    
      <category term="centos" scheme="http://kingge.top/tags/centos/"/>
    
      <category term="vmware" scheme="http://kingge.top/tags/vmware/"/>
    
  </entry>
  
  <entry>
    <title>查看虚拟机里的Centos7的IP</title>
    <link href="http://kingge.top/2017/10/18/%E6%9F%A5%E7%9C%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%87%8C%E7%9A%84Centos7%E7%9A%84IP/"/>
    <id>http://kingge.top/2017/10/18/查看虚拟机里的Centos7的IP/</id>
    <published>2017-10-18T07:48:43.000Z</published>
    <updated>2017-10-18T08:09:04.483Z</updated>
    
    <content type="html"><![CDATA[<h1 id="登录虚拟机"><a href="#登录虚拟机" class="headerlink" title="登录虚拟机"></a>登录虚拟机</h1><p>   输入用户名和密码（用户名一般是root）</p>
<h1 id="查看ip-指令"><a href="#查看ip-指令" class="headerlink" title="查看ip 指令"></a>查看ip 指令</h1><blockquote>
<p>ip addr 指令：  查看当前虚拟机ip </p>
</blockquote>
<p><img src="/2017/10/18/查看虚拟机里的Centos7的IP/TIM截图20171018155302.png" alt=""></p>
<p>我们发现ens32 没有 inet 这个属性，没有出现ip，那么说明在设置的时候没有开启，需要先去设置。</p>
<blockquote>
<p>当前位置：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[root@localhost ~]# pwd</div><div class="line">/root</div><div class="line">[root@localhost ~]#</div></pre></td></tr></table></figure></p>
<p>接着来查看ens32网卡的配置： vi /etc/sysconfig/network-scripts/ifcfg-ens32   注意vi后面加空格.  etc 文件夹的位置在于</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[root@localhost ~]# cd ..</div><div class="line">[root@localhost /]# ls</div><div class="line">bin   dev  home  lib64  mnt  proc  run   srv  tmp  var</div><div class="line">boot  etc  lib   media  opt  root  sbin  sys  usr</div></pre></td></tr></table></figure>
<blockquote>
<p>查看 ifcfg-ens32 的内容</p>
</blockquote>
<p><img src="/2017/10/18/查看虚拟机里的Centos7的IP/TIM截图20171018160037.png" alt=""></p>
<blockquote>
<p>从配置清单中可以发现 CentOS 7 默认是不启动网卡的（ONBOOT=no）。</p>
<blockquote>
<p>把这一项改为YES（ONBOOT=yes） – (按 i 进入编辑模式 ，修改完，按 esc退出编辑模式，然后 按 ctrl + shift + :  输入 wq 完成编辑)</p>
</blockquote>
<p>然后重启网络服务： sudo service network restart </p>
<p>然后我们再输入  ip addr 命令</p>
</blockquote>
<hr>
<p><img src="/2017/10/18/查看虚拟机里的Centos7的IP/TIM截图20171018160445.png" alt=""></p>
<hr>
<h1 id="使用第三方工具登录"><a href="#使用第三方工具登录" class="headerlink" title="使用第三方工具登录"></a>使用第三方工具登录</h1><blockquote>
<p>这里是用的是 xshell，你也可以用winscp（这个一般是用来传文件的）</p>
</blockquote>
<p><img src="/2017/10/18/查看虚拟机里的Centos7的IP/TIM截图20171018160611.png" alt=""></p>
<p>然后点击连接，输入用户名和密码，便可以进入命令界面</p>
<p><img src="/2017/10/18/查看虚拟机里的Centos7的IP/TIM截图20171018160844.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;登录虚拟机&quot;&gt;&lt;a href=&quot;#登录虚拟机&quot; class=&quot;headerlink&quot; title=&quot;登录虚拟机&quot;&gt;&lt;/a&gt;登录虚拟机&lt;/h1&gt;&lt;p&gt;   输入用户名和密码（用户名一般是root）&lt;/p&gt;
&lt;h1 id=&quot;查看ip-指令&quot;&gt;&lt;a href=&quot;#查看i
    
    </summary>
    
      <category term="Linux" scheme="http://kingge.top/categories/Linux/"/>
    
    
      <category term="linux" scheme="http://kingge.top/tags/linux/"/>
    
      <category term="centos" scheme="http://kingge.top/tags/centos/"/>
    
      <category term="vmware" scheme="http://kingge.top/tags/vmware/"/>
    
  </entry>
  
</feed>
