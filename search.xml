<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hessian 多系统访问]]></title>
    <url>%2F2019%2F06%2F01%2FHessian%20%E5%A4%9A%E7%B3%BB%E7%BB%9F%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new "My New Post" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[hadoop大数据(四)-Hadoop编译源码]]></title>
    <url>%2F2018%2F02%2F28%2Fhadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E5%9B%9B-Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop大数据(三)-Hadoop三种部署和运行方式]]></title>
    <url>%2F2018%2F02%2F26%2Fhadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%B8%89-Hadoop%E4%B8%89%E7%A7%8D%E9%83%A8%E7%BD%B2%E5%92%8C%E8%BF%90%E8%A1%8C%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop大数据(二)-运行环境搭建]]></title>
    <url>%2F2018%2F02%2F24%2Fhadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%BA%8C-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[三 Hadoop运行环境搭建3.0 前置准备需要linux相关的知识，和安装虚拟机。所以还不了解的请看：Linux基础 3.1 虚拟机网络模式设置为NAT ​ 最后，重新启动系统。 ​ [root@hadoop101 ~]# sync ​ [root@hadoop101 ~]# reboot 3.2 克隆虚拟机1）克隆虚拟机 2）启动虚拟机 3.3 修改为静态ip1）在终端命令窗口中输入[root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules 进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址 2）修改IP地址[root@hadoop101 /]# vim /etc/sysconfig/network-scripts/ifcfg-eth0 需要修改的内容有5项： IPADDR=192.168.1.101 GATEWAY=192.168.1.2 ONBOOT=yes BOOTPROTO=static DNS1=192.168.1.2 ​ （1）修改前 ​ （2）修改后 ：wq 保存退出 3）执行[root@hadoop101 /]# service network restart 4）如果报错，reboot，重启虚拟机。​ [root@hadoop101 /]# reboot 3.4 修改主机名1）修改linux的hosts文件（1）进入Linux系统查看本机的主机名。通过hostname命令查看。 [root@hadoop100 /]# hostname hadoop100 （2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件。 [root@hadoop100~]# vi /etc/sysconfig/network 修改文件中主机名称 NETWORKING=yes NETWORKING_IPV6=no HOSTNAME= hadoop101 注意：主机名称不要有“_”下划线 （3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。 （4）保存退出。 （5）打开/etc/hosts [root@hadoop100 ~]# vim /etc/hosts 添加如下内容 192.168.1.100 hadoop100 192.168.1.101 hadoop101 192.168.1.102 hadoop102 192.168.1.103 hadoop103 192.168.1.104 hadoop104 192.168.1.105 hadoop105 192.168.1.106 hadoop106 192.168.1.107 hadoop107 192.168.1.108 hadoop108 192.168.1.109 hadoop109 192.168.1.110 hadoop110 （6）并重启设备，重启后，查看主机名，已经修改成功 2）修改window7的hosts文件(可以不改)只不过是方便于在windows服务器中，使用域名的方式访问hadopp相关的组件。 ​ （1）进入C:\Windows\System32\drivers\etc路径 ​ （2）打开hosts文件并添加如下内容 192.168.1.100 hadoop100 192.168.1.101 hadoop101 192.168.1.102 hadoop102 192.168.1.103 hadoop103 192.168.1.104 hadoop104 192.168.1.105 hadoop105 192.168.1.106 hadoop106 192.168.1.107 hadoop107 192.168.1.108 hadoop108 192.168.1.109 hadoop109 192.168.1.110 hadoop110 3.5 关闭防火墙1）查看防火墙开机启动状态 [root@hadoop101 ~]# chkconfig iptables –list 2）关闭防火墙 [root@hadoop101 ~]# chkconfig iptables off 3.6 在opt目录下创建文件1）创建kingge用户​ 在root用户里面执行如下操作 [root@hadoop101 opt]# adduser atguigu [root@hadoop101 opt]# passwd atguigu 更改用户 test 的密码 。 新的 密码： 无效的密码： 它没有包含足够的不同字符 无效的密码： 是回文 重新输入新的 密码： passwd： 所有的身份验证令牌已经成功更新。 2）设置kingge用户具有root权限修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： [root@hadoop101 kingge]# vi /etc/sudoers ## Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) ALL 修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。 3）在/opt目录下创建文件夹（1）在root用户下创建module、software文件夹 [root@hadoop101 opt]# mkdir module [root@hadoop101 opt]# mkdir software （2）修改module、software文件夹的所有者 [root@hadoop101 opt]# chown kingge:kingge module [root@hadoop101 opt]# chown kingge:kingge sofrware [root@hadoop101 opt]# ls -al 总用量 16 drwxr-xr-x. 6 root root 4096 4月 24 09:07 . dr-xr-xr-x. 23 root root 4096 4月 24 08:52 .. drwxr-xr-x. 4 kingge kingge 4096 4月 23 16:26 module drwxr-xr-x. 2 kingge kingge 4096 4月 23 16:25 software 3.7 安装jdk1）卸载现有jdk（1）查询是否安装java软件： [root@hadoop101 opt]# rpm -qa|grep java （2）如果安装的版本低于1.7，卸载该jdk： [root@hadoop101 opt]# rpm -e 软件包 2）复制文件用SecureCRT工具将jdk、Hadoop-2.7.2.tar.gz导入到opt目录下面的software文件夹下面 3）在linux系统下的opt目录中查看软件包是否导入成功。[root@hadoop101opt]# cd software/ [root@hadoop101software]# ls hadoop-2.7.2.tar.gz jdk-8u144-linux-x64.tar.gz 4）解压jdk到/opt/module目录下​ [root@hadoop101software]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/ 5）配置jdk环境变量​ （1）先获取jdk路径： [root@hadoop101 jdk1.8.0_144]# pwd /opt/module/jdk1.8.0_144 ​ （2）打开/etc/profile文件： [root@hadoop101 jdk1.8.0_144]# vi /etc/profile ​ 在profie文件末尾添加jdk路径： ​ ##JAVA_HOME export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin ​ （3）保存后退出： :wq ​ （4）让修改后的文件生效： [root@hadoop101 jdk1.8.0_144]# source /etc/profile ​ （5）重启（如果java -version可以用就不用重启）： [root@hadoop101 jdk1.8.0_144]# sync ​ [root@hadoop101 jdk1.8.0_144]# reboot 6）测试jdk安装成功[root@hadoop101 jdk1.8.0_144]# java -version java version “1.8.0_144” 3.8 安装Hadoop 这里使用的是已经编译过后的hadoop源码，官网下载的是非编译过后的，需要编译。如何编译参见。 Hadoop编译源码-Hadoop编译源码) 1）进入到Hadoop安装包路径下： [root@hadoop101 ~]# cd /opt/software/ 2）解压安装文件到/opt/module下面 [root@hadoop101 software]# tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/ 3）查看是否解压成功 [root@hadoop101 software]# ls /opt/module/ hadoop-2.7.2 4）在/opt/module/hadoop-2.7.2/etc/hadoop路径下配置hadoop-env.sh （1）Linux系统中获取jdk的安装路径： [root@hadoop101 jdk1.8.0_144]# echo $JAVA_HOME /opt/module/jdk1.8.0_144 （2）修改hadoop-env.sh文件中JAVA_HOME 路径： [root@hadoop101 hadoop]# vi hadoop-env.sh 修改JAVA_HOME如下 export JAVA_HOME=/opt/module/jdk1.8.0_144 5）将hadoop添加到环境变量 （1）获取hadoop安装路径：[root@ hadoop101 hadoop-2.7.2]# pwd/opt/module/hadoop-2.7.2 （2）打开/etc/profile文件：[root@ hadoop101 hadoop-2.7.2]# vi /etc/profile 在profie文件末尾添加jdk路径：（shitf+g）\##HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin （3）保存后退出：:wq （4）让修改后的文件生效：[root@ hadoop101 hadoop-2.7.2]# source /etc/profile（5）重启(如果hadoop命令不能用再重启)： [root@ hadoop101 hadoop-2.7.2]# sync [root@ hadoop101 hadoop-2.7.2]# reboot 6）修改/opt目录下的所有文件所有者为kingge ​ [root@hadoop101 opt]# chown kingge:kingge -R /opt/ 7）切换到kingge用户 ​ [root@hadoop101 opt]# su kingge]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop大数据(一)-理论知识了解]]></title>
    <url>%2F2018%2F02%2F21%2Fhadoop%E5%A4%A7%E6%95%B0%E6%8D%AE-%E4%B8%80-%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E4%BA%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言首先本人要先声明的是，这一系列的大数据总结，只是对于整个大数据生态的一个稍微深入的总结。并非是非常深入的，但是能够满足大部分人的需求。 一 大数据概念大数据的概念： 指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 这个概念的解释来源于百度百科，这样的解释太过空洞，那么就用更浅显易懂的解释是： 解决海量数据的存储和海量数据的分析计算 1.2 大数据的特点 图片来源于网上 1.3 大数据应用场景 4.给顾客推荐访问过的商品，我们使用淘宝之类的，当我们浏览某个商品之后，下次进来时，他会默认给你推荐你上次浏览过的商品。 第八：人工智能，目前最火的的风口 二 Hadoop框架2.1 Hadoop是什么1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构 2）主要解决，海量数据的存储和海量数据的分析计算问题（很明显只是一句废话，哈哈哈）。 3）广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈 2.2 Hadoop发展历史1）Lucene–Doug Cutting开创的开源软件，用java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎 2）2001年年底成为apache基金会的一个子项目 3）对于大数量的场景，Lucene面对与Google同样的困难 4）学习和模仿Google解决这些问题的办法 ：微型版Nutch 5）可以说Google是hadoop的思想之源(Google在大数据方面的三篇论文) ​ GFS —&gt;HDFS ​ Map-Reduce —&gt;MR ​ BigTable —&gt;Hbase 6）2003-2004年，Google公开了部分GFS和Mapreduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和Mapreduce机制，使Nutch性能飙升 7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。2006 年 3 月份，Map-Reduce和Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中 8）名字来源于Doug Cutting儿子的玩具大象 9）Hadoop就此诞生并迅速发展，标志这云计算时代来临 2.3 Hadoop三大发行版本Hadoop 三大发行版本: Apache、Cloudera、Hortonworks。 Apache版本最原始（最基础）的版本，对于入门学习最好。 Cloudera在大型互联网企业中用的较多。（因为他解决了hadoop各个版本和其他框架的兼容问题） Hortonworks文档较好。 1）Apache Hadoop 官网地址：http://hadoop.apache.org/releases.html 下载地址：https://archive.apache.org/dist/hadoop/common/ 2）Cloudera Hadoop 官网地址：https://www.cloudera.com/downloads/cdh/5-10-0.html 下载地址：http://archive-primary.cloudera.com/cdh5/cdh/5/ （1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。 （2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support （3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。 （4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。 （5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。 3）Hortonworks Hadoop 官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform （1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。 （2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。 （3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。 （4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。 （5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 （6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的microsoft Windows平台上本地运行。定价以集群为基础，每10个节点每年为12500美元。 2.4 Hadoop的优势1）高可靠性：因为Hadoop假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理。2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度（根据文件快开启相应数量的map任务处理，reduce的并行数量是可控的）。4）高容错性：自动保存多份副本数据，并且能够自动将失败的任务重新分配。 2.5 Hadoop组成2.5.0 四个组成1）Hadoop HDFS：一个高可靠、高吞吐量的分布式文件系统。2）Hadoop MapReduce：一个分布式的离线并行计算框架。3）Hadoop YARN：作业调度与集群资源管理的框架。4）Hadoop Common：支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作）。 2.5.1 HDFS 组成： namenode：存储文件元数据。例如文件名称，文件目录结构，文件属性（生成时间、副本数，文件权限），以及每个文件的快列表和快所在的datanode。hdfs的文件目录维护中心。datanode：真正存储文件的单位，也就是统称的块。secondary namenode：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS的元数据的快照。帮助namenode合并镜像文件和操作日志，合并完成后同步给namenode，减少namenode的处理任务的压力。他不能够替代namenode，只能够做备份（方便namenode意外挂掉后，能够恢复数据） 2.5.2 YARN1）ResourceManager(rm)：处理客户端请求、启动/监控ApplicationMaster、监控NodeManager、资源分配与调度； 2）NodeManager(nm)：单个节点上的资源管理、处理来自ResourceManager的命令、处理来自ApplicationMaster的命令； 3）ApplicationMaster：数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错。 4）Container：对任务运行环境的抽象，封装了CPU、内存等多维资源以及环境变量、启动命令等任务运行相关的信息。 2.5.3 MapReduce主要是获取HDFS存储的数据，通过拆分块的形式进行数据获取分析处理，最后输出自己想要的结果。 MapReduce将计算过程分为两个阶段：Map和Reduce 1）Map阶段并行处理输入数据 2）Reduce阶段对Map结果进行汇总 上图简单的阐明了map和reduce的两个过程或者作用，虽然不够严谨，但是足以提供一个大概的认知，map过程是一个蔬菜到制成食物前的准备工作，reduce将准备好的材料合并进而制作出食物的过程。 2.6 大数据整个结构 图片来源于网上 图中涉及的技术名词解释如下：1）Sqoop：sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 一般是将关系型数据库的数据导入到hive或者HDFS中，目的就是为了分析这些数据，因为我们知道关系型数据库当数据量变得很大时，通过sql去统计查询，那么就会卡死。就是因为整个架构他们本省就是不支持的，他们通常进行的是扫表操作。2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息（3）支持通过Kafka服务器和消费机集群来分区消息。（4）支持Hadoop并行数据加载。4）Storm：Storm为分布式实时计算提供了一组通用原语，可被用于“流处理”之中，实时处理消息并更新数据库。这是管理队列及工作者集群的另一种方式。 Storm也可被用于“连续计算”（continuous computation），对数据流做连续查询，在计算时就将结果以流的形式输出给用户。5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。6）Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。Oozie协调作业就是通过时间（频率）和有效数据触发当前的Oozie工作流程。7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。8）Hive：hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。10）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。11）Mahout:Apache Mahout是个可扩展的机器学习和数据挖掘库，当前Mahout支持主要的4个用例：推荐挖掘：搜集用户动作并以此给用户推荐可能喜欢的事物。聚集：收集文件并进行相关文件分组。分类：从现有的分类文档中学习，寻找文档中的相似特征，并为无标签的文档进行正确的归类。频繁项集挖掘：将一组项分组，并识别哪些个别项会经常一起出现。12）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 好了到这里我们已经了解了hadoop整个生态相关的概念和组成，以及架构。那么接下来让我们进行hadoop环境的搭建。]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于2018年的计划]]></title>
    <url>%2F2018%2F02%2F19%2F%E5%85%B3%E4%BA%8E2018%E5%B9%B4%E7%9A%84%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[因为公司业务的转移和相关产品的开发，再加上大数据的火热，所以本人决定更新一些hadoop生态链相关的文章。所以敬请期待吧，哈哈哈哈哈。 Comming Soon！！！]]></content>
      <categories>
        <category>新年计划</category>
      </categories>
      <tags>
        <tag>新年计划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper知识学习]]></title>
    <url>%2F2018%2F02%2F02%2Fzookeeper%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[引言 最近公司开发saas模式的企业应用软件服务，所以下面是我个人使用和总结（掺杂了大数据相关的总结） 一、正文0.1 下载1）官网首页： https://zookeeper.apache.org/ 2）下载截图 1.1 概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 1.2 模型构造和特点 1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。 2）Leader负责进行投票的发起和决议，更新系统状态。 3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票。 4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。（例如现在zookeeper集群现在有四台，那么挂掉两台后就不能正常工作了。假设初始时只有三台，那么最多也是挂掉两台后就不能工作了。也就是说，部署三台和部署四台的效用其实是一样的，所以一般都是部署奇数台zookeeper，节省资源） 5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。 6）更新请求顺序进行（全局数据一致性的提现），来自同一个client的更新请求按其发送顺序依次执行。 7）数据更新原子性，一次数据更新要么成功，要么失败。 8）实时性，在一定时间范围内（数据一致性的更新会有延迟），client能读到最新数据。 9）一次性监听（缺点）-这个缺点我们在后面可以用代码解决。 1.3 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。每个节点的存储的数据量，也决定了他的应用场景并不是存储大量的数据。下面的1.4章节会阐述到他的作用-应用场景 1.4 应用场景如果某个需求：当某个节点发生变化，通知其他关注这个节点的其他节点。那么就可以使用**zookeeper** 项目中常用到分布式锁和配置管理 1.4.1 统一命名服务 意思就是：我们通常使用域名来访问某个网站，但是域名对应的ip我们是不需要关注的为了系统的容错性，我们访问Baidu的这个请求是会随机寻找一个正常运行的服务器去处理。那么我们就可以使用zookeeper来进行管理。管理可以访问到Baidu这个网址的ip列表。客户端每次请求百度时，只需要去请求这个zookeeper获取可访问的ip即可。实现动态ip的上下线管理 1.4.2 统一配置管理 1.4.3 统一集群管理集群管理结构图如下所示。 1.4.4 服务器节点动态上下线 1.4.5 软负载均衡 控制某个服务器的访问数，达到资源合理分配 1.4.6 分布式锁（主要原理是同一路径下的节点名称不能重复，不能重复创建）有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。（在下面的3.2章节会讲到zookeeper顺序节点的相关内容） 好的博客： https://my.oschina.net/aidelingyu/blog/1600979 https://www.jianshu.com/p/5d12a01018e1 1.4.7 队列管理两种类型的队列： 1、 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 2、队列按照 FIFO 方式进行入队和出队操作。 第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。 二 Zookeeper安装2.1 本地模式安装部署1）安装前准备： （1）安装jdk （2）通过cshell工具拷贝zookeeper到linux系统下 （3）修改tar包权限 [kingge@hadoop102 software]$ chmod u+x zookeeper-3.4.10.tar.gz （4）解压到指定目录 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 2）配置修改 将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； ​ 进入zoo.cfg文件：vim zoo.cfg ​ 修改dataDir路径为 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 在/opt/module/zookeeper-3.4.10/这个目录上创建zkData文件夹 ​ [kingge@hadoop102 zookeeper-3.4.10]$ mkdir zkData 3）操作zookeeper （1）启动zookeeper [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start （2）查看进程是否启动 ​ [kingge@hadoop102 zookeeper-3.4.10]$ jps 4020 Jps 4001 QuorumPeerMain （3）查看状态： [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: standalone （4）启动客户端： [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkCli.sh （5）退出客户端： [zk: localhost:2181(CONNECTED) 0] quit （6）停止zookeeper [kingge@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh stop 2.2 配置参数解读解读zoo.cfg文件中参数含义 1）tickTime=2000：通信心跳数，Zookeeper服务器心跳时间，单位毫秒 Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) 2）initLimit=10：Leader和Follower初始通信时限（10* tickTime – 也就是不要超过二十秒） 集群中的follower跟随者服务器与leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 投票选举新leader的初始化时间 Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 Leader允许Follower在initLimit时间内完成这个工作。 3）syncLimit=5：Leader和Follower同步通信时限（5* tickTime – 也就是不要超过十秒） 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。（默认超过十秒，leader就认为follwer已经碟机） 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 4）dataDir：数据文件目录+数据持久化路径 保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。 5）clientPort=2181：客户端连接端口 监听客户端连接的端口 2.3 分布式模式下的安装详见第四章节 三 Zookeeper内部原理3.1 选举机制1）半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。所以zookeeper**适合装在奇数台机器上**。 2）Zookeeper虽然在配置文件中并没有指定master和slave。但是，zookeeper工作时，是有一个节点为leader，其他则为follower，Leader是通过内部的选举机制临时产生的。 3）以一个简单的例子来说明整个选举的过程。 假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。 （1）服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是LOOKING状态。 （2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3 5/2=2.5 向上取整3)，所以服务器1、2还是继续保持LOOKING状态。 （3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的leader。 （4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。 （5）服务器5启动，同4一样当小弟。 3.2 节点类型1）Znode有两种类型： 短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 2）Znode有四种形式的目录节点（默认是persistent ） （1）持久化目录节点（PERSISTENT） ​ 客户端与zookeeper断开连接后，该节点依旧存在。 （2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） ​ 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。（保证创建的节点名称不会重复） （3）临时目录节点（EPHEMERAL） 客户端与zookeeper断开连接后，该节点被删除。 （4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 3）创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 4）在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序（应用场景1.4.6 分布式锁 第二种方式） 3.3 stat结构体1）czxid- 引起这个znode创建的zxid，创建节点的事务的zxid 每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。 事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。 2）ctime - znode被创建的毫秒数(从1970年开始) 3）mzxid - znode最后更新的zxid 4）mtime - znode最后修改的毫秒数(从1970年开始) 5）pZxid-znode最后更新的子节点zxid 6）cversion - znode子节点变化号，znode子节点修改次数 7）dataversion - znode数据变化号 8）aclVersion - znode访问控制列表的变化号 9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。 10）dataLength- znode的数据长度 11）numChildren - znode子节点数量 3.4 监听器原理 3.5 写数据流程 1.收到请求，先找到leader节点 2.广播请求给其他follower 3.哥哥follower写入数据，写入成功后，通知leader写入成功。（半数以上follower写入成功即为写入数据成功） 4.leader通知最初收到客户请求的server，数据写入成功，该server通知客户端写入数据成功 四 Zookeeper实战4.1 分布式安装部署0）集群规划 在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper。 1）解压安装 （1）解压zookeeper安装包到/opt/module/目录下 [kingge@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ （2）在/opt/module/zookeeper-3.4.10/这个目录下创建zkData ​ mkdir -p zkData （3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg ​ mv zoo_sample.cfg zoo.cfg 2）配置zoo.cfg文件 ​ （1）具体配置 ​ dataDir=/opt/module/zookeeper-3.4.10/zkData ​ 增加如下配置 ​ #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 （2）配置参数解读 Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器；（必须唯一） B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 3）集群操作 （1）在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件 ​ touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 （2）编辑myid文件 ​ vi myid ​ 在文件中添加与server对应的编号：如2 （3）拷贝配置好的zookeeper到其他机器上（可以用shell脚本进行分发数据） ​ scp -r zookeeper-3.4.10/ root@hadoop103.kingge.com:/opt/app/ ​ scp -r zookeeper-3.4.10/ root@hadoop104.kingge.com:/opt/app/ ​ 并分别修改myid文件中内容为3、4 （4）分别启动zookeeper ​ [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop104 zookeeper-3.4.10]# bin/zkServer.sh start （5）查看状态 [root@hadoop102 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop103 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower 分析：当第二个zookeeper启动时，因为2 &gt; 3/2=1.5，所以他被投票为了Leader，所以其他的节点就是follwer 4.2 客户端命令行操作 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建 -s 含有序列 -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 1）启动客户端（随便连接那个zookeeper都可以，因为他们内容都是一样的，下面连接的是103服务器） [kingge@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh 2）显示所有操作命令 [zk: localhost:2181(CONNECTED) 1] help 3）查看当前znode中所包含的内容 [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper] 4）查看当前节点数据并能看到更新次数等数据 [zk: localhost:2181(CONNECTED) 1] ls2 / [zookeeper] cZxid = 0x0 ctime = Thu Jan 01 08:00:00 CST 1970 mZxid = 0x0 mtime = Thu Jan 01 08:00:00 CST 1970 pZxid = 0x0 cversion = -1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 5）创建普通节点（注意创建节点时需要写入一些数据，否则创建不成功-例如create /app1 这样的话创建是没有效果的） [zk: localhost:2181(CONNECTED) 2] create /app1 “hello app1” Created /app1 [zk: localhost:2181(CONNECTED) 4] create /app1/server101 “192.168.1.101” Created /app1/server101 1.不支持递归创建节点，比如你要创建/app1/a,如果app1不存在，你就不能创建a( KeeperException.NoNode)。2.不可以再ephemeral类型的节点下创建子节点(KeeperException.NoChildrenForEphemerals)。（因为他本身是临时节点）3.如果指定的节点已经存在，会触发KeeperException.NodeExists 异常,当然了对于sequential类型的，不会抛出这个异常。（有编号类型的节点名称会自动递增）4.数据内容不能超过1M,否则将抛出KeeperException异常。 6）获得节点的值 [zk: localhost:2181(CONNECTED) 6] get /app1 hello app1 cZxid = 0x20000000a ctime = Mon Jul 17 16:08:35 CST 2017 mZxid = 0x20000000a mtime = Mon Jul 17 16:08:35 CST 2017 pZxid = 0x20000000b cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 10 numChildren = 1 [zk: localhost:2181(CONNECTED) 8] get /app1/server101 192.168.1.101 cZxid = 0x20000000b ctime = Mon Jul 17 16:11:04 CST 2017 mZxid = 0x20000000b mtime = Mon Jul 17 16:11:04 CST 2017 pZxid = 0x20000000b cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 13 numChildren = 0 7）创建短暂节点 [zk: localhost:2181(CONNECTED) 9] create -e /app-emphemeral 8888 （1）在当前客户端是能查看到的 [zk: localhost:2181(CONNECTED) 10] ls / [app1, app-emphemeral, zookeeper] （2）退出当前客户端然后再重启客户端 ​ [zk: localhost:2181(CONNECTED) 12] quit [kingge@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh （3）再次查看根目录下短暂节点已经删除 ​ [zk: localhost:2181(CONNECTED) 0] ls / [app1, zookeeper] 8）创建带序号的节点 ​ （1）先创建一个普通的根节点app2 ​ [zk: localhost:2181(CONNECTED) 11] create /app2 “app2” ​ （2）创建带序号的节点 ​ [zk: localhost:2181(CONNECTED) 13] create -s /app2/aa 888 Created /app2/aa0000000000 [zk: localhost:2181(CONNECTED) 14] create -s /app2/bb 888 Created /app2/bb0000000001 [zk: localhost:2181(CONNECTED) 15] create -s /app2/cc 888 Created /app2/cc0000000002 如果原节点下有1个节点，则再排序时从1开始，以此类推。 [zk: localhost:2181(CONNECTED) 16] create -s /app1/aa 888 Created /app1/aa0000000001 9）修改节点数据值 [zk: localhost:2181(CONNECTED) 2] set /app1 999 10）节点的值变化监听（一次性触发器）（Watch**的通知事件是从服务器发送给客户端的，是异步的**） ​ （1）在104主机上注册监听/app1节点数据变化（） 需要注意的是，注册一次监听，只能够响应一次，如果/app1节点的数据修改了两次，那么只显示第一次监听的信息，第二次不会有任何响应，想要得到响应，需要再次监听 [zk: localhost:2181(CONNECTED) 26] get /app1 watch ​ （2）在103主机上修改/app1节点的数据 [zk: localhost:2181(CONNECTED) 5] set /app1 777 ​ （3）观察104主机收到数据变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1 11）节点的子节点变化监听（路径变化） ​ （1）在104主机上注册监听/app1节点的子节点变化 [zk: localhost:2181(CONNECTED) 1] ls /app1 watch [aa0000000001, server101] ​ （2）在103主机/app1节点上创建子节点 [zk: localhost:2181(CONNECTED) 6] create /app1/bb 666 Created /app1/bb ​ （3）观察104主机收到子节点变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 12）删除节点 [zk: localhost:2181(CONNECTED) 4] delete /app1/bb 13）递归删除节点 [zk: localhost:2181(CONNECTED) 7] rmr /app2 14）查看节点状态 [zk: localhost:2181(CONNECTED) 12] stat /app1 cZxid = 0x20000000a ctime = Mon Jul 17 16:08:35 CST 2017 mZxid = 0x200000018 mtime = Mon Jul 17 16:54:38 CST 2017 pZxid = 0x20000001c cversion = 4 dataVersion = 2 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 3 numChildren = 2 15）exists 节点 这个函数很特殊，因为他可以监听一个尚未存在的节点，这是getData，getChildren不能做到的。exists可以监听一个节点的生命周期：从无到有，节点数据的变化，从有到无。 在传递给exists的watcher里，当path指定的节点被成功创建后，watcher会收到NodeCreated事件通知。当path所指定的节点的数据内容发送了改变后，wather会受到NodeDataChanged事件通知。 这里最需要注意的就是，exists可以监听一个未存在的节点，这是他与getData，getChildren本质的区别。 注意看上面的代码，其实我们已经实现了多次监听，解决了zookeeper单次监听的缺点。关键代码，我们在监听器里面，又再次声明了一次监听---zkClient.exists(&quot;eclipse&quot;,true) 16） getData 16） getChildren 4.3 API应用4.3.1 Eclipse环境搭建1）创建一个工程 2）解压zookeeper-3.4.10.tar.gz文件 3）拷贝zookeeper-3.4.10.jar、jline-0.9.94.jar、log4j-1.2.16.jar、netty-3.10.5.Final.jar、slf4j-api-1.6.1.jar、slf4j-log4j12-1.6.1.jar到工程的lib目录。并build一下，导入工程。 4）拷贝log4j.properties文件到项目根目录 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 4.3.2 创建ZooKeeper客户端private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void init() throws Exception &#123;//创建zookeeper连接的时候同时注册一个全局的默认的事件监听器 – // event.getType() 永远为null默认监听到None事件// //默认监听也可以使用register方法注册 //zkClient.register(watcherDefault); zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（用户的业务逻辑） System.out.println(event.getType() + &quot;--&quot; + event.getPath()); // 再次启动监听 - 解决zookeeper单次监听的缺点 try &#123; zkClient.getChildren(&quot;/&quot;, true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125;这里的watcher是该客户端总的监听方法，任何操作都会执行，而且是可以多次执行，并非单次。 4.3.3 创建子节点// 创建子节点@Testpublic void create() throws Exception &#123; // 数据的增删改查 // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型 String nodeCreated = zkClient.create(&quot;/eclipse&quot;, &quot;hello zk&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT);&#125; 4.3.4 获取子节点并监听// 获取子节点 @Test public void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren(&quot;/&quot;, true); for (String child : children) &#123; System.out.println(child); &#125; // 延时阻塞 Thread.sleep(Long.MAX_VALUE); &#125; 4.3.5 判断znode是否存在// 判断znode是否存在 @Test public void exist() throws Exception &#123; Stat stat = zkClient.exists(&quot;/eclipse&quot;, false); System.out.println(stat == null ? &quot;not exist&quot; : &quot;exist&quot;); &#125; 4.3.6 事件类型对照表 本表总结：exits和getData设置数据监视，而getChildren设置子节点监视 4.3.7 实现永久监听（伪）我们知道zookeeper的监听是一次性监听（on-time-trriger） 详情可查看 4.3.2代码 和 4.2 的15）exists 节点 4.4 案例总结1）需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线 2）需求分析 3）具体实现： （0）现在集群上创建/servers节点 [zk: localhost:2181(CONNECTED) 10] create /servers “servers” Created /servers （1）服务器端代码 package com.kingge.zkcase;import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.ZooDefs.Ids;public class DistributeServer &#123; private static String connectString = "hadoop102:2181,hadoop103:2181,hadoop104:2181"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = "/servers"; // 创建到zk的客户端连接 public void getConnect() throws IOException&#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); &#125; // 注册服务器 public void registServer(String hostname) throws Exception&#123; String create = zk.create(parentNode + "/server", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +" is noline "+ create); &#125; // 业务功能 public void business(String hostname) throws Exception&#123; System.out.println(hostname+" is working ..."); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 利用zk连接注册服务器信息 server.registServer(args[0]); // 启动业务功能 server.business(args[0]); &#125;&#125; （2）客户端代码 package com.kingge.zkcase;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = &quot;/servers&quot;; private volatile ArrayList&lt;String&gt; serversList = new ArrayList&lt;&gt;(); // 创建到zk的客户端连接 public void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 再次启动监听 try &#123; getServerList(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // public void getServerList() throws Exception &#123; // 获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); for (String child : children) &#123; byte[] data = zk.getData(parentNode + &quot;/&quot; + child, false, null); servers.add(new String(data)); &#125; // 把servers赋值给成员serverList，已提供给各业务线程使用 serversList = servers; System.out.println(serversList); &#125; // 业务功能 public void business() throws Exception &#123; System.out.println(&quot;client is working ...&quot;);Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 业务进程启动 client.business(); &#125;&#125; 4.5 zookeeper核心原理（事件） https://blog.csdn.net/yinwenjie/article/details/47685077 五 好的总结网站\1. https://blog.csdn.net/liu857279611/article/details/70495413 \2. https://www.jianshu.com/p/a1d7826073e6 \3. https://blog.csdn.net/yinwenjie/article/details/47685077 \4. https://www.jianshu.com/p/5d12a01018e1]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>hadoop，linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊分布式事务，再说说解决方案-cap]]></title>
    <url>%2F2017%2F10%2F18%2F%E8%81%8A%E8%81%8A%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%EF%BC%8C%E5%86%8D%E8%AF%B4%E8%AF%B4%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-cap%2F</url>
    <content type="text"><![CDATA[数据库事务 在说分布式事务之前，我们先从数据库事务说起。 数据库事务可能大家都很熟悉，在开发过程中也会经常使用到。但是即使如此，可能对于一些细节问题，很多人仍然不清楚。比如很多人都知道数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。但是再往下比如问到隔离性指的是什么的时候可能就不知道了，或者是知道隔离性是什么但是再问到数据库实现隔离的都有哪些级别，或者是每个级别他们有什么区别的时候可能就不知道了。 本文并不打算介绍这些数据库事务的这些东西，有兴趣可以搜索一下相关资料。不过有一个知识点我们需要了解，就是假如数据库在提交事务的时候突然断电，那么它是怎么样恢复的呢？ 为什么要提到这个知识点呢？ 因为分布式系统的核心就是处理各种异常情况，这也是分布式系统复杂的地方，因为分布式的网络环境很复杂，这种“断电”故障要比单机多很多，所以我们在做分布式系统的时候，最先考虑的就是这种情况。这些异常可能有 机器宕机、网络异常、消息丢失、消息乱序、数据错误、不可靠的TCP、存储数据丢失、其他异常等等… 我们接着说本地事务数据库断电的这种情况，它是怎么保证数据一致性的呢？我们使用SQL Server来举例，我们知道我们在使用 SQL Server 数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多。数据库进行任何写入操作的时候都是要先写日志的，同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。 接着，我们就说一下分布式事务。 分布式理论 当我们的单个数据库的性能产生瓶颈的时候，我们可能会对数据库进行分区，这里所说的分区指的是物理分区，分区之后可能不同的库就处于不同的服务器上了，这个时候单个数据库的ACID已经不能适应这种情况了，而在这种ACID的集群环境下，再想保证集群的ACID几乎是很难达到，或者即使能达到那么效率和性能会大幅下降，最为关键的是再很难扩展新的分区了，这个时候如果再追求集群的ACID会导致我们的系统变得很差，这时我们就需要引入一个新的理论原则来适应这种集群的情况，就是 CAP 原则或者叫CAP定理，那么CAP定理指的是什么呢？ CAP定理 CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性： 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成 具体地讲在分布式系统中，在任何数据库设计中，一个Web应用至多只能同时支持上面的两个属性。显然，任何横向扩展策略都要依赖于数据分区。因此，设计人员必须在一致性与可用性之间做出选择。 这个定理在迄今为止的分布式系统中都是适用的！ 为什么这么说呢？ 转载链接描述的很到位：http://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库中的undo和redo日志]]></title>
    <url>%2F2017%2F10%2F18%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84undo%E5%92%8Credo%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[转载好的博客解释1： http://blog.csdn.net/kobejayandy/article/details/50885693 转载好的博客解释2： http://www.cnblogs.com/Bozh/archive/2013/03/18/2966494.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vSphere与Workstation虚拟机交互的几种方法]]></title>
    <url>%2F2017%2F10%2F18%2FvSphere%E4%B8%8EWorkstation%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参见转载链接： http://wangchunhai.blog.51cto.com/225186/1884052]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看虚拟机里的Centos7的IP]]></title>
    <url>%2F2017%2F10%2F18%2F%E6%9F%A5%E7%9C%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%87%8C%E7%9A%84Centos7%E7%9A%84IP%2F</url>
    <content type="text"><![CDATA[登录虚拟机 输入用户名和密码（用户名一般是root） 查看ip 指令 ip addr 指令： 查看当前虚拟机ip 我们发现ens32 没有 inet 这个属性，没有出现ip，那么说明在设置的时候没有开启，需要先去设置。 当前位置：[root@localhost ~]# pwd/root[root@localhost ~]# 接着来查看ens32网卡的配置： vi /etc/sysconfig/network-scripts/ifcfg-ens32 注意vi后面加空格. etc 文件夹的位置在于 [root@localhost ~]# cd ..[root@localhost /]# lsbin dev home lib64 mnt proc run srv tmp varboot etc lib media opt root sbin sys usr 查看 ifcfg-ens32 的内容 从配置清单中可以发现 CentOS 7 默认是不启动网卡的（ONBOOT=no）。 把这一项改为YES（ONBOOT=yes） – (按 i 进入编辑模式 ，修改完，按 esc退出编辑模式，然后 按 ctrl + shift + : 输入 wq 完成编辑) 然后重启网络服务： sudo service network restart 然后我们再输入 ip addr 命令 使用第三方工具登录 这里是用的是 xshell，你也可以用winscp（这个一般是用来传文件的） 然后点击连接，输入用户名和密码，便可以进入命令界面]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[activity工作流框架——数据库表结构说明]]></title>
    <url>%2F2017%2F10%2F12%2Factivity%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%A1%86%E6%9E%B6%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E7%BB%93%E6%9E%84%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[本文转载于： http://www.jianshu.com/p/f9fd1cc02eae activity一共23张表 表的命名第一部分都是以 ACT_开头的。 表的命名第二部分是一个两个字符用例表的标识 act_ge_*： ‘ge’代表general（一般）。普通数据，各种情况都使用的数据。 act_gebytearray：二进制数据表，用来保存部署文件的大文本数据1.ID:资源文件编号，自增长2.REVINT:版本号3.NAME:资源文件名称4.DEPLOYMENTID:来自于父表act_redeployment的主键5.BYTES:大文本类型，存储文本字节流 act_geproperty：属性数据表，存储这整个流程引擎级别的数据。在初始化表结构时，会默认插入三条记录。1.NAME:属性名称2.VALUE_:属性值3.REV_INT:版本号 act_hi_*： hi’代表 history（历史）。就是这些表包含着历史的相关数据，如结束的流程实例、变量、任务、等等。 act_hiactinst：历史节点表1.ID : 标识2.PROC_DEFID :流程定义id3.PROC_INSTID : 流程实例id4.EXECUTIONID : 执行实例5.ACTID : 节点id6.ACTNAME : 节点名称7.ACTTYPE : 节点类型8.ASSIGNEE_ : 节点任务分配人9.STARTTIME : 开始时间10.ENDTIME : 结束时间11.DURATION : 经过时长 act_hi_attachment：历史附件表 act_hicomment：历史意见表1.ID :标识2.TYPE : 意见记录类型 为comment 时 为处理意见3.TIME : 记录时间4.USERID :5.TASKID ： 对应任务的id6.PROC_INSTID : 对应的流程实例的id7.ACTION ： 为AddComment 时为处理意见8.MESSAGE : 处理意见9.FULLMSG : act_hidetail：历史详情表，启动流程或者在任务complete之后,记录历史流程变量1.ID : 标识2.TYPE_ : variableUpdate 和 formProperty 两种值3.PROC_INSTID : 对应流程实例id4.EXECUTIONID : 对应执行实例id5.TASKID : 对应任务id6.ACT_INSTID : 对应节点id7.NAME : 历史流程变量名称，或者表单属性的名称8.VARTYPE : 定义类型9.REV : 版本10.TIME : 导入时间11.BYTEARRAYID12.DOUBLE : 如果定义的变量或者表单属性的类型为double，他的值存在这里13.LONG : 如果定义的变量或者表单属性的类型为LONG ,他的值存在这里14.TEXT : 如果定义的变量或者表单属性的类型为string，值存在这里15.TEXT2: act_hi_identitylink：历史流程人员表 act_hiprocinst： 历史流程实例表1.ID : 唯一标识2.PROC_INSTID : 流程ＩＤ3.BUSINESSKEY : 业务编号4.PROC_DEFID ： 流程定义id5.STARTTIME : 流程开始时间6.ENT_TIME : 结束时间7.DURATION : 流程经过时间8.START_USERID : 开启流程用户id9.START_ACTID : 开始节点10.END_ACTID： 结束节点11.SUPER_PROCESS_INSTANCEID : 父流程流程id12.DELETEREASON : 从运行中任务表中删除原因 act_hitaskinst： 历史任务实例表1.ID ： 标识2.PROC_DEFID ： 流程定义id3.TASK_DEFKEY : 任务定义id4.PROC_INSTID : 流程实例ｉｄ5.EXECUTIONID : 执行实例id6.PARENT_TASKID : 父任务id7.NAME : 任务名称8.DESCRIPTION : 说明9.OWNER : 拥有人（发起人）10.ASSIGNEE : 分配到任务的人11.START_TIME : 开始任务时间12.ENDTIME : 结束任务时间13.DURATION_ : 时长14.DELETEREASON :从运行时任务表中删除的原因15.PRIORITY_ : 紧急程度16.DUEDATE : act_hi_varinst：历史变量表 act_id_*： id’代表 identity（身份）。这些表包含着标识的信息，如用户、用户组、等等。 act_idgroup:用户组信息表，用来存储用户组信息。1.ID：用户组名2.REVINT:版本号3.NAME:用户组描述信息4.TYPE_:用户组类型 act_id_info：用户扩展信息表 act_id_membership：用户与用户组对应信息表，用来保存用户的分组信息1.USERID:用户名2.GROUPID:用户组名 act_iduser：用户信息表1.ID:用户名2.REVINT:版本号3.FIRST:用户名称4.LAST:用户姓氏5.EMAIL:邮箱6.PWD_:密码 act_re_*： ’re’代表 repository（仓库）。带此前缀的表包含的是静态信息，如，流程定义、流程的资源（图片、规则，等）。 act_redeployment:部署信息表,用来存储部署时需要持久化保存下来的信息1.ID:部署编号，自增长2.NAME_:部署包的名称3.DEPLOYTIME:部署时间 act_re_model 流程设计模型部署表 act_reprocdef:业务流程定义数据表1.ID:流程ID，由“流程编号：流程版本号：自增长ID”组成2.CATEGORY:流程命名空间（该编号就是流程文件targetNamespace的属性值）3.NAME:流程名称（该编号就是流程文件process元素的name属性值）4.KEY:流程编号（该编号就是流程文件process元素的id属性值）5.VERSION:流程版本号（由程序控制，新增即为1，修改后依次加1来完成的）6.DEPLOYMENTID:部署编号7.RESOURCENAME:资源文件名称8.DGRM_RESOURCENAME:图片资源文件名称9.HAS_START_FROMKEY:是否有Start From Key 注：此表和ACT_RE_DEPLOYMENT是多对一的关系，即，一个部署的bar包里可能包含多个流程定义文件，每个流程定义文件都会有一条记录在ACT_REPROCDEF表内，每个流程定义的数据，都会对于ACT_GE_BYTEARRAY表内的一个资源文件和PNG图片文件。和ACT_GE_BYTEARRAY的关联是通过程序用ACT_GE_BYTEARRAY.NAME与ACT_REPROCDEF.NAME完成的，在数据库表结构中没有体现。 act_ru_*： ’ru’代表 runtime（运行时）。就是这个运行时的表存储着流程变量、用户任务、变量、作业，等中的运行时的数据。 activiti 只存储流程实例执行期间的运行时数据，当流程实例结束时，将删除这些记录。这就使这些运行时的表保持 的小且快。 act_ru_event_subscr act_ruexecution：运行时流程执行实例表1.ID：主键，这个主键有可能和PROC_INSTID相同，相同的情况表示这条记录为主实例记录。2.REV_：版本，表示数据库表更新次数。3.PROC_INSTID：流程实例编号，一个流程实例不管有多少条分支实例，这个ID都是一致的。4.BUSINESSKEY：业务编号，业务主键，主流程才会使用业务主键，另外这个业务主键字段在表中有唯一约束。5.PARENTID：找到该执行实例的父级，最终会找到整个流程的执行实例6.PROC_DEFID：流程定义ID7.SUPEREXEC： 引用的执行模板，这个如果存在表示这个实例记录为一个外部子流程记录，对应主流程的主键ID。8.ACTID： 节点id，表示流程运行到哪个节点9.ISACTIVE： 是否活动流程实例10.ISCONCURRENT：是否并发。上图同步节点后为并发，如果是并发多实例也是为1。11.ISSCOPE： 主实例为1，子实例为0。12.TENANTID : 这个字段表示租户ID。可以应对多租户的设计。13.IS_EVENT_SCOPE: 没有使用到事件的情况下，一般都为0。14.SUSPENSIONSTATE：是否暂停。 act_ruidentitylink：运行时流程人员表，主要存储任务节点与参与者的相关信息1.ID： 标识2.REV_： 版本3.GROUPID： 组织id4.TYPE_： 类型5.USERID： 用户id6.TASKID： 任务id act_ru_job act_rutask：运行时任务节点表1.ID：2.REV_：3.EXECUTIONID： 执行实例的id4.PROC_INSTID： 流程实例的id5.PROC_DEFID： 流程定义的id,对应act_reprocdef 的id6.NAME_： 任务名称，对应 task 的name7.PARENT_TASKID : 对应父任务8.DESCRIPTION_：9.TASK_DEFKEY： task 的id10.OWNER : 发起人11.ASSIGNEE： 分配到任务的人12.DELEGATION : 委托人13.PRIORITY： 紧急程度14.CREATETIME： 发起时间15.DUETIME：审批时长 act_ruvariable：运行时流程变量数据表1.ID：标识2.REV：版本号3.TYPE：数据类型4.NAME_：变量名5.EXECUTIONID： 执行实例id6.PROC_INSTID： 流程实例id7.TASKID： 任务id8.BYTEARRAYID：9.DOUBLE：若数据类型为double ,保存数据在此列10.LONG： 若数据类型为Long保存数据到此列11.TEXT： string 保存到此列12.TEXT2： 结论及总结: 流程文件部署主要涉及到3个表，分别是：ACT_GE_BYTEARRAY、ACT_RE_DEPLOYMENT、ACT_RE_PROCDEF。主要完成“部署包”–&gt;“流程定义文件”–&gt;“所有包内文件”的解析部署关系。从表结构中可以看出，流程定义的元素需要每次从数据库加载并解析，因为流程定义的元素没有转化成数据库表来完成，当然流程元素解析后是放在缓存中的，具体的还需要后面详细研究。 流程定义中的java类文件不保存在数据库里 。 组织机构的管理相对较弱，如果要纳入单点登录体系内还需要改造完成，具体改造方法有待研究。 运行时对象的执行与数据库记录之间的关系需要继续研究 历史数据的保存及作用需要继续研究。]]></content>
      <categories>
        <category>activity</category>
      </categories>
      <tags>
        <tag>activity</tag>
        <tag>工作流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于web.xml中ServletContext、ServletContextListener、Filter、Servlet的执行顺序]]></title>
    <url>%2F2017%2F10%2F10%2F%E5%85%B3%E4%BA%8Eweb-xml%E4%B8%ADServletContext%E3%80%81ServletContextListener%E3%80%81Filter%E3%80%81Servlet%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言 今天跑一个web项目，想做一些初始化工作，于是使用Filter来实现，但是发现ServletContextListener，Servlet也是能够实现的。但是肯定会有先后顺序执行的问题，那么接下来探讨这个问题。 作者规则：为了节省部分人的时间，先说结论。结论就是标题的顺序：ServletContext - ServletContextListener- Filter、Servlet web加载 启动一个WEB项目的时候，WEB容器会去读取它的配置文件web.xml。 加载产生Servlet上下文实例，ServletContext 这个web项目的所有部分都将共享这个上下文。容器将转换为键值对，并交给servletContext。L例如我们在使用spring的时候，会配置applicationContext.xml &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; 依次加载Servlet的事件监听器 - ServletContextListener 并依次调用public void contextInitialized(ServletContextEvent sce)方法。加载和调用多个Listener的顺序由在web.xml中配置的依次顺序决定的。 &lt;listener&gt; &lt;listener-class&gt;com.wlx.core.application.ApplicaltionListener&lt;/listener-class&gt;&lt;/listener&gt;&lt;listener&gt; &lt;listener-class&gt;com.wlx.core.application.ApplicaltionListener2&lt;/listener-class&gt;&lt;/listener&gt;先执行 ApplicaltionListener的contextInitialized方法后执行ApplicaltionListener2的contextInitialized方法 我们可以通过这个方法做一些初始化工作：例如初始化数据库连接池，初始化redis，启动定时器服务，启动线程池做一些socket通讯服务等等工作。 然后在contextDestroyed方法关闭这些服务即可。 .依次加载Servlet的过滤器-Filter 并依次调用public void init(FilterConfig filterConfig) throws ServletException;方法加载和调用多个filter的顺序由在web.xml中配置的依次顺序决定的。 &lt;filter&gt; &lt;filter-name&gt;appFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.wlx.core.application.AppFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;appFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 依次加载Servlet Load-on-startup元素在web应用启动的时候指定了servlet被加载的顺序，它的值必须是一个整数。如果它的值是一个负整数或是这个元素不存在，那么容器会在该servlet被调用的时候(例如下面代码访问-/servlet/UploadFile 为后缀的时候才会去初始化init，并不会在项目启动时候访问init)，加载这个servlet。如果值是正整数或零，容器在配置的时候就加载并初始化这个servlet，容器必须保证值小的先被加载。如果值相等，容器可以自动选择先加载谁。 在servlet的配置当中，&lt;load-on-startup&gt;5&lt;/load-on-startup&gt;的含义是：标记容器是否在启动的时候就加载这个servlet。当值为0或者大于0时，表示容器在应用启动时就加载这个servlet；当是一个负数时或者没有指定时，则指示容器在该servlet被选择时才加载。正数的值越小，启动该servlet的优先级越高。 项目启动时会去调用 UploadFile的init方法&lt;servlet&gt; &lt;servlet-name&gt;UploadFile&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wlx.core.application.servlet.UploadFile&lt;/servlet-class&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;UploadFile&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/UploadFile&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 项目启动时不会去调用 EServlet的init方法，访问匹配规则的网址时才会去调用init，而且只调用一次 &lt;servlet&gt; &lt;servlet-name&gt;EServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wlx.core.application.servlet.EServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;EServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/EServlet&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 总结 以上是Web容器在启动时加载的顺序，启动加载只会加载一次。web.xml 的加载顺序是：ServletContext-&gt; context-param -&gt;listener -&gt; filter -&gt; servlet. 扩展知识-请求执行循序 在上面中我们总结web加载的执行顺序，那么一个请求的执行循序呢？实际上就是一个责任链模式的问题 依次执行过滤器filter的方法public void doFilter(ServletRequest request, ServletResponse response,FilterChain chain)，这个方法应用了责任链模式，当在该方法中使用chain.doFilter(request, response);则这个过滤器就调用下一个过滤器，直到过滤器链条完成调用，进入Servlet处理，这个时候doFilter并未执行完成，仅仅在servlet之前进行一连串的过滤处理。 进入相应Servlet并调用public void service(ServletRequest req, ServletResponse res)方法，或者说是GET和POST方法。public void doGet(HttpServletRequest request, HttpServletResponse respose)进行请求响应的业务处理。 Servlet处理完成后，执行chain.doFilter(request, response);执行其他过滤器链条的后置过滤处理，然后执行自己的后置处理。 以上Filter和Servlet的执行顺序有点像Spring AOP 的前置通知和后置通知与业务方法关系。在Filter的doFilter方法中的chain.doFilter(request, response);之前做的业务逻辑就像前置通知，之后的逻辑像后置通知。业务方法是Sevlet中的public void service(ServletRequest req, ServletResponse res)方法。并且可以由多个有序的过滤链条进行Servlet的过滤。 Filter的过滤请求的Servlet的范围与配置有关,Filter在每次访问Servlet时都会拦截过滤。 代码例子： public class MyFilter implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;执行MyFilter init&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;执行MyFilter doFilter&quot;); System.out.println(&quot;执行MyFilter doFilter before&quot;); chain.doFilter(request, response); System.out.println(&quot;执行MyFilter doFilter after&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;执行MyFilter destroy&quot;); &#125;&#125;-------------------------------------------------------------public class MyFilter1 implements Filter &#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(&quot;执行MyFilter1 init&quot;); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;执行MyFilter1 doFilter &quot;); System.out.println(&quot;执行MyFilter1 doFilter before&quot;); chain.doFilter(request, response); System.out.println(&quot;执行MyFilter1 doFilter after&quot;); &#125; @Override public void destroy() &#123; System.out.println(&quot;执行MyFilter1 destroy&quot;); &#125;&#125;------------------------------------------------------------------------public class MyServlet1 extends HttpServlet &#123; private static final long serialVersionUID = 1L; public void init() throws ServletException &#123; System.out.println(&quot;执行Servlet1 init()&quot;); &#125; public void destroy() &#123; System.out.println(&quot;执行Servlet1 destroy()&quot;); &#125; public void doGet(HttpServletRequest request, HttpServletResponse respose) throws ServletException, IOException &#123; System.out.println(&quot;执行Servlet1 service&quot;); &#125;&#125; 省略在web.xml中的配置 输出： 执行MyFilter doFilter执行MyFilter doFilter before执行MyFilter1 doFilter执行MyFilter1 doFilter before执行Servlet service执行MyFilter1 doFilter after执行MyFilter doFilter after]]></content>
      <categories>
        <category>javaweb</category>
      </categories>
      <tags>
        <tag>javaweb</tag>
        <tag>web.xml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软技能-代码之外的生存指南-把自己当做一个企业去思考]]></title>
    <url>%2F2017%2F10%2F09%2F%E8%BD%AF%E6%8A%80%E8%83%BD-%E4%BB%A3%E7%A0%81%E4%B9%8B%E5%A4%96%E7%9A%84%E7%94%9F%E5%AD%98%E6%8C%87%E5%8D%97-%E6%8A%8A%E8%87%AA%E5%B7%B1%E5%BD%93%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BC%81%E4%B8%9A%E5%8E%BB%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[《软技能》—— 把自己当做一个企业去思考]]></content>
      <categories>
        <category>读书系统</category>
      </categories>
      <tags>
        <tag>软技能</tag>
        <tag>代码之外的生存指南</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java到底是值传递还是引用传递]]></title>
    <url>%2F2017%2F09%2F26%2Fjava%E5%88%B0%E5%BA%95%E6%98%AF%E5%80%BC%E4%BC%A0%E9%80%92%E8%BF%98%E6%98%AF%E5%BC%95%E7%94%A8%E4%BC%A0%E9%80%92%2F</url>
    <content type="text"><![CDATA[引言 我们先给本文定下基调，java是值传递 有一种说法，引用传递实际上也就是值传递。这个说法很有意思，实际上这种说法也是有道理的，传递引用，这个引用实际上就是一个地址，也即是一个值。 什么是值传递和引用传递 首先，不要纠结于 Pass By Value 和 Pass By Reference 的字面上的意义，否则很容易陷入所谓的“一切传引用其实本质上是传值”这种并不能解决问题无意义论战中。更何况，要想知道Java到底是传值还是传引用，起码你要先知道传值和传引用含义。 一：搞清楚 基本类型 和 引用类型的不同之处 int num = 10;String str = &quot;hello&quot;; num是基本类型，值就直接保存在变量中。而str是引用类型，变量中保存的只是实际对象的地址。一般称这种变量为”引用”，引用指向实际对象，实际对象中保存着内容。 二：搞清楚赋值运算符（=）的作用 num = 20;str = &quot;java&quot;; 对于基本类型 num ，赋值运算符会直接改变变量的值，原来的值被覆盖掉。对于引用类型 str，赋值运算符会改变引用中所保存的地址，原来的地址被覆盖掉。但是原来的对象不会被改变（重要）。 例子 参数传递基本上就是赋值操作 第一个例子：基本类型void foo(int value) &#123; value = 100;&#125;foo(num); // num 没有被改变第二个例子：没有提供改变自身方法的引用类型void foo(String text) &#123; text = &quot;windows&quot;;&#125;foo(str); // str 也没有被改变第三个例子：提供了改变自身方法的引用类型StringBuilder sb = new StringBuilder(&quot;iphone&quot;);void foo(StringBuilder builder) &#123; builder.append(&quot;4&quot;);&#125;foo(sb); // sb 被改变了，变成了&quot;iphone4&quot;。第四个例子：提供了改变自身方法的引用类型，但是不使用，而是使用赋值运算符。StringBuilder sb = new StringBuilder(&quot;iphone&quot;);void foo(StringBuilder builder) &#123; builder = new StringBuilder(&quot;ipad&quot;);&#125;foo(sb); // sb 没有被改变，还是 &quot;iphone&quot;。 重点理解为什么，第三个例子和第四个例子结果不同？ 例子5 public class Employee &#123; public int age;&#125;public class Main &#123; public static void changeEmployee(Employee employee3) &#123; employee3 = new Employee(); // flag 1 employee3.age = 1000; &#125; public static void main(String[] args) &#123; Employee employee = new Employee(); employee.age = 100; changeEmployee(employee); System.out.println(employee.age); &#125;&#125;输出： 100如果把 flag 1 位置代码注释，那么程序结果输出1000---原因同上 总结 = 号的理解是最重要的，他是一个动词，可能会引起左边变量值的改变 java中方法参数传递方式是按值传递。 如果参数是基本类型，传递的是基本类型的字面量值的拷贝。也就是你我没有半毛钱关系 如果参数是引用类型，传递的是该参量所引用的对象在堆中地址值的拷贝。你我可能存在关系 = 是赋值操作（任何包含=的如+=、-=、 /=等等，都内含了赋值操作）。不再是你以前理解的数学含义了，而+ - /和 = 在java中更不是一个级别，换句话说， = 是一个动作，一个可以改变内存状态的操作，一个可以改变变量的符号，而+ - /却不会。这里的赋值操作其实是包含了两个意思：1、放弃了原有的值或引用；2、得到了 = 右侧变量的值或引用。Java中对 = 的理解很重要啊！！可惜好多人忽略了，或者理解了却没深思过。 对于基本数据类型变量，= 操作是完整地复制了变量的值。换句话说，“=之后，你我已无关联”；至于基本数据类型，就不在这科普了。 对于非基本数据类型变量，= 操作是复制了变量的引用。换句话说，“嘿，= 左侧的变量，你丫别给我瞎动！咱俩现在是一根绳上的蚂蚱，除非你再被 = 一次放弃现有的引用！！上面说了 = 是一个动作，所以我把 = 当作动词用啦！！”。而非基本数据类型变量你基本上可以参数本身是变量 参数传递本质就是一种 = 操作。参数是变量，所有我们对变量的操作、变量能有的行为，参数都有。所以把C语言里参数是传值啊、传指针啊的那套理论全忘掉，参数传递就是 = 操作。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java深入理解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[继承之上溯造型和下溯造型]]></title>
    <url>%2F2017%2F09%2F12%2F%E7%BB%A7%E6%89%BF%E4%B9%8B%E4%B8%8A%E6%BA%AF%E9%80%A0%E5%9E%8B%E5%92%8C%E4%B8%8B%E6%BA%AF%E9%80%A0%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[前言 我们在平时的开发编码中，都会用到上溯造型和下溯造型，只是我们并不知道他的官方叫法而已， 上溯造型跟继承和多态，以及动态绑定的关系很密切 ，关于这几个概念后面会有涉及到他们的概念。 继承和合成 继承：它的本质就是为了使得代码复用（可以基于已经存在的类构造一个新类。继承已经存在的类就可以复用这些类的方法和域。在此基础上，可以添加新的方法和域，从而扩充了类的功能。） 合成：在新类里创建原有的对象称为合成。这种方式可以重复利用现有的代码而不更改它的形式。 -----继承关键字extends表明新类派生于一个已经存在的类。已存在的类称为父类或基类，新类称为子类或派生类。例如:class Dog extends Animal &#123;&#125;类Dog继承了Animal，Animal类称为父类或基类，Dog类称为子类或派生类。---合成合成比较简单，就是在一个类中创建一个已经存在的类。class Dog &#123; Animal animal;&#125; 上溯造型 这个术语缘于继承关系图的传统画法：将基类至于顶部，而向下发展的就是派生类(子类)，发送给父类的消息亦可发给衍生类，父类包含子类。假设把子类赋值给父类，这个过程就称之为上溯造型— 这个时候只能够调用父类父类的方法，子类特有的方法不能够调用，子类变窄 //父类abstract class Animal &#123; public abstract void speak(); public void eat()&#123; &#125; &#125;//子类特有方法interface DoorGod &#123; void guard(); &#125; //Dog 子类和 Cat 子类class Cat extends Animal &#123; @Override public void eat() &#123; try &#123; Thread.sleep( 1000 ); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // super .eat(); System.out.println(&quot;cat eat&quot;); &#125; @Override public void speak() &#123; System.out.println( &quot; 喵喵 &quot; ); &#125; &#125; class Dog extends Animal implements DoorGod&#123; @Override public void speak() &#123; System.out.println( &quot; 汪汪 &quot; ); &#125; public void guard() &#123; while ( true )&#123; System.out.println( &quot; 汪汪 &quot; ); &#125; &#125; &#125; //测试方法public class TestShangSu&#123; public static void upcasting(Animal animal)&#123; animal.speak(); animal.eat(); &#125; @Test public void test1()&#123; Animal dog1 = new Dog(); upcasting(dog1); Animal cat = new Cat(); upcasting(cat); &#125; &#125;//输出 汪汪 喵喵 cat eat 这个时候为什么输出是：子类覆盖父类的方法，而不是父类的方法，这个涉及到动态绑定。后面再讲 由于upcasting(Animal animal)方法的参数是 Animal类型的，因此如果传入的参数是 Animal的子类，传入的参数就会被转换成父类Animal类型，这样你创建的Dog对象能使用的方法只是Animal中的签名方法；也就是说，在上溯的过程中，Dog的接口变窄了，它本身的一些方法（例如实现了 DoorGod的guard方法）就不可见了。如果你想使用Dog中存在而Animal中不存在的方法（比如guard方法），编译时不能通过的。由此可见，上溯造型是安全的类型转换。 如果Dog在上溯造型过程中想使用 DoorGod的guard方法，那么需要配合下溯造型和安全检查，来进行强制转换，讲Animal 下溯为 Dog类型。 注意的是：下溯是不安全的，由父类转化为子类，所以需要加上判断。 下溯造型 将基类转化为衍生类，不安全的操作，可能会引发ClassCastException。 上面的例子只需要加上这一层判断即可 public static void upcasting(Animal animal)&#123; if( animal instanceof Dog )&#123;//下溯造型判断 Dog dog = (Dog) animal; dog.guard(); &#125; animal.speak(); animal.eat(); &#125; 我们在使用注解实现请求方法的登录控制 登录拦截器里面有段关键代码使用的就是下溯造型 为什么使用上溯和下溯造型 上面的例子我们发现，关键的代码是upcasting方法，为什么在调用upcasting方法时要有意忽略调用它的对象类型呢？如果让upcasting方法简单地获取Dog句柄似乎更加直观易懂，但是那样会使衍生自Animal类的每一个新类都要实现专属自己的upcasting方法：例如Cat会实现一个重复的upcasting(Cat cat )这样的方法。 实现多态的好处和代码复利用。 动态绑定 在上面的upcasting方法，测试例子输出的是子类的方法，而非是父类的方法，但是我们使用的是父类去调用这些方法，为什么输出不是父类的呢？ upcasting它接收的是Animal句柄，当执行speak和eat方法时时，它是如何知道Animal句柄指向的是一个Dog对象而不是Cat对象呢？编译器是无从得知的，这涉及到接下来要说明的绑定问题。 Java实现了一种方法调用机制，可在运行期间判断对象的类型，然后调用相应的方法，这种在运行期间进行，以对象的类型为基础的绑定称为动态绑定。除非一个方法被声明为final，Java中的所有方法都是动态绑定的。 静态方法的绑定 他跟普通的方法不同，子类和父类方法都是静态的，子类如果去掉父类编译会错误 package Test;class Person &#123; static void eat() &#123; System.out.println(&quot;Person.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Person.speak()&quot;); &#125;&#125;class Boy extends Person &#123; static void eat() &#123; System.out.println(&quot;Boy.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Boy.speak()&quot;); &#125;&#125;class Girl extends Person &#123; static void eat() &#123; System.out.println(&quot;Girl.eat()&quot;); &#125; static void speak() &#123; System.out.println(&quot;Girl.speak()&quot;); &#125;&#125;public class Persons &#123; public static Person randPerson() &#123; switch ((int)(Math.random() * 2)) &#123; default: case 0: return new Boy(); case 1: return new Girl(); &#125; &#125; public static void main(String[] args) &#123; Person[] p = new Person[4]; for (int i = 0; i &lt; p.length; i++) &#123; p[i] = randPerson(); // 随机生成Boy或Girl &#125; for (int i = 0; i &lt; p.length; i++) &#123; p[i].eat(); &#125; &#125;&#125;//输出Person.eat()Person.eat()Person.eat()Person.eat() 对于静态方法而言，不管父类引用指向的什么子类对象，调用的都是父类的方法。 总结 上溯造型和动态绑定实际上就是多态的体现，下溯造型是为了解决因为上溯而导致衍生类功能变小的问题，继承则是上溯和下溯以及动态编译的基础。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>继承</tag>
        <tag>多态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注解实现请求方法的登录控制]]></title>
    <url>%2F2017%2F09%2F06%2F%E6%B3%A8%E8%A7%A3%E5%AE%9E%E7%8E%B0%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95%E7%9A%84%E7%99%BB%E5%BD%95%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[前言 之前一直使用的是，拦截器来统一验证当前用户是否登录，通过验证cookie或者session里面的是否存在已经登录标识来完成登录逻辑判断。但是会发现，这个很麻烦，而且有很多配置需要配置，例如免验证URL等等配置，无法实现可拔插式方法级别的控制。 public class RequestInterceptor extends HandlerInterceptorAdapter &#123; public String[] allowUrls;//配置不拦截的资源，所以在代码里面来排除. public void setAllowUrls(String[] allowUrls) &#123; this.allowUrls = allowUrls; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; // TODO Auto-generated method stub &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // TODO Auto-generated method stub request.setCharacterEncoding(&quot;UTF8&quot;); HttpSession session=request.getSession();//获取登录的SESSION String sessionid=request.getSession().getId();//获取登录的SESSIONID String requestPath=request.getServletPath();//获取客户请求页面 //先过滤掉不需要判断SESSION的请求 for(String url : allowUrls) &#123; if(requestPath.contains(url)) &#123; return true; &#125; &#125; Object attribute = request.getSession().getAttribute(&quot;sys_user&quot;); if( attribute == null )&#123; response.sendRedirect(&quot;/index.jsp&quot;); &#125; return true; &#125; 大体上是这样的，通过allowUrls来控制免登录url（上面的代码其实可以使用配置文件的方式来配置allowUrls的值，可以不通过setAllowUrls的方式来赋值，但是为了方面扩展就加入了。） 这里会面临一个问题，那就是如果网站网页多的话，那么allowUrls的值会变得很庞大，可能会缺漏。所以下面讲解本人用到的解决方式—-注解 和 spring配置方式（跟数组形式没有什么区别） spring 配置方式path 对所有的请求拦截使用/**，对某个模块下的请求拦截使用：/myPath/*&lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot; /&gt; &lt;bean class=&quot;com.kingge.oa.user.LoginInterceptor&quot; /&gt;&lt;/mvc:interceptor&gt; 或者 &lt;!-- 拦截是否登录 &lt;mvc:interceptor&gt; 需拦截的地址 二级目录 &lt;mvc:mapping path=&quot;/*/*&quot;/&gt; &lt;bean class=&quot;com.jk.ssm.interceptor.RequestInterceptor&quot; &gt; &lt;property name=&quot;allowUrls&quot;&gt; //回去调用拦截器的 setAllowUrls 方法 &lt;list&gt; 如果请求中包含以下路径，则不进行拦截 &lt;value&gt;/account/login.html&lt;/value&gt; &lt;value&gt;/captcha/image.html&lt;/value&gt; &lt;value&gt;/register/register.html&lt;/value&gt; &lt;value&gt;/error/400.html&lt;/value&gt; &lt;value&gt;/error/404.html&lt;/value&gt; &lt;value&gt;/error/500.html&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/mvc:interceptor&gt; 使用注解关于注解 官方说辞：JDK5开始，java增加了对元数据(MetaData)的支持，怎么支持？答：通过Annotation(注解）来实现。Annotation提供了为程序元素设置元数据的方法。元数据：描述数据的数据。 个人理解：首先什么是元数据，元数据就是对一类事物的统称，他不仅限于某个事物的描述。例如我们有ABC三个系统，分别使用oracle，mysql，db2，都有登录功能，他们的用户表字段名称是不一样的。那么有个需求，我想把A系统的用户数据pour到B系统中，那么进行映射操作？这个时候就需要一个描述用户数据的一个统一标识（元数据）这样我们就可以先把，A系统数据映射到元数据，然后再从元数据取数据映射到B系统中。 粗俗的理解，元数据就是一个类的属性，但是他所具备的职能的而应用范围，跟真正意义上类的属性数不一样的。传统的类的属性他只描述这个类，元数据可以描述多个具有共性的类。 再举个例子，我们现在常用的数据中心（DC）就是使用了元数据来作为数据传输的媒介。 元数据作用：：Annotation就像代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取。读取到了程序元素的元数据，就可以执行相应的处理。通过注解，程序开发人员可以在不改变原有逻辑的情况下，在源代码文件中嵌入一些补充信息。代码分析工具、开发工具和部署工具可以通过解析这些注解获取到这些补充信息，从而进行验证或者进行部署等。 到java8为止一共提供了五个 注解 unchecked异常：运行时异常。是RuntimeException的子类，不需要在代码中显式地捕获unchecked异常做处理。Java异常 @SafeVarargs (java7新增）：java7的“堆污染”警告与@SafeVarargs堆污染：把一个不带泛型的对象赋给一个带泛型的变量是，就会发生堆污染。例如：下面代码引起堆污染，会给出警告List l2 = new ArrayList&lt;Number&gt;();List&lt;String&gt; ls = l2;3中方式去掉这个警告 3种方式去掉这个警告：使用注解@SafeVarargs修饰引发该警告的方法或构造器。使用@SuppressWarnings(“unchecked”) 修饰。使用编译器参数命令：-Xlint:varargs @Functionlnterface （java8新增）：修饰函数式接口使用该注解修饰的接口必须是函数式接口，不然编译会出错。那么什么是函数式接口？答：如果接口中只有一个抽象方法（可以包含多个默认方法或static方法），就是函数式接口。 五个基本元注解 元注解：描述注解的注解（概念跟元数据类似）。 java提供了6个元注解（Meta Annotation)，在java.lang.annotation中。其中5个用于修饰其他的Annonation定义。而@Repeatable专门用于定义Java8新增的重复注解。所以要定义注解必须使用到5个元注解来定义( 五个注解用法 详情百度 ) @Inherited @Documented @Retention（英文：保留） @Target ( 目标) 自定义注解 参见下面，例子或者白度，具体就不阐述了。 使用注解解决登录问题定义一个枚举类 作用： 是否进行验证权限（因为后期可能会增加权限判断注解，而且是否登录也可以说是权限判断的一种，所以这里的枚举类的作用就是保存是否进行权限判断信息） public enum Action&#123; Normal(&quot;0&quot;,&quot;执行权限验证&quot;), Skip(&quot;1&quot;, &quot;跳过权限验证&quot;); private final String key; private final String desc; private Action(String key, String desc) &#123; this.key = key; this.desc = desc; &#125; //省略get set方法 定义登录和权限注解 Login属性是action ，属性类型是Action（上面的枚举类） @Target(ElementType.METHOD)@Documented@Retention(RetentionPolicy.RUNTIME)public @interface Login&#123; Action action() default Action.Normal;&#125; @Target(&#123;java.lang.annotation.ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Permission&#123; String value() default &quot;&quot;; // 这里我是保存一个权限代码，例如赋值为4000，表示当前用户的必须具备4000的权限才能够访问方法 Action action() default Action.Normal;&#125; 拦截器public class LoginInterceptor extends HandlerInterceptorAdapter&#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception&#123; if(handler instanceof HandlerMethod)&#123; //是否为请求方法 HandlerMethod handlerMethod = (HandlerMethod) handler; Login login = handlerMethod.getMethodAnnotation(Login.class);//当前请求方法是否添加了Login注解 if( login != null &amp;&amp; &quot;0&quot;.equals(login.action().getKey()) )&#123;//判断属性的值是否是0-表示需要进行登录验证 Object attribute = request.getSession().getAttribute(&quot;sys_user&quot;); if( attribute == null )&#123; response.sendRedirect(&quot;/index.jsp&quot;); &#125; &#125; return true; &#125; return true; &#125;&#125; 在spring中配置拦截器&lt;mvc:interceptors&gt;&lt;bean class=&quot;com.kingge.oa.user.LoginInterceptor&quot;&gt;&lt;/bean&gt;&lt;/mvc:interceptors&gt; 给请求方法添加权限控制 @Login(action=Action.Skip) //不需要进行登录校验 @Permission(value=&quot;4000&quot;,action=Action.normal)//需要进行权限号为4000的权限校验 @RequestMapping(&quot;/list&quot;) public String list(Model model,HttpServletRequest request) &#123; request.getSession().setAttribute(&quot;sys_user&quot;, &quot;denglule&quot;); List&lt;User&gt; userList = userService.findAllObjects(); System.out.println( userList ); model.addAttribute(&quot;userList&quot;,userList ); return &quot;list&quot;; &#125; @Login(action=Action.Normal)//添加操作，需要校验是否登录 @RequestMapping(value=&quot;/add&quot;, method=RequestMethod.POST) public String add( User user ) &#123; System.out.println( user ); userService.insert(user); return &quot;forward:/user/list&quot;; &#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java注解</tag>
        <tag>登录控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8新特性]]></title>
    <url>%2F2017%2F08%2F29%2Fjava8%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Java 8可谓是自Java 5以来最具革命性的版本了，她在语言、编译器、类库、开发工具以及Java虚拟机等方面都带来了不少新特性。我们来一一回顾一下这些特性。 一、Lambda表达式 Lambda表达式可以说是Java 8最大的卖点，她将函数式编程引入了Java。Lambda允许把函数作为一个方法的参数，或者把代码看成数据。Lambda 是一个匿名函数。 一个Lambda表达式可以由用逗号分隔的参数列表、–&gt;符号与函数体三部分表示。例如： 例子1 需求： 比较TreeSet中数据，按小到大输出 使用匿名内部类实现一个排序功能 //采用匿名内部类的方式-实现比较器 Comparator&lt;Integer&gt; comparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return Integer.compare(o1, o2);//关键代码 &#125; &#125;;//传入比较器 TreeSet&lt;Integer&gt; tree2 = new TreeSet&lt;&gt;(comparator ); tree2.add(12); tree2.add(-12); tree2.add(100);System.out.println(tree2) //输出 -12 12 100 我们不难发现上面的代码存在一个问题：其实关键代码只有第七行，其他代码都是冗余的 使用Lambda表达式实现同样功能 //使用Lambda表达式，抽取关键代码，减少代码量Comparator&lt;Integer&gt; comparator2 = (x, y) -&gt; Integer.compare(x, y); //关键代码 TreeSet&lt;Integer&gt; tree = new TreeSet&lt;&gt;(comparator2 ); tree.add(12); tree.add(-12); tree.add(100); tree.forEach(System.out::println);//代替System.out.println 代码瞬间就变得很简短，你可能觉得这个有什么，没什么感觉。那么我们在进入第二个例子 例子2 需求：1.获取公司中年龄小于 35 的员工信息2.获取公司中工资大于 5000 的员工信息。。。。。。 前期准备实现一个Employee类,有四个属性 private int id;private String name;private int age;private double salary;忽略get/set方法和构造器 初始化一个List： List&lt;Employee&gt; emps = Arrays.asList( new Employee(101, &quot;张三&quot;, 18, 9999.99), new Employee(102, &quot;李四&quot;, 59, 6666.66), new Employee(103, &quot;王五&quot;, 28, 3333.33), new Employee(104, &quot;赵六&quot;, 8, 7777.77), new Employee(105, &quot;田七&quot;, 38, 5555.55)); 常规方法实现实现两个方法，然后传入需要过滤的源数据，返回过滤后的结果集 //需求：获取公司中年龄小于 35 的员工信息public List&lt;Employee&gt; filterEmployeeAge(List&lt;Employee&gt; emps)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee emp : emps) &#123; if(emp.getAge() &lt;= 35)&#123;//比较代码 list.add(emp); &#125; &#125; return list;&#125;//需求：获取公司中工资大于 5000 的员工信息public List&lt;Employee&gt; filterEmployeeSalary(List&lt;Employee&gt; emps)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee emp : emps) &#123; if(emp.getSalary() &gt;= 5000)&#123;//比较代码 list.add(emp); &#125; &#125; return list;&#125; 我们不难发现上面的代码存在一个问题：那就是两个方法除了比较部分不同，其他逻辑是一样的，存在大量冗余，假设有新的需求（例如求得求得名字姓王的员工）那么就需要再创建一个 filterEmployee**方法对应新的需求。 使用策略设计模式实现 提供父借口 和 两个 实现类（两个需求对应的逻辑实现类） // 父接口 @FunctionalInterfacepublic interface MyPredicate&lt;T&gt; &#123; public boolean test(T t); &#125;//需求1 实现类-年龄小于35public class FilterEmployeeForAge implements MyPredicate&lt;Employee&gt;&#123; @Override public boolean test(Employee t) &#123; return t.getAge() &lt;= 35; &#125;&#125;//需求1 实现类-工资大于5000public class FilterEmployeeForSalary implements MyPredicate&lt;Employee&gt; &#123; @Override public boolean test(Employee t) &#123; return t.getSalary() &gt;= 5000; &#125;&#125; 测试代码 // 通用过滤方法 public List&lt;Employee&gt; filterEmployee(List&lt;Employee&gt; emps, MyPredicate&lt;Employee&gt; mp)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee employee : emps) &#123; if(mp.test(employee))&#123; list.add(employee); &#125; &#125; return list; &#125; @Test public void test4()&#123; //传入实现年龄过滤的实现类 List&lt;Employee&gt; list = filterEmployee(emps, new FilterEmployeeForAge()); for (Employee employee : list) &#123; System.out.println(employee); &#125; System.out.println(&quot;------------------------------------------&quot;); List&lt;Employee&gt; list2 = filterEmployee(emps, new FilterEmployeeForSalary()); for (Employee employee : list2) &#123; System.out.println(employee); &#125; &#125; 使用策略模式比上一个的好处是：代码很清晰，便于维护，新的需求我们只需要再实现对应的需求实现类即可，然后传入MyPredicate```接口即可。缺点是：需要实现对应的需求类然后实现``` MyPredicate&lt;T&gt;```接口### **匿名内部类**这种方法类似于例子1中的 Comparator这个接口的实现```JAVA//直接使用 MyPredicate&lt;Employee&gt;接口，不去实现对应的需求类（上面的FilterEmployeeForSalary 和 FilterEmployeeForAge ） @Test public void test5()&#123; List&lt;Employee&gt; list = filterEmployee(emps, new MyPredicate&lt;Employee&gt;() &#123; @Override public boolean test(Employee t) &#123; return t.getId() &lt;= 103; &#125; &#125;); for (Employee employee : list) &#123; System.out.println(employee); &#125; &#125; 我们不难发现上面的代码存在一个问题：跟例子1一样，存在大量的冗余。 Lambda 表达式实现前期准备public List&lt;Employee&gt; filterEmployee(List&lt;Employee&gt; emps, MyPredicate&lt;Employee&gt; mp)&#123; List&lt;Employee&gt; list = new ArrayList&lt;&gt;(); for (Employee employee : emps) &#123; if(mp.test(employee))&#123; list.add(employee); &#125; &#125; return list;&#125; @Testpublic void test6()&#123; List&lt;Employee&gt; list = filterEmployee(emps, (e) -&gt; e.getAge() &lt;= 35); list.forEach(System.out::println); System.out.println("------------------------------------------"); List&lt;Employee&gt; list2 = filterEmployee(emps, (e) -&gt; e.getSalary() &gt;= 5000); list2.forEach(System.out::println);&#125; 我们不难发现上面的代码存在一个问题：这个代码，是不是已经非常简短了，感觉已经是终极的最简代码。但是实际上还有更简短的代码（使用stream api）缺点：太过依赖 MyPredicate 这个接口，假设这个接口不存在，该怎么办呢？（我们这里仅仅是做个假设） 终极实现方式：Stream API@Testpublic void test7()&#123; emps.stream() .filter((e) -&gt; e.getAge() &lt;= 35) .forEach(System.out::println); System.out.println(&quot;----------------------------------------------&quot;); emps.stream() .filter((e) -&gt; e.getSalary() &gt;= 5000) .forEach(System.out::println); System.out.println(&quot;----------------------------------------------&quot;); // 可以使用map 指定输出那个属性的值，代替普通的便利输出 emps.stream() .map(Employee::getName) .limit(3)// 输出前三个 .sorted()//排序 .forEach(System.out::println); &#125; 输出 Employee [id=101, name=张三, age=18, salary=9999.99]Employee [id=103, name=王五, age=28, salary=3333.33]Employee [id=104, name=赵六, age=8, salary=7777.77]----------------------------------------------Employee [id=101, name=张三, age=18, salary=9999.99]Employee [id=102, name=李四, age=59, salary=6666.66]Employee [id=104, name=赵六, age=8, salary=7777.77]Employee [id=105, name=田七, age=38, salary=5555.55]----------------------------------------------张三李四王五 我们不难发现上面的代码存在一个问题：这个代码，是非常潇洒，舒服的，不依赖我们上面所说的接口。 函数式接口 为了使现有函数更好的支持Lambda表达式，Java 8引入了函数式接口的概念。函数式接口就是只有一个方法的普通接口。java.lang.Runnable与java.util.concurrent.Callable是函数式接口最典型的例子。为此，Java 8增加了一种特殊的注解@FunctionalInterface： –也就是说：这个接口里面只能够存在一个接口方法，多个就会报错 例子：@FunctionalInterfacepublic interface Functional &#123; void method();&#125; 认识Lambda表达式概念 一、Lambda 表达式的基础语法：Java8中引入了一个新的操作符 “-&gt;” 该操作符称为箭头操作符或 Lambda 操作符 箭头操作符将 Lambda 表达式拆分成两部分： 左侧：Lambda 表达式的参数列表 右侧：Lambda 表达式中所需执行的功能， 即 Lambda 体 上面的例子：List list = filterEmployee(emps, (e) -&gt; e.getAge() &lt;= 35); 第二个参数他会去找 MyPredicate&lt;T&gt; 接口里面的 public boolean test(T t);test方法，lambda表达式左边的(e) 对应的是test方法的入参, ambda表达式右边的e.getAge() &lt;= 35 对应得是test方法的实现 那么你可能会有疑问，假设MyPredicate接口里面有很多个接口方法，那么他会去调用那个呢？他怎么知道去找test方法呢？ 引入了：@FunctionalInterface这个函数式接口的概念，解决了这个问题。 * 语法格式一：无参数，无返回值 * () -&gt; System.out.println(&quot;Hello Lambda!&quot;); &gt; 例如 Runnable接口的 run方法就是无参数无返回值： @Test public void test1()&#123; int num = 0;//jdk 1.7 前，我们知道匿名内部引用局部变量必须声明为final //但jdk1.8，它默认给我们添加了final，不用显示声明。 Runnable r = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Hello World!&quot; + num); //这里如果改为 num++是会报错的，因为他本质上是一个final &#125; &#125;; r.run(); System.out.println(&quot;-------------------------------&quot;); Runnable r1 = () -&gt; System.out.println(&quot;Hello Lambda!&quot;); r1.run(); &#125;这两个是等效的 * * 语法格式二：有一个参数，并且无返回值* (x) -&gt; System.out.println(x)* 例子：Consumer这个类jdk自带--有参数无返回值@Testpublic void test2()&#123; Consumer&lt;String&gt; con = x -&gt; System.out.println(x); con.accept(&quot;我是你泽精哥！&quot;);&#125; * 语法格式三：若只有一个参数，小括号可以省略不写* x -&gt; System.out.println(x)* * 语法格式四：有两个以上的参数，有返回值，并且 Lambda 体中有多条语句* Comparator&lt;Integer&gt; com = (x, y) -&gt; &#123;* System.out.println(&quot;函数式接口&quot;);* return Integer.compare(x, y);* &#125;; * 语法格式五：若 Lambda 体中只有一条语句， return 和 大括号都可以省略不写* Comparator&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x, y);* * 语法格式六：Lambda 表达式的参数列表的数据类型可以省略不写，因为JVM编译器通过上下文推断出，数据类型，即“类型推断”* (Integer x, Integer y) -&gt; Integer.compare(x, y); 类型推断 : jdk1.8后，添加了这个功能String[] strs = {“aaa”, “bbb”, “ccc”} ; 它自动会转换里面的数据为String类型的数据改为： String[] strs;strs = &#123;&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;&#125;;//会报错--因为这样无法进行类型推断 类型推断例子2 public void show(Map&lt;String, Integer&gt; map)&#123;&#125;//方法 show(new HashMap&lt;&gt;());//调用方法我们发现在调用方法的时候入参我们并没有明确声明类型，但是在jdk1.8中是可以编译通过的。这里也是运用了类型推断（注意：jdk1.7中编译会失败） 热身例子一 //函数是接口@FunctionalInterfacepublic interface MyFun &#123; public Integer getValue(Integer num);&#125;//测试 //需求：对一个数进行运算 @Test public void test6()&#123; Integer num = operation(100, (x) -&gt; x * x); System.out.println(num); System.out.println(operation(200, (y) -&gt; y + 200)); &#125; public Integer operation(Integer num, MyFun mf)&#123; return mf.getValue(num); &#125; 热身例子二//函数接口 @FunctionalInterface //约束当前接口只能有一个方法public interface CalcLong&lt;K,T&gt;&#123; // public K getMultiply(T t, T tt); K getMultiply(T t, T tt);&#125;//需求：求得两个数的和 String result = getMuyl(10L,10L,(e,ee)-&gt;&#123; System.out.println(e+ &quot; &quot; + ee); return e+ee+&quot;&quot;; &#125;); System.out.println(result); public String getMuyl(Long l,Long ll,CalcLong&lt;String,Long&gt; mf)&#123; return mf.getMultiply(l, ll); &#125; 看到这里可能会有疑惑？我靠，使用lambda表达式还得声明一个函数接口，这么麻烦。实际上，java内部已经帮我们实现了很多个接口供我们使用，不需要重新自己定义，除非有特别操作。 java8内置四大函数式接口 为了解决接口需要自定义问题 /* * Java8 内置的四大核心函数式接口 * * Consumer&lt;T&gt; : 消费型接口 * void accept(T t); * * Supplier&lt;T&gt; : 供给型接口 * T get(); * * Function&lt;T, R&gt; : 函数型接口 * R apply(T t); * * Predicate&lt;T&gt; : 断言型接口 * boolean test(T t); * */ 例子消费型接口//Consumer&lt;T&gt; 消费型接口 :@Testpublic void test1()&#123; String p; happy(10000, (m) -&gt; System.out.println(&quot;桑拿，每次消费：&quot; + m + &quot;元&quot;));&#125; public void happy(double money, Consumer&lt;Double&gt; con)&#123; con.accept(money);&#125; Supplier 供给型接口 //Supplier&lt;T&gt; 供给型接口 :@Testpublic void test2()&#123; List&lt;Integer&gt; numList = getNumList(10, () -&gt; (int)(Math.random() * 100)); for (Integer num : numList) &#123; System.out.println(num); &#125;&#125;//需求：产生指定个数的整数，并放入集合中public List&lt;Integer&gt; getNumList(int num, Supplier&lt;Integer&gt; sup)&#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; num; i++) &#123; Integer n = sup.get(); list.add(n); &#125; return list;&#125; Function 函数型接口 //Function&lt;T, R&gt; 函数型接口：@Testpublic void test3()&#123; String newStr = strHandler(&quot;\t\t\t 去除前后空格 &quot;, (str) -&gt; str.trim()); System.out.println(newStr); String subStr = strHandler(&quot;截取字符串你知不知道&quot;, (str) -&gt; str.substring(2, 5)); System.out.println(subStr);&#125;//需求：用于处理字符串public String strHandler(String str, Function&lt;String, String&gt; fun)&#123; return fun.apply(str);&#125; Predicate 断言型接口 //Predicate&lt;T&gt; 断言型接口：@Testpublic void test4()&#123; List&lt;String&gt; list = Arrays.asList(&quot;Hello&quot;, &quot;atguigu&quot;, &quot;Lambda&quot;, &quot;www&quot;, &quot;ok&quot;); List&lt;String&gt; strList = filterStr(list, (s) -&gt; s.length() &gt; 3); for (String str : strList) &#123; System.out.println(str); &#125;&#125;//需求：将满足条件的字符串，放入集合中public List&lt;String&gt; filterStr(List&lt;String&gt; list, Predicate&lt;String&gt; pre)&#123; List&lt;String&gt; strList = new ArrayList&lt;&gt;(); for (String str : list) &#123; if(pre.test(str))&#123; strList.add(str); &#125; &#125; return strList;&#125; 四大内置函数衍生的子函数 二、接口的默认方法与静态方法 我们可以在接口中定义默认方法，使用default关键字，并提供默认的实现。所有实现这个接口的类都会接受默认方法的实现，除非子类提供的自己的实现。例如：public interface DefaultFunctionInterface &#123; default String defaultFunction() &#123; return &quot;default function&quot;; &#125;&#125; 我们还可以在接口中定义静态方法，使用static关键字，也可以提供实现。例如：public interface StaticFunctionInterface &#123; static String staticFunction() &#123; return &quot;static function&quot;; &#125;&#125; 接口的默认方法和静态方法的引入，其实可以认为引入了C＋＋中抽象类的理念，以后我们再也不用在每个实现类中都写重复的代码了。 三、方法引用 通常与Lambda表达式联合使用，可以直接引用已有Java类或对象的方法。一般有四种不同的方法引用： 构造器引用 构造器引用。语法是Class::new，构造器的参数列表，需要与函数式接口中参数列表保持一致！也就是说，决定Class::new调用那一个构造器得是：接口函数的方法的参数 //构造器引用@Testpublic void test7()&#123; // Supplier 的接口方法 T get(); --所以调用无参构造器 Supplier&lt;Employee&gt; fun0 = Employee::new; //Function 的接口方法 R apply(T t);-调用一个参数构造器 Function&lt;String, Employee&gt; fun = Employee::new; //BiFunction 的接口方法 R apply(T t, U u); -调用二参构造器 BiFunction&lt;String, Integer, Employee&gt; fun2 = Employee::new;&#125; 对象静态方法引用（类名::静态方法） 静态方法引用。语法是Class::static_method，要求接受一个Class类型的参数； //类名 :: 静态方法名//max和compare 都是静态方法@Testpublic void test4()&#123; Comparator&lt;Integer&gt; com = (x, y) -&gt; Integer.compare(x, y); System.out.println(&quot;-------------------------------------&quot;); Comparator&lt;Integer&gt; com2 = Integer::compare; BiFunction&lt;Double, Double, Double&gt; fun = (x, y) -&gt; Math.max(x, y); System.out.println(fun.apply(1.5, 22.2)); System.out.println(&quot;------------------------------------&quot;); BiFunction&lt;Double, Double, Double&gt; fun2 = Math::max; System.out.println(fun2.apply(1.2, 1.5));&#125; 对象实例方法引用（对象引用::实例方法名） 特定类的任意对象方法引用。它的语法是Class::method。要求方法是没有参数的； //对象的引用 :: 实例方法名@Testpublic void test2()&#123; Employee emp = new Employee(101, &quot;张三&quot;, 18, 9999.99); Supplier&lt;String&gt; sup = () -&gt; emp.getName(); System.out.println(sup.get()); System.out.println(&quot;----------------------------------&quot;); Supplier&lt;String&gt; sup2 = emp::getName; System.out.println(sup2.get());&#125; 类名实例方法引用(类名::实例方法名) 我们知道一般是有对象才能够引用实例方法，但是有种特殊情况是可以直接使用类名引用实例方法若Lambda 的参数列表的第一个参数，是实例方法的调用者，第二个参数(或无参)是实例方法的参数时，格式： ClassName::MethodName //类名 :: 实例方法名//按照常规是String st = new String(&quot;123&quot;); st::equals,//对象调用实例方法，但是下面因为符合第四种引用的规则，//所以可以使用类名调用实例方法@Testpublic void test5()&#123;//第一个参数为实例方法调用者，第二个参数为为实例方法参数 BiPredicate&lt;String, String&gt; bp = (x, y) -&gt; x.equals(y); System.out.println(bp.test(&quot;abcde&quot;, &quot;abcde&quot;)); System.out.println(&quot;-------------------------------------&quot;); BiPredicate&lt;String, String&gt; bp2 = String::equals;System.out.println(bp2.test(&quot;abc&quot;, &quot;abc&quot;)); System.out.println(&quot;---------------------------------------&quot;); //第一个参数为实例方法调用者，第二个参数为空Function&lt;Employee, String&gt; fun = (e) -&gt; e.show();System.out.println(fun.apply(new Employee()));System.out.println(&quot;--------------------------------------&quot;); Function&lt;Employee, String&gt; fun2 = Employee::show; System.out.println(fun2.apply(new Employee())); &#125; 注意： ①方法体所引用的方法的参数列表与返回值类型，需要与函数式接口中抽象方法的参数列表和返回值类型保持一致！ ②若Lambda 的参数列表的第一个参数，是实例方法的调用者，第二个参数(或无参)是实例方法的参数时，格式：ClassName::MethodName (针对于第四种方法引用) 数组引用（类型[] :: new）//数组引用@Testpublic void test8()&#123; Function&lt;Integer, String[]&gt; fun = (args) -&gt; new String[args]; String[] strs = fun.apply(10); System.out.println(strs.length); System.out.println(&quot;--------------------------&quot;); Function&lt;Integer, Employee[]&gt; fun2 = Employee[] :: new; Employee[] emps = fun2.apply(20); System.out.println(emps.length);&#125; 四、重复注解在Java 5中使用注解有一个限制，即相同的注解在同一位置只能声明一次。Java 8引入重复注解，这样相同的注解在同一地方也可以声明多次。重复注解机制本身需要用@Repeatable注解。Java 8在编译器层做了优化，相同注解会以集合的方式保存，因此底层的原理并没有变化。 五、扩展注解的支持Java 8扩展了注解的上下文，几乎可以为任何东西添加注解，包括局部变量、泛型类、父类与接口的实现，连方法的异常也能添加注解。 六、OptionalJava 8引入Optional类来防止空指针异常，Optional类最先是由Google的Guava项目引入的。Optional类实际上是个容器：它可以保存类型T的值，或者保存null。使用Optional类我们就不用显式进行空指针检查了。 七、Stream前言 Stream API是把真正的函数式编程风格引入到Java中。其实简单来说可以把Stream理解为MapReduce，当然Google的MapReduce的灵感也是来自函数式编程。她其实是一连串支持连续、并行聚集操作的元素。从语法上看，也很像linux的管道、或者链式编程，代码写起来简洁明了，非常酷帅！ 八、Date/Time API (JSR 310)Java 8新的Date-Time API (JSR 310)受Joda-Time的影响，提供了新的java.time包，可以用来替代 java.util.Date和java.util.Calendar。一般会用到Clock、LocaleDate、LocalTime、LocaleDateTime、ZonedDateTime、Duration这些类，对于时间日期的改进还是非常不错的。 九、JavaScript引擎NashornNashorn允许在JVM上开发运行JavaScript应用，允许Java与JavaScript相互调用。 十、Base64在Java 8中，Base64编码成为了Java类库的标准。Base64类同时还提供了对URL、MIME友好的编码器与解码器。 除了这十大新特性之外，还有另外的一些新特性： 更好的类型推测机制：Java 8在类型推测方面有了很大的提高，这就使代码更整洁，不需要太多的强制类型转换了。 编译器优化：Java 8将方法的参数名加入了字节码中，这样在运行时通过反射就能获取到参数名，只需要在编译时使用-parameters参数。 并行（parallel）数组：支持对数组进行并行处理，主要是parallelSort()方法，它可以在多核机器上极大提高数组排序的速度。 并发（Concurrency）：在新增Stream机制与Lambda的基础之上，加入了一些新方法来支持聚集操作。 Nashorn引擎jjs：基于Nashorn引擎的命令行工具。它接受一些JavaScript源代码为参数，并且执行这些源代码。 类依赖分析器jdeps：可以显示Java类的包级别或类级别的依赖。 JVM的PermGen空间被移除：取代它的是Metaspace（JEP 122），元空间直接采用的是物理空间，也即是我们电脑的内存，电脑内存多大，元空间就有多大。]]></content>
      <categories>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
        <tag>java8新特性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[哈希表冲突解决方式之开放地址法和链地址法]]></title>
    <url>%2F2017%2F08%2F29%2F%E5%93%88%E5%B8%8C%E8%A1%A8%E5%86%B2%E7%AA%81%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F%E4%B9%8B%E5%BC%80%E6%94%BE%E5%9C%B0%E5%9D%80%E6%B3%95%E5%92%8C%E9%93%BE%E5%9C%B0%E5%9D%80%E6%B3%95%2F</url>
    <content type="text"><![CDATA[基本定义 散列技术是在记录的存储位置和它的关键字之间建立一个确定的对应关系 f，使得每个关键字key对应一个存储位置f(key)。 这种对应关系f称为散列或哈希函数 采用上述思想将数据存储在一块连续的存储空间中，这块连续的存储空间称为散列或哈希表 关键字对应的存储位置称为散列地址 如果碰到两个不同的关键字key1≠key2，但却有相同的f(key1)=f(key2)，这种现象称为冲突， 并把key1和key2 称为这个散列函数的同义词（synonym） 散列函数构造方法好的散列函数参考如下两个原则： 计算简单 散列地址分布均匀 最常用的方法是除留余数法，对于散列表长度为m的散列函数是 f(key)=key mod p (p≤m) 处理散列冲突 开放地址法 开放地址法就是一旦发生冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列表总能找到，并存入。开放地址法又分为线性探测法，二次探测法和随机探测法。 链地址法 将所有同义词的关键字存储在同一个单链表中，称这个单链表为同义词子表，在散列表中只存储同义词子表的头指针。只要有冲突，就在同义词的子表中增加结点。(java中的HashMap就是采用这种方法) 开放地址法]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数据结构</tag>
        <tag>哈希表</tag>
        <tag>哈希解决冲突</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8之Hashmap]]></title>
    <url>%2F2017%2F08%2F28%2Fjava8%E4%B9%8BHashmap%2F</url>
    <content type="text"><![CDATA[Java8-HashMap变化 数据的存储结构从：数组+链表 演变为了 数组+链表+红黑树 Map 家庭族谱 HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 总结对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 HashMap简介 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 我们知道HashMap的数据存储结构就是：数组加上链表。通过对于key的值做hash运算，获得对应的值找到对应的数组下标，然后再存储值。存储值的过程中可能当前数组已经存在值（这个称之为冲突） 然后再生成一个链表存储冲突的值。 HashCode() 和 Hash() 方法实现得足够好，能够尽可能地减少冲突的产生，那么对 HashMap 的操作几乎等价于对数组的随机访问操作，具有很好的性能。但是，如果 HashCode() 或者 Hash() 方法实现较差，在大量冲突产生的情况下，HashMap 事实上就退化为几个链表，对 HashMap 的操作等价于遍历链表，此时性能很差。 解决冲突的方法：开放地址法和链地址法 HashMap特点 允许null为key 输出无序 如果想要输出有序，那以使用继承他的LinkedHashMap，元素输出顺序跟输入顺序一致,他提供了一个节点保存输入的元素的顺序。想要对元素的值进行排序 推荐TreeMap（因为他继承了SortedMap) 非线程安全 数组+链表存储方式 Java8特性 HashMap是数组+链表+红黑树 存储算法： map.put(&quot;kingge&quot;,&quot;shuai&quot;)系统将调用kingge”这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 好的hash算法和扩容机制是解决冲突和高效存取的命题 HashMap 重要的几个属性int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; Node[] table(Hash桶)始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 分析HashMap的put方法put方法图解，详情可以去看源码 public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); &#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 为什么说HashMap是线程不安全的 个人觉得有两个表现，如果还有其他的希望大家补充，或者以后等楼主源码研究透了再补充 表现一 我们知道当插入数据超过了threshold(threshold=length * Load factor),那么就会扩容，扩容会去调用resize和transfer方法，这个时候原先hash桶里面的所有数据都会重新计算，对应的位置–称之为rehash，这个成本很大 最根本的原因是出现时死循环-也就是在死锁问题出现在了transfer方法上面,而且是因为在扩容转换的过程中采用的是链表的头插法的形式进行插入数据。例如原来在数组arr[0]的位置又链表1–&gt;2–&gt;3 那么扩容后，采用头插法就变成了arr[0]：3–&gt;2–&gt;1 为什么采用头插法，因为时间复杂度为O(1)，想象一下尾插法，那么需要遍历找到最尾元素然后插入时间复杂度是O(n) 具体源码分析参见：http://www.importnew.com/22011.html 表现二多个线程同时操作一个hashmap就可能出现不安全的情况：比如A B两个线程(A线程获数据 B线程存数据) 同时操作myHashMap1.B线程执行存放数据modelHashMap.put(“1”,”2”);2.A线程执行get获取数据modelHashMap.get(“1”)A线程获取的值本来应该是2，但是如果A线程在刚到达获取的动作还没执行的时候，线程执行的机会又跳到线程B，此时线程B又对modelHashMap赋值 如：modelHashMap.put(“1”,”3”);然后线程虚拟机又执行线程A，A取到的值为3，这样map中第一个存放的值 就会丢失。。。。。—原子性 解决HashMap非线程安全其实上面我已经有提过了： 三个方法： Hashtable替换HashMap Collections.synchronizedMap将HashMap包装起来 private Map map = Collections.synchronizedMap(new HashMap());替换private HashMap map = new HashMap(); ConcurrentHashMap替换HashMap private ConcurrentHashMap map = new ConcurrentHashMap();替换private HashMap map = new HashMap(); 好的博文http://blog.csdn.net/lyg468088/article/details/49464121]]></content>
      <categories>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java8</tag>
        <tag>java8新特性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员未来规划]]></title>
    <url>%2F2017%2F08%2F28%2F%E7%A8%8B%E5%BA%8F%E5%91%98%E6%9C%AA%E6%9D%A5%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[http://www.jianshu.com/p/9d29a441ee17?utm_source=desktop&amp;utm_medium=timeline]]></content>
      <categories>
        <category>心情</category>
      </categories>
      <tags>
        <tag>心情</tag>
        <tag>大龄程序员</tag>
        <tag>规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java之ClassLoader源码分析]]></title>
    <url>%2F2017%2F08%2F24%2Fjava%E4%B9%8BClassLoader%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[类加载器ClassLoader的含义 不论多么简单的java程序，都是由一个或者多个java文件组成，java内部实现了程序所需要的功能逻辑，类之间可能还存在着依赖关系。当程序运行的时候，类加载器会把一部分类编译为class后加载到内存中，这样程序才能够调用里面的方法并运行。 类之间如果存在依赖关系，那么类加载会去帮你加载相关的类到内存中，这样才能够完成调用。如果找不到相关的类，那么他就会抛出我们在开发经常见到的异常：ClassNotFoundException Java中的所有类，必须被装载到jvm中才能运行，这个装载工作是由jvm中的类装载器完成的，类装载器所做的工作实质是把类文件从硬盘读取到内存中，JVM在加载类的时候，都是通过ClassLoader的loadClass（）方法来加载class的,与此同时在loadClass中存在着三种加载策略，loadClass使用双亲委派模式。 所以Classloader就是用来动态加载Class文件到内存当中用的。 Java默认提供的三个ClassLoader1.Bootstrap ClassLoader 称为启动类加载器，是Java类加载层次中最顶层的类加载器，负责加载JDK中的核心类库，预设上它负责搜寻JRE所在目录的classes或lib目录下（实际上是由系统参数sun.boot.class.path指定）。如：rt.jar、resources.jar、charsets.jar等，可通过如下程序获得该类加载器从哪些地方加载了相关的jar或class文件： URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i &lt; urls.length; i++) &#123; System.out.println(urls[i].toExternalForm()); &#125; &gt; 这两种输出的内容都是一样的。 System.out.println(System.getProperty("sun.boot.class.path")); 输出 file:/F:/JDK/jdk1.8/jre/lib/resources.jarfile:/F:/JDK/jdk1.8/jre/lib/rt.jarfile:/F:/JDK/jdk1.8/jre/lib/sunrsasign.jarfile:/F:/JDK/jdk1.8/jre/lib/jsse.jarfile:/F:/JDK/jdk1.8/jre/lib/jce.jarfile:/F:/JDK/jdk1.8/jre/lib/charsets.jarfile:/F:/JDK/jdk1.8/jre/lib/jfr.jarfile:/F:/JDK/jdk1.8/jre/classesF:\JDK\jdk1.8\jre\lib\resources.jar;F:\JDK\jdk1.8\jre\lib\rt.jar;F:\JDK\jdk1.8\jre\lib\sunrsasign.jar;F:\JDK\jdk1.8\jre\lib\jsse.jar;F:\JDK\jdk1.8\jre\lib\jce.jar;F:\JDK\jdk1.8\jre\lib\charsets.jar;F:\JDK\jdk1.8\jre\lib\jfr.jar;F:\JDK\jdk1.8\jre\classes 2.Extension ClassLoader 称为扩展类加载器，负责加载Java的扩展类库，默认加载JAVA_HOME/jre/lib/ext/目下的所有jar（实际上是由系统参数java.ext.dirs指定） 3.App ClassLoader 称为系统类加载器，负责加载应用程序classpath目录下的所有jar和class文件（由系统参数java.class.path指定） 总结 Extension ClassLoader和App ClassLoader 这两个类加载器实际上都是继承了ClassLoader类，但是Bootstrap ClassLoader不继承自ClassLoader，因为它不是一个普通的Java类，底层由C++编写，已嵌入到了JVM内核当中，当JVM启动后，Bootstrap ClassLoader也随着启动，负责加载完核心类库后，并构造Extension ClassLoader和App ClassLoader类加载器，也就是说： Bootstrap Loader会在JVM启动之后载入，之后它会载入ExtClassLoader并将ExtClassLoader的parent设为Bootstrap Loader，然后BootstrapLoader再加载AppClassLoader，并将AppClassLoader的parent设定为 ExtClassLoader。 ClassLoader加载类的原理双亲委托加载模式 我们知道除了顶级的 Bootstrap Loader他的parent属性为null之外，其他的两个或者自定义的类加载器都是存在parent 的。 当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader 进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象 为什么要使用双亲委托这种模型呢 java是一门具有很高的安全性的语言，使用这种加载策略的目的是为了：防止重载，父类如果已经找到了需要的类并加载到了内存，那么子类加载器就不需要再去加载该类。安全性问题。两个原因 JVM在搜索类的时候，又是如何判定两个class是相同的 类名是否相同 否由同一个类加载器实例加载 看一个例子public class TestClassLoader&#123; public static void main(String[] args) &#123; ClassLoader loader = TestClassLoader.class.getClassLoader(); //获得加载ClassLoaderTest.class这个类的类加载器 while(loader != null) &#123; System.out.println(loader); loader = loader.getParent(); //获得父类加载器的引用 &#125; System.out.println(loader); &#125;&#125; 输出 sun.misc.Launcher$AppClassLoader@2a139a55sun.misc.Launcher$ExtClassLoader@7852e922null 结论 第一行结果说明：ClassLoaderTest的类加载器是AppClassLoader。 第二行结果说明：AppClassLoader的类加器是ExtClassLoader，即parent=ExtClassLoader。 第三行结果说明：ExtClassLoader的类加器是Bootstrap ClassLoader，因为Bootstrap ClassLoader不是一个普通的Java类，所以ExtClassLoader的parent=null，所以第三行的打印结果为null就是这个原因。 测试2 将ClassLoaderTest.class打包成ClassLoaderTest.jar，放到Extension ClassLoader的加载目录下（JAVA_HOME/jre/lib/ext），然后重新运行这个程序，得到的结果会是什么样呢？ 输出 sun.misc.Launcher$ExtClassLoader@7852e922null 结果分析： 为什么第一行的结果是ExtClassLoader呢？ 因为ClassLoader的委托模型机制，当我们要用ClassLoaderTest.class这个类的时候，AppClassLoader在试图加载之前，先委托给Bootstrcp ClassLoader，Bootstracp ClassLoader发现自己没找到，它就告诉ExtClassLoader，兄弟，我这里没有这个类，你去加载看看，然后Extension ClassLoader拿着这个类去它指定的类路径（JAVA_HOME/jre/lib/ext）试图加载，唉，它发现在ClassLoaderTest.jar这样一个文件中包含ClassLoaderTest.class这样的一个文件，然后它把找到的这个类加载到内存当中，并生成这个类的Class实例对象，最后把这个实例返回。所以ClassLoaderTest.class的类加载器是ExtClassLoader。 第二行的结果为null，是因为ExtClassLoader的父类加载器是Bootstrap ClassLoader。 测试3: 用Bootstrcp ClassLoader来加载ClassLoaderTest.class，有两种方式：1、在jvm中添加-Xbootclasspath参数，指定Bootstrcp ClassLoader加载类的路径，并追加我们自已的jar（ClassTestLoader.jar）2、将class文件放到JAVA_HOME/jre/classes/目录下（上面有提到）(将ClassLoaderTest.jar解压后，放到JAVA_HOME/jre/classes目录下，如下图所示：)提示：jre目录下默认没有classes目录，需要自己手动创建一个提供者：Java团长 自定义ClassLoader前言 实现自定义类加载的目的是，假设我们的类他不是存在特定的位置，可能是某个磁盘或者某个远程服务器上面，那么我们就需要自定义类加载器去加载这些类。 继承继承java.lang.ClassLoader 重写父类的findClass方法 在findClass()方法中调用defineClass()。 这个方法在编写自定义classloader的时候非常重要，它能将class二进制内容转换成Class对象，如果不符合要求的会抛出各种异常 注意： 一个ClassLoader创建时如果没有指定parent，那么它的parent默认就是AppClassLoader。 为什么不去重定义loadClass方法呢？其实也可以，但是loadClass方法内部已经实现了搜索类的策略。除非你是非常熟悉否则还是不建议这样去做。这里建议重载findClass方法，因为在loadClass中最后会去调用findClass方法去加载类。而且这个方法内部默认是空的。 分析loadClass方法源码/*** A class loader is an object that is responsible for loading classes. The class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to locate or generate data that constitutes a definition for the class. A typical strategy is to transform the name into a file name and then read a &quot;class file&quot; of that name from a file system.**/ 大致意思如下：class loader是一个负责加载classes的对象，ClassLoader类是一个抽象类，需要给出类的二进制名称，class loader尝试定位或者产生一个class的数据，一个典型的策略是把二进制名字转换成文件名然后到文件系统中找到该文件。 protected synchronized Class loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; // 首先检查该name指定的class是否有被加载 Class c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; //如果parent不为null，则调用parent的loadClass进行加载 c = parent.loadClass(name, false); &#125;else&#123; //parent为null，则调用BootstrapClassLoader进行加载 c = findBootstrapClass0(name); &#125; &#125;catch(ClassNotFoundException e) &#123; //如果仍然无法加载成功，则调用自身的findClass进行加载 c = findClass(name); // &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; 自定义类加载器package com.kingge.com;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.InputStream;import java.net.URL;public class PersonalClassLoader extends ClassLoader&#123; private String rootUrl; public PersonalClassLoader(String rootUrl) &#123; this.rootUrl = rootUrl; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; Class clazz = null;//this.findLoadedClass(name); // 父类已加载 //if (clazz == null) &#123; //检查该类是否已被加载过 byte[] classData = getClassData(name); //根据类的二进制名称,获得该class文件的字节码数组 if (classData == null) &#123; throw new ClassNotFoundException(); &#125; clazz = defineClass(name, classData, 0, classData.length); //将class的字节码数组转换成Class类的实例 //ClassLoader内置方法 /* * Converts an array of bytes into an instance of class * Before the &lt;tt&gt;Class&lt;/tt&gt; can be used it must be resolved.*/ //&#125; return clazz; &#125; private byte[] getClassData(String name) &#123; InputStream is = null; try &#123; String path = classNameToPath(name); URL url = new URL(path); byte[] buff = new byte[1024*4]; int len = -1; is = url.openStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); while((len = is.read(buff)) != -1) &#123; baos.write(buff,0,len); &#125; return baos.toByteArray(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (is != null) &#123; try &#123; is.close(); &#125; catch(IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return null; &#125; private String classNameToPath(String name) &#123; return rootUrl + &quot;/&quot; + name.replace(&quot;.&quot;, &quot;/&quot;) + &quot;.class&quot;; &#125;&#125; 测试类： package com.kingge.com;public class ClassLoaderTest&#123; public static void main(String[] args) &#123; try &#123; /*ClassLoader loader = ClassLoaderTest.class.getClassLoader(); //获得ClassLoaderTest这个类的类加载器 while(loader != null) &#123; System.out.println(loader); loader = loader.getParent(); //获得父加载器的引用 &#125; System.out.println(loader);*/ String rootUrl = &quot;http://localhost:8080/console/res&quot;; PersonalClassLoader networkClassLoader = new PersonalClassLoader(rootUrl); String classname = &quot;HelloWorld&quot;; Class clazz = networkClassLoader.loadClass(classname); System.out.println(clazz.getClassLoader()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出： com.kingge.com.PersonalClassLoader@65b54208 其中HelloWorld.class文件的位置在于： 其实很多服务器都自定义了类加载器 用于加载web应用指定目录下的类库（jar或class），如：Weblogic、Jboss、tomcat等，下面我以Tomcat为例，展示该web容器都定义了哪些个类加载器： 下面以tomcat为例子 1、新建一个web工程httpweb 2、新建一个ClassLoaderServletTest，用于打印web容器中的ClassLoader层次结构 一下代码来自网上：import java.io.IOException; import java.io.PrintWriter; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; public class ClassLoaderServletTest extends HttpServlet &#123; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.setContentType(&quot;text/html&quot;); PrintWriter out = response.getWriter(); ClassLoader loader = this.getClass().getClassLoader(); while(loader != null) &#123; out.write(loader.getClass().getName()+&quot;&lt;br/&gt;&quot;); loader = loader.getParent(); &#125; out.write(String.valueOf(loader)); out.flush(); out.close(); &#125; public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125; &#125; 3、配置Servlet，并启动服务 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app version=&quot;2.4&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;&gt; &lt;servlet&gt; &lt;servlet-name&gt;ClassLoaderServletTest&lt;/servlet-name&gt; &lt;servlet-class&gt;ClassLoaderServletTest&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;ClassLoaderServletTest&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/ClassLoaderServletTest&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;/web-app&gt; 运行截图： 总结 这种自定义的方式目的就是为了，能够控制类的加载流程，那么这种远程加载类的方式类似于我们常用的Hessian 来访问多个系统获取类 好的网站http://blog.csdn.net/briblue/article/details/54973413]]></content>
      <categories>
        <category>java深入理解</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>ClassLoader</tag>
        <tag>类加载器</tag>
        <tag>自定义类加载器</tag>
        <tag>类加载器源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么毕业后选择留在小城市]]></title>
    <url>%2F2017%2F08%2F21%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AF%95%E4%B8%9A%E5%90%8E%E9%80%89%E6%8B%A9%E7%95%99%E5%9C%A8%E5%B0%8F%E5%9F%8E%E5%B8%82%2F</url>
    <content type="text"><![CDATA[前言经常看到大学要毕业的学生，会有一种纠结感。越是临近毕业，这种感觉就越是强烈这种感觉实际上就是一种选择恐惧症，又或者说是小城综合征。他们对于毕业后究竟是选择去一线城市就业还是去二三线城市就业，产生了很大的选择恐惧。 为什么会产生这种心理大致的原因有那么几个： 身边的认识的人，都是选择回到自己的家乡进行就业，自己收到了影响。 大城市的生活节奏会比小城市更加的紧凑，你会很忙。 父母希望自己回去，离家近的地方工作。 恋人不跟随自己，她或他选择了回到了故乡就业，自己左右为难。 有些人已经在大城市实习过，对于大城市已经厌倦。 作者的选择 本人就是属于最后一种，大三的时候去的深圳实习，在一家IT互联网公司上班，加班很多，虽然加班这种现象在深圳是一种常态。但是每天加班到晚上一点多，也是很累，所以毕业后也就选择回到了自己的家乡就业。 回到了]]></content>
      <categories>
        <category>心情</category>
      </categories>
      <tags>
        <tag>心情</tag>
        <tag>总结</tag>
        <tag>有感</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[max-allowed-packet的问题]]></title>
    <url>%2F2017%2F08%2F17%2Fmax-allowed-packet%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言 近期，因启动项目有个批量插入的sql结果太大，超过了mysql自带的缓存，报了这个错误 修改： 定位到mysql的安装目录下面，然后修改my.ini 的max_allowed_packet = 8M默认是1M]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>异常</tag>
        <tag>sql异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YUM仓库配置]]></title>
    <url>%2F2017%2F08%2F06%2FYUM%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.1 概述YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。-需要联网 1.2 为什么要制作本地YUM源YUM源虽然可以简化我们在Linux上安装软件的过程，但是生产环境通常无法上网，不能连接外网的YUM源，说以就无法使用yum命令安装软件了。为了在内网中也可以使用yum安装相关的软件，就要配置yum源。 YUM源其实就是一个保存了多个RPM包的服务器，可以通过http的方式来检索、下载并安装相关的RPM包。 1.3 yum的常用命令1）基本语法： yum install -y rpm软件包 （功能描述：安装httpd并确认安装） yum list （功能描述：列出所有可用的package和package组） yum clean all （功能描述：清除所有缓冲数据） yum deplist rpm软件包 （功能描述：列出一个包所有依赖的包） yum remove rpm软件包 （功能描述：删除httpd） 2）案例实操 ​ yum install -y tree //安装文档树结构展示插件 1.4 关联网络yum源1）前期文件准备 （1）前提条件linux系统必须可以联网 （2）在Linux环境中访问该网络地址：http://mirrors.163.com/.help/centos.html，在使用说明中点击CentOS6-&gt;再点击保存 （3）查看文件保存的位置 在打开的终端中输入如下命令，就可以找到文件的保存位置。 [kingge@hadoop101 下载]$ pwd /home/kingge/下载 2）替换本地yum文件 ​ （1）把下载的文件移动到/etc/yum.repos.d/目录 [root@hadoop101 下载]# mv CentOS6-Base-163.repo /etc/yum.repos.d/ ​ （2）进入到/etc/yum.repos.d/目录 [root@hadoop101 yum.repos.d]# pwd /etc/yum.repos.d ​ （3）用CentOS6-Base-163.repo替换CentOS-Base.rep [root@hadoop101 yum.repos.d]# mv CentOS6-Base-163.repo CentOS-Base.rep 3）安装命令 ​ （1）[root@hadoop101 yum.repos.d]#yum clean all ​ （2）[root@hadoop101 yum.repos.d]#yum makecache ​ （3）[root@hadoop101 yum.repos.d]# yum install -y createrepo （4）[root@hadoop101 yum.repos.d]#yum install -y httpd 1.5 制作本地yum源1.5.1 制作只有本机能访问的本地YUM源（1）准备一台Linux服务器，版本CentOS-6.8-x86_64-bin-DVD1.iso （2）配置好这台服务器的IP地址 （3）将CentOS-6.8-x86_64-bin-DVD1.iso镜像挂载到/mnt/cdrom目录 [root@hadoop101 /]# mkdir /mnt/cdrom [root@hadoop101 /]# mount -t iso9660 /dev/cdrom /mnt/cdrom （4）安装相应的软件 [root@hadoop101 yum.repos.d]#yum install -y httpd ​ （5）启动httpd服务 [root@hadoop101 yum.repos.d]#service httpd start （6）使用浏览器访问http://192.168.1.101:80（如果访问不通，检查防火墙是否开启了80端口或关闭防火墙），测试网络是否畅通 （7）将YUM源配置到httpd（Apache Server）中 [root@hadoop101 html]# mkdir Packages [root@hadoop101 html]# chown kingge:kingge Packages/ [root@hadoop101 html]# cp -r /mnt/cdrom/Packages/* /var/www/html/Packages/ （8）执行创建仓库命令：createrepo 路径 [root@hadoop101 Packages]# createrepo ./ （9）修改本机上的YUM源配置文件，将源指向自己 备份原有的YUM源的配置文件 [root@hadoop101 /]# cd /etc/yum.repos.d/ [root@hadoop101 yum.repos.d]# cp CentOS-Base.repo CentOS-Base.repo.bak ​ 编辑CentOS-Base.repo文件 [root@hadoop101 yum.repos.d]# vi CentOS-Base.repo [base] name=CentOS-Local baseurl=file:///var/www/html/Packages gpgcheck=0 enabled=1 #增加改行，使能 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 添加上面内容保存退出 （10）清除YUM缓存 [root@hadoop101 yum.repos.d]# yum clean all （11）列出可用的YUM仓库 [root@hadoop101 yum.repos.d]# yum repolist （12）安装相应的软件 [root@hadoop101 yum.repos.d]# yum install -y tree [root@hadoop101 Packages]# yum install -y firefox-45.0.1-1.el6.centos.x86_64.rpm 1.5.2 制作其他主机通过网络能访问的本地YUM源前期准备：检查yum源服务器的httpd服务是否启动： ​ Ps aux | grep httpd –查看该进程是否存在 或者 netstat –anp | grep 80 直接查看是否监听80端口 ​ 启动 httpd服务： service httpd start/restart 启动后，可能使用yum源的服务器访问不到yum仓库（例如下面的101服务器访问不到102服务器的yum），那么有可能是101服务器防火墙屏蔽了80端口，应该设置一下防火墙规则 （1）让其他需要安装RPM包的服务器指向这个YUM源，准备一台新的服务器，备份或删除原有的YUM源配置文件 备份原有的YUM源的配置文件 [root@hadoop102 /]#cd /etc/yum.repos.d/ [root@hadoop102 yum.repos.d]# cp CentOS-Base.repo CentOS-Base.repo.bak ​ 编辑CentOS-Base.repo文件 [root@hadoop102 yum.repos.d]# vi CentOS-Base.repo [base] name=CentOS-hadoop101 baseurl=http://192.168.1.101/Packages gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 添加上面内容保存退出 （2）在这台新的服务器上执行YUM的命令 [root@hadoop102 yum.repos.d]# yum clean all [root@hadoop102 yum.repos.d]# yum repolist （3）安装软件 [root@hadoop102 yum.repos.d]# yum install -y httpd]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell编程]]></title>
    <url>%2F2017%2F08%2F06%2FShell%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、引言因为公司遜要开发一款saas系统，需要使用自动部署脚本，部署web项目，所以就去了解了一下相关的知识，这里并不做深入的研究。 1.1 概述Shell是一个命令行解释器，它为用户提供了一个向Linux内核发送请求以便运行程序的界面系统级程序，用户可以用Shell来启动、挂起、停止甚至是编写一些程序。 Shell还是一个功能相当强大的编程语言，易编写、易调试、灵活性强。Shell是解释执行的脚本语言，在Shell中可以调用Linux系统命令。 1.2 shell脚本的执行方式1）echo输出内容到控制台 ​ （1）基本语法：​ echo [选项] [输出内容] 选项： -e： 支持反斜线控制的字符转换 控制字符 作 用 \ 输出\本身 \a 输出警告音 \b 退格键，也就是向左删除键 \c 取消输出行末的换行符。和“-n”选项一致 \e ESCAPE键 \f 换页符 \n 换行符 \r 回车键 \t 制表符，也就是Tab键 \v 垂直制表符 \0nnn 按照八进制ASCII码表输出字符。其中0为数字零，nnn是三位八进制数 \xhh 按照十六进制ASCII码表输出字符。其中hh是两位十六进制数 ​（2）案例 ​ [kingge@hadoop101 sbin]$ echo &quot;helloworld&quot; helloworld 2）脚本格式 （1）脚本以 #!/bin/bash 开头（2）脚本必须有可执行权限 3）第一个Shell脚本 （1）需求：创建一个Shell脚本，输出helloworld （2）实操： [kingge@hadoop101 datas]$ touch helloworld.sh [kingge@hadoop101 datas]$ vi helloworld.sh ​ 在helloworld.sh中输入如下内容 #!**/**bin**/**bash echo &quot;helloworld&quot; 4）脚本的常用执行方式 第一种：输入脚本的绝对路径或相对路径 （1）首先要赋予helloworld.sh 脚本的+x权限 [kingge@hadoop101 datas]$ chmod 777 helloworld.sh （2）执行脚本 ​ /root/helloWorld.sh ​ ./helloWorld.sh 第二种：bash或sh+脚本（不用赋予脚本+x权限） ​ sh /root/helloWorld.sh ​ sh helloWorld.sh 1.3 shell中的变量1）Linux Shell中的变量分为，系统变量和用户自定义变量。 2）系统变量：$HOME、$PWD、$SHELL、$USER等等 3）显示当前shell中所有变量：set 1.3.1 定义变量1）基本语法： ​ （1）定义变量：变量=值 （2）撤销变量：unset 变量 （3）声明静态变量：readonly变量，注意：不能unset 2）变量定义规则 ​ （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头。 ​ （2）等号两侧不能有空格 ​ （3）变量名称一般习惯为大写 3）案例 ​ （1）定义变量A A=8 ​ （2）撤销变量A unset A ​ （3）声明静态的变量B=2，不能unset readonly B=2 ​ （4）可把变量提升为全局环境变量，可供其他shell程序使用 export 变量名 1.3.2 将命令的返回值赋给变量​ （1）A=ls -la 反引号，运行里面的命令，并把结果返回给变量A ​ （2）A=$(ls -la) 等价于反引号 1.3.3 设置环境变量1）基本语法： ​ （1）export 变量名=变量值 （功能描述：设置环境变量的值） （2）source 配置文件 （功能描述：让修改后的配置信息立即生效） （3）echo $变量名 （功能描述：查询环境变量的值） 2）案例： ​ （1）在/etc/profile文件中定义JAVA_HOME环境变量 ​ export JAVA_HOME=/opt/module/jdk1.7.0_79 ​ export PATH=$PATH:$JAVA_HOME/bin （2）查看环境变量JAVA_HOME的值 ​ [kingge@hadoop101 datas]$ echo $JAVA_HOME /opt/module/jdk1.7.0_79 1.3.4 位置参数变量1）基本语法 ​ $n （功能描述：n为数字，$0代表命令本身，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如$&#123;10&#125;）$* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）$@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）$# （功能描述：这个变量代表命令行中所有参数的个数） 2）案例 （1）输出输入的的参数1，参数2，所有参数，参数个数 #!/bin/bashecho &quot;$0 $1 $2&quot;echo &quot;$*&quot;echo &quot;$@&quot;echo &quot;$#&quot; （2）$*与$@的区别 #!/bin/bash for i in &quot;$*&quot; #$*中的所有参数看成是一个整体，所以这个for循环只会循环一次 do echo &quot;The parameters is: $i&quot; done x=1 for y in &quot;$@&quot; #$@中的每个参数都看成是独立的，所以“$@”中有几个参数，就会循环几次 do echo &quot;The parameter$x is: $y&quot; x=$(( $x +1 )) done a）$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数 b）当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“​$1 $2 …​$n”的形式输出所有参数；“$@”会将各个参数分开，以“​$1” “$2”…”​$n”的形式输出所有参数 1.3.5 预定义变量1）基本语法： （1）“$((运算式))”或“$[运算式]”（2）expr m + n 注意expr运算符间要有空格（3）expr m - n（4）expr \*, /, % 乘，除，取余 2）案例 计算（2+3）X4的值 （1）采用$[运算式]方式[root@hadoop101 datas]# S=$[(2+3)*4] [root@hadoop101 datas]# echo $S （2）expr分布计算 S=`expr 2 + 3` expr $S \* 4 （3）expr一步完成计算 expr `expr 2 + 3` \* 4 1.4 运算符1）基本语法： （1）“$((运算式))”或“$[运算式]” （2）expr m + n 注意expr运算符间要有空格 （3）expr m - n （4）expr *, /, % 乘，除，取余 2）案例：计算（2+3）X4的值 ​ （1）采用$[运算式]方式 ​ [root@hadoop101 datas]# S=$[(2+3)*4] ​ [root@hadoop101 datas]# echo $S ​ （2）expr分布计算 ​ S=expr 2 + 3 ​ expr $S * 4 ​ （3）expr一步完成计算 ​ expr expr 2 + 3 * 4 1.5 条件判断1.5.1 判断语句1）基本语法： [ condition ]（注意condition前后要有空格） #非空返回true，可使用$?验证（0为true，&gt;1为false） 2）案例： [kingge] 返回true [] 返回false [condition] &amp;&amp; echo OK || echo notok 条件满足，执行后面的语句 1.5.2 常用判断条件1）两个整数之间比较 = 字符串比较 -lt 小于 -le 小于等于 -eq 等于 -gt 大于 -ge 大于等于 -ne 不等于 2）按照文件权限进行判断 -r 有读的权限 -w 有写的权限 -x 有执行的权限 3）按照文件类型进行判断 -f 文件存在并且是一个常规的文件 -e 文件存在 -d 文件存在并是一个目录 4）案例 ​ （1）23是否大于等于22 ​ [ 23 -ge 22 ] ​ （2）student.txt是否具有写权限 ​ [ -w student.txt ] ​ （3）/root/install.log目录中的文件是否存在 ​ [ -e /root/install.log ] 1.6 流程控制1.6.1 if判断1）基本语法： if [ 条件判断式 ];then 程序 fi 或者 if [ 条件判断式 ] then ​ 程序 fi ​ 注意事项：（1）[ 条件判断式 ]，中括号和条件判断式之间必须有空格 2）案例 #!/bin/bashif [ $1 -eq &quot;123&quot; ]then echo &quot;123&quot;elif [ $1 -eq &quot;456&quot; ]then echo &quot;456&quot;fi 1.6.2 case语句1）基本语法： case $变量名 in “值1”） ​ 如果变量的值等于值1，则执行程序1 ​ ;; “值2”） ​ 如果变量的值等于值2，则执行程序2 ​ ;; …省略其他分支… *） ​ 如果变量的值都不是以上的值，则执行此程序 ​ ;; esac 2）案例 !/bin/bashcase $1 in&quot;1&quot;) echo &quot;1&quot;;;&quot;2&quot;) echo &quot;2&quot;;;*) echo &quot;other&quot;;;esac 1.6.3 for循环1）基本语法1： for 变量 in 值1 值2 值3… do ​ 程序 done 2）案例： ​ （1）打印输入参数 #!/bin/bash#打印数字for i in &quot;$*&quot; do echo &quot;The num is $i &quot; donefor j in &quot;$@&quot; do echo &quot;The num is $j&quot; done 3）基本语法2： ​ for (( 初始值;循环控制条件;变量变化 )) do ​ 程序 done 4）案例 （1）从1加到100 #!/bin/bashs=0for((i=0;i&lt;=100;i++))do s=$[$s+$i]doneecho &quot;$s&quot; 1.6.4 while循环1）基本语法： while [ 条件判断式 ] do ​ 程序 done 2）案例 ​ （1）从1加到100 #!/bin/bashs=0i=1while [ $i -le 100 ]do s=$[$s+$i] i=$[$i+1]doneecho $s 1.7 read读取控制台输入1）基本语法： ​ read(选项)(参数) ​ 选项： -p：指定读取值时的提示符； -t：指定读取值时等待的时间（秒）。 参数 ​ 变量：指定读取值的变量名 2）案例 ​ 读取控制台输入的名称 #!/bin/bashread -t 7 -p &quot;please 7 miao input your name &quot; NAMEecho $NAME 1.8 函数1.8.1 系统函数1）basename基本语法 basename [pathname] [suffix] basename [string] [suffix] （功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。 选项： suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。 2）案例 [kingge@hadoop101 opt]$ basename /opt/test.txt test.txt [kingge@hadoop101 opt]$ basename /opt/test.txt .txt test 3）dirname基本语法 ​ dirname 文件绝对路径 （功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）） 4）案例 ​ [kingge@hadoop101 opt]$ dirname /opt/test.txt /opt 1.8.2 自定义函数1）基本语法： [ function ] funname[()]&#123; Action; [return int;]&#125; funname 注意： ​ （1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。 ​ （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255) 2）案例 ​ （1）计算输入参数的和 #!/bin/bashfunction sum()&#123; s=0 s=$[ $1 + $2 ] echo &quot;$s&quot;&#125;read -p &quot;Please input the number1: &quot; n1;read -p &quot;Please input the number2: &quot; n2;sum $n1 $n2;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现多数据量的导入数据库]]></title>
    <url>%2F2017%2F08%2F01%2F%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%95%B0%E6%8D%AE%E9%87%8F%E7%9A%84%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[引言 在做一个项目的时候，涉及到需要从一个表格中获取百万条数据然后插入到数据库中，最后采用JDBC的executeBantch方法实现这个功能。 采取的策略 尽量关闭字段索引（因为再插入数据的时候还是需要维护索引的，在创建索引和维护索引 会耗费时间,随着数据量的增加而增加，可以在插入数据后再去为字段创建索引） 虽然索引可以提高查询速度但是，插入数据的时候会导致索性的更新。索性越多，插入会越慢。可以看文档描述:Although it can be tempting to create an indexes for every possible column used in a query, unnecessary indexes waste space and waste time for MySQL to determine which indexes to use. Indexes also add to the cost of inserts, updates, and deletes because each index must be updated. You must find the right balance to achieve fast queries using the optimal set of indexes. 分批次提交数据 在分布式条件下，还可以考虑在不同的数据库结点提交，有底层的消息系统完成数据扩展 过滤预处理数据 预处理数据的场景：为了避免插入的数据（假设ListA）跟数据库中某些数据重复，那么我们会把要插入的数据去数据库中查询是否已经存在，获得返回的已经存在数据（ListB）。然后在插入数据的时候判断当前数据是否在ListB中，那么当前数据不能够插入数据库。过滤出来，最后得到一个可以插入数据库的ListC 代码关键代码/*数据分析结束*/ /*往数据库写数据开始*/ Connection conn=null; PreparedStatement idsUserAdd=null; try &#123; Class.forName("com.mysql.jdbc.Driver") ; conn = DriverManager.getConnection(ConfigTool.getProperty("jdbc.url").toString() , ConfigTool.getProperty("jdbc.username").toString() , ConfigTool.getProperty("jdbc.password").toString()); conn.setAutoCommit(false); //构造预处理statement idsUserAdd = conn.prepareStatement("INSERT INTO dc_matedata ("+ " ID,`NAME`, DATATYPE,`CODE`,TYPE_ID,`LENGTH`, "+ " DATANAME, VALUEAREA,`RESTRICT`, REMARK,MD_DATE)"+ " values(?,?,?,?,?,?,?,?,?,?,now())"); //最大列表的数目当做循环次数 int xhcs=addMetadataList.size();//addMetadataList需要插入的数据 for(int i=0;i&lt;xhcs;i++)&#123; idsUserAdd.setString(1,addMetadataList.get(i).get("id").toString()); idsUserAdd.setString(2,addMetadataList.get(i).get("name").toString()); idsUserAdd.setString(3,addMetadataList.get(i).get("dataType").toString()); idsUserAdd.setString(4,addMetadataList.get(i).get("code").toString()); idsUserAdd.setString(5,addMetadataList.get(i).get("typeId").toString()); idsUserAdd.setString(6,addMetadataList.get(i).get("dataLength").toString()); idsUserAdd.setString(7,addMetadataList.get(i).get("dataName").toString()); idsUserAdd.setString(8,addMetadataList.get(i).get("valueArea").toString()); idsUserAdd.setString(9,addMetadataList.get(i).get("dataRestrict").toString()); idsUserAdd.setString(10,addMetadataList.get(i).get("dataRemark").toString()); idsUserAdd.addBatch(); //每10000次提交一次 if(i%10000==0||i==xhcs-1)&#123;//可以设置不同的大小；如50，100，500，1000等等 i==xhcs-1（最后一次） idsUserAdd.executeBatch(); conn.commit(); idsUserAdd.clearBatch(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw e; &#125;finally &#123; try &#123; if(idsUserAdd!=null) idsUserAdd.close(); if(conn!=null) conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); throw e; &#125; &#125; /*往数据库写数据结束*/ 完整代码/** * 校验需要导入的元数据信息，封装错误信息并批量插入数据库 */ @Override public List&lt;Map&lt;String, Object&gt;&gt; saveDCMetadataBatch(List&lt;Map&lt;String, Object&gt;&gt; list, boolean valid, boolean addError) throws Exception&#123; List&lt;Map&lt;String,Object&gt;&gt; errorList=new ArrayList&lt;Map&lt;String,Object&gt;&gt;();//获得不能够添加成功的数据 Map&lt;String,Object&gt; map=new HashMap&lt;String,Object&gt;();//查询条件 Map&lt;String,String&gt; codeMap=new HashMap&lt;String,String&gt;();//每个分类对应的元数据的编号最大值 Map&lt;String,Object&gt; metaName=new HashMap&lt;String,Object&gt;();//查询数据库中是否存在相同的数据（这里校验的是：元数据的中文简称） Map&lt;String,Object&gt; metaDataName=new HashMap&lt;String,Object&gt;();//查询数据库中是否存在相同的数据（这里校验的是：元数据的数据项名称） map.put("metaName",list);//需要查询的元数据中文名称 map.put("metaDataTypeId",list);//导入的元数据的编号 List&lt;Map&lt;String, Object&gt;&gt; metaExistList = dCMatedataDao.getDCMetadata(map);//根据元数据名称查询当前分类下是否存在同样元数据 map.put("metaName",null);//置空 map.put("metaDataName",list); List&lt;Map&lt;String, Object&gt;&gt; metaExistListTwo = dCMatedataDao.getDCMetadata(map);//根据元数据数据项名称查询存在的元数据 //保存重复的信息 for(int i=0;i&lt;metaExistList.size();i++) metaName.put(metaExistList.get(i).get("name").toString()+metaExistList.get(i).get("code").toString() ,metaExistList.get(i).get("id"));//添加父类的编号为后缀-唯一性保证 for(int i=0;i&lt;metaExistListTwo.size();i++) metaDataName.put(metaExistListTwo.get(i).get("dataname").toString()+metaExistListTwo.get(i).get("code").toString(), metaExistListTwo.get(i).get("id")); /*整理出来的数据-开始*/ List&lt;Map&lt;String,Object&gt;&gt; addMetadataList=new ArrayList&lt;Map&lt;String,Object&gt;&gt;(); /*整理出来的数据-结束*/ for (int i = 0; i &lt; list.size(); i++) &#123; Map&lt;String, Object&gt; MetadataObj = list.get(i); try &#123; String metadatId = StringUtil.getUUID();//元数据id /*校验开始*/ if (valid)&#123; if(validUser(MetadataObj,"name",addError)!=null)&#123;//验证输入的数据是否符合格式和必填。 errorList.add(MetadataObj); continue; &#125; &#125; /*前端校验结束*/ /*校验是否存在同名的元数据*/ String dataCodeCheck = MetadataObj.get("dataCode").toString().trim(); //元数据父分类编号 String name = MetadataObj.get("name").toString().trim();//元数据中文简称 if (metaName.containsKey(name+dataCodeCheck)) &#123; if (addError) &#123; MetadataObj.put("errInfo", "中文简称已存在"); &#125; errorList.add(MetadataObj); continue; &#125; /*校验是否存在相同数据项的元数据*/ String dataName = MetadataObj.get("dataName").toString().trim();//数据项名 if (metaDataName.containsKey(dataName+dataCodeCheck)) &#123; if (addError) &#123; MetadataObj.put("errInfo", "数据项名已存在"); &#125; errorList.add(MetadataObj); continue; &#125; String dataCode = MetadataObj.get("dataCode").toString().trim(); //元数据父分类编号 List&lt;Map&lt;String, Object&gt;&gt; footCount = dCMatedataDao.getFootCount(dataCode); if( footCount.size() &gt; 0)&#123; if (addError) &#123; MetadataObj.put("errInfo", "分类编码不是最后一级分类"); &#125; errorList.add(MetadataObj); continue; &#125; Map&lt;String, Object&gt; typeByCode = dCMatedataDao.getMetadataTypeByCode(dataCode); if( typeByCode == null || typeByCode.size() &lt; 1)&#123; if (addError) &#123; MetadataObj.put("errInfo", "分类编码不存在，请先添加分类"); &#125; errorList.add(MetadataObj); continue; &#125; //校验是在添加的List中是否存在相同的数据项名或者中文简称 //校验导入文件中是否存在一样的中文简称或者数据项名 boolean nameExist = false; boolean dataNameExist = false; for (int j = 0; j &lt; addMetadataList.size(); j++)&#123; Map&lt;String, Object&gt; map2 = addMetadataList.get(j); String typeId = map2.get("typeId").toString(); String nameE = map2.get("name").toString(); String dataNameE = map2.get("dataName").toString(); if( typeId.equals(typeByCode.get("id").toString()) &amp;&amp; nameE.equals(name))&#123; nameExist=true; break; &#125; if( typeId.equals(typeByCode.get("id").toString()) &amp;&amp; dataNameE.equals(dataName))&#123; dataNameExist=true; break; &#125; &#125; if( nameExist )&#123; if (addError) &#123; MetadataObj.put("errInfo", "中文简称已存在"); &#125; errorList.add(MetadataObj); continue; &#125; if( dataNameExist )&#123; if (addError) &#123; MetadataObj.put("errInfo", "数据项名已存在"); &#125; errorList.add(MetadataObj); continue; &#125; //进入这里说明校验结束，开始填充添加的数据 String type_id = typeByCode.get("id").toString();//元数据所属分类id String dataType = MetadataObj.get("dataType").toString().trim(); //元数据类型 String dataLength = MetadataObj.get("dataLength").toString().trim(); //元数据长度 String code = ""; //// if( codeMap.get(dataCode) == null||StringUtil.isEmpty(codeMap.get(dataCode)) )&#123;//表示当前分类不存在已经添加的元数据--因为编码map中不存在对应分类的最大编码 Map maxCodeByPid = this.selectMetadataMaxCode(type_id); if( maxCodeByPid == null )&#123;//表示当前分类下不存在任何子分类 code = StringUtil.getCode("0", dataCode);//则从01开始编号 codeMap.put(dataCode, "01");//保存当前分类下元数据编号最大值 &#125;else&#123; String object = (String) maxCodeByPid.get("codeNum");//当前分类节点下的元数据的编号最大值。 int pSituation = object.indexOf(dataCode); int pLength = pSituation+dataCode.length() ; String substring = object.substring(pLength); //截取出最大编号值得最大值 code = StringUtil.getCode(substring, dataCode); int temp = Integer.parseInt(substring);//保存当前分类下元数据编号最大值 temp+=1; codeMap.put(dataCode, temp+""); &#125; &#125;else&#123; String maxCode = codeMap.get(dataCode); code = StringUtil.getCode(maxCode, dataCode); //保存当前分类下元数据编号最大值 int temp = Integer.parseInt(maxCode); temp+=1; codeMap.put(dataCode, temp+""); &#125; /// Map&lt;String, Object&gt; metadatList = new LinkedHashMap&lt;String, Object&gt;(); metadatList.put("id", metadatId); metadatList.put("name",name); metadatList.put("dataType",dataType); metadatList.put("code",code); metadatList.put("typeId",type_id); metadatList.put("dataLength",dataLength); metadatList.put("dataName",dataName); metadatList.put("valueArea", MetadataObj.get("valueArea")==null?"":MetadataObj.get("valueArea") ); metadatList.put("dataRestrict",MetadataObj.get("dataRestrict")==null?"":MetadataObj.get("dataRestrict")); metadatList.put("dataRemark",MetadataObj.get("dataRemark")==null?"":MetadataObj.get("dataRemark")); metadatList.put("mdDate",new Date()); addMetadataList.add(metadatList); &#125; catch (Exception e)&#123; if(addError) &#123; MetadataObj.put("errInfo", e.getMessage()); &#125; errorList.add(MetadataObj); &#125; &#125; /*数据分析结束*/ /*往数据库写数据开始*/ Connection conn=null; PreparedStatement idsUserAdd=null; try &#123; Class.forName("com.mysql.jdbc.Driver") ; conn = DriverManager.getConnection(ConfigTool.getProperty("jdbc.url").toString() , ConfigTool.getProperty("jdbc.username").toString() , ConfigTool.getProperty("jdbc.password").toString()); conn.setAutoCommit(false); //构造预处理statement idsUserAdd = conn.prepareStatement("INSERT INTO dc_matedata ("+ " ID,`NAME`, DATATYPE,`CODE`,TYPE_ID,`LENGTH`, "+ " DATANAME, VALUEAREA,`RESTRICT`, REMARK,MD_DATE)"+ " values(?,?,?,?,?,?,?,?,?,?,now())"); //最大列表的数目当做循环次数 int xhcs=addMetadataList.size(); for(int i=0;i&lt;xhcs;i++)&#123; idsUserAdd.setString(1,addMetadataList.get(i).get("id").toString()); idsUserAdd.setString(2,addMetadataList.get(i).get("name").toString()); idsUserAdd.setString(3,addMetadataList.get(i).get("dataType").toString()); idsUserAdd.setString(4,addMetadataList.get(i).get("code").toString()); idsUserAdd.setString(5,addMetadataList.get(i).get("typeId").toString()); idsUserAdd.setString(6,addMetadataList.get(i).get("dataLength").toString()); idsUserAdd.setString(7,addMetadataList.get(i).get("dataName").toString()); idsUserAdd.setString(8,addMetadataList.get(i).get("valueArea").toString()); idsUserAdd.setString(9,addMetadataList.get(i).get("dataRestrict").toString()); idsUserAdd.setString(10,addMetadataList.get(i).get("dataRemark").toString()); idsUserAdd.addBatch(); //每10000次提交一次 if(i%10000==0||i==xhcs-1)&#123;//可以设置不同的大小；如50，100，500，1000等等 idsUserAdd.executeBatch(); conn.commit(); idsUserAdd.clearBatch(); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); throw e; &#125;finally &#123; try &#123; if(idsUserAdd!=null) idsUserAdd.close(); if(conn!=null) conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); throw e; &#125; &#125; /*往数据库写数据结束*/ return errorList; &#125; 总结 有些网友发现使用StringBuffer 来拼接入参，不通过prepareStatement的预处理，虽然前者速度很快，但是使用prepareStatement可以防止SQL注入 有的好的建议大家都可以提出来]]></content>
      <categories>
        <category>JDBC</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>JDBC</tag>
        <tag>批量导入</tag>
        <tag>SSM</tag>
        <tag>项目经验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java工程师书单（初级、中级、高级）]]></title>
    <url>%2F2017%2F06%2F21%2FJava%E5%B7%A5%E7%A8%8B%E5%B8%88%E4%B9%A6%E5%8D%95%EF%BC%88%E5%88%9D%E7%BA%A7%E3%80%81%E4%B8%AD%E7%BA%A7%E3%80%81%E9%AB%98%E7%BA%A7%EF%BC%89%2F</url>
    <content type="text"><![CDATA[当你的能力承受不住你的欲望，你就应该静下心来读书 初级书籍《编写高质量代码——改善Java程序的151个建议》 这是一本值得入门java的人放在床头的书。此书内容广泛、要点翔实。大多数优秀程序设计书籍都需要看老外写的，但是这本讲述提高java编程水平的书还是不错的，适合具有基本java编程能力的人。对于程序猿而言，工作久了，就感觉编程习惯对一个人很重要。习惯好，不仅工作效率告，而且bug少。这本书对提高个人的好的编程习惯很有帮助。 《Java程序员修炼之道》 此书涵盖了Java7的新特性和Java开发的关键技术，对当前大量开源技术并存，多核处理器、并发以及海量数据给Java开发带来的挑战作出了精辟的分析，提供了实践前沿的深刻洞见，涉及依赖注入、现代并发、类与字节码、性能调优等底层概念的剖析。**书中的道理很浅显，可是对于菜鸟却是至理名言。基本为你勾勒了一个成熟软件程序员专家所需要的所有特性。。 《Java8实战》 没看过。嘻嘻嘻 《有效的单元测试》 此书由敏捷技术实践专家撰写，系统且深入地阐释单元测试用于软件设计的工具、方法、原则和佳实践；深入剖析各种测试常见问题，包含大量实践案例，可操作性强，能为用户高效编写测试提供系统实践指南。**介绍了单元测试的各个方面，TDD、test double、测试的坏味道、可测试的设计等等，每个主题需要深入的话，还需要配合其它书籍和实践，非常适合入门单元测试。书中例子非常全面，看完对使用 Junit 进行单元测试会有一个大的长进，而且用java语言编写，内容很新 《Java核心技术：卷1》 不推荐卷2，因为这个作为初级书单来讲，太难了。 《代码整洁之道》 没看过 《数据结构与算法分析-Java语言描述》 本书是java数据结构与算法方面的三宝之一，除了这三本其他的已经没有意义了。这三宝分别是:**黑宝书《数据结构与算法分析java语言描述》mark allen weiss蓝宝书《java数据结构和算法》robert lafore红宝书《算法》robert sedgewick黑宝书胜在公式推理和证明以及算法的简洁和精炼，此外习题较多。蓝宝书胜在对算法的深入浅出的讲解，演示和举例，让艰涩的理论变得很容易理解。红宝书胜在系出名门斯坦福，演示通俗易懂，内容丰富。有了这三宝，算法不用愁，学完以后再看《算法导论》就容易多了。本书从讲解什么是数据结构开始，延伸至高级数据结构和算法分析，强调数据结构和问题求解技术。本书的目的是从抽象思维和问题求解的观点提供对数据结构的实用介绍，试图包含有关数据结构、算法分析及其Java实现的所有重要的细节 中级书单《重构：改善既有代码的设计》 重构，绝对是写程序过程中最重要的事之一。在写程序之前我们不可能事先了解所有的需求，设计肯定会有考虑不周的地方，而且随着项目需求的修改，也有可能原来的设计已经被改得面目全非了。更何况，我们很少有机会从头到尾完成一个项目，基本上都是接手别人的代码，我们要做的是重构，从小范围的重构开始。**重构是设计,设计是art,重构也是art. 一个函数三行只是语不惊人死不休的说法,是对成百上千行代码的矫枉过正。 更一个般的看法是一个函数应该写在一页纸内。 《Effective Java》 必读 《Java并发编程实战》 没看过： 本书深入浅出地介绍了Java线程和并发，是一本完美的Java并发参考手册。书中从并发性和线程安全性的基本概念出发，介绍了如何使用类库提供的基本并发构建块，用于避免并发危险、构造线程安全的类及验证线程安全的规则，如何将小的线程安全类组合成更大的线程安全类，如何利用线程来提高并发应用程序的吞吐量。**java进阶必看，多线程的最佳书籍。 实战Java高并发程序设计》 没看过 《算法》 没看过 《Head First 设计模式》 这是我看过最幽默最搞笑最亲切同时又让我收获巨大的技术书籍！ 森森的膜拜Freeman(s)！Amen！ 深入浅出，娓娓道来，有的地方能笑死你！写得很有趣，图文并茂，比起四人帮的那本，好懂了不知道多少倍。计算机世界的head first系列基本都是经典。不过只看书学明白设计模式是不可能的，这些只是前人的总结，我们唯有实践实践再实践了。**读这本书不仅仅是学习知识，而是在学习一种思考的方法，学习一种认知的技巧，学习一种成长的阶梯。 总之，用你闲暇的时间来读这本书，并不亚于你专注的工作或学习。笔者强烈推荐此书，要成长为一名高级程序员，设计模式已经是必备技能了。 《Java编程思想》 没看过 高级书单 《深入理解Java虚拟机》 没看过 《Java性能权威指南》 没看过 《深入分析Java Web技术内幕》 没看过 《大型网站系统与Java中间件实践》 没看过 《大型网站技术架构：核心原理与案例分析》 没看过 《企业应用架构模式》 没看过 Spring3.x企业应用开发实战 这本书适合初学者看或者当做一本参考书。对于提高者而言，略看就行 Spring揭秘 没看过 Java程序性能优化:让你的Java程序更快、更稳定 没看过 总结talk is less show me your code，希望大家有好的书籍也可以推荐]]></content>
      <categories>
        <category>读书系统</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>书籍推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-PRM软件包管理工具]]></title>
    <url>%2F2017%2F06%2F13%2Flinux-PRM%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[1.1 概述RPM（RedHat Package Manager），Rethat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。 RPM包的名称格式 Apache-1.3.23-11.i386.rpm - “apache” 软件名称 - “1.3.23-11”软件的版本号，主版本和此版本 - “i386”是软件所运行的硬件平台 - “rpm”文件扩展名，代表RPM包 1.2 常用命令1.2.1 查询（rpm -qa）1）基本语法： rpm -qa （功能描述：查询所安装的所有rpm软件包） 过滤 rpm -qa | grep rpm软件包 2）案例 [root@hadoop101 Packages]# rpm -qa |grep firefox firefox-45.0.1-1.el6.centos.x86_64 1.2.2 卸载（rpm -e）1）基本语法： （1）rpm -e RPM软件包 或者（2） rpm -e –nodeps 软件包 –nodeps 如果该RPM包的安装依赖其它包，即使其它包没装，也强迫安装。 2）案例 [root@hadoop101 Packages]# rpm -e firefox 1.2.3 安装（rpm -ivh）1）基本语法： ​ rpm -ivh RPM包全名 ​ -i=install，安装 ​ -v=verbose，显示详细信息 ​ -h=hash，进度条 ​ –nodeps，不检测依赖进度 2）案例 [root@hadoop101 Packages]# pwd /media/CentOS_6.8_Final/Packages [root@hadoop101 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm warning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY Preparing… ########################################### [100%] 1:firefox ########################################### [100%]]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux基本操作命令]]></title>
    <url>%2F2017%2F06%2F13%2Flinux%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[一、常用基本命令此文章紧接linux基础文章，如果没有安装linux，那么情先参见（linux基础 ） 1.1 帮助命令1.1.1 man 获得帮助信息1）基本语法： ​ man [命令或配置文件] （功能描述：获得帮助信息） ​ （1）显示说明 NAME 命令的名称和单行描述 SYNOPSIS 怎样使用命令 DESCRIPTION 命令功能的深入讨论 EXAMPLES 怎样使用命令的例子 SEE ALSO 相关主题（通常是手册页） （2）数字说明 1.用户在shell环境中可以操作的命令或是可执行的文件 2.系统内核(kernel)可以调用的函数 3.常用的函数or函数库 4.设备配置文件 5.配置文件的格式 6.游戏相关 7.linux网络协议和文件系统 8.系统管理员可以用的命令 9.跟内核有关系的文件 2）案例 [root@hadoop106 home]# man ls 1.1.2 help 获得shell内置命令的帮助信息1）基本语法： ​ help 命令 （功能描述：获得shell内置命令的帮助信息） 2）案例： ​ [root@hadoop101 bin]# help cd 1.1.3 常用快捷键1）ctrl + c：停止进程 2）ctrl+l：清屏 3）ctrl + q：退出 4）善于用tab键 5）上下键：查找执行过的命令 6）ctrl +alt：linux和Windows之间切换 1.2 文件目录类1.2.1 pwd 显示当前工作目录的绝对路径1）基本语法： ​ pwd （功能描述：显示当前工作目录的绝对路径） ​ 2）案例 [root@hadoop101 home]# pwd /home 1.2.2 ls 列出目录的内容1）基本语法： ls [选项] [目录或是文件] 选项： -a ：全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用) -l ：长数据串列出，包含文件的属性与权限等等数据；(常用) 每行列出的信息依次是： 文件类型与权限 链接数 文件属主 文件属组 文件大小用byte来表示 建立或最近修改的时间 名字 2）案例 [kingge@hadoop101 ~]$ ls -al 总用量 44 drwx——. 5 kingge kingge 4096 5月 27 15:15 . drwxr-xr-x. 3 root root 4096 5月 27 14:03 .. drwxrwxrwx. 2 root root 4096 5月 27 14:14 hello -rwxrw-r–. 1 kingge kingge 34 5月 27 14:20 test.txt 1.2.3 mkdir 创建一个新的目录1）基本语法： ​ mkdir [-p] 要创建的目录 ​ 选项： -p：创建多层目录 2）案例 [root@hadoop101 opt]# mkdir test [root@hadoop101 opt]# mkdir -p user/kingge 1.2.4 rmdir 删除一个空的目录1）基本语法： ​ rmdir 要删除的空目录 2）案例 [root@hadoop101 opt]# mkdir test [root@hadoop101 opt]# rmdir test 1.2.5 touch 创建空文件1）基本语法： ​ touch 文件名称 2）案例 [root@hadoop101 opt]# touch test.java 1.2.6 cd 切换目录1）基本语法： ​ （1）cd 绝对路径 ​ （2）cd 相对路径 ​ （3）cd ~或者cd （功能描述：回到自己的家目录） ​ （4）cd - （功能描述：回到上一次所在目录） ​ （5）cd .. （功能描述：回到当前目录的上一级目录） ​ （6）cd -P （功能描述：跳转到实际物理路径，而非快捷方式路径） 2）案例 （1）使用 mkdir 命令创建kingge目录 [root@hadoop101 ~]# mkdir kingge （2）使用绝对路径切换到kingge目录 [root@hadoop101 ~]# cd /root/kingge/ （3）使用相对路径切换到kingge目录 [root@hadoop101 ~]# cd ./kingge/ （4）表示回到自己的家目录，亦即是 /root 这个目录 [root@hadoop101 kingge]# cd ~ （5）cd- 回到上一次所在目录 [root@hadoop101 kingge]# cd - （6）表示回到当前目录的上一级目录，亦即是 /root 的上一级目录的意思； [root@hadoop101 ~]# cd .. 1.2.7 cp 复制文件或目录1）基本语法： （1）cp source dest （功能描述：复制source文件到dest） （2）cp -r sourceFolder targetFolder （功能描述：递归复制整个文件夹） 2）案例 （1）复制文件 [root@hadoop101 opt]# cp test.java test （2）递归复制整个文件夹 [root@hadoop101 opt]# cp -r test test1 1.2.8 rm 移除文件或目录1）基本语法 ​ （1）rmdir deleteEmptyFolder （功能描述：删除空目录） （2）rm -rf deleteFile （功能描述：递归删除目录中所有内容） 2）案例 ​ 1）删除空目录 [root@hadoop101 opt]# rmdir test 2）递归删除目录中所有内容 [root@hadoop101 opt]# rm -rf test1 1.2.9 mv 移动文件与目录或重命名1）基本语法： ​ （1）mv oldNameFile newNameFile （功能描述：重命名） ​ （2）mv /temp/movefile /targetFolder （功能描述：递归移动文件） 2）案例： ​ 1）重命名 [root@hadoop101 opt]# mv test.java test1.java 2）移动文件 [root@hadoop101 opt]# mv test1.java test1 1.2.10 cat 查看文件内容查看文件内容，从第一行开始显示。 1）基本语法 ​ cat [选项] 要查看的文件 选项： -A ：相当于 -vET 的整合选项，可列出一些特殊字符而不是空白而已； -b ：列出行号，仅针对非空白行做行号显示，空白行不标行号！ -E ：将结尾的断行字节 $ 显示出来； -n ：列出行号，连同空白行也会有行号，与 -b 的选项不同； -T ：将 [tab] 按键以 ^I 显示出来； -v ：列出一些看不出来的特殊字符 2）案例 [kingge@hadoop101 ~]$ cat -A test.txt hellda $ dasadf ^I$ da^I^I^I$ das$ 1.2.11 tac查看文件内容查看文件内容，从最后一行开始显示，可以看出 tac 是 cat 的倒著写。 1）基本语法： ​ tac [选项参数] 要查看的文件 2）案例 [root@hadoop101 test1]# cat test1.java hello kingge kingge1 [root@hadoop101 test1]# tac test1.java kingge1 kingge hello 1.2.12 more 查看文件内容查看文件内容，一页一页的显示文件内容。 1）基本语法： ​ more 要查看的文件 2）功能使用说明 空白键 (space)：代表向下翻一页； Enter:代表向下翻『一行』； q:代表立刻离开 more ，不再显示该文件内容。 Ctrl+F 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 :f 输出文件名和当前行的行号 3）案例 [root@hadoop101 test1]# more test1.java 1.2.13 less 查看文件内容less 的作用与 more 十分相似，都可以用来浏览文字档案的内容，不同的是 less 允许使用[pageup] [pagedown]往回滚动。 1）基本语法： ​ less 要查看的文件 2）功能使用说明 空白键 ：向下翻动一页； [pagedown]：向下翻动一页； [pageup] ：向上翻动一页； /字串 ：向下搜寻『字串』的功能；n：向下查找；N：向上查找； ?字串 ：向上搜寻『字串』的功能；n：向上查找；N：向下查找； q ：离开 less 这个程序； 3）案例 [root@hadoop101 test1]# less test1.java 1.2.14 head查看文件内容查看文件内容，只看头几行。 1）基本语法 head -n 10 文件 （功能描述：查看文件头10行内容，10可以是任意行数） 2）案例 [root@hadoop101 test1]# head -n 2 test1.java hello kingge 1.2.15 tail 查看文件内容查看文件内容，只看尾巴几行。 1）基本语法 （1）tail -n 10 文件 （功能描述：查看文件头10行内容，10可以是任意行数） （2）tail -f 文件 （功能描述：实时追踪该文档的所有更新） 2）案例 （1）查看文件头1行内容 [root@hadoop101 test1]# tail -n 1 test1.java kingge （2）实时追踪该档的所有更新 [root@hadoop101 test1]# tail -f test1.java hello kingge kingge 1.2.16 重定向命令1）基本语法： （1）ls -l &gt;文件 （功能描述：列表的内容写入文件a.txt中（覆盖写）） （2）ls -al &gt;&gt;文件 （功能描述：列表的内容追加到文件aa.txt的末尾） 2）案例 ​ （1）[root@hadoop101 opt]# ls -l&gt;t.txt （2）[root@hadoop101 opt]# ls -l&gt;&gt;t.txt （3）[root@hadoop101 test1]# echo hello&gt;&gt;test1.java 1.2.17 echo1）基本语法： （1）echo 要显示的内容 &gt;&gt; 存储内容的的文件 （功能描述：将要显示的内容，存储到文件中） ​ （2）echo 变量 （功能描述：显示变量的值） 2）案例 [root@hadoop101 test1]# echo $JAVA_HOME /opt/module/jdk1.7.0_79 1.2.18 ln软链接1）基本语法： ln -s [原文件] [目标文件] （功能描述：给原文件创建一个软链接，软链接存放在目标文件目录） 删除软链接： rm -rf kingge，而不是rm -rf kingge/ 2）案例： [root@hadoop101 module]# ln -s /opt/module/test.txt /opt/t.txt [root@hadoop101 opt]# ll lrwxrwxrwx. 1 root root 20 6月 17 12:56 t.txt -&gt; /opt/module/test.txt 创建一个软链接 [kingge@hadoop101 opt]$ ln -s /opt/module/hadoop-2.7.2/ /opt/software/hadoop cd不加参数进入是软链接的地址 [kingge@hadoop101 software]$ cd hadoop [kingge@hadoop101 hadoop]$ pwd /opt/software/hadoop cd加参数进入是实际的物理地址 [kingge@hadoop101 software]$ cd -P hadoop [kingge@hadoop101 hadoop-2.7.2]$ pwd /opt/module/hadoop-2.7.2 1.2.19 history查看所敲命令历史1）基本语法： ​ history 2）案例 [root@hadoop101 test1]# history 1.3 时间日期类1）基本语法 date [OPTION]… [+FORMAT] 1.3.1 date显示当前时间1）基本语法： ​ （1）date （功能描述：显示当前时间） ​ （2）date +%Y （功能描述：显示当前年份） （3）date +%m （功能描述：显示当前月份） （4）date +%d （功能描述：显示当前是哪一天） （5）date +%Y%m%d date +%Y/%m/%d … （功能描述：显示当前年月日各种格式 ） ​ （6）date “+%Y-%m-%d %H:%M:%S” （功能描述：显示年月日时分秒） 2）案例 [root@hadoop101 /]# date 2017年 06月 19日 星期一 20:53:30 CST [root@hadoop101 /]# date +%Y%m%d 20170619 [root@hadoop101 /]# date “+%Y-%m-%d %H:%M:%S” 2017-06-19 20:54:58 1.3.2 date显示非当前时间1）基本语法： （1）date -d ‘1 days ago’ （功能描述：显示前一天日期） （2）date -d yesterday +%Y%m%d （同上） （3）date -d next-day +%Y%m%d （功能描述：显示明天日期） （4）date -d ‘next monday’ （功能描述：显示下周一时间） 2）案例： [root@hadoop101 /]# date -d ‘1 days ago’ 2017年 06月 18日 星期日 21:07:22 CST [root@hadoop101 /]# date -d next-day +%Y%m%d 20170620 [root@hadoop101 /]# date -d ‘next monday’ 2017年 06月 26日 星期一 00:00:00 CST 1.3.3 date设置系统时间1）基本语法： ​ date -s 字符串时间 2）案例 ​ [root@hadoop106 /]# date -s “2017-06-19 20:52:18” 1.3.4 cal查看日历1）基本语法： cal [选项] （功能描述：不加选项，显示本月日历） 选项： -3 ，显示系统前一个月，当前月，下一个月的日历 具体某一年，显示这一年的日历。 2）案例： [root@hadoop101 /]# cal [root@hadoop101 /]# cal -3 ​ [root@hadoop101 /]# cal 2016 1.4 用户管理命令1.4.1 useradd 添加新用户1）基本语法： ​ useradd 用户名 （功能描述：添加新用户） 2）案例： ​ [root@hadoop101 opt]# user kingge 1.4.2 passwd 设置用户密码1）基本语法： ​ passwd 用户名 （功能描述：设置用户密码） 2）案例 ​ [root@hadoop101 opt]# passwd kingge 1.4.3 id 判断用户是否存在1）基本语法： ​ id 用户名 2）案例： ​ [root@hadoop101 opt]#id kingge 1.4.4 su 切换用户1）基本语法： su 用户名称 （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量） su - 用户名称 （功能描述：切换到用户并获得该用户的环境变量及执行权限） 2）案例 [root@hadoop101 opt]#su kingge [root@hadoop101 opt]#su - kingge 1.4.5 userdel 删除用户1）基本语法： ​ （1）userdel 用户名 （功能描述：删除用户但保存用户主目录） （2）userdel -r 用户名 （功能描述：用户和用户主目录，都删除） 2）案例： （1）删除用户但保存用户主目录 ​ [root@hadoop101 opt]#userdel kingge （2）删除用户和用户主目录，都删除 ​ [root@hadoop101 opt]#userdel -r kingge 1.4.6 who 查看登录用户信息1）基本语法 ​ （1）whoami （功能描述：显示自身用户名称） （2）who am i （功能描述：显示登录用户的用户名） （3）who （功能描述：看当前有哪些用户登录到了本台机器上） 2）案例 [root@hadoop101 opt]# whoami [root@hadoop101 opt]# who am i ​ [root@hadoop101 opt]# who 1.4.7 设置kingge普通用户具有root权限1）修改配置文件 修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) ALL或者配置成采用sudo命令时，不需要输入密码 Allow root to run any commands anywhere root ALL=(ALL) ALL kingge ALL=(ALL) NOPASSWD:ALL修改完毕，现在可以用kingge帐号登录，然后用命令 su - ，即可获得root权限进行操作。 2）案例 [kingge@hadoop101 opt]$ sudo mkdir module [root@hadoop101 opt]# chown kingge:kingge module/ 1.4.8 cat /etc/passwd 查看创建了哪些用户cat /etc/passwd 1.4.9 usermod修改用户1）基本语法： usermod -g 用户组 用户名 2）案例： 将用户kingge加入dev用户组 [root@hadoop101 opt]#usermod -g dev kingge 1.5 用户组管理命令每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同， 如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 1.5.1 groupadd 新增组1）基本语法 groupadd 组名 2）案例： ​ 添加一个kingge组 [root@hadoop101 opt]#groupadd kingge 1.5.2 groupdel删除组1）基本语法： groupdel 组名 2）案例 [root@hadoop101 opt]# groupdel kingge 1.5.3 groupmod修改组1）基本语法： groupmod -n 新组名 老组名 2）案例 ​ 修改kingge组名称为kingge1 [root@hadoop101 kingge]# groupmod -n kingge1 kingge 1.5.4 cat /etc/group 查看创建了哪些组cat /etc/group 1.5.5 综合案例[root@hadoop101 kingge]# groupadd dev [root@hadoop101 kingge]# groupmod -n device dev [root@hadoop101 kingge]# usermod -g device kingge [root@hadoop101 kingge]# su kingge [kingge@hadoop101 ~]$ mkdir kingge [kingge@hadoop101 ~]$ ls -l drwxr-xr-x. 2 kingge device 4096 5月 27 16:31 kingge [root@hadoop101 kingge]# usermod -g kingge kingge 1.6 文件权限类1.6.1 文件属性Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls –l命令来显示一个文件的属性以及文件所属的用户和组。 1）从左到右的10个字符表示： 如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示: （1）0首位表示类型 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等 - 代表文件 d 代表目录 c 字符流，装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置) s socket p 管道 l 链接文档(link file)； b 设备文件，装置文件里面的可供储存的接口设备(可随机存取装置) （2）第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User （3）第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group （4）第7-9位确定其他用户拥有该文件的权限 —Other 文件类型 属主权限 属组权限 其他用户权限 0 1 2 3 4 5 6 7 8 9 d R w x R - x R - x 目录文件 读 写 执行 读 写 执行 读 写 执行 2）rxw作用文件和目录的不同解释 （1）作用到文件： [ r ]代表可读(read): 可以读取，查看 [ w ]代表可写(write): 可以修改，但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件. [ x ]代表可执行(execute):可以被系统执行 （2）作用到目录： [ r ]代表可读(read): 可以读取，ls查看目录内容 [ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录 [ x ]代表可执行(execute):可以进入该目录 3）案例 [kingge@hadoop101 ~]$ ls -l 总用量 8 drwxrwxr-x. 2 kingge kingge 4096 5月 27 14:14 hello -rw-rw-r–. 1 kingge kingge 34 5月 27 14:20 test.txt （1）如果查看到是文件：链接数指的是硬链接个数。创建硬链接方法 ln [原文件] [目标文件] [root@hadoop101 xiyou]# ln sunhouzi/shz.txt ./shz.txt （2）如果查看的是文件夹：链接数指的是子文件夹个数。 [root@hadoop101 xiyou]# ls -al kingge/ 总用量 8 drwxr-xr-x. 2 root root 4096 9月 3 19:02 . drwxr-xr-x. 5 root root 4096 9月 3 21:21 .. 1.6.2 chmod改变权限1）基本语法： ​ chmod [{ugoa}{+-=}{rwx}] [文件或目录] [mode=421 ] [文件或目录] 2）功能描述 改变文件或者目录权限 文件: r-查看；w-修改；x-执行文件 目录: r-列出目录内容；w-在目录中创建和删除；x-进入目录 删除一个文件的前提条件:该文件所在的目录有写权限，你才能删除该文件。 3）案例 [root@hadoop101 test1]# chmod u+x test1.java [root@hadoop101 test1]# chmod g+x test1.java [root@hadoop101 test1]# chmod o+x test1.java [root@hadoop101 test1]# chmod 777 test1.java [root@hadoop101 test1]# chmod -R 777 testdir 1.6.3 chown改变所有者1）基本语法： chown [最终用户] [文件或目录] （功能描述：改变文件或者目录的所有者） 2）案例 [root@hadoop101 test1]# chown kingge test1.java [root@hadoop101 test1]# ls -al -rwxr-xr-x. 1 kingge kingge 551 5月 23 13:02 test1.java 修改前： [root@hadoop101 xiyou]# ll drwxrwxrwx. 2 root root 4096 9月 3 21:20 sunhouzi 修改后 [root@hadoop101 xiyou]# chown -R kingge:kingge sunhouzi/ [root@hadoop101 xiyou]# ll drwxrwxrwx. 2 kingge kingge 4096 9月 3 21:20 sunhouzi 1.6.4 chgrp改变所属组1）基本语法： ​ chgrp [最终用户组] [文件或目录] （功能描述：改变文件或者目录的所属组） 2）案例 [root@hadoop101 test1]# chgrp kingge test1.java [root@hadoop101 test1]# ls -al -rwxr-xr-x. 1 root kingge 551 5月 23 13:02 test1.java 1.6.5 su 切换用户1）基本语法： su -username （功能描述：切换用户） 2）案例 [root@hadoop101 kingge]# su kingge [kingge@hadoop101 ~]$ [kingge@hadoop101 ~]$ su root 密码： [root@hadoop101 kingge]# 1.7 磁盘分区类1.7.1 fdisk查看分区1）基本语法： ​ fdisk -l （功能描述：查看磁盘分区详情） ​ 注意：在root用户下才能使用 2）功能说明： ​ （1）Linux分区 这个硬盘是20G的，有255个磁面；63个扇区；2610个磁柱；每个 cylinder（磁柱）的容量是 8225280 bytes=8225.280 K（约为）=8.225280M（约为）； Device Boot Start End Blocks Id System 分区序列 引导 从X磁柱开始 到Y磁柱结束 容量 分区类型ID 分区类型 （2）Win7分区 3）案例 [root@hadoop101 /]# fdisk -l Disk /dev/sda: 21.5 GB, 21474836480 bytes 255 heads, 63 sectors/track, 2610 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x0005e654 Device Boot Start End Blocks Id System /dev/sda1 * 1 26 204800 83 Linux Partition 1 does not end on cylinder boundary. /dev/sda2 26 1332 10485760 83 Linux /dev/sda3 1332 1593 2097152 82 Linux swap / Solaris 1.7.2 df查看硬盘1）基本语法： ​ df 参数 （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况） 参数： -a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统； -k ：以 KBytes 的容量显示各文件系统； -m ：以 MBytes 的容量显示各文件系统； -h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示； -H ：以 M=1000K 取代 M=1024K 的进位方式； -T ：显示文件系统类型，连同该 partition 的 filesystem 名称 (例如 ext3) 也列出； -i ：不用硬盘容量，而以 inode 的数量来显示 2）案例 [root@hadoop101 ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda2 15G 3.5G 11G 26% / tmpfs 939M 224K 939M 1% /dev/shm /dev/sda1 190M 39M 142M 22% /boot 1.7.3 mount/umount挂载/卸载对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构 Linux中每个分区都是用来组成整个文件系统的一部分，她在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。 0**）挂载前准备（必须要有光盘或者已经连接镜像文件）** 1**）挂载光盘语法：** mount [-t vfstype] [-o options] device dir （1）-t vfstype 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。 常用类型有： 光盘或光盘镜像：iso9660 DOS fat16文件系统：msdos Windows 9x fat32文件系统：vfat Windows NT ntfs文件系统：ntfs Mount Windows文件网络共享：smbfs UNIX(LINUX) 文件网络共享：nfs （2）-o options 主要用来描述设备或档案的挂接方式。常用的参数有： loop：用来把一个文件当成硬盘分区挂接上系统 ro：采用只读方式挂接设备 rw：采用读写方式挂接设备 iocharset：指定访问文件系统所用字符集 （3）device 要挂接(mount)的设备 （4）dir设备在系统上的挂接点(mount point) 2**）案例** （1）光盘镜像文件的挂载 [root@hadoop101 ~]# mkdir /mnt/cdrom/ 建立挂载点 [root@hadoop101 ~]# mount -t iso9660 /dev/cdrom /mnt/cdrom/ 设备/dev/cdrom挂载到 挂载点 ： /mnt/cdrom中 [root@hadoop101 ~]# ll /mnt/cdrom/ 3**）卸载光盘语法：** [root@hadoop101 ~]# umount 设备文件名或挂载点 4**）案例** [root@hadoop101 ~]# umount /mnt/cdrom 5**）开机自动挂载语法：** [root@hadoop101 ~]# vi /etc/fstab 添加红框中内容，保存退出。 1.8 搜索查找类1.8.1 find 查找文件或者目录1）基本语法： ​ find [搜索范围] [匹配条件] 2）案例 （1）按文件名：根据名称查找/目录下的filename.txt文件。 [root@hadoop101 ~]# find /opt/ -name *.txt （2）按拥有者：查找/opt目录下，用户名称为-user的文件 [root@hadoop101 ~]# find /opt/ -user kingge ​ （3）按文件大小：在/home目录下查找大于200m的文件（+n 大于 -n小于 n等于） [root@hadoop101 ~]find /home -size +204800 1.8.2 grep 过滤查找及“|”管道符0）管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理 1）基本语法 grep+参数+查找内容+源文件 参数： -c：只输出匹配行的计数。 -I：不区分大小写(只适用于单字符)。 -h：查询多文件时不显示文件名。 -l：查询多文件时只输出包含匹配字符的文件名。 -n：显示匹配行及行号。 -s：不显示不存在或无匹配文本的错误信息。 -v：显示不包含匹配文本的所有行。 2）案例 [root@hadoop101 opt]# ls | grep -n test 4:test1 5:test2 1.8.3 which 文件搜索命令1）基本语法： ​ which 命令 （功能描述：搜索命令所在目录及别名信息） 2）案例 ​ [root@hadoop101 opt]# which ls ​ /bin/ls 1.9 进程线程类进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。 1.9.1 ps查看系统中所有进程1）基本语法： ​ ps -aux （功能描述：查看系统中所有进程） 2）功能说明 ​ USER：该进程是由哪个用户产生的 ​ PID：进程的ID号 %CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源； %MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源； VSZ：该进程占用虚拟内存的大小，单位KB； RSS：该进程占用实际物理内存的大小，单位KB； TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。 STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台 START：该进程的启动时间 TIME：该进程占用CPU的运算时间，注意不是系统时间 COMMAND：产生此进程的命令名 3）案例 ​ [root@hadoop101 datas]# ps -aux 1.9.2 top查看系统健康状态1）基本命令 ​ top [选项] ​ （1）选项： ​ -d 秒数：指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令： -i：使top不显示任何闲置或者僵死进程。 -p：通过指定监控进程ID来仅仅监控某个进程的状态。 ​ （2）操作选项： P： 以CPU使用率排序，默认就是此项 M： 以内存的使用率排序 N： 以PID排序 q： 退出top ​ （3）查询结果字段解释 第一行信息为任务队列信息 内容 说明 12:26:46 系统当前时间 up 1 day, 13:32 系统的运行时间，本机已经运行1天 13小时32分钟 2 users 当前登录了两个用户 load average: 0.00, 0.00, 0.00 系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。 第二行为进程信息 Tasks: 95 total 系统中的进程总数 1 running 正在运行的进程数 94 sleeping 睡眠的进程 0 stopped 正在停止的进程 0 zombie 僵尸进程。如果不是0，需要手工检 查僵尸进程 第三行为CPU信息 Cpu(s): 0.1%us 用户模式占用的CPU百分比 0.1%sy 系统模式占用的CPU百分比 0.0%ni 改变过优先级的用户进程占用的CPU百分比 99.7%id 空闲CPU的CPU百分比 0.1%wa 等待输入/输出的进程的占用CPU百分比 0.0%hi 硬中断请求服务占用的CPU百分比 0.1%si 软中断请求服务占用的CPU百分比 0.0%st st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。 第四行为物理内存信息 Mem: 625344k total 物理内存的总量，单位KB 571504k used 已经使用的物理内存数量 53840k free 空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了 65800k buffers 作为缓冲的内存数量 第五行为交换分区（swap）信息 Swap: 524280k total 交换分区（虚拟内存）的总大小 0k used 已经使用的交互分区的大小 524280k free 空闲交换分区的大小 409280k cached 作为缓存的交互分区的大小 2）案例 ​ [root@hadoop101 kingge]# top -d 1 [root@hadoop101 kingge]# top -i [root@hadoop101 kingge]# top -p 2575 执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。 1.9.3 pstree查看进程树1）基本语法： ​ pstree [选项] ​ 选项 -p： 显示进程的PID -u： 显示进程的所属用户 2）案例： ​ [root@hadoop101 datas]# pstree -u [root@hadoop101 datas]# pstree -p 1.9.4 kill终止进程1）基本语法： ​ kill -9 pid进程号 ​ 选项 -9 表示强迫进程立即停止 2）案例： ​ 启动mysql程序 ​ 切换到root用户执行 ​ [root@hadoop101 桌面]# kill -9 5102 1.9.5 netstat显示网络统计信息1）基本语法： ​ netstat -anp （功能描述：此命令用来显示整个系统目前的网络情况。例如目前的连接、数据包传递数据、或是路由表内容） ​ 选项： ​ -an 按一定顺序排列输出 ​ -p 表示显示哪个进程在调用 ​ -nltp 查看tcp协议进程端口号 2）案例 查看端口50070的使用情况 [root@hadoop101 hadoop-2.7.2]# netstat -anp | grep 50070 tcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 6816/java ​ 端口号 进程号 1.9.6 前后台进程切换1）基本语法： fg %1 （功能描述：把后台进程转换成前台进程） ctrl+z bg %1 （功能描述：把前台进程发到后台） 1.10 压缩和解压类1.10.1 gzip/gunzip压缩1）基本语法： gzip+文件 （功能描述：压缩文件，只能将文件压缩为*.gz文件） gunzip+文件.gz （功能描述：解压缩文件命令） 2）特点： （1）只能压缩文件不能压缩目录 （2）不保留原来的文件 3）案例 （1）gzip压缩 [root@hadoop101 opt]# ls test.java [root@hadoop101 opt]# gzip test.java [root@hadoop101 opt]# ls test.java.gz （2）gunzip解压缩文件 [root@hadoop101 opt]# gunzip test.java.gz [root@hadoop101 opt]# ls test.java 1.10.2 zip/unzip压缩1）基本语法： zip + 参数 + XXX.zip + 将要压缩的内容 （功能描述：压缩文件和目录的命令，window/linux通用且可以压缩目录且保留源文件） 参数： -r 压缩目录 2）案例： （1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip [root@hadoop101 opt]# zip test.zip test1.java test.java adding: test1.java (stored 0%) adding: test.java (stored 0%) [root@hadoop101 opt]# ls test1.java test.java test.zip （2）解压 mypackage.zip [root@hadoop101 opt]# unzip test.zip Archive: test.zip extracting: test1.java extracting: test.java ​ [root@hadoop101 opt]# ls test1.java test.java test.zip 1.10.3 tar打包1）基本语法： tar + 参数 + XXX.tar.gz + 将要打包进去的内容 （功能描述：打包目录，压缩后的文件格式.tar.gz） 参数： -c 产生.tar打包文件 -v 显示详细信息 -f 指定压缩后的文件名 -z 打包同时压缩 -x 解包.tar文件 2）案例 （1）压缩：tar -zcvf XXX.tar.gz n1.txt n2.txt ​ 压缩多个文件 [root@hadoop101 opt]# tar -zcvf test.tar.gz test1.java test.java test1.java test.java [root@hadoop101 opt]# ls test1.java test.java test.tar.gz 压缩目录 [root@hadoop101 opt]# tar -zcvf test.java.tar.gz test1 test1/ test1/hello test1/test1.java test1/test/ test1/test/test.java [root@hadoop106 opt]# ls test1 test.java.tar.gz （2）解压：tar -zxvf XXX.tar.gz ​ 解压到当前目录 [root@hadoop101 opt]# tar -zxvf test.tar.gz 解压到/opt目录 [root@hadoop101 opt]# tar -zxvf test.tar.gz -C /opt .11 后台服务管理类.11.1 service后台服务管理1）service network status 查看指定服务的状态 2）service network stop 停止指定服务 3）service network start 启动指定服务 4）service network restart 重启指定服务 5）service –status-all 查看系统中所有的后台服务 .11.2 chkconfig设置后台服务的自启配置1）chkconfig 查看所有服务器自启配置 2）chkconfig iptables off 关掉指定服务的自动启动 3）chkconfig iptables on 开启指定服务的自动启动 1.12 crond系统定时任务1.12.1 crond服务管理[root@hadoop101 ~]# service crond restart （重新启动服务） 1.12.2 crontab定时任务设置1）基本语法 crontab [选项] 选项： -e： 编辑crontab定时任务 -l： 查询crontab任务 -r： 删除当前用户所有的crontab任务 2）参数说明 ​ [root@hadoop101 ~]# crontab -e （1）进入crontab编辑界面。会打开vim编辑你的工作。 * 执行的任务 项目 含义 范围 第一个“*” 一小时当中的第几分钟 0-59 第二个“*” 一天当中的第几小时 0-23 第三个“*” 一个月当中的第几天 1-31 第四个“*” 一年当中的第几月 1-12 第五个“*” 一周当中的星期几 0-7（0和7都代表星期日） （2）特殊符号 特殊符号 含义 * 代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。 ， 代表不连续的时间。比如“0 8,12,16 * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 - 代表连续的时间范围。比如“0 5 1-6命令”，代表在周一到周六的凌晨5点0分执行命令 */n 代表每隔多久执行一次。比如“/10 * 命令”，代表每隔10分钟就执行一遍命令 （3）特定时间执行命令 时间 含义 45 22 * 命令 在22点45分执行命令 0 17 1 命令 每周1 的17点0分执行命令 0 5 1,15 命令 每月1号和15号的凌晨5点0分执行命令 40 4 1-5 命令 每周一到周五的凌晨4点40分执行命令 /10 4 命令 每天的凌晨4点，每隔10分钟执行一次命令 0 0 1,15 * 1 命令 每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。 3）案例： /5 * /bin/echo ”11” &gt;&gt; /tmp/test]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux基础]]></title>
    <url>%2F2017%2F06%2F12%2Flinux%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[一 、Linux入门概述linux 系统也是接触了许久，不过一直没有机会总结一下，所以决定出个linux相关安装和配置以及常用指令的专栏。 1.1 概述​ Linux内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。 ​ Linux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。Linux能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 ​ 目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debain、Fedora、SuSE、OpenSUSE ​ 下面的操作，我使用的是Centos 1.2 下载地址centos下载地址： 网易镜像：http://mirrors.163.com/centos/6/isos/ 1.3 Linux特点 Linux里面一切皆是文件 Linux里面没有后缀名这一说 1.4 Linux和Windows区别目前国内Linux更多的是应用在服务器上，而桌面操作系统更多使用的是window。主要区别如下。 二 、VM安装相关（运行环境）2.1 安装VMWare虚拟机 详情这里就不说了，自行百度。 2.2 安装CentOS ​ 需要注意的是：下面的步骤，按需省略。 1 检查BIOS虚拟化支持 2 新建虚拟机 3 新建虚拟机向导 4 创建虚拟空白光盘 5 安装Linux系统对应的CentOS版 6 虚拟机命名和定位磁盘位置 7 处理器配置，看自己是否是双核、多核 8 设置内存为2GB 9 网络设置NAT 10 选择IO控制器类型 11 选择磁盘类型 12 新建虚拟磁盘 13 设置磁盘容量 14 你在哪里存储这个磁盘文件 15 新建虚拟机向导配置完成 16 VM设置 17 加载ISO 18 加电并安装配置CentOS 19 加电后初始化欢迎进入页面 回车选择第一个开始安装配置，此外，在Ctrl+Alt可以实现Windows主机和VM之间窗口的切换 20 是否对CD媒体进行测试，直接跳过**Skip** 21 CentOS欢迎页面，直接点击Next 22 选择简体中文进行安装 23 选择语言键盘 23 选择存储设备 24 给计算机起名 25 设置网络环境 安装成功后再设置。 26 选择时区 27 设置root密码 （一定记住） 28 硬盘分区-1 29 根分区新建 l Boot l swap分区设置 l 分区完成 30 程序引导，直接下一步 31 现在定制系统软件 32 Web环境 33 可扩展文件系统支持 34 基本系统 35 应用程序 36 开发、弹性存储、数据库、服务器 可以都不勾，有需要，以后使用中有需要再手动安装 37 桌面 除了KDE，其他都选就可以了。 38 语言支持 39 系统管理、虚拟化、负载平衡器、高可用性可以都不选 40 完成配置，开始安装CentOS 41 等待安装完成，等待等待等待等待……20分钟左右 42 安装完成，重新引导 43 欢迎引导页面 44 许可证 45 创建用户，可以先不创建，用root账户登录就行 46 时间和日期 47 Kdump,去掉 48 重启后用root登录 2.3 安装VMTools工具1）什么是VMtools VM tools顾名思义就是Vmware的一组工具。主要用于虚拟主机显示优化与调整，另外还可以方便虚拟主机与本机的交互，如允许共享文件夹，甚至可以直接从本机向虚拟主机拖放文件、鼠标无缝切换、显示分辨率调整等，十分实用。 安装过程自行百度 三 、Linux目录结构3.1 概览 3.2 树状目录结构 /bin：是Binary的缩写，这个目录存放着系统必备执行命令/boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件，自己的安装别放这里/dev：Device(设备)的缩写，该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。/etc：所有的系统管理所需要的配置文件和子目录。/home：存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。/lib：系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。/lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。/media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。/misc: 该目录可以用来存放杂项文件或目录，即那些用途或含义不明确的文件或目录可以存放在该目录下。/mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。/net 存放着和网络相关的一些文件./opt：这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。/proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。/root：该目录为系统管理员，也称作超级权限者的用户主目录。/sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。/selinux：这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙/srv：service缩写，该目录存放一些服务启动之后需要提取的数据。/sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。/tmp：这个目录是用来存放一些临时文件的。/usr： 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。/var：这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 四 VI/VIM编辑器4.1 概述所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。但是目前我们使用比较多的是 vim 编辑器。 Vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 简单的来说vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。vim 则可以说是程序开发者的一项很好用的工具。连vim 的官方网站 (http://www.vim.org) 自己也说 vim 是一个程序开发工具而不是文字处理软件。 4.2 一般模式以 vi 打开一个档案就直接进入一般模式了(这是默认的模式)。在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、粘贴』来处理你的文件数据。 常用语法 1）yy （功能描述：复制光标当前一行） y数字y （功能描述：复制一段(从第几行到第几行)） 2）p （功能描述：箭头移动到目的行粘贴） 3）u （功能描述：撤销上一步） 4）dd （功能描述：删除光标当前行） d数字d （功能描述：删除光标(含)后多少行） 5）x （功能描述：删除一个字母，相当于del） X （功能描述：删除一个字母，相当于Backspace） 6）yw （功能描述：复制一个词） 7）dw （功能描述：删除一个词） 8）shift+^ （功能描述：移动到行头） 9）shift+$ （功能描述：移动到行尾） 10）1+shift+g （功能描述：移动到页头，数字） 11）shift+g （功能描述：移动到页尾） 12）数字N+shift+g （功能描述：移动到目标行） 4.3 编辑模式在一般模式中可以进行删除、复制、贴上等等的动作，但是却无法编辑文件内容的！ 要等到你按下『i, I, o, O, a, A, r, R』等任何一个字母之后才会进入编辑模式。 注意了！通常在 Linux 中，按下这些按键时，在画面的左下方会出现『INSERT 或 REPLACE 』的字样，此时才可以进行编辑。而如果要回到一般模式时， 则必须要按下『Esc』这个按键即可退出编辑模式。 常用语法 1）进入编辑模式 （1）i 当前光标前 （2）a 当前光标后 （3）o 当前光标行的下一行 2）退出编辑模式 按『Esc』键 4.4 指令模式在一般模式当中，输入『 : / ?』3个中的任何一个按钮，就可以将光标移动到最底下那一行。 在这个模式当中， 可以提供你『搜寻资料』的动作，而读取、存盘、大量取代字符、离开 vi 、显示行号等动作是在此模式中达成的！ 常用语法 1）基本语法 （1）: 选项 ​ 选项： w 保存 q 退出 ！ 感叹号强制执行 （2）/ 查找，/被查找词，n是查找下一个，shift+n是往上查找 （3）? 查找，?被查找词，n是查找上一个，shift+n是往下查找 2）案例 :wq! 强制保存退出 五 系统管理操作5.1 查看网络IP和网关1）查看虚拟网络编辑器 2）修改ip地址 3）查看网关 5.2 配置网络ip地址 0）查看当前ip基本语法： &gt; &gt; &gt; [root@hadoop101 /]# ifconfig&gt; &gt; 1）在终端命令窗口中输入（如果不是克隆的虚拟机可以跳过这一步）*******&gt; &gt; [root@hadoop101 /]#vim /etc/udev/rules.d/70-persistent-net.rules&gt; &gt; 进入如下页面，删除eth0该行；将eth1修改为eth0，同时复制物理ip地址&gt; 2）修改IP地址 [root@hadoop101 /]#vim /etc/sysconfig/network-scripts/ifcfg-eth0需要修改的内容有5项：IPADDR=192.168.1.101GATEWAY=192.168.1.2ONBOOT=yesBOOTPROTO=staticDNS1=192.168.1.2 （1）修改前 ​ （2）修改后 ：wq 保存退出 3）执行service network restart 3）执行service network restart 4）如果报错，reboot，重启虚拟机 5.3 配置主机名0）查看主机名基本语法： [root@hadoop101 /]#hostname 1）修改linux的主机映射文件（hosts文件） （1）进入Linux系统查看本机的主机名。通过hostname命令查看[root@hadoop101 ~]# hostnamehadoop100（2）如果感觉此主机名不合适，我们可以进行修改。通过编辑/etc/sysconfig/network文件[root@hadoop101 /]# vi /etc/sysconfig/network文件中内容NETWORKING=yesNETWORKING_IPV6=noHOSTNAME= hadoop101注意：主机名称不要有“_”下划线（3）打开此文件后，可以看到主机名。修改此主机名为我们想要修改的主机名hadoop101。（4）保存退出。（5）打开/etc/hosts[root@hadoop101 /]# vim /etc/hosts添加如下内容192.168.1.101 hadoop101（6）并重启设备，重启后，查看主机名，已经修改成功 2）修改window7的主机映射文件（hosts文件）–方面在电脑使用域名进行访问hadoop相关的组件-例如hdfs，mapreduce等等。 ​ （1）进入C:\Windows\System32\drivers\etc路径 （2）打开hosts文件并添加如下内容192.168.1.101 hadoop101192.168.1.102 hadoop102192.168.1.103 hadoop103192.168.1.104 hadoop104192.168.1.105 hadoop105192.168.1.106 hadoop106192.168.1.107 hadoop107192.168.1.108 hadoop108 5.4 防火墙1）基本语法： service iptables status （功能描述：查看防火墙状态）chkconfig iptables -list （功能描述：查看防火墙开机启动状态）service iptables stop （功能描述：临时关闭防火墙）chkconfig iptables off （功能描述：关闭防火墙开机启动）chkconfig iptables on （功能描述：开启防火墙开机启动） 2）扩展 Linux系统有7个运行级别(runlevel)运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆运行级别2：多用户状态(没有NFS)运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式运行级别4：系统未使用，保留运行级别5：X11控制台，登陆后进入图形GUI模式运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 5.5 关机重启在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机 。 正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt 1）基本语法： ​ （1）sync （功能描述：将数据由内存同步到硬盘中）​ （2）shutdown [选项] 时间 ​ 选项：​ -h：关机​ -r：重启（3）halt （功能描述：关闭系统，等同于shutdown -h now 和 poweroff）（4）reboot （功能描述：就是重启，等同于 shutdown -r now） 2）案例 （1）将数据由内存同步到硬盘中[root@hadoop101 /]#sync （2）计算机将在10分钟后关机，并且会显示在登录用户的当前屏幕中[root@hadoop101 /]#shutdown -h 10 ‘This server will shutdown after 10 mins’（3）立马关机[root@hadoop101 /]# shutdown -h now （4）系统立马重启[root@hadoop101 /]# shutdown -r now（5）重启（等同于 shutdown -r now）[root@hadoop101 /]# reboot （6）关机（等同于shutdown -h now 和 poweroff）[root@hadoop101 /]#halt 注意：不管是重启系统还是关闭系统，首先要运行sync命令，把内存中的数据写到磁盘中。 5.6 找回root密码重新安装系统吗？当然不用！进入单用户模式更改一下root密码即可。 1）重启Linux，见到下图，在3秒钟之内按下回车 2）三秒之内要按一下回车，出现如下界面 3）按下e键就可以进入下图 4）移动到下一行，再次按e键 5）移动到下一行，进行修改 修改完成后回车键，然后按b键进行重新启动进入系统 6）移动到下一行，进行修改 最终修改完密码，reboot一下即可。 6.1 安装远程连接linux服务器工具Linux一般作为服务器使用，而服务器一般放在机房，你不可能在机房操作你的Linux服务器。这时我们就需要远程登录到Linux服务器来管理维护系统。 Linux系统中是通过SSH服务实现的远程登录功能，默认ssh服务端口号为 22。Window系统上 Linux 远程登录客户端有SecureCRT, Putty, SSH Secure Shell,XShell等 我这里安装的是 xshell。安装流程自行百度，比较简单。 # 好了到此，linux 相关的环境安装就已经结束了，linux相关的指令操作，可以参考下一章节。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql索引详解]]></title>
    <url>%2F2016%2F08%2F01%2FMysql%E7%B4%A2%E5%BC%95%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言 索引对查询的速度有着至关重要的影响，理解索引也是进行数据库性能调优的起点。考虑如下情况，假设数据库中一个表有10^6条记录，DBMS的页面大小为4K，并存储100条记录。如果没有索引，查询将对整个表进行扫描，最坏的情况下，如果所有数据页都不在内存，需要读取10^4个页面，如果这10^4个页面在磁盘上随机分布，需要进行10^4次I/O，假设磁盘每次I/O时间为10ms(忽略数据传输时间)，则总共需要100s(但实际上要好很多很多)。如果对之建立B-Tree索引，则只需要进行log100(10^6)=3次页面读取，最坏情况下耗时30ms。这就是索引带来的效果，很多时候，当你的应用程序进行SQL查询速度很慢时，应该想想是否可以建索引。进入正题： 有些硬啃的干货还是得了解的，下面先了解索引的基本知识 索引分类 单列索引 主键索引 唯一索引 普通索引 组合索引用到的表CREATE TABLE `award` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '用户id', `aty_id` varchar(100) NOT NULL DEFAULT '' COMMENT '活动场景id', `nickname` varchar(12) NOT NULL DEFAULT '' COMMENT '用户昵称', `is_awarded` tinyint(1) NOT NULL DEFAULT 0 COMMENT '用户是否领奖', `award_time` int(11) NOT NULL DEFAULT 0 COMMENT '领奖时间', `account` varchar(12) NOT NULL DEFAULT '' COMMENT '帐号', `password` char(32) NOT NULL DEFAULT '' COMMENT '密码', `message` varchar(255) NOT NULL DEFAULT '' COMMENT '获奖信息', `created_time` int(11) NOT NULL DEFAULT 0 COMMENT '创建时间', `updated_time` int(11) NOT NULL DEFAULT 0 COMMENT '更新时间', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='获奖信息表'; 单列索引普通索引 这个是最基本的索引 创建语法：其sql格式是： 第一种方式 : CREATE INDEX IndexName ON `TableName`(`字段名`(length)) 第二种方式 : ALTER TABLE TableName ADD INDEX IndexName(`字段名`(length)) 创建例子：第一种方式 : CREATE INDEX account_Index ON `award`(`account`);第二种方式: ALTER TABLE award ADD INDEX account_Index(`account`) 唯一索引 与普通索引类似,但是不同的是唯一索引要求所有的类的值是唯一的,这一点和主键索引一样.但是他允许有空值 创建语法：其sql格式是： 第一种方式 : CREATE UNIQUE INDEX IndexName ON `TableName`(`字段名`(length)); 第二种方式 : ALTER TABLE TableName ADD UNIQUE (column_list) 创建例子：CREATE UNIQUE INDEX account_UNIQUE_Index ON `award`(`account`); 主键索引 他与唯一索引的不同在于不允许有空值(在B+TREE中的InnoDB引擎中,主键索引起到了至关重要的地位) 创建语法：其sql格式是： 第一种方式 : CREATE UNIQUE INDEX IndexName ON `TableName`(`字段名`(length)); 第二种方式 : ALTER TABLE TableName ADD UNIQUE (column_list) 创建例子：CREATE UNIQUE INDEX account_UNIQUE_Index ON `award`(`account`); 单列索引的总结mysql&gt;SELECT ｀uid｀ FROM people WHERE lname｀='Liu' AND ｀fname｀='Zhiqun' AND ｀age｀=26因为我们不想扫描整表，故考虑用索引。单列索引：ALTER TABLE people ADD INDEX lname (lname);将lname列建索引，这样就把范围限制在lname='Liu'的结果集1上，之后扫描结果集1，产生满足fname='Zhiqun'的结果集2，再扫描结果集2，找到 age=26的结果集3，即最终结果。由 于建立了lname列的索引，与执行表的完全扫描相比，效率提高了很多，但我们要求扫描的记录数量仍旧远远超过了实际所需 要的。虽然我们可以删除lname列上的索引，再创建fname或者age 列的索引，但是，不论在哪个列上创建索引搜索效率仍旧相似。&gt; 所以就需要组合索引 组合索引 一个表中含有多个单列索引不代表是组合索引,通俗一点讲 组合索引是:包含多个字段但是只有索引名称 创建语法：其sql格式是： CREATE INDEX IndexName On `TableName`(`字段名`(length),`字段名`(length),...); 创建例子：CREATE INDEX nickname_account_createdTime_Index ON `award`(`nickname`, `account`, `created_time`); 如果你建立了 组合索引(nickname_account_createdTime_Index) 那么他实际包含的是3个索引 (nickname) (nickname,account)(nickname,account,created_time) 组合索引的最左前缀 上面的例子中给nickname,account,created_time 这三个字段建立索引他会去创建三个索引，但是在执行查询的时候只会用其中一个索引去查询，mysql会选择一个最严格(获得结果集记录数最少)的索引，所以where子句中使用最频繁的一列放在最左边。所谓最左前缀原则就是先要看第一列，在第一列满足的条件下再看左边第二列 全文索引 文本字段上(text)如果建立的是普通索引,那么只有对文本的字段内容前面的字符进行索引,其字符大小根据索引建立索引时申明的大小来规定.如果文本中出现多个一样的字符,而且需要查找的话,那么其条件只能是 where column lick &apos;%xxxx%&apos; 这样做会让索引失效.这个时候全文索引就祈祷了作用了ALTER TABLE tablename ADD FULLTEXT(column1, column2)有了全文索引，就可以用SELECT查询命令去检索那些包含着一个或多个给定单词的数据记录了。ELECT * FROM tablenameWHERE MATCH(column1, column2) AGAINST(‘xxx′, ‘sss′, ‘ddd′)这条命令将把column1和column2字段里有xxx、sss和ddd的数据记录全部查询出来。 总结使用索引的优点 可以通过建立唯一索引或者主键索引,保证数据库表中每一行数据的唯一性. 建立索引可以大大提高检索的数据,以及减少表的检索行数 在表连接的连接条件 可以加速表与表直接的相连 在分组和排序字句进行数据检索,可以减少查询时间中 分组 和 排序时所消耗的时间(数据库的记录会重新排序) 建立索引,在查询中使用索引 可以提高性能 使用索引的缺点 在创建索引和维护索引 会耗费时间,随着数据量的增加而增加 索引文件会占用物理空间,除了数据表需要占用物理空间之外,每一个索引还会占用一定的物理空间 当对表的数据进行 INSERT,UPDATE,DELETE 的时候,索引也要动态的维护,这样就会降低数据的维护速度,(建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快)。 使用索引需要注意的地方 在经常需要搜索的列上,可以加快索引的速度 主键列上可以确保列的唯一性 在表与表的而连接条件上加上索引,可以加快连接查询的速度 在经常需要排序(order by),分组(group by)和的distinct 列上加索引 可以加快排序查询的时间, (单独order by 用不了索引，索引考虑加where 或加limit) 在一些where 之后的 &lt; &lt;= &gt; &gt;= BETWEEN IN 以及某个情况下的like 建立字段的索引(B-TREE) like语句的 如果你对nickname字段建立了一个索引.当查询的时候的语句是 nickname lick ‘%ABC%’ 那么这个索引讲不会起到作用.而nickname lick ‘ABC%’ 那么将可以用到索引 索引不会包含NULL列,如果列中包含NULL值都将不会被包含在索引中,复合索引中如果有一列含有NULL值那么这个组合索引都将失效,一般需要给默认值0或者 ‘ ‘字符串 使用短索引,如果你的一个字段是Char(32)或者int(32),在创建索引的时候指定前缀长度 比如前10个字符 (前提是多数值是唯一的..)那么短索引可以提高查询速度,并且可以减少磁盘的空间,也可以减少I/0操作. 不要在列上进行运算,这样会使得mysql索引失效,也会进行全表扫描 选择越小的数据类型越好,因为通常越小的数据类型通常在磁盘,内存,cpu,缓存中 占用的空间很少,处理起来更快 什么情况下不建立索引 查询中很少使用到的列 不应该创建索引,如果建立了索引然而还会降低mysql的性能和增大了空间需求. 很少数据的列也不应该建立索引,比如 一个性别字段 0或者1,在查询中,结果集的数据占了表中数据行的比例比较大,mysql需要扫描的行数很多,增加索引,并不能提高效率 定义为text和image和bit数据类型的列不应该增加索引 当表的修改(UPDATE,INSERT,DELETE)操作远远大于检索(SELECT)操作时不应该创建索引,这两个操作是互斥的关系 好的文章转：SQL优化转：MySQL索引原理及慢查询优化]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++文件流操作的读与写]]></title>
    <url>%2F2014%2F11%2F08%2FC-%E6%96%87%E4%BB%B6%E6%B5%81%E6%93%8D%E4%BD%9C%E7%9A%84%E8%AF%BB%E4%B8%8E%E5%86%99%2F</url>
    <content type="text"><![CDATA[对文件的写入put和&lt;&lt; 写入方式 put的操作：是对文件进行写入的操作，写入一个字符（可以使字母也可以是asci码值） file.put(' A');file.put('\n');file &lt;&lt; "xiezejing1994"; 输出： &nbsp;&nbsp;&nbsp;&nbsp;A// 注意到A这里有几个空格 但是不影响左对齐xiezejing1994// 也就是说A的前面不会有空格 ##操作和&lt;&lt; 读写方式区别 put操作和 file &lt;&lt;‘A’这个基本上是一样的，但是有个区别就是他不可以这样file &lt;&lt;’ A’;（A的前面有空格）因为他是格式化输入 所以中间不能有”空格“但是这样file &lt;&lt;”‘ A”;（也就是以字符串的格式输入则会有空格） 文件的读操作1.getline（） getline（ cin ，string类型 ） getline( cin, z ); file1 &lt;&lt; z; （file1 为文件流对象） 例子： char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ); cout &lt;&lt; c; &#125; 假设文件1.txt内有' A xiezejing1994 这样文本它的输出：' Axiezejing1994 也就是说他没有读到换行的功能 不会输出' A xiezejing1994（原因就是getlibe其实里面有三个参数，第三个参数默认为'\n'） 2.getline（ fstream，string ）while ( getline( file,z ) )&#123; cout &lt;&lt; z;&#125; 3.get（） char c[100]; while ( !file.eof() ) &#123; //file.getline( c,100 ,'\0'); file.get( c,100 ,'\0'); cout &lt;&lt; c; &#125;输出同getline一样----必须要写三个参数 否则只会输出一行（第三个参数为'\n'也是只会输出一行）。非常严格的输出。 4.get操作 char c; file.get(c); while ( !file.eof() ) &#123; cout &lt;&lt; c; file.get(c); &#125;-----和getline的区别在于 他是读取单个字符的，所以会读取到结束符号故会输出' Axiezejing1994 对文件是否读到末尾的判断1.feof（） 该函数只有“已经读取了”结束标志时 feof（）才会返回非零值 也就是说当文件读取到文件结束标志位时他的返回值不是非零还是零 故还要在进行一次读. 例子 假设在1.txt中只有abc三个字符在进行 while（！feof(fp)） &#123; ch = getc(fp); putchar(ch); &#125;//实际上输出的是四个字符改为ch = getc（fp）；while （ ！feof（fp））&#123; putchar（ch）； ch = getc（fp）；&#125;// 这样就可以正常运行3. 可以不调用函数eof 直接就是 while （ file ） // file 就是文件流的对象&#123; 。。。。操作&#125;4.char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ,'\0'); cout &lt;&lt; c; &#125;这个 和char c[100]; while ( !file.eof() ) &#123; file.getline( c,100 ,'\n'); cout &lt;&lt; c; &#125;假设文本为上面的。输出分别为' A xiezejing1994' Axiezejing1994 读写1.read( 数组名，接收的个数 )2.write( 数组名，gcount函数 )#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main()&#123; ifstream file( "D:\\jjj.txt"); ofstream file1( "D:\\j.txt" , ios::app); string z; if ( !file ) &#123; cout &lt;&lt; " 无法打开\n "; return 1; &#125; char c[100]; while ( !file.eof() ) &#123; file.read( c,100 ); file1.write( c, file.gcount() ); &#125; file.close(); file.close(); return 0;&#125; **判断打开是否正确** 1. if( !file )2.if ( !file.good() ) &#123; cout &lt;&lt; " 无法打开\n "; return 1; &#125;3. if ( !file.is_open() ) &#123; cout &lt;&lt; " 无法打开\n "; return 1; &#125;4. if ( file.fail() ) &#123; cout &lt;&lt; " 无法打开\n "; return 1; &#125;]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>文件</tag>
        <tag>C++</tag>
        <tag>文件读写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文章例子]]></title>
    <url>%2F2013%2F12%2F02%2Fceshi%2F</url>
    <content type="text"><![CDATA[前言使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的； 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行； 博客内容可以轻松打包、转移、发布到其它平台； 等等；]]></content>
      <categories>
        <category>默认分类</category>
      </categories>
      <tags>
        <tag>tag1</tag>
        <tag>tag2</tag>
        <tag>tag3</tag>
      </tags>
  </entry>
</search>
