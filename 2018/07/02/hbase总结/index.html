<!DOCTYPE html>



  




<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>
<script>
    (function(){
        if(''){
            if (prompt('OOPS, 主人请输入文章密码才能够观看哟') !== ''){
                alert('哎呀，密码错误哟！');
                history.back();
            }
        }
    })();
</script>


  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|40:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="hbase,数据结构化," />





  <link rel="alternate" href="/atom.xml" title="King哥" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="一、HBaes介绍1.1、HBase的起源HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储（非结构化数据也可存储-数据挖掘）。 官方网站：http://hbase.apache.org – 2006年Google发表BigTable白皮书。 – 2006年开始开发HBase。 – 2008年北京成功开奥运">
<meta name="keywords" content="hbase,数据结构化">
<meta property="og:type" content="article">
<meta property="og:title" content="hbase总结">
<meta property="og:url" content="http://kingge.top/2018/07/02/hbase总结/index.html">
<meta property="og:site_name" content="King哥">
<meta property="og:description" content="一、HBaes介绍1.1、HBase的起源HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储（非结构化数据也可存储-数据挖掘）。 官方网站：http://hbase.apache.org – 2006年Google发表BigTable白皮书。 – 2006年开始开发HBase。 – 2008年北京成功开奥运">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/clip_image002.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/clip_image003.jpg">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/clip_image005.jpg">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510113948557.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/clip_image002.jpg">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510114341556.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/clip_image02202.jpg">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/123.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510114823112.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510114832543.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510114845784.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510115403745.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510115417538.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510115442300.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510115920725.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510120021250.png">
<meta property="og:image" content="http://kingge.top/2018/07/02/hbase总结/image-20200510120028174.png">
<meta property="og:updated_time" content="2020-05-10T08:24:38.743Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hbase总结">
<meta name="twitter:description" content="一、HBaes介绍1.1、HBase的起源HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储（非结构化数据也可存储-数据挖掘）。 官方网站：http://hbase.apache.org – 2006年Google发表BigTable白皮书。 – 2006年开始开发HBase。 – 2008年北京成功开奥运">
<meta name="twitter:image" content="http://kingge.top/2018/07/02/hbase总结/clip_image002.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://kingge.top/2018/07/02/hbase总结/"/>





  <title>hbase总结 | King哥</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">King哥</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">To know everything, no words don't talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-picture">
          <a href="/picture/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-camera"></i> <br />
            
            照片
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://kingge.top/2018/07/02/hbase总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeremy Kinge">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="King哥">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">hbase总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-02T22:22:10+08:00">
                2018-07-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  15,967
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  70
                </span>
              
            </div>
          

          

		  
		  
        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="一、HBaes介绍"><a href="#一、HBaes介绍" class="headerlink" title="一、HBaes介绍"></a>一、HBaes介绍</h1><h2 id="1-1、HBase的起源"><a href="#1-1、HBase的起源" class="headerlink" title="1.1、HBase的起源"></a>1.1、HBase的起源</h2><p>HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储（非结构化数据也可存储-数据挖掘）。</p>
<p>官方网站：<a href="http://hbase.apache.org" target="_blank" rel="external">http://hbase.apache.org</a></p>
<p>– 2006年Google发表BigTable白皮书。</p>
<p>– 2006年开始开发HBase。</p>
<p>– 2008年北京成功开奥运会，程序员默默地将HBase弄成了Hadoop的子项目。</p>
<p>– 2010年HBase成为Apache顶级项目。</p>
<p>– 现在很多公司二次开发出了很多发行版本，你也开始使用了。</p>
<h2 id="1-2、HBase的角色"><a href="#1-2、HBase的角色" class="headerlink" title="1.2、HBase的角色"></a>1.2、HBase的角色</h2><h3 id="1-2-1、HMaster"><a href="#1-2-1、HMaster" class="headerlink" title="1.2.1、HMaster"></a>1.2.1、HMaster</h3><p><strong>功能：</strong></p>
<p>1) 监控RegionServer</p>
<p>2) 处理RegionServer故障转移</p>
<p>3) 处理元数据的变更</p>
<p>4) 处理region的分配或移除</p>
<p>5) 在空闲时间进行数据的负载均衡</p>
<p>6) 通过Zookeeper发布自己的位置给客户端</p>
<h3 id="1-2-2、RegionServer"><a href="#1-2-2、RegionServer" class="headerlink" title="1.2.2、RegionServer"></a>1.2.2、RegionServer</h3><p><strong>功能：</strong></p>
<p>1) 负责存储HBase的实际数据</p>
<p>2) 处理分配给它的Region</p>
<p>3) 刷新缓存到HDFS </p>
<p>4) 维护HLog（保存数据本身和对数据的操作）</p>
<p>5) 执行压缩</p>
<p>6) 负责处理Region分片</p>
<p><strong>组件：</strong></p>
<p><strong>1) Write-Ahead logs</strong></p>
<p>HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p>
<p><strong>2) HFile</strong></p>
<p>这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。</p>
<p><strong>3) Store</strong></p>
<p>HFile存储在Store中，一个Store对应HBase表中的一个列族。</p>
<p><strong>4) MemStore</strong></p>
<p>顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。</p>
<p><strong>5) Region</strong></p>
<p>Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。</p>
<h2 id="1-3、HBase的架构"><a href="#1-3、HBase的架构" class="headerlink" title="1.3、HBase的架构"></a>1.3、HBase的架构</h2><p>HBase一种是作为存储的分布式文件系统，另一种是作为数据处理模型的MR框架。因为日常开发人员比较熟练的是结构化的数据进行处理，但是在HDFS直接存储的文件往往不具有结构化，所以催生出了HBase在HDFS上的操作。如果需要查询数据，只需要通过键值便可以成功访问。</p>
<p>架构图如下图所示：</p>
<p><img src="/2018/07/02/hbase总结/clip_image002.png" alt="img"> </p>
<p>HBase内置有Zookeeper，但一般我们会有其他的Zookeeper集群来监管master和regionserver，Zookeeper通过选举，保证任何时候，集群中只有一个活跃的HMaster，HMaster与HRegionServer 启动时会向ZooKeeper注册，存储所有HRegion的寻址入口，实时监控HRegionserver的上线和下线信息。并实时通知给HMaster，存储HBase的schema和table元数据，默认情况下，HBase 管理ZooKeeper 实例，Zookeeper的引入使得HMaster不再是单点故障。一般情况下会启动两个HMaster，非Active的HMaster会定期的和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个HMaster反而增加了Active HMaster的负担。</p>
<p>一个RegionServer可以包含多个HRegion，每个RegionServer维护一个HLog，和多个HFiles以及其对应的MemStore。RegionServer运行于DataNode上，数量可以与DatNode数量一致，请参考如下架构图：</p>
<p><img src="/2018/07/02/hbase总结/clip_image003.jpg" alt="http://upload-images.jianshu.io/upload_images/4951489-8dee031e87bde745.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<p><img src="/2018/07/02/hbase总结/clip_image005.jpg" alt="img"></p>
<h1 id="二、HBase部署与使用"><a href="#二、HBase部署与使用" class="headerlink" title="二、HBase部署与使用"></a>二、HBase部署与使用</h1><h2 id="2-1、部署"><a href="#2-1、部署" class="headerlink" title="2.1、部署"></a>2.1、部署</h2><h3 id="2-1-1、Zookeeper正常部署"><a href="#2-1-1、Zookeeper正常部署" class="headerlink" title="2.1.1、Zookeeper正常部署"></a>2.1.1、Zookeeper正常部署</h3><p>首先保证Zookeeper集群的正常部署，并启动之：</p>
<p>  $ ~/modules/zookeeper-3.4.5/bin/zkServer.sh  start  </p>
<h3 id="2-1-2、Hadoop正常部署"><a href="#2-1-2、Hadoop正常部署" class="headerlink" title="2.1.2、Hadoop正常部署"></a>2.1.2、Hadoop正常部署</h3><p>Hadoop集群的正常部署并启动：</p>
<p>  $  ~/modules/hadoop-2.7.2/sbin/start-dfs.sh  $  ~/modules/hadoop-2.7.2/sbin/start-yarn.sh  </p>
<h3 id="2-1-3、HBase的解压"><a href="#2-1-3、HBase的解压" class="headerlink" title="2.1.3、HBase的解压"></a>2.1.3、HBase的解压</h3><p>解压HBase到指定目录：</p>
<p>  $ tar -zxf  ~/softwares/installations/hbase-1.3.1-bin.tar.gz -C ~/modules/  </p>
<h3 id="2-1-4、HBase的配置文件"><a href="#2-1-4、HBase的配置文件" class="headerlink" title="2.1.4、HBase的配置文件"></a>2.1.4、HBase的配置文件</h3><p>需要修改HBase对应的配置文件。</p>
<p><strong>hbase-env.sh**</strong>修改内容：**</p>
<p>  export JAVA_HOME=/home/admin/modules/jdk1.8.0_121  export HBASE_MANAGES_ZK=false  //是否使用hbase内嵌的zookeeper  </p>
<p><strong>hbase-site.xml**</strong>修改内容：**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">	&lt;property&gt;     </div><div class="line">		&lt;name&gt;hbase.rootdir&lt;/name&gt;     </div><div class="line">		&lt;value&gt;hdfs://hadoop101:9000/hbase&lt;/value&gt;  	</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">//这个地址要跟hadoop 的 core-site.xml 的namenode地址一致</div><div class="line"></div><div class="line">&lt;!—是否开启分布式--&gt;</div><div class="line">	&lt;property&gt;   </div><div class="line">		&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</div><div class="line">		&lt;value&gt;true&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">   &lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 和reginserver通信的端口 --&gt;</div><div class="line">	&lt;property&gt;</div><div class="line">		&lt;name&gt;hbase.master.port&lt;/name&gt;</div><div class="line">		&lt;value&gt;16000&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;property&gt;   </div><div class="line">		&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</div><div class="line">		&lt;value&gt;hadoop101:2181, hadoop102:2181, hadoop103:2181&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line"></div><div class="line">	&lt;property&gt;   </div><div class="line">		&lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</div><div class="line">	 &lt;value&gt; /opt/module/zookeeper-3.4.5/zkData&lt;/value&gt;</div><div class="line">	&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p><strong>regionservers</strong>：</p>
<p>linux01  </p>
<p>linux02 </p>
<p>linux03  </p>
<h3 id="2-1-5、HBase需要依赖的Jar包"><a href="#2-1-5、HBase需要依赖的Jar包" class="headerlink" title="2.1.5、HBase需要依赖的Jar包"></a>2.1.5、HBase需要依赖的Jar包</h3><p>由于HBase需要依赖Hadoop，所以替换HBase的lib目录下的jar包，以解决兼容问题：</p>
<p>1) 删除原有的jar：</p>
<p>  $ rm -rf  /home/admin/modules/hbase-1.3.1/lib/hadoop-*  $ rm -rf  /home/admin/modules/hbase-1.3.1/lib/zookeeper-3.4.6.jar  </p>
<p>2) 拷贝新jar，涉及的jar有：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">hadoop-annotations-2.7.2.jar</div><div class="line">hadoop-auth-2.7.2.jar</div><div class="line">hadoop-client-2.7.2.jar</div><div class="line">hadoop-common-2.7.2.jar</div><div class="line">hadoop-hdfs-2.7.2.jar</div><div class="line">hadoop-mapreduce-client-app-2.7.2.jar</div><div class="line">hadoop-mapreduce-client-common-2.7.2.jar</div><div class="line">hadoop-mapreduce-client-core-2.7.2.jar</div><div class="line">hadoop-mapreduce-client-hs-2.7.2.jar</div><div class="line">hadoop-mapreduce-client-hs-plugins-2.7.2.jar</div><div class="line">hadoop-mapreduce-client-jobclient-2.7.2.jar</div><div class="line">hadoop-mapreduce-client-jobclient-2.7.2-tests.jar</div><div class="line">hadoop-mapreduce-client-shuffle-2.7.2.jar</div><div class="line">hadoop-yarn-api-2.7.2.jar</div><div class="line">hadoop-yarn-applications-distributedshell-2.7.2.jar</div><div class="line">hadoop-yarn-applications-unmanaged-am-launcher-2.7.2.jar</div><div class="line">hadoop-yarn-client-2.7.2.jar</div><div class="line">hadoop-yarn-common-2.7.2.jar</div><div class="line">hadoop-yarn-server-applicationhistoryservice-2.7.2.jar</div><div class="line">hadoop-yarn-server-common-2.7.2.jar</div><div class="line">hadoop-yarn-server-nodemanager-2.7.2.jar</div><div class="line">hadoop-yarn-server-resourcemanager-2.7.2.jar</div><div class="line">hadoop-yarn-server-tests-2.7.2.jar</div><div class="line">hadoop-yarn-server-web-proxy-2.7.2.jar</div><div class="line">zookeeper-3.4.5.jar</div></pre></td></tr></table></figure>
<p>尖叫提示：这些jar包的对应版本应替换成你目前使用的hadoop版本，具体情况具体分析。</p>
<p>查找jar包举例：</p>
<p>  $ find /home/admin/modules/hadoop-2.7.2/  -name hadoop-annotations*  </p>
<p>然后将找到的jar包复制到HBase的lib目录下即可。</p>
<h3 id="2-1-6、HBase软连接Hadoop配置"><a href="#2-1-6、HBase软连接Hadoop配置" class="headerlink" title="2.1.6、HBase软连接Hadoop配置"></a>2.1.6、HBase软连接Hadoop配置</h3><p>  $ ln  -s ~/modules/hadoop-2.7.2/etc/hadoop/core-site.xml ~/modules/hbase-1.3.1/conf/core-site.xml  $ ln  -s ~/modules/hadoop-2.7.2/etc/hadoop/hdfs-site.xml  ~/modules/hbase-1.3.1/conf/hdfs-site.xml  </p>
<h3 id="2-1-7、HBase远程scp到其他集群"><a href="#2-1-7、HBase远程scp到其他集群" class="headerlink" title="2.1.7、HBase远程scp到其他集群"></a>2.1.7、HBase远程scp到其他集群</h3><p>  $ scp -r /home/admin/modules/hbase-1.3.1/  linux02:/home/admin/modules/  $ scp -r /home/admin/modules/hbase-1.3.1/  linux03:/home/admin/modules/  </p>
<h3 id="2-1-8、HBase服务的启动"><a href="#2-1-8、HBase服务的启动" class="headerlink" title="2.1.8、HBase服务的启动"></a>2.1.8、HBase服务的启动</h3><p><strong>启动方式1**</strong>：**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ bin/hbase-daemon.sh start master </div><div class="line">$ bin/hbase-daemon.sh start regionserver</div><div class="line">$ bin/hbase-daemon.sh stop regionserver</div></pre></td></tr></table></figure>
<p>尖叫提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。</p>
<p>修复提示：</p>
<p>a、同步时间服务</p>
<p>请参看帮助文档：《大数据帮助文档》</p>
<p>b、属性：hbase.master.maxclockskew设置更大的值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">        &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt;</div><div class="line">        &lt;value&gt;180000&lt;/value&gt;</div><div class="line">        &lt;description&gt;Time difference of regionserver from master&lt;/description&gt;</div><div class="line"> &lt;/property&gt;</div></pre></td></tr></table></figure>
<p><strong>启动方式2*：</strong></p>
<p>  $ bin/start-hbase.sh  </p>
<p>对应的停止服务：</p>
<p>  $ bin/stop-hbase.sh  </p>
<p>尖叫提示：如果使用的是JDK8以上版本，则应在hbase-evn.sh中移除“HBASE_MASTER_OPTS”和“HBASE_REGIONSERVER_OPTS”配置。</p>
<h3 id="2-1-9、查看Hbse页面"><a href="#2-1-9、查看Hbse页面" class="headerlink" title="2.1.9、查看Hbse页面"></a>2.1.9、查看Hbse页面</h3><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：</p>
<p>  <a href="http://hadoop101:16010" target="_blank" rel="external">http://hadoop101:16010</a>    </p>
<h2 id="2-2、简单使用"><a href="#2-2、简单使用" class="headerlink" title="2.2、简单使用"></a>2.2、简单使用</h2><h3 id="2-2-1、基本操作"><a href="#2-2-1、基本操作" class="headerlink" title="2.2.1、基本操作"></a>2.2.1、基本操作</h3><p><strong>1)</strong> <strong>进入HBase**</strong>客户端命令行**</p>
<p>  $ bin/hbase  shell  </p>
<p><strong>2)</strong> <strong>查看帮助命令</strong></p>
<p>  hbase(main)&gt; help  </p>
<p><strong>3)</strong> <strong>查看当前数据库中有哪些表</strong></p>
<p>  hbase(main)&gt; list  </p>
<h3 id="2-2-2、表的操作"><a href="#2-2-2、表的操作" class="headerlink" title="2.2.2、表的操作"></a>2.2.2、表的操作</h3><p><strong>1)</strong> <strong>创建表（同时创建列镞info**</strong>）**</p>
<p>  hbase(main)&gt; create ‘student’,’info’  </p>
<p><strong>2)</strong> <strong>插入数据到表（列镞中的列可以动态生成-*</strong>info*<strong>*</strong>这个列镞必须是已经存在的）-student类似一个map集合</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">hbase(main) &gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:name&apos;,&apos;Thomas&apos;</div><div class="line">hbase(main) &gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:sex&apos;,&apos;male&apos;</div><div class="line">hbase(main) &gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:age&apos;,&apos;18&apos;</div><div class="line">hbase(main) &gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:name&apos;,&apos;Janna&apos;</div><div class="line">hbase(main) &gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:sex&apos;,&apos;female&apos;</div><div class="line">hbase(main) &gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:age&apos;,&apos;20&apos;</div></pre></td></tr></table></figure>
<p>Put相同的键值，会覆盖列的值，跟map的性质一样。</p>
<p>例如：put ‘student’,’1001’,’info:name’,’kingge’  –那么这一列的值就会被覆盖为kingge.</p>
<p><strong><em>这个时候产生一个问题，原先的Thomas***</em></strong>就消失了嘛？其实不是，之前的值还是存在的。通过时间戳来区分***。（version 版本）</p>
<p><strong>3)</strong> <strong>扫描查看表数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">hbase(main) &gt; scan &apos;student&apos;</div><div class="line">hbase(main) &gt; scan &apos;student&apos;,&#123;STARTROW =&gt; &apos;1001&apos;, STOPROW  =&gt; &apos;1001&apos;&#125;</div><div class="line">hbase(main) &gt; scan &apos;student&apos;,&#123;STARTROW =&gt; &apos;1001&apos;&#125;</div></pre></td></tr></table></figure>
<p><img src="/2018/07/02/hbase总结/image-20200510113948557.png" alt="image-20200510113948557"></p>
<p>时间戳是在插入数据时，默认添加的，目的是为了保存冗余的数据。</p>
<p>注意在hbase中比较大小时按位比较。那么比较大小时STARTROW，<strong><em>根据***</em></strong>ascall*<strong>*</strong>码按位比较***。例如存在行的rowkey有abc、aaa、abcd、e、ac。那么STARTROW =&gt; ‘ac’.首先比较第一个字母a，这五个rowkey都满足，再比较第二个字母c，最终只有e、ac满足。故输出这两行的值</p>
<p><strong>4)</strong> <strong>查看表结构</strong></p>
<p>  hbase(main):012:0&gt; describe ‘student’  </p>
<p><strong>5)</strong> <strong>更新指定字段的数据（原数据依旧存在，跟java**</strong>的map<strong>**有所不同）</strong></p>
<p>  hbase(main) &gt; put  ‘student’,’1001’,’info:name’,’Nick’  </p>
<p>hbase(main) &gt; put  ‘student’,’1001’,’info:age’,’100’  </p>
<p><strong>6)</strong> <strong>查看“指定行”或“指定列族:**</strong>列”的数据**</p>
<p>  hbase(main) &gt; get ‘student’,’1001’  </p>
<p>hbase(main) &gt; get  ‘student’,’1001’,’info:name’  </p>
<p><strong>7)</strong> <strong>删除数据</strong></p>
<p><strong>删除某rowkey**</strong>的全部数据：**</p>
<p>  hbase(main) &gt; deleteall  ‘student’,’1001’  </p>
<p><strong>删除某rowkey**</strong>的某一列数据：**</p>
<p>  hbase(main) &gt; delete ‘student’,’1002’,’info:sex’  </p>
<p><strong>8)</strong> <strong>清空表数据</strong></p>
<p>  hbase(main) &gt; truncate ‘student’  </p>
<p>尖叫提示：清空表的操作顺序为先disable，然后再truncating。</p>
<p><strong>9)</strong> <strong>删除表</strong></p>
<p><strong>首先需要先让该表为disable**</strong>状态：**</p>
<p>  hbase(main) &gt; disable ‘student’  </p>
<p><strong>然后才能drop**</strong>这个表：**</p>
<p>  hbase(main) &gt; drop ‘student’  </p>
<p>尖叫提示：如果直接drop表，会报错：Drop the named table. Table must first be disabled</p>
<p>ERROR: Table student is enabled. Disable it first.</p>
<p><strong>10)</strong> <strong>统计表数据行数（按照rowkey**</strong>统计）**</p>
<p>  hbase(main) &gt; count ‘student’  </p>
<p><strong>11)</strong> <strong>变更表信息</strong></p>
<p>将info列族中的数据存放3个版本：（默认只保留一个版本）</p>
<p>  hbase(main) &gt; alter ‘student’,{NAME=&gt;’info’,VERSIONS=&gt;3}  </p>
<h3 id="2-2-3-hbase的表结构"><a href="#2-2-3-hbase的表结构" class="headerlink" title="2.2.3 hbase的表结构"></a>2.2.3 hbase的表结构</h3><p><img src="/2018/07/02/hbase总结/clip_image002.jpg" alt="img"></p>
<p>每个表必须有一个唯一的rowkey，已经时间戳（ts）,info是一个列镞，也就是说info里面可能包含多个列（图中就包含name和sex两个列- nick和male是这两个列的值）。</p>
<p>那么我们需要给name列赋值，就需要通过一系列属性定位- </p>
<p> <img src="/2018/07/02/hbase总结/image-20200510114341556.png" alt="image-20200510114341556"></p>
<h2 id="2-3、读写流程"><a href="#2-3、读写流程" class="headerlink" title="2.3、读写流程"></a>2.3、读写流程</h2><h3 id="2-3-0-Region的寻址"><a href="#2-3-0-Region的寻址" class="headerlink" title="2.3.0 Region的寻址"></a>2.3.0 Region的寻址</h3><p><img src="/2018/07/02/hbase总结/clip_image02202.jpg" alt="img"></p>
<p><img src="/2018/07/02/hbase总结/123.png" alt="img"></p>
<h3 id="2-3-1、HBase读数据流程"><a href="#2-3-1、HBase读数据流程" class="headerlink" title="2.3.1、HBase读数据流程"></a>2.3.1、HBase读数据流程</h3><p><strong>1)</strong> HRegionServer保存着.META.的这样一张表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取-ROOT-表所在位置，进而找到.META.表所在的位置信息，即找到这个.META.表在哪个HRegionServer上保存着。</p>
<p><strong>2)</strong> 接着Client通过刚才获取到的HRegionServer的IP来访问.META.表所在的HRegionServer，从而读取到.META.，进而获取到.META.表中存放的元数据。</p>
<p><strong>3)</strong> Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。</p>
<p><strong>4)</strong> 最后HRegionServer把查询到的数据响应给Client。-详细的请看视频，涉及到memorystore（存储写入的数据，内存存储）和blockcache（存储读取的数据。内存存储）的hfile（物理逻辑上的输出-在hdfs中）的读取。</p>
<p><img src="/2018/07/02/hbase总结/image-20200510114823112.png" alt="image-20200510114823112"></p>
<h3 id="2-3-2、HBase写数据流程"><a href="#2-3-2、HBase写数据流程" class="headerlink" title="2.3.2、HBase写数据流程"></a>2.3.2、HBase写数据流程</h3><p><strong>1)</strong> Client也是先访问zookeeper，找到-ROOT-表，进而找到.META.表，并获取.META.表信息。</p>
<p><strong>2)</strong> 确定当前将要写入的数据所对应的RegionServer服务器和Region。</p>
<p><strong>3)</strong> Client向该RegionServer服务器发起写入数据请求，然后RegionServer收到请求并响应。</p>
<p><strong>4)</strong> Client先把数据写入到HLog，以防止数据丢失。</p>
<p><strong>5)</strong> 然后将数据写入到Memstore。</p>
<p><strong>6)</strong> 如果Hlog和Memstore均写入成功，则这条数据写入成功。在此过程中，如果Memstore达到阈值，会把Memstore中的数据flush到StoreFile中。(溢出后，会重新创建一块memostore存储接下来需要写入的数据。当前溢出的memostore会把数据打包放入到一个队列中，等到flush，也即是说一个memstore溢出后写入，对应着一个storefile，一对多的关系)</p>
<p><strong>7)</strong> 当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile（避免datanode出现大量小文件）。当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。</p>
<p>尖叫提示：因为内存空间是有限的，所以说溢写过程必定伴随着大量的小文件产生。</p>
<p><img src="/2018/07/02/hbase总结/image-20200510114832543.png" alt="image-20200510114832543"></p>
<p>从上图可以看出氛围3步骤：</p>
<p>第1步：Client获取数据写入的Region所在的RegionServer<br> 第2步：请求写Hlog<br> 第3步：请求写MemStore</p>
<p>只有当写Hlog和写MemStore都成功了才算请求写入完成。MemStore后续会逐渐刷到HDFS中。</p>
<p>备注：Hlog存储在HDFS，当RegionServer出现异常，需要使用Hlog来恢复数据。</p>
<p><strong>重申强调上述涉及到的3个机制：</strong></p>
<p><strong><em>\</em> Flush**</strong>机制：**</p>
<p>当MemStore达到阈值，将Memstore中的数据Flush进Storefile</p>
<p><strong>涉及属性：</strong></p>
<p><strong>hbase.hregion.memstore.flush.size**</strong>：134217728**</p>
<p>即：128M就是Memstore的默认阈值</p>
<p><strong>hbase.regionserver.global.memstore.upperLimit**</strong>：0.4**</p>
<p>即： 这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。 RegionServer的flush是通过将请求添加一个队列，模拟生产消费模式来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请 求时，可能会导致内存陡增，最坏的情况是触发OOM。</p>
<p><strong>hbase.regionserver.global.memstore.lowerLimit**</strong>：0.38**</p>
<p>即： 当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个 MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于 hbase.regionserver.global.memstore.lowerLimit。</p>
<p><strong><em>\</em> Compact**</strong>机制：**</p>
<p>把小的Memstore文件合并成大的Storefile文件。</p>
<p><strong><em>\</em> Split**</strong>机制**</p>
<p>当Region达到阈值，会把过大的Region一分为二。</p>
<h2 id="2-4、JavaAPI"><a href="#2-4、JavaAPI" class="headerlink" title="2.4、JavaAPI"></a>2.4、JavaAPI</h2><h3 id="2-4-1、安装Maven并配置环境变量"><a href="#2-4-1、安装Maven并配置环境变量" class="headerlink" title="2.4.1、安装Maven并配置环境变量"></a>2.4.1、安装Maven并配置环境变量</h3><p>  $ tar  -zxf ~/softwares/installations/apache-maven-3.5.0-bin.tar.gz -C ~/modules/  </p>
<p>在环境变量中添加：</p>
<p>  MAVEN_HOME=/home/admin/modules/apache-maven-3.5.0  export PATH=$PATH:$MAVEN_HOME/bin  </p>
<h3 id="2-4-2、新建Maven-Project"><a href="#2-4-2、新建Maven-Project" class="headerlink" title="2.4.2、新建Maven Project"></a>2.4.2、新建Maven Project</h3><p><img src="/2018/07/02/hbase总结/image-20200510114845784.png" alt="image-20200510114845784"></p>
<p> <strong>需要三个配置文件信息：core-site.xml**</strong>、hdfs-site.xml<strong>**、hbase-site.xml</strong></p>
<p>新建项目后在pom.xml中添加依赖：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.3.1&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div><div class="line"></div><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.3.1&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div><div class="line"></div><div class="line">----下面这个可以不导入</div><div class="line">&lt;dependency&gt;</div><div class="line">	&lt;groupId&gt;jdk.tools&lt;/groupId&gt;</div><div class="line">	&lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;</div><div class="line">	&lt;version&gt;1.6&lt;/version&gt;</div><div class="line">	&lt;scope&gt;system&lt;/scope&gt;</div><div class="line">	&lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure>
<h3 id="2-4-3、编写HBaseAPI"><a href="#2-4-3、编写HBaseAPI" class="headerlink" title="2.4.3、编写HBaseAPI"></a>2.4.3、编写HBaseAPI</h3><p>注意，这部分的学习内容，我们先学习使用老版本的API，接着再写出新版本的API调用方式。因为在企业中，有些时候我们需要一些过时的API来提供更好的兼容性。</p>
<p><strong>1)</strong> <strong>首先需要获取Configuration**</strong>对象：**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static Configuration conf;</div><div class="line">static&#123;</div><div class="line">	//使用HBaseConfiguration的单例方法实例化</div><div class="line">	conf = HBaseConfiguration.create();</div><div class="line">conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.216.20&quot;);</div><div class="line">conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>2)</strong> <strong>判断表是否存在：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static boolean isTableExist(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123;</div><div class="line">	//在HBase中管理、访问表需要先创建HBaseAdmin对象</div><div class="line">//Connection connection = ConnectionFactory.createConnection(conf);</div><div class="line">//HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();-新api</div><div class="line">	HBaseAdmin admin = new HBaseAdmin(conf);-老api</div><div class="line">	return admin.tableExists(tableName);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>3)</strong> <strong>创建表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123;</div><div class="line">	HBaseAdmin admin = new HBaseAdmin(conf);</div><div class="line">	//判断表是否存在</div><div class="line">	if(isTableExist(tableName))&#123;</div><div class="line">		System.out.println(&quot;表&quot; + tableName + &quot;已存在&quot;);</div><div class="line">		//System.exit(0);</div><div class="line">	&#125;else&#123;</div><div class="line">		//创建表属性对象,表名需要转字节</div><div class="line">		HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName));</div><div class="line">		//创建多个列族</div><div class="line">		for(String cf : columnFamily)&#123;</div><div class="line">			descriptor.addFamily(new HColumnDescriptor(cf));</div><div class="line">		&#125;</div><div class="line">		//根据对表的配置，创建表</div><div class="line">		admin.createTable(descriptor);</div><div class="line">		System.out.println(&quot;表&quot; + tableName + &quot;创建成功！&quot;);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>4)</strong> <strong>删除表</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123;</div><div class="line">	HBaseAdmin admin = new HBaseAdmin(conf);</div><div class="line">	if(isTableExist(tableName))&#123;</div><div class="line">		admin.disableTable(tableName);</div><div class="line">		admin.deleteTable(tableName);</div><div class="line">		System.out.println(&quot;表&quot; + tableName + &quot;删除成功！&quot;);</div><div class="line">	&#125;else&#123;</div><div class="line">		System.out.println(&quot;表&quot; + tableName + &quot;不存在！&quot;);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>5)</strong> <strong>向表中插入数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static void addRowData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException&#123;</div><div class="line">	//创建HTable对象</div><div class="line">	HTable hTable = new HTable(conf, tableName);</div><div class="line">	//向表中插入数据</div><div class="line">	Put put = new Put(Bytes.toBytes(rowKey));</div><div class="line">	//向Put对象中组装数据</div><div class="line">	put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value));</div><div class="line">	hTable.put(put);</div><div class="line">	hTable.close();</div><div class="line">	System.out.println(&quot;插入数据成功&quot;);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>6)</strong> <strong>删除多行数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static void deleteMultiRow(String tableName, String... rows) throws IOException&#123;</div><div class="line">	HTable hTable = new HTable(conf, tableName);</div><div class="line">	List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;();</div><div class="line">	for(String row : rows)&#123;</div><div class="line">		Delete delete = new Delete(Bytes.toBytes(row));</div><div class="line">		deleteList.add(delete);</div><div class="line">	&#125;</div><div class="line">	hTable.delete(deleteList);</div><div class="line">	hTable.close();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>7)</strong> <strong>得到所有数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static void getAllRows(String tableName) throws IOException&#123;</div><div class="line">	HTable hTable = new HTable(conf, tableName);</div><div class="line">	//得到用于扫描region的对象</div><div class="line">	Scan scan = new Scan();</div><div class="line">	//使用HTable得到resultcanner实现类的对象</div><div class="line">	ResultScanner resultScanner = hTable.getScanner(scan);</div><div class="line">	for(Result result : resultScanner)&#123;</div><div class="line">		Cell[] cells = result.rawCells();</div><div class="line">		for(Cell cell : cells)&#123;</div><div class="line">			//得到rowkey</div><div class="line">			System.out.println(&quot;行键:&quot; + Bytes.toString(CellUtil.cloneRow(cell)));</div><div class="line">			//得到列族</div><div class="line">			System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell)));</div><div class="line">			System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell)));</div><div class="line">			System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell)));</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>8)</strong> <strong>得到某一行所有数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static void getRow(String tableName, String rowKey) throws IOException&#123;</div><div class="line">	HTable table = new HTable(conf, tableName);</div><div class="line">	Get get = new Get(Bytes.toBytes(rowKey));</div><div class="line">	//get.setMaxVersions();显示所有版本</div><div class="line">    //get.setTimeStamp();显示指定时间戳的版本</div><div class="line">	Result result = table.get(get);</div><div class="line">	for(Cell cell : result.rawCells())&#123;</div><div class="line">		System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow()));</div><div class="line">		System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell)));</div><div class="line">		System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell)));</div><div class="line">		System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell)));</div><div class="line">		System.out.println(&quot;时间戳:&quot; + cell.getTimestamp());</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>9)</strong> <strong>获取某一行指定“列族:**</strong>列”的数据**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static void getRowQualifier(String tableName, String rowKey, String family, String qualifier) throws IOException&#123;</div><div class="line">	HTable table = new HTable(conf, tableName);</div><div class="line">	Get get = new Get(Bytes.toBytes(rowKey));</div><div class="line">	get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier));</div><div class="line">	Result result = table.get(get);</div><div class="line">	for(Cell cell : result.rawCells())&#123;</div><div class="line">		System.out.println(&quot;行键:&quot; + Bytes.toString(result.getRow()));</div><div class="line">		System.out.println(&quot;列族&quot; + Bytes.toString(CellUtil.cloneFamily(cell)));</div><div class="line">		System.out.println(&quot;列:&quot; + Bytes.toString(CellUtil.cloneQualifier(cell)));</div><div class="line">		System.out.println(&quot;值:&quot; + Bytes.toString(CellUtil.cloneValue(cell)));</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><img src="/2018/07/02/hbase总结/image-20200510115403745.png" alt="image-20200510115403745"></p>
<h2 id="2-5、MapReduce"><a href="#2-5、MapReduce" class="headerlink" title="2.5、MapReduce"></a>2.5、MapReduce</h2><p>通过HBase的相关JavaAPI，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析。</p>
<h3 id="2-5-1、官方HBase-MapReduce"><a href="#2-5-1、官方HBase-MapReduce" class="headerlink" title="2.5.1、官方HBase-MapReduce"></a>2.5.1、官方HBase-MapReduce</h3><p><strong>1)</strong> <strong>查看HBase**</strong>的MapReduce<strong><strong>任务的所需的依赖-</strong></strong>也就是说当前hbase<strong><strong>想要执行mapreduce</strong></strong>所需要的依赖**</p>
<p>  $ bin/hbase mapredcp  </p>
<p><strong>2)</strong> <strong>执行环境变量的导入（*注意此次环境变量只会在当前会话框有效，临时的*）</strong></p>
<p>  $ export HBASE_HOME= /opt/module/hbase-1.3.1/  $ export HADOOP_CLASSPATH=<code>${HBASE_HOME}/bin/hbase  mapredcp</code>  </p>
<p><strong>2.2)</strong> <strong>执行环境变量的导入（永久-**</strong>在hadoop<strong><strong>启动时自动加载jar</strong></strong>包到环境中）**</p>
<p> \1. 打开hadoop的 hadoop-env.sh</p>
<p><img src="/2018/07/02/hbase总结/image-20200510115417538.png" alt="image-20200510115417538"></p>
<p>写入命令脚本（如果配置了hbase_home那么可以不书写hbase的全路径，用${HBASE_HOME} 代替即可）</p>
<p><img src="/2018/07/02/hbase总结/image-20200510115442300.png" alt="image-20200510115442300"></p>
<p>必须放在上面for循环下面，避免被覆盖，同时也要保留for循环已经为hadoop_classpath赋的值。（<strong><em>为了避免被覆盖推荐放到***</em></strong>hadoop-env.sh*<strong>*</strong>的最末尾***）</p>
<p>export HADOOP_CLASSPATH=<code>/opt/module/hbase-1.3.1/bin/hbase mapredcp</code></p>
<p>\3. hadoop-env.sh分发到hadoop集群</p>
<p><strong>3)</strong> <strong>运行官方的MapReduce**</strong>任务**</p>
<p><strong>–</strong> <strong>案例一：统计Student**</strong>表中有多少行数据**</p>
<p>  $  ~/modules/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar rowcounter  student  </p>
<p><strong>–</strong> <strong>案例二：使用MapReduce**</strong>将本地数据导入到HBase**</p>
<p><strong>(1)</strong> <strong>在本地创建一个tsv**</strong>格式的文件：fruit.tsv**</p>
<p>  1001    Apple   Red  1002    Pear    Yellow  1003    Pineapple Yellow  </p>
<p>尖叫提示：上面的这个数据不要从word中直接复制，有格式错误</p>
<p><strong>(2)</strong> <strong>创建HBase**</strong>表**</p>
<p>  hbase(main):001:0&gt; create  ‘fruit’,’info’  </p>
<p><strong>(3)</strong> <strong>在HDFS**</strong>中创建input_fruit<strong><strong>文件夹并上传fruit.tsv</strong></strong>文件**</p>
<p>  $  ~/modules/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/ </p>
<p> $  ~/modules/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/  </p>
<p><strong>(4)</strong> <strong>执行MapReduce**</strong>到HBase<strong><strong>的fruit</strong></strong>表中（*fruit*<strong>*</strong>表必须已经存在，否则报错*<strong>**）</strong></p>
<p>  $ ~/modules/hadoop-2.7.2/bin/yarn  jar lib/hbase-server-1.3.1.jar importtsv \  -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color  fruit \  hdfs://linux01:8020/input_fruit  </p>
<p><strong>(5)</strong> <strong>使用scan**</strong>命令查看导入后的结果**</p>
<p>  hbase(main):001:0&gt; scan ‘fruit’  </p>
<p><strong><em>–***</em></strong>需要注意，从mapreduce*<strong>*</strong>导入数据到hbase*<strong>*</strong>，一般都是结构化的数据，因为非结构化数据导入，你无法确定某一列的数据究竟是对应的是hbase*<strong>*</strong>表那个列镞的列。***</p>
<h3 id="2-5-2、自定义HBase-MapReduce1（hbase导入到hbase）"><a href="#2-5-2、自定义HBase-MapReduce1（hbase导入到hbase）" class="headerlink" title="2.5.2、自定义HBase-MapReduce1（hbase导入到hbase）"></a>2.5.2、自定义HBase-MapReduce1（hbase导入到hbase）</h3><p><strong>目标：</strong>将fruit表中的一部分数据，通过MR迁入到fruit_mr表中。</p>
<p><strong>分步实现：</strong></p>
<p><strong>1)</strong> <strong>构建ReadFruitMapper**</strong>类，用于读取fruit<strong>**表中的数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.z.hbase_mr;</div><div class="line"></div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.hbase.Cell;</div><div class="line">import org.apache.hadoop.hbase.CellUtil;</div><div class="line">import org.apache.hadoop.hbase.client.Put;</div><div class="line">import org.apache.hadoop.hbase.client.Result;</div><div class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</div><div class="line">import org.apache.hadoop.hbase.mapreduce.TableMapper;</div><div class="line">import org.apache.hadoop.hbase.util.Bytes;</div><div class="line">// ImmutableBytesWritable 相当于rowkey、 Put rowkey对应一行的数据</div><div class="line">public class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; &#123;</div><div class="line"></div><div class="line">	@Override</div><div class="line">	protected void map(ImmutableBytesWritable key, Result value, Context context) </div><div class="line">	throws IOException, InterruptedException &#123;</div><div class="line">	//将fruit的name和color提取出来，相当于将每一行数据读取出来放入到Put对象中。</div><div class="line">		Put put = new Put(key.get());</div><div class="line">		//遍历添加column行</div><div class="line">		for(Cell cell: value.rawCells())&#123;</div><div class="line">			//添加/克隆列族:info</div><div class="line">			if(&quot;info&quot;.equals(Bytes.toString(CellUtil.cloneFamily(cell))))&#123;</div><div class="line">				//添加/克隆列：name</div><div class="line">				if(&quot;name&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</div><div class="line">					//将该列cell加入到put对象中</div><div class="line">					put.add(cell);</div><div class="line">					//添加/克隆列:color</div><div class="line">				&#125;else if(&quot;color&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</div><div class="line">					//向该列cell加入到put对象中</div><div class="line">					put.add(cell);</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		//将从fruit读取到的每行数据写入到context中作为map的输出</div><div class="line">		context.write(key, put);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>2)</strong> <strong>构建WriteFruitMRReducer**</strong>类，用于将读取到的fruit<strong><strong>表中的数据写入到fruit_mr</strong></strong>表中**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.z.hbase_mr;</div><div class="line"></div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.hbase.client.Put;</div><div class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</div><div class="line">import org.apache.hadoop.hbase.mapreduce.TableReducer;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line"></div><div class="line">public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123;</div><div class="line">	@Override</div><div class="line">	protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) </div><div class="line">	throws IOException, InterruptedException &#123;</div><div class="line">		//读出来的每一行数据写入到fruit_mr表中</div><div class="line">		for(Put put: values)&#123;</div><div class="line">			context.write(NullWritable.get(), put);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>3)</strong> <strong>构建Fruit2FruitMRRunner extends Configured implements Tool**</strong>用于组装运行Job<strong>**任务</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">	private Configuration conf = null;</div><div class="line">	public void setConf(Configuration conf) &#123;//需要注意的是这里获取的conf是hadoop//生成的配置文件，我们这里运行的是hbase，那么需要覆盖为hbase的环境变量。</div><div class="line">      this.conf = new HBaseConfiguration(conf);		</div><div class="line">	&#125;</div><div class="line">// HBaseConfiguration实际上是hadoop的configuration的重载</div><div class="line">	public Configuration getConf() &#123;</div><div class="line">		return this.conf;</div><div class="line">	&#125;</div><div class="line">//组装Job</div><div class="line">	public int run(String[] args) throws Exception &#123;</div><div class="line">		//得到Configuration</div><div class="line">		Configuration conf = this.getConf();</div><div class="line">		//创建Job任务</div><div class="line">		Job job = Job.getInstance(conf, this.getClass().getSimpleName());</div><div class="line">		job.setJarByClass(Fruit2FruitMRRunner.class);</div><div class="line"></div><div class="line">		//配置Job</div><div class="line">		Scan scan = new Scan();</div><div class="line">		scan.setCacheBlocks(false);</div><div class="line">		scan.setCaching(500);</div><div class="line"></div><div class="line">		//设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本</div><div class="line">		TableMapReduceUtil.initTableMapperJob(</div><div class="line">		&quot;fruit&quot;, //数据源的表名</div><div class="line">		scan, //scan扫描控制器</div><div class="line">		ReadFruitMapper.class,//设置Mapper类</div><div class="line">		ImmutableBytesWritable.class,//设置Mapper输出key类型</div><div class="line">		Put.class,//设置Mapper输出value值类型</div><div class="line">		job//设置给哪个JOB</div><div class="line">		);</div><div class="line">		//设置Reducer</div><div class="line">		TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRReducer.class, job);</div><div class="line">		//设置Reduce数量，最少1个</div><div class="line">		job.setNumReduceTasks(1);</div><div class="line"></div><div class="line">		boolean isSuccess = job.waitForCompletion(true);</div><div class="line">		if(!isSuccess)&#123;</div><div class="line">			throw new IOException(&quot;Job running with error&quot;);</div><div class="line">		&#125;</div><div class="line">		return isSuccess ? 0 : 1;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p><strong>4)</strong> <strong>主函数中调用运行该Job**</strong>任务**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public static void main( String[] args ) throws Exception&#123;</div><div class="line">Configuration conf = HBaseConfiguration.create();</div><div class="line">int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args);</div><div class="line">System.exit(status);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>5)</strong> <strong>打包运行任务</strong></p>
<p>  $  ~/modules/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/hbase-0.0.1-SNAPSHOT.jar  com.z.hbase.mr1.Fruit2FruitMRRunner  </p>
<p>尖叫提示：运行任务前，如果待数据导入的表不存在，则需要提前创建之。</p>
<p>尖叫提示：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</p>
<h3 id="2-5-3、自定义HBase-MapReduce2"><a href="#2-5-3、自定义HBase-MapReduce2" class="headerlink" title="2.5.3、自定义HBase-MapReduce2"></a>2.5.3、自定义HBase-MapReduce2</h3><p><strong>目标：</strong>实现将HDFS中的数据写入到HBase表中。</p>
<p><strong>分步实现：</strong></p>
<p><strong>1)</strong> <strong>构建ReadFruitFromHDFSMapper**</strong>于读取HDFS<strong>**中的文件数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.z.hbase.mr2;</div><div class="line"></div><div class="line">import java.io.IOException;</div><div class="line"></div><div class="line">import org.apache.hadoop.hbase.client.Put;</div><div class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</div><div class="line">import org.apache.hadoop.hbase.util.Bytes;</div><div class="line">import org.apache.hadoop.io.LongWritable;</div><div class="line">import org.apache.hadoop.io.Text;</div><div class="line">import org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line">public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; &#123;</div><div class="line">	@Override</div><div class="line">	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</div><div class="line">		//从HDFS中读取的数据</div><div class="line">		String lineValue = value.toString();</div><div class="line">		//读取出来的每行数据使用\t进行分割，存于String数组</div><div class="line">		String[] values = lineValue.split(&quot;\t&quot;);</div><div class="line">		</div><div class="line">		//根据数据中值的含义取值</div><div class="line">		String rowKey = values[0];</div><div class="line">		String name = values[1];</div><div class="line">		String color = values[2];</div><div class="line">		</div><div class="line">		//初始化rowKey</div><div class="line">		ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey));</div><div class="line">		</div><div class="line">		//初始化put对象</div><div class="line">		Put put = new Put(Bytes.toBytes(rowKey));</div><div class="line">		</div><div class="line">		//参数分别:列族、列、值  </div><div class="line">        put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;),  Bytes.toBytes(name)); </div><div class="line">        put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;color&quot;),  Bytes.toBytes(color)); </div><div class="line">        </div><div class="line">        context.write(rowKeyWritable, put);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>2)</strong> <strong>构建WriteFruitMRFromTxtReducer**</strong>类**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.z.hbase.mr2;</div><div class="line"></div><div class="line">import java.io.IOException;</div><div class="line">import org.apache.hadoop.hbase.client.Put;</div><div class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</div><div class="line">import org.apache.hadoop.hbase.mapreduce.TableReducer;</div><div class="line">import org.apache.hadoop.io.NullWritable;</div><div class="line"></div><div class="line">public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123;</div><div class="line">	@Override</div><div class="line">	protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123;</div><div class="line">		//读出来的每一行数据写入到fruit_hdfs表中</div><div class="line">		for(Put put: values)&#123;</div><div class="line">			context.write(NullWritable.get(), put);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>3)</strong> <strong>创建Txt2FruitRunner**</strong>组装Job**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">public int run(String[] args) throws Exception &#123;</div><div class="line">//得到Configuration</div><div class="line">Configuration conf = this.getConf();</div><div class="line"></div><div class="line">//创建Job任务</div><div class="line">Job job = Job.getInstance(conf, this.getClass().getSimpleName());</div><div class="line">job.setJarByClass(Txt2FruitRunner.class);</div><div class="line">Path inPath = new Path(&quot;hdfs://linux01:8020/input_fruit/fruit.tsv&quot;);</div><div class="line">FileInputFormat.addInputPath(job, inPath);</div><div class="line"></div><div class="line">//设置Mapper</div><div class="line">job.setMapperClass(ReadFruitFromHDFSMapper.class);</div><div class="line">job.setMapOutputKeyClass(ImmutableBytesWritable.class);</div><div class="line">job.setMapOutputValueClass(Put.class);</div><div class="line"></div><div class="line">//设置Reducer</div><div class="line">TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRFromTxtReducer.class, job);</div><div class="line"></div><div class="line">//设置Reduce数量，最少1个</div><div class="line">job.setNumReduceTasks(1);</div><div class="line"></div><div class="line">boolean isSuccess = job.waitForCompletion(true);</div><div class="line">if(!isSuccess)&#123;</div><div class="line">throw new IOException(&quot;Job running with error&quot;);</div><div class="line">&#125;</div><div class="line"></div><div class="line">return isSuccess ? 0 : 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>4)</strong> <strong>调用执行Job</strong></p>
<p>  public  static void main(String[] args) throws Exception {         Configuration conf = HBaseConfiguration.create();          int status = ToolRunner.run(conf, new Txt2FruitRunner(), args);          System.exit(status);  }  </p>
<p><strong>5)</strong> <strong>打包运行</strong></p>
<p>  $  ~/modules/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/hbase-0.0.1-SNAPSHOT.jar  com.z.hbase.mr2.Txt2FruitRunner  </p>
<p>尖叫提示：运行任务前，如果待数据导入的表不存在，则需要提前创建之。</p>
<p>尖叫提示：maven打包命令：-P local clean package或-P dev clean package install（将第三方jar包一同打包，需要插件：maven-shade-plugin）</p>
<h2 id="2-6、与Hive的集成"><a href="#2-6、与Hive的集成" class="headerlink" title="2.6、与Hive的集成"></a>2.6、与Hive的集成</h2><h3 id="2-6-1、HBase与Hive的对比"><a href="#2-6-1、HBase与Hive的对比" class="headerlink" title="2.6.1、HBase与Hive的对比"></a>2.6.1、HBase与Hive的对比</h3><p><strong>1) Hive</strong></p>
<p><strong>(1)</strong> <strong>数据仓库</strong></p>
<p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p>
<p><strong>(2)</strong> <strong>用于数据分析、清洗</strong></p>
<p>Hive适用于离线的数据分析和清洗，延迟较高。</p>
<p><strong>(3)</strong> <strong>基于HDFS**</strong>、MapReduce**</p>
<p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</p>
<p><strong>2) HBase</strong></p>
<p><strong>(1)</strong> <strong>数据库</strong></p>
<p>是一种面向列存储的非关系型数据库。</p>
<p><strong>(2)</strong> <strong>用于存储结构化和非结构话的数据</strong></p>
<p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p>
<p><strong>(3)</strong> <strong>基于HDFS</strong></p>
<p>数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。</p>
<p><strong>(4)</strong> <strong>延迟较低，接入在线业务使用</strong></p>
<p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p>
<h3 id="2-6-2、HBase与Hive集成使用"><a href="#2-6-2、HBase与Hive集成使用" class="headerlink" title="2.6.2、HBase与Hive集成使用"></a>2.6.2、HBase与Hive集成使用</h3><p>尖叫提示：HBase与Hive的集成在最新的两个版本中无法兼容。所以，我们只能含着泪勇敢的重新编译：hive-hbase-handler-1.2.2.jar！！好气！！</p>
<p><strong>环境准备</strong></p>
<p>因为我们后续可能会在操作Hive的同时对HBase也会产生影响，所以Hive需要持有操作HBase的Jar，那么接下来拷贝Hive所依赖的Jar包（或者使用软连接的形式）。记得还有把zookeeper的jar包考入到hive的lib目录下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ export HBASE_HOME=/home/admin/modules/hbase-1.3.1</div><div class="line">$ export HIVE_HOME=/home/admin/modules/apache-hive-1.2.2-bin</div><div class="line"></div><div class="line">$ ln -s $HBASE_HOME/lib/hbase-common-1.3.1.jar  $HIVE_HOME/lib/hbase-common-1.3.1.jar</div><div class="line">$ ln -s $HBASE_HOME/lib/hbase-server-1.3.1.jar $HIVE_HOME/lib/hbase-server-1.3.1.jar</div><div class="line">$ ln -s $HBASE_HOME/lib/hbase-client-1.3.1.jar $HIVE_HOME/lib/hbase-client-1.3.1.jar</div><div class="line">$ ln -s $HBASE_HOME/lib/hbase-protocol-1.3.1.jar $HIVE_HOME/lib/hbase-protocol-1.3.1.jar</div><div class="line">$ ln -s $HBASE_HOME/lib/hbase-it-1.3.1.jar $HIVE_HOME/lib/hbase-it-1.3.1.jar</div><div class="line">$ ln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jar</div><div class="line">$ ln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar</div><div class="line">$ ln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.3.1.jar</div></pre></td></tr></table></figure>
<p><strong>同时在hive-site.xml**</strong>中修改zookeeper<strong>**的属性，如下：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</div><div class="line">  &lt;value&gt;linux01,linux02,linux03&lt;/value&gt;</div><div class="line">  &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt;</div><div class="line">  &lt;value&gt;2181&lt;/value&gt;</div><div class="line">  &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<p><strong>1)</strong> <strong>案例一</strong></p>
<p><strong>目标：</strong>建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。</p>
<p><strong>分步实现：</strong></p>
<p><strong>(1)</strong> <strong>在Hive**</strong>中创建表同时关联HBase<strong><em>*（\</em>会自动的在\</strong>hbase*<strong>*</strong>中创建相应的表*<strong><strong>）（数据存储在hbase</strong></strong>中<strong>**）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">CREATE TABLE hive_hbase_emp_table(</div><div class="line">empno int,</div><div class="line">ename string,</div><div class="line">job string,</div><div class="line">mgr int,</div><div class="line">hiredate string,</div><div class="line">sal double,</div><div class="line">comm double,</div><div class="line">deptno int)</div><div class="line">STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;</div><div class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)</div><div class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;);</div></pre></td></tr></table></figure>
<p>尖叫提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表</p>
<p><strong>(2)</strong> <strong>在Hive**</strong>中创建临时中间表，用于load<strong>**文件中的数据</strong></p>
<p>尖叫提示：不能将数据直接load进Hive所关联HBase的那张表中</p>
<p>  CREATE  TABLE emp(  empno  int,  ename  string,  job  string,  mgr  int,  hiredate  string,  sal  double,  comm  double,  deptno  int)  row  format delimited fields terminated by ‘\t’;  </p>
<p><strong>(3)</strong> <strong>向Hive**</strong>中间表中load<strong>**数据</strong></p>
<p>  hive&gt; load data local inpath ‘/home/admin/softwares/data/emp.txt’  into table emp;  </p>
<p><strong>(4)</strong> <strong>通过insert**</strong>命令将中间表中的数据导入到Hive<strong><strong>关联HBase</strong></strong>的那张表中**</p>
<p>  hive&gt; insert into table  hive_hbase_emp_table select * from emp;  </p>
<p><strong>(5)</strong> <strong>查看Hive**</strong>以及关联的HBase<strong>**表中是否已经成功的同步插入了数据</strong></p>
<p><strong>Hive**</strong>：**</p>
<p>  hive&gt; select * from  hive_hbase_emp_table;  </p>
<p><strong>HBase**</strong>：**</p>
<p>  hbase&gt; scan ‘hbase_emp_table’  </p>
<p><strong>2)</strong> <strong>案例二（常用场景）</strong></p>
<p><strong>目标：</strong>在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。</p>
<p><strong>注：</strong>该案例2紧跟案例1的脚步，所以完成此案例前，请先完成案例1。</p>
<p><strong>分步实现：</strong></p>
<p><strong>(1)</strong> <strong>在Hive**</strong>中创建外部表**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">CREATE EXTERNAL TABLE relevance_hbase_emp(</div><div class="line">empno int,</div><div class="line">ename string,</div><div class="line">job string,</div><div class="line">mgr int,</div><div class="line">hiredate string,</div><div class="line">sal double,</div><div class="line">comm double,</div><div class="line">deptno int)</div><div class="line">STORED BY </div><div class="line">&apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;</div><div class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = </div><div class="line">&quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) </div><div class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;);</div></pre></td></tr></table></figure>
<p><strong>(2)</strong> <strong>关联后就可以使用Hive**</strong>函数进行一些分析操作了（数据存储在hbase<strong><strong>中</strong></strong>）**</p>
<p>  hive  (default)&gt; select * from relevance_hbase_emp;  </p>
<h3 id="2-6-3-需要注意"><a href="#2-6-3-需要注意" class="headerlink" title="2.6.3 需要注意"></a>2.6.3 需要注意</h3><p>删除表的时候，要先删除hive的表然后再删除habse的。如果反之，删除完hbase的表，那么再去删除hive的表时就会报错（这个时候，退出hive重新登录即可，但是不推荐）</p>
<h2 id="2-7、与Sqoop的集成"><a href="#2-7、与Sqoop的集成" class="headerlink" title="2.7、与Sqoop的集成"></a>2.7、与Sqoop的集成</h2><p>Sqoop supports additional import targets beyond HDFS and Hive. Sqoop can also import records into a table in HBase.</p>
<p>之前我们已经学习过如何使用Sqoop在Hadoop集群和关系型数据库中进行数据的导入导出工作，接下来我们学习一下利用Sqoop在HBase和RDBMS中进行数据的转储。</p>
<p><strong>相关参数：</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>–column-family <family></family></td>
<td>Sets the target column family for the  import  设置导入的目标列族。</td>
</tr>
<tr>
<td>–hbase-create-table</td>
<td>If specified, create missing HBase tables  是否自动创建不存在的HBase表（这就意味着，不需要手动提前在HBase中先建立表）</td>
</tr>
<tr>
<td>–hbase-row-key <col></td>
<td>Specifies which input column to use as  the row key.In case, if input table contains composite  key, then <col> must be in the form  of a  comma-separated list of composite key  attributes.  mysql中哪一列的值作为HBase的rowkey，如果rowkey是个组合键，则以逗号分隔。（注：避免rowkey的重复）</td>
</tr>
<tr>
<td>–hbase-table <table-name></table-name></td>
<td>Specifies an HBase table to use as the  target instead of HDFS.  指定数据将要导入到HBase中的哪张表中。</td>
</tr>
<tr>
<td>–hbase-bulkload</td>
<td>Enables bulk loading.  是否允许bulk形式的导入。</td>
</tr>
</tbody>
</table>
<p><strong>1)</strong> <strong>案例</strong></p>
<p><strong>目标：</strong>将RDBMS中的数据抽取到HBase中</p>
<p><strong>分步实现：</strong></p>
<p><strong>(1)</strong> <strong>配置sqoop-env.sh**</strong>，添加如下内容：**</p>
<p>  export HBASE_HOME=/home/admin/modules/hbase-1.3.1  </p>
<p><strong>(2)</strong> <strong>在Mysql**</strong>中新建一个数据库db_library<strong>**，一张表book</strong></p>
<p>  CREATE DATABASE db_library;  CREATE TABLE db_library.book(  id int(4) PRIMARY KEY NOT NULL  AUTO_INCREMENT,   name VARCHAR(255) NOT NULL,   price VARCHAR(255) NOT NULL);  </p>
<p><strong>(3)</strong> <strong>向表中插入一些数据</strong></p>
<p>  INSERT INTO db_library.book (name, price)  VALUES(‘Lie Sporting’, ‘30’);   INSERT INTO db_library.book (name, price)  VALUES(‘Pride &amp; Prejudice’, ‘70’);     INSERT INTO db_library.book (name, price)  VALUES(‘Fall of Giants’, ‘50’);  </p>
<p><strong>(4)</strong> <strong>执行Sqoop**</strong>导入数据的操作**</p>
<p>  $ bin/sqoop import \  –connect jdbc:mysql://linux01:3306/db_library  \  –username root \  –password 123456 \  –table book \  –columns “id,name,price” \  –column-family “info” \  –hbase-create-table \  –hbase-row-key “id” \  –hbase-table “hbase_book” \  –num-mappers 1 \  –split-by id //一个id一条数据的分割方式，导入hbase  </p>
<p>尖叫提示：sqoop1.4.6只支持HBase1.0.1之前的版本的自动创建HBase表的功能</p>
<p>解决方案：手动创建HBase表</p>
<p>  hbase&gt; create ‘hbase_book’,’info’  </p>
<p><strong>(5)</strong> <strong>在HBase**</strong>中scan<strong>**这张表得到如下内容</strong></p>
<p>  hbase&gt; scan ‘hbase_book’  </p>
<p><strong>思考：</strong>尝试使用复合键作为导入数据时的rowkey。</p>
<h2 id="2-8、常用的Shell操作"><a href="#2-8、常用的Shell操作" class="headerlink" title="2.8、常用的Shell操作"></a>2.8、常用的Shell操作</h2><p><strong>1) satus</strong></p>
<p>例如：显示服务器状态</p>
<p>  hbase&gt; status ‘linux01’  </p>
<p><strong>2) whoami</strong></p>
<p>显示HBase当前用户，例如：</p>
<p>  hbase&gt; whoami  </p>
<p><strong>3) list</strong></p>
<p>显示当前所有的表</p>
<p>  hbase&gt; list  </p>
<p><strong>4) count</strong></p>
<p>统计指定表的记录数，例如：</p>
<p>  hbase&gt; count ‘hbase_book’  </p>
<p><strong>5) describe</strong></p>
<p>展示表结构信息</p>
<p>  hbase&gt; describe ‘hbase_book’  </p>
<p><strong>6) exist</strong></p>
<p>检查表是否存在，适用于表量特别多的情况</p>
<p>  hbase&gt; exist ‘hbase_book’  </p>
<p><strong>7) is_enabled/is_disabled</strong></p>
<p>检查表是否启用或禁用</p>
<p>  hbase&gt; is_enabled ‘hbase_book’  hbase&gt; is_disabled ‘hbase_book’  </p>
<p><strong>8) alter</strong></p>
<p>该命令可以改变表和列族的模式，例如：</p>
<p><strong>为当前表增加列族：</strong></p>
<p>  hbase&gt; alter ‘hbase_book’, NAME =&gt; ‘CF2’,  VERSIONS =&gt; 2  </p>
<p><strong>为当前表删除列族：</strong></p>
<p>  hbase&gt; alter ‘hbase_book’, ‘delete’  =&gt; ‘CF2’  </p>
<p><strong>9) disable</strong></p>
<p>禁用一张表</p>
<p>  hbase&gt; disable ‘hbase_book’  </p>
<p><strong>10) drop</strong></p>
<p>删除一张表，记得在删除表之前必须先禁用</p>
<p>  hbase&gt; drop ‘hbase_book’  </p>
<p><strong>11) delete</strong></p>
<p>删除一行中一个单元格的值，例如：</p>
<p>  hbase&gt; delete ‘hbase_book’, ‘rowKey’,  ‘CF:C’  </p>
<p><strong>12) truncate</strong></p>
<p>清空表数据，即禁用表-删除表-创建表</p>
<p>  hbase&gt; truncate ‘hbase_book’  </p>
<p><strong>13) create</strong></p>
<p>创建表，例如：</p>
<p>  hbase&gt; create ‘table’, ‘cf’  </p>
<p>创建多个列族：</p>
<p>  hbase&gt; create ‘t1’, {NAME =&gt; ‘f1’},  {NAME =&gt; ‘f2’}, {NAME =&gt; ‘f3’}  </p>
<h2 id="2-9、数据的备份与恢复"><a href="#2-9、数据的备份与恢复" class="headerlink" title="2.9、数据的备份与恢复"></a>2.9、数据的备份与恢复</h2><h3 id="2-9-1、备份"><a href="#2-9-1、备份" class="headerlink" title="2.9.1、备份"></a>2.9.1、备份</h3><p>停止HBase服务后，使用distcp命令运行MapReduce任务进行备份，将数据备份到另一个地方，可以是同一个集群，也可以是专用的备份集群。</p>
<p>即，把数据转移到当前集群的其他目录下（也可以不在同一个集群中）:</p>
<p>  $  bin/hadoop distcp \  hdfs://linux01:8020/hbase  \  hdfs://linux01:8020/HbaseBackup/backup20171009  </p>
<p>尖叫提示：执行该操作，一定要开启Yarn服务</p>
<h3 id="2-9-2、恢复"><a href="#2-9-2、恢复" class="headerlink" title="2.9.2、恢复"></a>2.9.2、恢复</h3><p>非常简单，与备份方法一样，将数据整个移动回来即可。</p>
<p>  $  bin/hadoop distcp \  hdfs://linux01:8020/HbaseBackup/backup20170930  \  hdfs://linux01:8020/hbase  </p>
<h2 id="2-10、节点的管理"><a href="#2-10、节点的管理" class="headerlink" title="2.10、节点的管理"></a>2.10、节点的管理</h2><h3 id="2-10-1、服役（commissioning）"><a href="#2-10-1、服役（commissioning）" class="headerlink" title="2.10.1、服役（commissioning）"></a>2.10.1、服役（commissioning）</h3><p>当启动regionserver时，regionserver会向HMaster注册并开始接收本地数据，开始的时候，新加入的节点不会有任何数据，平衡器开启的情况下，将会有新的region移动到开启的RegionServer上。如果启动和停止进程是使用ssh和HBase脚本，那么会将新添加的节点的主机名加入到conf/regionservers文件中。</p>
<h3 id="2-10-2、退役（decommissioning）"><a href="#2-10-2、退役（decommissioning）" class="headerlink" title="2.10.2、退役（decommissioning）"></a>2.10.2、退役（decommissioning）</h3><p>顾名思义，就是从当前HBase集群中删除某个RegionServer，这个过程分为如下几个过程：</p>
<p><strong>1)</strong> <strong>停止负载平衡器（HMaster**</strong>上操作）**</p>
<p>  hbase&gt; balance_switch false  </p>
<p><strong>2)</strong> <strong>在退役节点上停止RegionServer</strong></p>
<p>  hbase&gt; hbase-daemon.sh stop  regionserver  </p>
<p><strong>3) RegionServer**</strong>一旦停止，会关闭维护的所有region**</p>
<p><strong>4) Zookeeper**</strong>上的该RegionServer<strong>**节点消失</strong></p>
<p><strong>5) Master**</strong>节点检测到该RegionServer<strong>**下线，开启平衡器</strong></p>
<p><strong>6)</strong> <strong>下线的RegionServer**</strong>的region<strong>**服务得到重新分配</strong></p>
<p>该关闭方法比较传统，需要花费一定的时间，而且会造成部分region短暂的不可用。</p>
<p><strong>另一种方案：</strong></p>
<p><strong>1) RegionServer**</strong>先卸载所管理的region**</p>
<p>  $ bin/graceful_stop.sh  <regionserver-hostname>  </regionserver-hostname></p>
<p><strong>2)</strong> <strong>自动平衡数据</strong></p>
<p><strong>3)</strong> <strong>和之前的2~6**</strong>步是一样的**</p>
<h2 id="2-11、版本的确界"><a href="#2-11、版本的确界" class="headerlink" title="2.11、版本的确界"></a>2.11、版本的确界</h2><p><strong>1)</strong> <strong>版本的下界</strong></p>
<p>默认的版本下界是0，即禁用。row版本使用的最小数目是与生存时间（TTL Time To Live）相结合的，并且我们根据实际需求可以有0或更多的版本，使用0，即只有1个版本的值写入cell。</p>
<p><strong>2)</strong> <strong>版本的上界</strong></p>
<p>之前默认的版本上界是3，也就是一个row保留3个副本（基于时间戳的插入）。该值不要设计的过大，一般的业务不会超过100。如果cell中存储的数据版本号超过了3个，再次插入数据时，最新的值会将最老的值覆盖。（现版本已默认为1）</p>
<h1 id="三、HBase的优化"><a href="#三、HBase的优化" class="headerlink" title="三、HBase的优化"></a>三、HBase的优化</h1><h2 id="3-1、高可用"><a href="#3-1、高可用" class="headerlink" title="3.1、高可用"></a>3.1、高可用</h2><p>在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。（<strong><em>也就是说，加入***</em></strong>HMaster*<strong>*</strong>挂掉了，hbase*<strong>*</strong>集群还是能工作的，只不过此时所有的读写都操作同一个reginserver*<strong>*</strong>，那么会把它呈报，后面也会不工作。因为失去了HMaster*<strong>*</strong>，失去了负债均衡的能力***）</p>
<p><strong>1)</strong> <strong>关闭HBase**</strong>集群（如果没有开启则跳过此步）**</p>
<p>  $ bin/stop-hbase.sh  </p>
<p><strong>2)</strong> <strong>在conf**</strong>目录下创建backup-masters<strong>**文件</strong></p>
<p>  $ touch conf/backup-masters  </p>
<p><strong>3)</strong> <strong>在backup-masters**</strong>文件中配置高可用HMaster<strong>**节点</strong></p>
<p>  $ echo linux02 &gt; conf/backup-masters  </p>
<p><strong>4)</strong> <strong>将整个conf**</strong>目录scp<strong>**到其他节点</strong></p>
<p>  $ scp -r conf/ linux02:/opt/modules/cdh/hbase-0.98.6-cdh5.3.6/  $ scp -r conf/ linux03:/opt/modules/cdh/hbase-0.98.6-cdh5.3.6/  </p>
<p><strong>5)</strong> <strong>重新启动HBase**</strong>后打开页面测试查看**</p>
<p>  0.98版本之前：<a href="http://linux01:60010" target="_blank" rel="external">http://linux01:60010</a>  0.98版本之后：<a href="http://linux01:16010" target="_blank" rel="external">http://linux01:16010</a>  </p>
<h2 id="3-2、Hadoop的通用性优化"><a href="#3-2、Hadoop的通用性优化" class="headerlink" title="3.2、Hadoop的通用性优化"></a>3.2、Hadoop的通用性优化</h2><p><strong>1) NameNode**</strong>元数据备份使用SSD**</p>
<p><strong>2)</strong> <strong>定时备份NameNode**</strong>上的元数据** </p>
<p>每小时或者每天备份，如果数据极其重要，可以5~10分钟备份一次。备份可以通过定时任务复制元数据目录即可。</p>
<p><strong>3)</strong> <strong>为NameNode**</strong>指定多个元数据目录**</p>
<p>使用dfs.name.dir或者dfs.namenode.name.dir指定。这样可以提供元数据的冗余和健壮性，以免发生故障。</p>
<p><strong>4) NameNode**</strong>的dir<strong>**自恢复</strong></p>
<p>设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。</p>
<p><strong>5) HDFS**</strong>保证RPC<strong>**调用会有较多的线程数</strong></p>
<p><strong>hdfs-site.xml</strong></p>
<p>  属性：dfs.namenode.handler.count  解释：该属性是NameNode服务默认线程数，的默认值是10，根据机器的可用内存可以调整为50~100     属性：dfs.datanode.handler.count  解释：该属性默认值为10，是DataNode的处理线程数，如果HDFS客户端程序读写请求比较多，可以调高到15~20，设置的值越大，内存消耗越多，不要调整的过高，一般业务中，5~10即可。  </p>
<p><strong>6) HDFS**</strong>副本数的调整**</p>
<p><strong>hdfs-site.xml</strong></p>
<p>  属性：dfs.replication  解释：如果数据量巨大，且不是非常之重要，可以调整为2~3，如果数据非常之重要，可以调整为3~5。  </p>
<p><strong>7) HDFS**</strong>文件块大小的调整**</p>
<p><strong>hdfs-site.xml</strong></p>
<p>  属性：dfs.blocksize  解释：块大小定义，该属性应该根据存储的大量的单个文件大小来设置，如果大量的单个文件都小于100M，建议设置成64M块大小，对于大于100M或者达到GB的这种情况，建议设置成256M，一般设置范围波动在64M~256M之间。  </p>
<p><strong>8) MapReduce Job**</strong>任务服务线程数调整**</p>
<p><strong>mapred-site.xml</strong></p>
<p>  属性：mapreduce.jobtracker.handler.count  解释：该属性是Job任务线程数，默认值是10，根据机器的可用内存可以调整为50~100  </p>
<p><strong>9) Http**</strong>服务器工作线程数**</p>
<p><strong>mapred-site.xml</strong></p>
<p>  属性：mapreduce.tasktracker.http.threads  解释：定义HTTP服务器工作线程数，默认值为40，对于大集群可以调整到80~100  </p>
<p><strong>10)</strong> <strong>文件排序合并优化</strong></p>
<p><strong>mapred-site.xml</strong></p>
<p>  属性：mapreduce.task.io.sort.factor  解释：文件排序时同时合并的数据流的数量，这也定义了同时打开文件的个数，默认值为10，如果调高该参数，可以明显减少磁盘IO，即减少文件读取的次数。  </p>
<p><strong>11)</strong> <strong>设置任务并发</strong></p>
<p><strong>mapred-site.xml</strong></p>
<p>  属性：mapreduce.map.speculative  解释：该属性可以设置任务是否可以并发执行，如果任务多而小，该属性设置为true可以明显加快任务执行效率，但是对于延迟非常高的任务，建议改为false，这就类似于迅雷下载。  </p>
<p><strong>12) MR**</strong>输出数据的压缩**</p>
<p><strong>mapred-site.xml</strong></p>
<p>  属性：mapreduce.map.output.compress、mapreduce.output.fileoutputformat.compress  解释：对于大集群而言，建议设置Map-Reduce的输出为压缩的数据，而对于小集群，则不需要。  </p>
<p><strong>13)</strong> <strong>优化Mapper**</strong>和Reducer<strong>**的个数</strong></p>
<p><strong>mapred-site.xml</strong></p>
<p>  属性：  mapreduce.tasktracker.map.tasks.maximum  mapreduce.tasktracker.reduce.tasks.maximum  解释：以上两个属性分别为一个单独的Job任务可以同时运行的Map和Reduce的数量。  设置上面两个参数时，需要考虑CPU核数、磁盘和内存容量。假设一个8核的CPU，业务内容非常消耗CPU，那么可以设置map数量为4，如果该业务不是特别消耗CPU类型的，那么可以设置map数量为40，reduce数量为20。这些参数的值修改完成之后，一定要观察是否有较长等待的任务，如果有的话，可以减少数量以加快任务执行，如果设置一个很大的值，会引起大量的上下文切换，以及内存与磁盘之间的数据交换，这里没有标准的配置数值，需要根据业务和硬件配置以及经验来做出选择。  在同一时刻，不要同时运行太多的MapReduce，这样会消耗过多的内存，任务会执行的非常缓慢，我们需要根据CPU核数，内存容量设置一个MR任务并发的最大值，使固定数据量的任务完全加载到内存中，避免频繁的内存和磁盘数据交换，从而降低磁盘IO，提高性能。  </p>
<p><strong>大概估算公式：</strong></p>
<p>map = 2 + ⅔cpu_core</p>
<p>reduce = 2 + ⅓cpu_core</p>
<h2 id="3-3、Linux优化"><a href="#3-3、Linux优化" class="headerlink" title="3.3、Linux优化"></a>3.3、Linux优化</h2><p><strong>1)</strong> <strong>开启文件系统的预读缓存可以提高读取速度（kb**</strong>）**</p>
<p>  $ sudo blockdev –setra  32768 /dev/sda  </p>
<p>尖叫提示：ra是readahead的缩写</p>
<p><strong>2)</strong> <strong>关闭进程睡眠池</strong></p>
<p>即不允许后台进程进入睡眠状态，如果进程空闲，则直接kill掉释放资源</p>
<p>  $ sudo sysctl -w vm.swappiness=0  </p>
<p><strong>3)</strong> <strong>调整ulimit**</strong>上限，默认值为比较小的数字**</p>
<p>  $ ulimit -n 查看允许最大进程数  $ ulimit -u 查看允许打开最大文件数  </p>
<p><strong>优化修改：</strong></p>
<p>  $ sudo  vi /etc/security/limits.conf 修改打开文件数限制  末尾添加：  <em>        soft  nofile     1024000  </em>        hard  nofile     1024000  Hive       -    nofile     1024000  hive       -    nproc      1024000      $ sudo  vi /etc/security/limits.d/90-nproc.conf 修改用户打开进程数限制  修改为：  #<em>     soft  nproc    4096  #root    soft    nproc   unlimited  </em>     soft  nproc    40960  root    soft    nproc   unlimited  </p>
<p><strong>4)</strong> <strong>开启集群的时间同步NTP</strong></p>
<p>集群中某台机器同步网络时间服务器的时间，集群中其他机器则同步这台机器的时间。</p>
<p><strong>5)</strong> <strong>更新系统补丁</strong></p>
<p>更新补丁前，请先测试新版本补丁对集群节点的兼容性。</p>
<h2 id="3-4、Zookeeper优化"><a href="#3-4、Zookeeper优化" class="headerlink" title="3.4、Zookeeper优化"></a>3.4、Zookeeper优化</h2><p><strong>1)</strong> <strong>优化Zookeeper**</strong>会话超时时间**</p>
<p><strong>hbase-site.xml</strong></p>
<p>  参数：zookeeper.session.timeout  解释：In hbase-site.xml, set zookeeper.session.timeout to 30 seconds or  less to bound failure detection (20-30 seconds is a good start).该值会直接关系到master发现服务器宕机的最大周期，默认值为30秒（不同的HBase版本，该默认值不一样），如果该值过小，会在HBase在写入大量数据发生而GC时，导致RegionServer短暂的不可用，从而没有向ZK发送心跳包，最终导致认为从节点shutdown。一般20台左右的集群需要配置5台zookeeper。  </p>
<h2 id="3-5、HBase优化"><a href="#3-5、HBase优化" class="headerlink" title="3.5、HBase优化"></a>3.5、HBase优化</h2><h3 id="3-5-1、预分区-避免region被无线切分-（他的本质实际上就是预估rowkey的范围）"><a href="#3-5-1、预分区-避免region被无线切分-（他的本质实际上就是预估rowkey的范围）" class="headerlink" title="3.5.1、预分区(避免region被无线切分)（他的本质实际上就是预估rowkey的范围）"></a>3.5.1、预分区(避免region被无线切分)（他的本质实际上就是预估rowkey的范围）</h3><p>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据索要投放的分区提前大致的规划好，以提高HBase性能。</p>
<p><strong>1)</strong> <strong>手动设定预分区（很明显生成五个分区</strong> <strong>负无穷到1000,1000-2000,2000-3000**</strong>，3000-4000,4000-<strong>**正无穷）</strong></p>
<p>  hbase&gt; create animal,’info’,’partition1’,SPLITS  =&gt; [‘1000’,’2000’,’3000’,’4000’]  </p>
<p><img src="/2018/07/02/hbase总结/image-20200510115920725.png" alt="image-20200510115920725"></p>
<p><strong>2)</strong> <strong>生成16**</strong>进制序列预分区(<strong><strong>直接指定15</strong></strong>个分区)**</p>
<p>  create  ‘staff2’,’info’,’partition2’,{NUMREGIONS =&gt; 15, SPLITALGO =&gt;  ‘HexStringSplit’}  </p>
<p><strong>3)</strong> <strong>按照文件中设置的规则预分区</strong></p>
<p>创建splits.txt文件内容如下：</p>
<p>aaaa  </p>
<p>bbbb  </p>
<p>cccc  </p>
<p>dddd  </p>
<p>然后执行：</p>
<p>  create ‘staff3’,’partition3’,SPLITS_FILE  =&gt; ‘splits.txt’  </p>
<p><strong>4)</strong> <strong>使用JavaAPI**</strong>创建预分区**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//自定义算法，产生一系列Hash散列值存储在二维数组中</div><div class="line">byte[][] splitKeys = 某个散列值函数</div><div class="line">//创建HBaseAdmin实例</div><div class="line">HBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create());</div><div class="line">//创建HTableDescriptor实例</div><div class="line">HTableDescriptor tableDesc = new HTableDescriptor(tableName);</div><div class="line">//通过HTableDescriptor实例和散列值二维数组创建带有预分区的HBase表</div><div class="line">hAdmin.createTable(tableDesc, splitKeys);</div></pre></td></tr></table></figure>
<p>注意：分区的rowkey必须是有序的递增。否则没有意义</p>
<p> 例如：create animal,’info’,’partition1’,SPLITS =&gt; [‘4000’,’2000’]</p>
<p>上面的分区脚本很明显会分成三个分区，分别是：-无穷大到4000,4000到2000,2000到正无穷大。这样的分区很明显是存在重叠的。</p>
<h3 id="3-5-2、RowKey设计"><a href="#3-5-2、RowKey设计" class="headerlink" title="3.5.2、RowKey设计"></a>3.5.2、RowKey设计</h3><p>一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案。</p>
<p><strong>1)</strong> <strong>生成随机数、hash**</strong>、散列值**</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">比如：</div><div class="line">原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7</div><div class="line">原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd</div><div class="line">原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913</div><div class="line">在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。</div></pre></td></tr></table></figure>
<p><strong>2)</strong> <strong>字符串反转</strong></p>
<p>  20170524000001转成10000042507102  20170524000002转成20000042507102  </p>
<p>这样也可以在一定程度上散列逐步put进来的数据。</p>
<p><strong>3)</strong> <strong>字符串拼接</strong></p>
<p>  20170524000001_a12e  20170524000001_93i7  </p>
<h3 id="3-5-3、内存优化"><a href="#3-5-3、内存优化" class="headerlink" title="3.5.3、内存优化"></a>3.5.3、内存优化</h3><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p>
<h3 id="3-5-4、基础优化"><a href="#3-5-4、基础优化" class="headerlink" title="3.5.4、基础优化"></a>3.5.4、基础优化</h3><p><strong>1)</strong> <strong>允许在HDFS**</strong>的文件中追加内容**</p>
<p>不是不允许追加内容么？没错，请看背景故事：</p>
<p><a href="http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/" target="_blank" rel="external">http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/</a></p>
<p><strong>hdfs-site.xml**</strong>、hbase-site.xml**</p>
<p>  属性：dfs.support.append  解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。  </p>
<p><strong>2)</strong> <strong>优化DataNode**</strong>允许的最大文件打开数**</p>
<p><strong>hdfs-site.xml</strong></p>
<p>  属性：dfs.datanode.max.transfer.threads  解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096  </p>
<p><strong>3)</strong> <strong>优化延迟高的数据操作的等待时间</strong></p>
<p><strong>hdfs-site.xml</strong></p>
<p>  属性：dfs.image.transfer.timeout  解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。  </p>
<p><strong>4)</strong> <strong>优化数据的写入效率</strong></p>
<p><strong>mapred-site.xml</strong></p>
<p>  属性：  mapreduce.map.output.compress  mapreduce.map.output.compress.codec  解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。  </p>
<p><strong>5)</strong> <strong>优化DataNode**</strong>存储**</p>
<p>  属性：dfs.datanode.failed.volumes.tolerated  解释： 默认为0，意思是当DataNode中有一个磁盘出现故障，则会认为该DataNode shutdown了。如果修改为1，则一个磁盘出现故障时，数据会被复制到其他正常的DataNode上，当前的DataNode继续工作。  </p>
<p><strong>6)</strong> <strong>设置RPC**</strong>监听数量**</p>
<p><strong>hbase-site.xml</strong></p>
<p>  属性：hbase.regionserver.handler.count  解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。  </p>
<p><strong>7)</strong> <strong>优化HStore**</strong>文件大小**</p>
<p><strong>hbase-site.xml</strong></p>
<p>  属性：hbase.hregion.max.filesize  解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。  </p>
<p><strong>8)</strong> <strong>优化hbase**</strong>客户端缓存**</p>
<p><strong>hbase-site.xml</strong></p>
<p>  属性：hbase.client.write.buffer  解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。  </p>
<p><strong>9)</strong> <strong>指定scan.next**</strong>扫描HBase<strong>**所获取的行数</strong></p>
<p><strong>hbase-site.xml</strong></p>
<p>  属性：hbase.client.scanner.caching  解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。  </p>
<p><strong>10) flush**</strong>、compact<strong><strong>、split</strong></strong>机制**</p>
<p>当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。</p>
<p><strong>涉及属性：</strong></p>
<p>即：128M就是Memstore的默认阈值</p>
<p>  hbase.hregion.memstore.flush.size：134217728  </p>
<p>即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。</p>
<p>  hbase.regionserver.global.memstore.upperLimit：0.4  hbase.regionserver.global.memstore.lowerLimit：0.38  </p>
<p>即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit</p>
<h1 id="四、HBase项目"><a href="#四、HBase项目" class="headerlink" title="四、HBase项目"></a>四、HBase项目</h1><h2 id="4-1、涉及概念梳理：命名空间"><a href="#4-1、涉及概念梳理：命名空间" class="headerlink" title="4.1、涉及概念梳理：命名空间"></a>4.1、涉及概念梳理：命名空间</h2><h3 id="4-1-1、命名空间的结构"><a href="#4-1-1、命名空间的结构" class="headerlink" title="4.1.1、命名空间的结构"></a>4.1.1、命名空间的结构</h3><p><img src="/2018/07/02/hbase总结/image-20200510120021250.png" alt="image-20200510120021250"></p>
<p><strong>1) Table</strong>：表，所有的表都是命名空间的成员，即表必属于某个命名空间，如果没有指定，则在default默认的命名空间中。</p>
<p><strong>2) RegionServer group**</strong>：**一个命名空间包含了默认的RegionServer Group。</p>
<p><strong>3) Permission**</strong>：**权限，命名空间能够让我们来定义访问控制列表ACL（Access Control List）。例如，创建表，读取表，删除，更新等等操作。</p>
<p><strong>4) Quota**</strong>：**限额，可以强制一个命名空间可包含的region的数量。（属性：hbase.quota.enabled）</p>
<h3 id="4-1-2、命名空间的使用"><a href="#4-1-2、命名空间的使用" class="headerlink" title="4.1.2、命名空间的使用"></a>4.1.2、命名空间的使用</h3><p>1) <strong>创建命名空间</strong></p>
<p>  hbase(main):002:0&gt; create_namespace ‘ns_school’  </p>
<p>2) <strong>创建表时指定命名空间</strong></p>
<p>  hbase(main):004:0&gt; create  ‘ns_school:tbl_student’,’info’  </p>
<p><strong>3)</strong> <strong>观察HDFS**</strong>中的目录结构的变化**</p>
<p><img src="/2018/07/02/hbase总结/image-20200510120028174.png" alt="image-20200510120028174"></p>
<h2 id="4-2、微博系统"><a href="#4-2、微博系统" class="headerlink" title="4.2、微博系统"></a>4.2、微博系统</h2><h3 id="4-1-1、需求分析"><a href="#4-1-1、需求分析" class="headerlink" title="4.1.1、需求分析"></a>4.1.1、需求分析</h3><p>1) 微博内容的浏览，数据库表设计</p>
<p>2) 用户社交体现：关注用户，取关用户</p>
<p>3) 拉取关注的人的微博内容</p>
<h3 id="4-1-2、代码实现"><a href="#4-1-2、代码实现" class="headerlink" title="4.1.2、代码实现"></a>4.1.2、代码实现</h3><p><strong>代码设计总览：</strong></p>
<p>1) 创建命名空间以及表名的定义</p>
<p>2) 创建微博内容表</p>
<p>3) 创建用户关系表</p>
<p>4) 创建用户微博内容接收邮件表</p>
<p>5) 发布微博内容</p>
<p>6) 添加关注用户</p>
<p>7) 移除（取关）用户</p>
<p>8) 获取关注的人的微博内容</p>
<p>9) 测试</p>
<p><strong>1)</strong> <strong>创建命名空间以及表名的定义</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">//获取配置conf</div><div class="line">private Configuration conf = HBaseConfiguration.create();</div><div class="line">//微博内容表的表名</div><div class="line">private static final byte[] TABLE_CONTENT = Bytes.toBytes(&quot;ns_weibo:content&quot;);</div><div class="line">//用户关系表的表名</div><div class="line">private static final byte[] TABLE_RELATION = Bytes.toBytes(&quot;ns_weibo:relation&quot;);</div><div class="line">//微博收件箱表的表名</div><div class="line">private static final byte[] TABLE_INBOX = Bytes.toBytes(&quot;ns_weibo:inbox&quot;);</div><div class="line"></div><div class="line">/**</div><div class="line"> * 初始化命名空间</div><div class="line"> * @param args</div><div class="line"> */</div><div class="line">public void initNamespace()&#123;</div><div class="line">	HBaseAdmin admin = null;</div><div class="line">	try &#123;</div><div class="line">		Connection connection = ConnectionFactory.createConnection(conf);</div><div class="line">		admin = (HBaseAdmin) connection.getAdmin();</div><div class="line">		//命名空间类似于关系型数据库中的schema，可以想象成文件夹</div><div class="line">		NamespaceDescriptor weibo = NamespaceDescriptor</div><div class="line">				.create(&quot;ns_weibo&quot;)</div><div class="line">				.addConfiguration(&quot;creator&quot;, &quot;Jinji&quot;)</div><div class="line">				.addConfiguration(&quot;create_time&quot;, System.currentTimeMillis() + &quot;&quot;)</div><div class="line">				.build();</div><div class="line">		admin.createNamespace(weibo);</div><div class="line">	&#125; catch (MasterNotRunningException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125; catch (ZooKeeperConnectionException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125; catch (IOException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;finally&#123;</div><div class="line">		if(null != admin)&#123;</div><div class="line">			try &#123;</div><div class="line">				admin.close();</div><div class="line">			&#125; catch (IOException e) &#123;</div><div class="line">				e.printStackTrace();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>2)</strong> <strong>创建微博内容表</strong></p>
<p><strong>表结构：</strong></p>
<table>
<thead>
<tr>
<th><strong>方法名</strong></th>
<th>creatTableeContent</th>
</tr>
</thead>
<tbody>
<tr>
<td>Table Name</td>
<td>ns_weibo:content</td>
</tr>
<tr>
<td>RowKey</td>
<td>用户ID_时间戳</td>
</tr>
<tr>
<td>ColumnFamily</td>
<td>info</td>
</tr>
<tr>
<td>ColumnLabel</td>
<td>标题,内容,图片</td>
</tr>
<tr>
<td>Version</td>
<td>1个版本</td>
</tr>
</tbody>
</table>
<p><strong>代码：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/**</div><div class="line"> * 创建微博内容表</div><div class="line"> * Table Name:ns_weibo:content</div><div class="line"> * RowKey:用户ID_时间戳</div><div class="line"> * ColumnFamily:info</div><div class="line"> * ColumnLabel:标题,内容,图片URL</div><div class="line"> * Version:1个版本</div><div class="line"> */</div><div class="line">public void createTableContent()&#123;</div><div class="line">	HBaseAdmin admin = null;</div><div class="line">	Connection connection = null;</div><div class="line">	try &#123;</div><div class="line">		connection = ConnectionFactory.createConnection(conf);</div><div class="line">		admin = (HBaseAdmin) connection.getAdmin();</div><div class="line">		//创建表表述</div><div class="line">		HTableDescriptor contentTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_CONTENT));</div><div class="line">		//创建列族描述</div><div class="line">		HColumnDescriptor infoColumnDescriptor = new HColumnDescriptor(Bytes.toBytes(&quot;info&quot;));</div><div class="line">		//设置块缓存</div><div class="line">		infoColumnDescriptor.setBlockCacheEnabled(true);</div><div class="line">		//设置块缓存大小</div><div class="line">		infoColumnDescriptor.setBlocksize(2097152);</div><div class="line">		//设置压缩方式</div><div class="line">//			infoColumnDescriptor.setCompressionType(Algorithm.SNAPPY);</div><div class="line">		//设置版本确界</div><div class="line">		infoColumnDescriptor.setMaxVersions(1);</div><div class="line">		infoColumnDescriptor.setMinVersions(1);</div><div class="line">		contentTableDescriptor.addFamily(infoColumnDescriptor);</div><div class="line">		admin.createTable(contentTableDescriptor);</div><div class="line">		</div><div class="line">	&#125; catch (IOException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125; finally&#123;</div><div class="line">		if(null != admin)&#123;</div><div class="line">			try &#123;</div><div class="line">				admin.close();</div><div class="line">				connection.close();</div><div class="line">			&#125; catch (IOException e) &#123;</div><div class="line">				e.printStackTrace();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>3)</strong> <strong>创建用户关系表</strong></p>
<p><strong>表结构：</strong></p>
<table>
<thead>
<tr>
<th><strong>方法名</strong></th>
<th>createTableRelations</th>
</tr>
</thead>
<tbody>
<tr>
<td>Table Name</td>
<td>ns_weibo:relation</td>
</tr>
<tr>
<td>RowKey</td>
<td>用户ID</td>
</tr>
<tr>
<td>ColumnFamily</td>
<td>attends、fans</td>
</tr>
<tr>
<td>ColumnLabel</td>
<td>关注用户ID，粉丝用户ID</td>
</tr>
<tr>
<td>ColumnValue</td>
<td>用户ID</td>
</tr>
<tr>
<td>Version</td>
<td>1个版本</td>
</tr>
</tbody>
</table>
<p><strong>代码：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/**</div><div class="line"> * 用户关系表</div><div class="line"> * Table Name:ns_weibo:relation</div><div class="line"> * RowKey:用户ID</div><div class="line"> * ColumnFamily:attends,fans</div><div class="line"> * ColumnLabel:关注用户ID，粉丝用户ID</div><div class="line"> * ColumnValue:用户ID</div><div class="line"> * Version：1个版本</div><div class="line"> */</div><div class="line">public void createTableRelation()&#123;</div><div class="line">	HBaseAdmin admin = null;</div><div class="line">	try &#123;</div><div class="line">		Connection connection = ConnectionFactory.createConnection(conf);</div><div class="line">		admin = (HBaseAdmin) connection.getAdmin();</div><div class="line">		HTableDescriptor relationTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_RELATION));</div><div class="line">		</div><div class="line">		//关注的人的列族</div><div class="line">		HColumnDescriptor attendColumnDescriptor = new HColumnDescriptor(Bytes.toBytes(&quot;attends&quot;));</div><div class="line">		//设置块缓存</div><div class="line">		attendColumnDescriptor.setBlockCacheEnabled(true);</div><div class="line">		//设置块缓存大小</div><div class="line">		attendColumnDescriptor.setBlocksize(2097152);</div><div class="line">		//设置压缩方式</div><div class="line">//			attendColumnDescriptor.setCompressionType(Algorithm.SNAPPY);</div><div class="line">		//设置版本确界</div><div class="line">		attendColumnDescriptor.setMaxVersions(1);</div><div class="line">		attendColumnDescriptor.setMinVersions(1);</div><div class="line">		</div><div class="line">		//粉丝列族</div><div class="line">		HColumnDescriptor fansColumnDescriptor = new HColumnDescriptor(Bytes.toBytes(&quot;fans&quot;));</div><div class="line">		fansColumnDescriptor.setBlockCacheEnabled(true);</div><div class="line">		fansColumnDescriptor.setBlocksize(2097152);</div><div class="line">		fansColumnDescriptor.setMaxVersions(1);</div><div class="line">		fansColumnDescriptor.setMinVersions(1);</div><div class="line">		</div><div class="line">		relationTableDescriptor.addFamily(attendColumnDescriptor);</div><div class="line">		relationTableDescriptor.addFamily(fansColumnDescriptor);</div><div class="line">		admin.createTable(relationTableDescriptor);</div><div class="line">		</div><div class="line">	&#125; catch (MasterNotRunningException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125; catch (ZooKeeperConnectionException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125; catch (IOException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;finally&#123;</div><div class="line">		if(null != admin)&#123;</div><div class="line">			try &#123;</div><div class="line">				admin.close();</div><div class="line">			&#125; catch (IOException e) &#123;</div><div class="line">				e.printStackTrace();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>4)</strong> <strong>创建微博收件箱表</strong></p>
<p><strong>表结构：</strong></p>
<table>
<thead>
<tr>
<th><strong>方法名</strong></th>
<th>createTableInbox</th>
</tr>
</thead>
<tbody>
<tr>
<td>Table Name</td>
<td>ns_weibo:inbox</td>
</tr>
<tr>
<td>RowKey</td>
<td>用户ID</td>
</tr>
<tr>
<td>ColumnFamily</td>
<td>info</td>
</tr>
<tr>
<td>ColumnLabel</td>
<td>用户ID</td>
</tr>
<tr>
<td>ColumnValue</td>
<td>取微博内容的RowKey</td>
</tr>
<tr>
<td>Version</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p><strong>代码：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/**</div><div class="line"> * 创建微博收件箱表</div><div class="line"> * Table Name: ns_weibo:inbox</div><div class="line"> * RowKey:用户ID</div><div class="line"> * ColumnFamily:info</div><div class="line"> * ColumnLabel:用户ID_发布微博的人的用户ID</div><div class="line"> * ColumnValue:关注的人的微博的RowKey</div><div class="line"> * Version:1000</div><div class="line"> */</div><div class="line">public void createTableInbox()&#123;</div><div class="line">	HBaseAdmin admin = null;</div><div class="line">	try &#123;</div><div class="line">		Connection connection = ConnectionFactory.createConnection(conf);</div><div class="line">		admin = (HBaseAdmin) connection.getAdmin();</div><div class="line">		</div><div class="line">		HTableDescriptor inboxTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_INBOX));</div><div class="line">		HColumnDescriptor infoColumnDescriptor = new HColumnDescriptor(Bytes.toBytes(&quot;info&quot;));</div><div class="line">		</div><div class="line">		infoColumnDescriptor.setBlockCacheEnabled(true);</div><div class="line">		infoColumnDescriptor.setBlocksize(2097152);</div><div class="line">		infoColumnDescriptor.setMaxVersions(1000);</div><div class="line">		infoColumnDescriptor.setMinVersions(1000);</div><div class="line">		</div><div class="line">		inboxTableDescriptor.addFamily(infoColumnDescriptor);;</div><div class="line">		admin.createTable(inboxTableDescriptor);</div><div class="line">	&#125; catch (MasterNotRunningException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125; catch (ZooKeeperConnectionException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125; catch (IOException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;finally&#123;</div><div class="line">		if(null != admin)&#123;</div><div class="line">			try &#123;</div><div class="line">				admin.close();</div><div class="line">			&#125; catch (IOException e) &#123;</div><div class="line">				e.printStackTrace();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>5)</strong> <strong>发布微博内容</strong></p>
<p>a、微博内容表中添加1条数据</p>
<p>b、微博收件箱表对所有粉丝用户添加数据</p>
<p><strong>代码：Message.java</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">package com.z.hbase.weibo;</div><div class="line"></div><div class="line">public class Message &#123;</div><div class="line">	private String uid;</div><div class="line">	private String timestamp;</div><div class="line">	private String content;</div><div class="line">	</div><div class="line">	public String getUid() &#123;</div><div class="line">		return uid;</div><div class="line">	&#125;</div><div class="line">	public void setUid(String uid) &#123;</div><div class="line">		this.uid = uid;</div><div class="line">	&#125;</div><div class="line">	public String getTimestamp() &#123;</div><div class="line">		return timestamp;</div><div class="line">	&#125;</div><div class="line">	public void setTimestamp(String timestamp) &#123;</div><div class="line">		this.timestamp = timestamp;</div><div class="line">	&#125;</div><div class="line">	public String getContent() &#123;</div><div class="line">		return content;</div><div class="line">	&#125;</div><div class="line">	public void setContent(String content) &#123;</div><div class="line">		this.content = content;</div><div class="line">	&#125;</div><div class="line">	@Override</div><div class="line">	public String toString() &#123;</div><div class="line">		return &quot;Message [uid=&quot; + uid + &quot;, timestamp=&quot; + timestamp + &quot;, content=&quot; + content + &quot;]&quot;;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>代码：public void publishContent(String uid, String content)</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/**</div><div class="line"> * 发布微博</div><div class="line"> * a、微博内容表中数据+1</div><div class="line"> * b、向微博收件箱表中加入微博的Rowkey</div><div class="line"> */</div><div class="line">public void publishContent(String uid, String content)&#123;</div><div class="line">	Connection connection = null;</div><div class="line">	try &#123;</div><div class="line">		connection = ConnectionFactory.createConnection(conf);</div><div class="line">		</div><div class="line">		//a、微博内容表中添加1条数据，首先获取微博内容表描述</div><div class="line">		Table contentTable = connection.getTable(TableName.valueOf(TABLE_CONTENT));</div><div class="line">		//组装Rowkey</div><div class="line">		long timestamp = System.currentTimeMillis();</div><div class="line">		String rowKey = uid + &quot;_&quot; + timestamp;</div><div class="line">		//添加微博内容</div><div class="line">		Put put = new Put(Bytes.toBytes(rowKey));</div><div class="line">		put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;content&quot;), timestamp, Bytes.toBytes(content));</div><div class="line">		contentTable.put(put);</div><div class="line">		</div><div class="line">		//b、向微博收件箱表中加入发布的Rowkey</div><div class="line">		//b.1、查询用户关系表，得到当前用户有哪些粉丝</div><div class="line">		Table relationTable = connection.getTable(TableName.valueOf(TABLE_RELATION));</div><div class="line">		//b.2、取出目标数据</div><div class="line">		Get get = new Get(Bytes.toBytes(uid));</div><div class="line">		get.addFamily(Bytes.toBytes(&quot;fans&quot;));</div><div class="line">		</div><div class="line">		Result result = relationTable.get(get);</div><div class="line">		List&lt;byte[]&gt; fans = new ArrayList&lt;byte[]&gt;();</div><div class="line">		</div><div class="line">		//遍历取出当前发布微博的用户的所有粉丝数据</div><div class="line">		for(Cell cell : result.rawCells())&#123;</div><div class="line">			fans.add(CellUtil.cloneQualifier(cell));</div><div class="line">		&#125;</div><div class="line">		//如果该用户没有粉丝，则直接return</div><div class="line">		if(fans.size() &lt;= 0) return;	</div><div class="line">		//开始操作收件箱表</div><div class="line">		Table inboxTable = connection.getTable(TableName.valueOf(TABLE_INBOX));</div><div class="line">		//每一个粉丝，都要向收件箱中添加该微博的内容，所以每一个粉丝都是一个Put对象</div><div class="line">		List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;();</div><div class="line">		for(byte[] fan : fans)&#123;</div><div class="line">			Put fansPut = new Put(fan);</div><div class="line">			fansPut.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(uid), timestamp, Bytes.toBytes(rowKey));</div><div class="line">			puts.add(fansPut);</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		inboxTable.put(puts);</div><div class="line">	&#125; catch (IOException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;finally&#123;</div><div class="line">		if(null != connection)&#123;</div><div class="line">			try &#123;</div><div class="line">				connection.close();</div><div class="line">			&#125; catch (IOException e) &#123;</div><div class="line">				e.printStackTrace();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>6)</strong> <strong>添加关注用户</strong></p>
<p>a、在微博用户关系表中，对当前主动操作的用户添加新关注的好友</p>
<p>b、在微博用户关系表中，对被关注的用户添加新的粉丝</p>
<p>c、微博收件箱表中添加所关注的用户发布的微博</p>
<p><strong>代码实现：public void addAttends(String uid, String… attends)</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/**</div><div class="line"> * 关注用户逻辑</div><div class="line"> * a、在微博用户关系表中，对当前主动操作的用户添加新的关注的好友</div><div class="line"> * b、在微博用户关系表中，对被关注的用户添加粉丝（当前操作的用户）</div><div class="line"> * c、当前操作用户的微博收件箱添加所关注的用户发布的微博rowkey</div><div class="line"> */</div><div class="line">public void addAttends(String uid, String... attends)&#123;</div><div class="line">	//参数过滤</div><div class="line">	if(attends == null || attends.length &lt;= 0 || uid == null || uid.length() &lt;= 0)&#123;</div><div class="line">		return;</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	Connection connection = null;</div><div class="line">	try &#123;</div><div class="line">		connection = ConnectionFactory.createConnection(conf);</div><div class="line">		</div><div class="line">		//用户关系表操作对象（连接到用户关系表）</div><div class="line">		Table relationTable = connection.getTable(TableName.valueOf(TABLE_RELATION));</div><div class="line">		List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;();</div><div class="line">		//a、在微博用户关系表中，添加新关注的好友</div><div class="line">		Put attendPut = new Put(Bytes.toBytes(uid));</div><div class="line">		for(String attend : attends)&#123;</div><div class="line">			//为当前用户添加关注的人</div><div class="line">			attendPut.addColumn(Bytes.toBytes(&quot;attends&quot;), Bytes.toBytes(attend), Bytes.toBytes(attend));</div><div class="line">			//b、为被关注的人，添加粉丝</div><div class="line">			Put fansPut = new Put(Bytes.toBytes(attend));</div><div class="line">			fansPut.addColumn(Bytes.toBytes(&quot;fans&quot;), Bytes.toBytes(uid), Bytes.toBytes(uid));</div><div class="line">			//将所有关注的人一个一个的添加到puts（List）集合中</div><div class="line">			puts.add(fansPut);</div><div class="line">		&#125;</div><div class="line">		puts.add(attendPut);</div><div class="line">		relationTable.put(puts);</div><div class="line">		</div><div class="line">		//c.1、微博收件箱添加关注的用户发布的微博内容（content）的rowkey</div><div class="line">		Table contentTable = connection.getTable(TableName.valueOf(TABLE_CONTENT));</div><div class="line">		Scan scan = new Scan();</div><div class="line">		//用于存放取出来的关注的人所发布的微博的rowkey</div><div class="line">		List&lt;byte[]&gt; rowkeys = new ArrayList&lt;byte[]&gt;();</div><div class="line">		</div><div class="line">		for(String attend : attends)&#123;</div><div class="line">			//过滤扫描rowkey，即：前置位匹配被关注的人的uid_</div><div class="line">			RowFilter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(attend + &quot;_&quot;));</div><div class="line">			//为扫描对象指定过滤规则</div><div class="line">			scan.setFilter(filter);</div><div class="line">			//通过扫描对象得到scanner</div><div class="line">			ResultScanner result = contentTable.getScanner(scan);</div><div class="line">			//迭代器遍历扫描出来的结果集</div><div class="line">			Iterator&lt;Result&gt; iterator = result.iterator();</div><div class="line">			while(iterator.hasNext())&#123;</div><div class="line">				//取出每一个符合扫描结果的那一行数据</div><div class="line">				Result r = iterator.next();</div><div class="line">				for(Cell cell : r.rawCells())&#123;</div><div class="line">					//将得到的rowkey放置于集合容器中</div><div class="line">					rowkeys.add(CellUtil.cloneRow(cell));</div><div class="line">				&#125;</div><div class="line">				</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		//c.2、将取出的微博rowkey放置于当前操作的用户的收件箱中</div><div class="line">		if(rowkeys.size() &lt;= 0) return;</div><div class="line">		//得到微博收件箱表的操作对象</div><div class="line">		Table inboxTable = connection.getTable(TableName.valueOf(TABLE_INBOX));</div><div class="line">		//用于存放多个关注的用户的发布的多条微博rowkey信息</div><div class="line">		List&lt;Put&gt; inboxPutList = new ArrayList&lt;Put&gt;();</div><div class="line">		for(byte[] rk : rowkeys)&#123;</div><div class="line">			Put put = new Put(Bytes.toBytes(uid));</div><div class="line">			//uid_timestamp</div><div class="line">			String rowKey = Bytes.toString(rk);</div><div class="line">			//截取uid</div><div class="line">			String attendUID = rowKey.substring(0, rowKey.indexOf(&quot;_&quot;));</div><div class="line">			long timestamp = Long.parseLong(rowKey.substring(rowKey.indexOf(&quot;_&quot;) + 1));</div><div class="line">			//将微博rowkey添加到指定单元格中</div><div class="line">			put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(attendUID), timestamp, rk);</div><div class="line">			inboxPutList.add(put);</div><div class="line">		&#125;</div><div class="line">		inboxTable.put(inboxPutList);</div><div class="line">		</div><div class="line">	&#125; catch (IOException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;finally&#123;</div><div class="line">		if(null != connection)&#123;</div><div class="line">			try &#123;</div><div class="line">				connection.close();</div><div class="line">			&#125; catch (IOException e) &#123;</div><div class="line">				// TODO Auto-generated catch block</div><div class="line">				e.printStackTrace();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>7)</strong> <strong>移除（取关）用户</strong></p>
<p>a、在微博用户关系表中，对当前主动操作的用户移除取关的好友(attends)</p>
<p>b、在微博用户关系表中，对被取关的用户移除粉丝</p>
<p>c、微博收件箱中删除取关的用户发布的微博</p>
<p><strong>代码：public void removeAttends(String uid, String… attends)</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/**</div><div class="line"> * 取消关注（remove)</div><div class="line"> * a、在微博用户关系表中，对当前主动操作的用户删除对应取关的好友</div><div class="line"> * b、在微博用户关系表中，对被取消关注的人删除粉丝（当前操作人）</div><div class="line"> * c、从收件箱中，删除取关的人的微博的rowkey</div><div class="line"> * </div><div class="line"> */</div><div class="line">public void removeAttends(String uid, String... attends)&#123;</div><div class="line">	//过滤数据</div><div class="line">	if(uid == null || uid.length() &lt;= 0 || attends == null || attends.length &lt;= 0) return;</div><div class="line">	</div><div class="line">	try &#123;</div><div class="line">		Connection connection = ConnectionFactory.createConnection(conf);</div><div class="line">		</div><div class="line">		//a、在微博用户关系表中，删除已关注的好友</div><div class="line">		Table relationTable = connection.getTable(TableName.valueOf(TABLE_RELATION));</div><div class="line">		//待删除的用户关系表中的所有数据</div><div class="line">		List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;();</div><div class="line">		//当前取关操作者的uid对应的Delete对象</div><div class="line">		Delete attendDelete = new Delete(Bytes.toBytes(uid));</div><div class="line">		//遍历取关，同时每次取关都要将被取关的人的粉丝-1</div><div class="line">		for(String attend : attends)&#123;</div><div class="line">			attendDelete.addColumn(Bytes.toBytes(&quot;attends&quot;), Bytes.toBytes(attend));</div><div class="line">			//b、在微博用户关系表中，对被取消关注的人删除粉丝（当前操作人）</div><div class="line">			Delete fansDelete = new Delete(Bytes.toBytes(attend));</div><div class="line">			fansDelete.addColumn(Bytes.toBytes(&quot;fans&quot;), Bytes.toBytes(uid));</div><div class="line">			deleteList.add(fansDelete);</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		deleteList.add(attendDelete);</div><div class="line">		relationTable.delete(deleteList);</div><div class="line">		</div><div class="line">		//c、删除取关的人的微博rowkey 从 收件箱表中</div><div class="line">		Table inboxTable = connection.getTable(TableName.valueOf(TABLE_INBOX));</div><div class="line">		</div><div class="line">		Delete inboxDelete = new Delete(Bytes.toBytes(uid));</div><div class="line">		for(String attend : attends)&#123;</div><div class="line">			inboxDelete.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(attend));</div><div class="line">		&#125;</div><div class="line">		inboxTable.delete(inboxDelete);</div><div class="line">	&#125; catch (IOException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>8)</strong> <strong>获取关注的人的微博内容</strong></p>
<p>a、从微博收件箱中获取所关注的用户的微博RowKey </p>
<p>b、根据获取的RowKey，得到微博内容</p>
<p><strong>代码实现：public List getAttendsContent(String uid)</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/**</div><div class="line"> * 获取微博实际内容</div><div class="line"> * a、从微博收件箱中获取所有关注的人的发布的微博的rowkey</div><div class="line"> * b、根据得到的rowkey去微博内容表中得到数据</div><div class="line"> * c、将得到的数据封装到Message对象中</div><div class="line"> */</div><div class="line">public List&lt;Message&gt; getAttendsContent(String uid)&#123;</div><div class="line">	Connection connection = null;</div><div class="line">	try &#123;</div><div class="line">		connection = ConnectionFactory.createConnection(conf);</div><div class="line">		</div><div class="line">		Table inboxTable = connection.getTable(TableName.valueOf(TABLE_INBOX));</div><div class="line">		//a、从收件箱中取得微博rowKey</div><div class="line">		Get get = new Get(Bytes.toBytes(uid));</div><div class="line">		//设置最大版本号</div><div class="line">		get.setMaxVersions(5);</div><div class="line">		List&lt;byte[]&gt; rowkeys = new ArrayList&lt;byte[]&gt;();</div><div class="line">		Result result = inboxTable.get(get);</div><div class="line">		for(Cell cell : result.rawCells())&#123;</div><div class="line">			rowkeys.add(CellUtil.cloneValue(cell));</div><div class="line">		&#125;</div><div class="line">		//b、根据取出的所有rowkey去微博内容表中检索数据</div><div class="line">		Table contentTable = connection.getTable(TableName.valueOf(TABLE_CONTENT));</div><div class="line">		List&lt;Get&gt; gets = new ArrayList&lt;Get&gt;();</div><div class="line">		//根据rowkey取出对应微博的具体内容</div><div class="line">		for(byte[] rk : rowkeys)&#123;</div><div class="line">			Get g = new Get(rk);</div><div class="line">			gets.add(g);</div><div class="line">		&#125;</div><div class="line">		//得到所有的微博内容的result对象</div><div class="line">		Result[] results = contentTable.get(gets);</div><div class="line">		//将每一条微博内容都封装为消息对象</div><div class="line">		List&lt;Message&gt; messages = new ArrayList&lt;Message&gt;();</div><div class="line">		for(Result res : results)&#123;</div><div class="line">			for(Cell cell : res.rawCells())&#123;</div><div class="line">				Message message = new Message();</div><div class="line">				</div><div class="line">				String rowKey = Bytes.toString(CellUtil.cloneRow(cell));</div><div class="line">				String userid = rowKey.substring(0, rowKey.indexOf(&quot;_&quot;));</div><div class="line">				String timestamp = rowKey.substring(rowKey.indexOf(&quot;_&quot;) + 1);</div><div class="line">				String content = Bytes.toString(CellUtil.cloneValue(cell));</div><div class="line">				</div><div class="line">				message.setContent(content);</div><div class="line">				message.setTimestamp(timestamp);</div><div class="line">				message.setUid(userid);</div><div class="line">				</div><div class="line">				messages.add(message);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		return messages;</div><div class="line">	&#125; catch (IOException e) &#123;</div><div class="line">		e.printStackTrace();</div><div class="line">	&#125;finally&#123;</div><div class="line">		try &#123;</div><div class="line">			connection.close();</div><div class="line">		&#125; catch (IOException e) &#123;</div><div class="line">			e.printStackTrace();</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	return null;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>9)</strong> <strong>测试</strong></p>
<p><strong>–</strong> <strong>测试发布微博内容</strong> </p>
<p>public void testPublishContent(WeiBo wb)</p>
<p><strong>–</strong> <strong>测试添加关注</strong></p>
<p>public void testAddAttend(WeiBo wb)</p>
<p><strong>–</strong> <strong>测试取消关注</strong></p>
<p>public void testRemoveAttend(WeiBo wb)</p>
<p><strong>–</strong> <strong>测试展示内容</strong></p>
<p>public void testShowMessage(WeiBo wb)</p>
<p><strong>代码：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/**</div><div class="line"> * 发布微博内容</div><div class="line"> * 添加关注</div><div class="line"> * 取消关注</div><div class="line"> * 展示内容</div><div class="line"> */</div><div class="line">public void testPublishContent(WeiBo wb)&#123;</div><div class="line">	wb.publishContent(&quot;0001&quot;, &quot;今天买了一包空气，送了点薯片，非常开心！！&quot;);</div><div class="line">	wb.publishContent(&quot;0001&quot;, &quot;今天天气不错。&quot;);</div><div class="line">&#125;</div><div class="line"></div><div class="line">public void testAddAttend(WeiBo wb)&#123;</div><div class="line">	wb.publishContent(&quot;0008&quot;, &quot;准备下课！&quot;);</div><div class="line">	wb.publishContent(&quot;0009&quot;, &quot;准备关机！&quot;);</div><div class="line">	wb.addAttends(&quot;0001&quot;, &quot;0008&quot;, &quot;0009&quot;);</div><div class="line">&#125;</div><div class="line"></div><div class="line">public void testRemoveAttend(WeiBo wb)&#123;</div><div class="line">	wb.removeAttends(&quot;0001&quot;, &quot;0008&quot;);</div><div class="line">&#125;</div><div class="line"></div><div class="line">public void testShowMessage(WeiBo wb)&#123;</div><div class="line">	List&lt;Message&gt; messages = wb.getAttendsContent(&quot;0001&quot;);</div><div class="line">	for(Message message : messages)&#123;</div><div class="line">		System.out.println(message);</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">public static void main(String[] args) &#123;</div><div class="line">	WeiBo weibo = new WeiBo();</div><div class="line">	weibo.initTable();	</div><div class="line">	</div><div class="line">	weibo.testPublishContent(weibo);</div><div class="line">	weibo.testAddAttend(weibo);</div><div class="line">	weibo.testShowMessage(weibo);</div><div class="line">	weibo.testRemoveAttend(weibo);</div><div class="line">	weibo.testShowMessage(weibo);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>不一定所有的企业都会使用HBase，大数据的框架可以是相互配合相互依赖的，同时，根据不同的业务，部分框架之间的使用也可以是相互独立的。例如有些企业在处理整个业务时，只是用HDFS+Spark部分的内容。所以在学习HBase框架时，一定要有宏观思维，了解其框架特性，不一定非要在所有的业务中使用所有的框架，要具体情况具体分析，酌情选择。</p>
<h2 id="5-1、HBase在商业项目中的能力"><a href="#5-1、HBase在商业项目中的能力" class="headerlink" title="5.1、HBase在商业项目中的能力"></a>5.1、HBase在商业项目中的能力</h2><p><strong>每天：</strong></p>
<p>1) 消息量：发送和接收的消息数超过60亿-一天每秒七万次请求</p>
<p>2) 将近1000亿条数据的读写</p>
<p>3) 高峰期每秒150万左右操作</p>
<p>4) 整体读取数据占有约55%，写入占有45%</p>
<p>5) 超过2PB的数据，涉及冗余共6PB数据</p>
<p>6) 数据每月大概增长300千兆字节。</p>
<h2 id="5-2、HBase2-0新特性"><a href="#5-2、HBase2-0新特性" class="headerlink" title="5.2、HBase2.0新特性"></a>5.2、HBase2.0新特性</h2><p>2017年8月22日凌晨2点左右，HBase发布了2.0.0 alpha-2，相比于上一个版本，修复了500个补丁，我们来了解一下2.0版本的HBase新特性。</p>
<p>最新文档：</p>
<p><a href="http://hbase.apache.org/book.html#ttl" target="_blank" rel="external">http://hbase.apache.org/book.html#ttl</a></p>
<p>官方发布主页：</p>
<p><a href="http://mail-archives.apache.org/mod_mbox/www-announce/201708.mbox/%3CCADcMMgFzmX0xYYso-UAYbU7V8z-Obk1J4pxzbGkRzbP5Hps+iA@mail.gmail.com" target="_blank" rel="external">http://mail-archives.apache.org/mod_mbox/www-announce/201708.mbox/%3CCADcMMgFzmX0xYYso-UAYbU7V8z-Obk1J4pxzbGkRzbP5Hps+iA@mail.gmail.com</a></p>
<p><strong>举例：</strong></p>
<p><strong>1) region**</strong>进行了多份冗余**</p>
<p>主region负责读写，从region维护在其他HregionServer中，负责读以及同步主region中的信息，如果同步不及时，是有可能出现client在从region中读到了脏数据（主region还没来得及把memstore中的变动的内容flush）。</p>
<p><strong>2)</strong> <strong>更多变动</strong></p>
<p><a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12340859&amp;styleName=&amp;projectId=12310753&amp;Create=Create&amp;atl_token=A5KQ-2QAV-T4JA-FDED|e6f233490acdf4785b697d4b457f7adb0a72b69f|lout" target="_blank" rel="external">https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12340859&amp;styleName=&amp;projectId=12310753&amp;Create=Create&amp;atl_token=A5KQ-2QAV-T4JA-FDED%7Ce6f233490acdf4785b697d4b457f7adb0a72b69f%7Clout</a></p>
<h1 id="六．好的总结文档"><a href="#六．好的总结文档" class="headerlink" title="六．好的总结文档"></a>六．好的总结文档</h1><p><a href="https://www.cnblogs.com/qcloud1001/p/7615526.html" target="_blank" rel="external">https://www.cnblogs.com/qcloud1001/p/7615526.html</a></p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>如果你感觉文章对你又些许感悟，你可以支持我！！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/uploads/wechatpay.png" alt="Jeremy Kinge WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/uploads/alipay.png" alt="Jeremy Kinge Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
	
	<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/hbase/" rel="tag"><i class="fa fa-tag"></i> hbase</a>
          
            <a href="/tags/数据结构化/" rel="tag"><i class="fa fa-tag"></i> 数据结构化</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/28/flume总结/" rel="next" title="flume总结">
                <i class="fa fa-chevron-left"></i> flume总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/18/oozie总结/" rel="prev" title="oozie总结">
                oozie总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_fav">收藏夹</a>
<a class="jiathis_button_copy">复制网址</a>
<a class="jiathis_button_email">邮件</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_qzone">QQ空间</a>
<a class="jiathis_button_tqq">腾讯微薄</a>
<a class="jiathis_button_douban">豆瓣</a>
<a class="jiathis_button_share">一键分享</a>

<a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
var jiathis_config={
  data_track_clickback:true,
  summary:"",
  shortUrl:false,
  hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script>
<!-- JiaThis Button END -->
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zMDUxMS83MDY1"></div>
    
  </div>


      <div id="lv-container" data-id="city" data-uid="MTAyMC8zMDUxMS83MDY1"></div>

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="Jeremy Kinge" />
          <p class="site-author-name" itemprop="name">Jeremy Kinge</p>
           
              <p class="site-description motion-element" itemprop="description">To know everything, no words don't talk, listening to people is enough to cause alarm（知无不言，言无不尽 言者无罪，闻者足戒）</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">72</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">31</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">120</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/JeremyKinge" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/LJBANANABLUE?s=09" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                    
                      Twitter
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://m.weibo.cn/u/3991058874?from=1078095010&wm=20005_0002&sourceType=qq&uid=3991058874" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      Weibo
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.facebook.com/profile.php?id=100010100689349" target="_blank" title="FB Page">
                  
                    <i class="fa fa-fw fa-facebook"></i>
                  
                    
                      FB Page
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/your-user-name" target="_blank" title="StackOverflow">
                  
                    <i class="fa fa-fw fa-stack-overflow"></i>
                  
                    
                      StackOverflow
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://xiezejingzhazha.blog.163.com/" target="_blank" title="网易博客">
                  
                    <i class="fa fa-fw fa-spinner"></i>
                  
                    
                      网易博客
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

		<div id="music163player">
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=451768026&auto=0&height=66"></iframe>
</div>
		
        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一、HBaes介绍"><span class="nav-number">1.</span> <span class="nav-text">一、HBaes介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1、HBase的起源"><span class="nav-number">1.1.</span> <span class="nav-text">1.1、HBase的起源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2、HBase的角色"><span class="nav-number">1.2.</span> <span class="nav-text">1.2、HBase的角色</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1、HMaster"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1、HMaster</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2、RegionServer"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2.2、RegionServer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3、HBase的架构"><span class="nav-number">1.3.</span> <span class="nav-text">1.3、HBase的架构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二、HBase部署与使用"><span class="nav-number">2.</span> <span class="nav-text">二、HBase部署与使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1、部署"><span class="nav-number">2.1.</span> <span class="nav-text">2.1、部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1、Zookeeper正常部署"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1、Zookeeper正常部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2、Hadoop正常部署"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2、Hadoop正常部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3、HBase的解压"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.3、HBase的解压</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-4、HBase的配置文件"><span class="nav-number">2.1.4.</span> <span class="nav-text">2.1.4、HBase的配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-5、HBase需要依赖的Jar包"><span class="nav-number">2.1.5.</span> <span class="nav-text">2.1.5、HBase需要依赖的Jar包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-6、HBase软连接Hadoop配置"><span class="nav-number">2.1.6.</span> <span class="nav-text">2.1.6、HBase软连接Hadoop配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-7、HBase远程scp到其他集群"><span class="nav-number">2.1.7.</span> <span class="nav-text">2.1.7、HBase远程scp到其他集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-8、HBase服务的启动"><span class="nav-number">2.1.8.</span> <span class="nav-text">2.1.8、HBase服务的启动</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-9、查看Hbse页面"><span class="nav-number">2.1.9.</span> <span class="nav-text">2.1.9、查看Hbse页面</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2、简单使用"><span class="nav-number">2.2.</span> <span class="nav-text">2.2、简单使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1、基本操作"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1、基本操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2、表的操作"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2、表的操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-hbase的表结构"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3 hbase的表结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3、读写流程"><span class="nav-number">2.3.</span> <span class="nav-text">2.3、读写流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-0-Region的寻址"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.0 Region的寻址</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1、HBase读数据流程"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.1、HBase读数据流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2、HBase写数据流程"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3.2、HBase写数据流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4、JavaAPI"><span class="nav-number">2.4.</span> <span class="nav-text">2.4、JavaAPI</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1、安装Maven并配置环境变量"><span class="nav-number">2.4.1.</span> <span class="nav-text">2.4.1、安装Maven并配置环境变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2、新建Maven-Project"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.4.2、新建Maven Project</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-3、编写HBaseAPI"><span class="nav-number">2.4.3.</span> <span class="nav-text">2.4.3、编写HBaseAPI</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5、MapReduce"><span class="nav-number">2.5.</span> <span class="nav-text">2.5、MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-1、官方HBase-MapReduce"><span class="nav-number">2.5.1.</span> <span class="nav-text">2.5.1、官方HBase-MapReduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-2、自定义HBase-MapReduce1（hbase导入到hbase）"><span class="nav-number">2.5.2.</span> <span class="nav-text">2.5.2、自定义HBase-MapReduce1（hbase导入到hbase）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-3、自定义HBase-MapReduce2"><span class="nav-number">2.5.3.</span> <span class="nav-text">2.5.3、自定义HBase-MapReduce2</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6、与Hive的集成"><span class="nav-number">2.6.</span> <span class="nav-text">2.6、与Hive的集成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-1、HBase与Hive的对比"><span class="nav-number">2.6.1.</span> <span class="nav-text">2.6.1、HBase与Hive的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-2、HBase与Hive集成使用"><span class="nav-number">2.6.2.</span> <span class="nav-text">2.6.2、HBase与Hive集成使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-3-需要注意"><span class="nav-number">2.6.3.</span> <span class="nav-text">2.6.3 需要注意</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7、与Sqoop的集成"><span class="nav-number">2.7.</span> <span class="nav-text">2.7、与Sqoop的集成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-8、常用的Shell操作"><span class="nav-number">2.8.</span> <span class="nav-text">2.8、常用的Shell操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-9、数据的备份与恢复"><span class="nav-number">2.9.</span> <span class="nav-text">2.9、数据的备份与恢复</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-9-1、备份"><span class="nav-number">2.9.1.</span> <span class="nav-text">2.9.1、备份</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-9-2、恢复"><span class="nav-number">2.9.2.</span> <span class="nav-text">2.9.2、恢复</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-10、节点的管理"><span class="nav-number">2.10.</span> <span class="nav-text">2.10、节点的管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-10-1、服役（commissioning）"><span class="nav-number">2.10.1.</span> <span class="nav-text">2.10.1、服役（commissioning）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-10-2、退役（decommissioning）"><span class="nav-number">2.10.2.</span> <span class="nav-text">2.10.2、退役（decommissioning）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-11、版本的确界"><span class="nav-number">2.11.</span> <span class="nav-text">2.11、版本的确界</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三、HBase的优化"><span class="nav-number">3.</span> <span class="nav-text">三、HBase的优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1、高可用"><span class="nav-number">3.1.</span> <span class="nav-text">3.1、高可用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2、Hadoop的通用性优化"><span class="nav-number">3.2.</span> <span class="nav-text">3.2、Hadoop的通用性优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3、Linux优化"><span class="nav-number">3.3.</span> <span class="nav-text">3.3、Linux优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4、Zookeeper优化"><span class="nav-number">3.4.</span> <span class="nav-text">3.4、Zookeeper优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5、HBase优化"><span class="nav-number">3.5.</span> <span class="nav-text">3.5、HBase优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-1、预分区-避免region被无线切分-（他的本质实际上就是预估rowkey的范围）"><span class="nav-number">3.5.1.</span> <span class="nav-text">3.5.1、预分区(避免region被无线切分)（他的本质实际上就是预估rowkey的范围）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-2、RowKey设计"><span class="nav-number">3.5.2.</span> <span class="nav-text">3.5.2、RowKey设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-3、内存优化"><span class="nav-number">3.5.3.</span> <span class="nav-text">3.5.3、内存优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-4、基础优化"><span class="nav-number">3.5.4.</span> <span class="nav-text">3.5.4、基础优化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#四、HBase项目"><span class="nav-number">4.</span> <span class="nav-text">四、HBase项目</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1、涉及概念梳理：命名空间"><span class="nav-number">4.1.</span> <span class="nav-text">4.1、涉及概念梳理：命名空间</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1、命名空间的结构"><span class="nav-number">4.1.1.</span> <span class="nav-text">4.1.1、命名空间的结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2、命名空间的使用"><span class="nav-number">4.1.2.</span> <span class="nav-text">4.1.2、命名空间的使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2、微博系统"><span class="nav-number">4.2.</span> <span class="nav-text">4.2、微博系统</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1、需求分析"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.1.1、需求分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2、代码实现"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.1.2、代码实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#五、总结"><span class="nav-number">5.</span> <span class="nav-text">五、总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1、HBase在商业项目中的能力"><span class="nav-number">5.1.</span> <span class="nav-text">5.1、HBase在商业项目中的能力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2、HBase2-0新特性"><span class="nav-number">5.2.</span> <span class="nav-text">5.2、HBase2.0新特性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#六．好的总结文档"><span class="nav-number">6.</span> <span class="nav-text">六．好的总结文档</span></a></li></ol></div>
            

          </div>
		  

		  
        </section>
      <!--/noindex-->
      

      

    </div>
	

	
  </aside>
  

  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2022 &mdash; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeremy Kinge</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">
      465.5k
    </span>
  
</div>


<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv">
  
</div>



  <span class="post-meta-divider">|</span>
<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>
<span class="post-meta-divider">|</span>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共465.5k字</span>
</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  




  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
</html>
